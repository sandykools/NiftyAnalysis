===============================================
TRADING_APP ñ COMPLETE CODEBASE SCAN
Root Path : G:\trading_app
Generated : 2026-01-29 13:56:56
===============================================

========== DIRECTORY HIERARCHY ==========

[DIR] \.streamlit
[DIR] \backtest
[DIR] \config
[DIR] \core
[DIR] \core\signals
[DIR] \data
[DIR] \data\tokens
[DIR] \features
[DIR] \intelligence
[DIR] \ml
[DIR] \risk
[DIR] \scripts
[DIR] \storage
[DIR] \ui
[DIR] \utils

========== END DIRECTORY HIERARCHY ==========


========== FILE LIST ==========

[FILE] G:\trading_app\app - Copy (2).py
[FILE] G:\trading_app\app - Copy.py
[FILE] G:\trading_app\app.py
[FILE] G:\trading_app\backtest\__init__.py
[FILE] G:\trading_app\backtest\engine.py
[FILE] G:\trading_app\backtest\metrics.py
[FILE] G:\trading_app\backtest\run_backtest.py
[FILE] G:\trading_app\codebase_export.txt
[FILE] G:\trading_app\config\nifty_weights.json
[FILE] G:\trading_app\config\settings.json
[FILE] G:\trading_app\core\debug_session.py
[FILE] G:\trading_app\core\feature_pipeline.py
[FILE] G:\trading_app\core\scheduler.py
[FILE] G:\trading_app\core\session - Copy.py
[FILE] G:\trading_app\core\session.py
[FILE] G:\trading_app\core\signals\state_machine.py
[FILE] G:\trading_app\data\instrument_master.py
[FILE] G:\trading_app\data\instrument_master_fixed.py
[FILE] G:\trading_app\data\upstox_client.py
[FILE] G:\trading_app\features\breadth.py
[FILE] G:\trading_app\features\option_features.py
[FILE] G:\trading_app\features\price_features.py
[FILE] G:\trading_app\FULL_CODEBASE_DUMP.txt
[FILE] G:\trading_app\intelligence\market_state.py
[FILE] G:\trading_app\intelligence\signal_logic.py
[FILE] G:\trading_app\ml\feature_contract.py
[FILE] G:\trading_app\ml\inference.py
[FILE] G:\trading_app\ml\training.py
[FILE] G:\trading_app\risk\position_sizing.py
[FILE] G:\trading_app\risk\stoploss.py
[FILE] G:\trading_app\scripts\fix_database.py
[FILE] G:\trading_app\scripts\initialize_database.py
[FILE] G:\trading_app\storage\repository - Copy.py
[FILE] G:\trading_app\storage\repository.py
[FILE] G:\trading_app\storage\schema.sql
[FILE] G:\trading_app\ui\charts.py
[FILE] G:\trading_app\ui\heatmap.py
[FILE] G:\trading_app\ui\option_chain.py
[FILE] G:\trading_app\ui\signals.py
[FILE] G:\trading_app\utils\expiry_utils.py

========== END FILE LIST ==========


========== FILE CONTENTS ==========

--------------------------------------------------
FILE PATH : G:\trading_app\app - Copy (2).py
SIZE      : 82239 bytes
--------------------------------------------------

"""
ENHANCED TRADING TOOL - Research-Based Live Feature Engine
Incorporates OI Velocity, Gamma Exposure, Walls/Traps, and Divergence analysis.
"""
# Add this near the top of your imports
import streamlit as st
print(f"Streamlit version: {st.__version__}")

# Check if fragments are available
if hasattr(st, 'fragment'):
    print("‚úÖ Fragments are available in this Streamlit version")
else:
    print("‚ùå Fragments are NOT available. You need Streamlit >= 1.37.0")
    
    
import os
import sys
import threading
import time
from datetime import datetime
from streamlit.runtime.scriptrunner import get_script_run_ctx, add_script_run_ctx


# Fix Conda environment issue
if 'conda' in sys.version.lower():
    try:
        # Try to fix conda path issues
        os.environ['PATH'] = os.path.join(os.environ.get('CONDA_PREFIX', ''), 'bin') + ':' + os.environ['PATH']
    except:
        pass
# ==============================
# STREAMLIT CONFIG FIX - ADD THIS
# ==============================
import os
os.environ['STREAMLIT_SERVER_FILE_WATCHER_TYPE'] = 'none'
import time 
import threading  # <-- ADD THIS LINE
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from datetime import datetime, timezone
import json
from pathlib import Path
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Enhanced core components
from core.session import UpstoxSession, initialize_session, display_session_status
from data.upstox_client import UpstoxClient, MarketAnalytics, display_market_analytics
from data.instrument_master import get_option_keys

try:
    from core.feature_pipeline import build_and_store_features, create_feature_summary
except ImportError as e:
    st.error(f"Feature pipeline import error: {e}")
    # Define dummy functions to prevent crash
    def build_and_store_features(*args, **kwargs):
        return {"error": "Feature pipeline not loaded"}
    def create_feature_summary(*args, **kwargs):
        return {"error": "Feature pipeline not loaded"}
from core.scheduler import MarketScheduler
from core.signals.state_machine import SignalStateMachine, SignalValidator, create_signal_engine
from features.breadth import build_constituents_df
from ml.feature_contract import FEATURE_VERSION

# Enhanced storage
from storage.repository import (
    fetch_latest_features,
    insert_signal,
    signal_exists,
    validate_new_signals,
    expire_old_signals,
    fetch_active_signals,
    fetch_recent_analytics,
    get_database_stats,
    fetch_signal_performance,
    insert_research_analytics
)

# ==============================
# PAGE CONFIGURATION
# ==============================

st.set_page_config(
    page_title="Algorithmic Trading Research Platform",
    page_icon="üìà",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E3A8A;
        font-weight: 700;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #374151;
        font-weight: 600;
        margin-top: 1.5rem;
        margin-bottom: 1rem;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
    }
    .research-insight {
        background: #F0F9FF;
        border-left: 4px solid #3B82F6;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
    .trap-alert {
        background: #FEF3C7;
        border-left: 4px solid #F59E0B;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
    .divergence-alert {
        background: #FEE2E2;
        border-left: 4px solid #EF4444;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
</style>
""", unsafe_allow_html=True)

# ==============================
# APP HEADER
# ==============================

st.markdown('<div class="main-header">üìä Algorithmic Trading Research Platform</div>', unsafe_allow_html=True)
st.markdown("""
<div style='color: #6B7280; margin-bottom: 2rem;'>
    Advanced derivatives analytics with OI Velocity, Gamma Exposure, Structural Walls/Traps, and Spot Divergence detection.
    Version: Research v2.0
</div>
""", unsafe_allow_html=True)

# ==============================
# INITIALIZATION
# ==============================

def initialize_app_state():
    """Initialize application state with research components."""
    
    # Initialize session
    initialize_session()
    
    # Initialize state variables
    if "initialized" not in st.session_state:
        st.session_state.initialized = True
        
        # Market data series
        for key in ["price_series", "volume_series", "ccc_history", "oi_velocity_history", "gamma_history"]:
            if key not in st.session_state:
                st.session_state[key] = pd.Series(dtype=float)
        
        # Research analytics
        if "research_analytics" not in st.session_state:
            st.session_state.research_analytics = []
        
        # Signal engine
        if "signal_engine" not in st.session_state:
            st.session_state.signal_engine = create_signal_engine(
                signal_expiry_minutes=5,
                confidence_threshold=0.2
            )
        
        # Scheduler
        if "scheduler" not in st.session_state:
            st.session_state.scheduler = MarketScheduler(interval_seconds=30)
            # Initialize scheduler properties
            st.session_state.scheduler._last_run = None
            st.session_state.scheduler._total_executions = 0
        
        # Load Nifty weights
        if "nifty_weights" not in st.session_state:
            try:
                with open("config/nifty_weights.json", "r") as f:
                    st.session_state.nifty_weights = json.load(f)
            except:
                st.session_state.nifty_weights = {}
        
        # Instrument master path
        if "instrument_master" not in st.session_state:
            st.session_state.instrument_master = Path("G:/trading_app/data/instruments.json.gz")
        
        # Configuration - SMART EXPIRY SELECTION
        if "config" not in st.session_state:
            from utils.expiry_utils import get_trading_expiry
            
            # Get smart expiry
            underlying = "NIFTY"
            expiry_date_str = get_trading_expiry(underlying)
            
            if not expiry_date_str:
                # Fallback to next available date from instrument master
                from data.instrument_master import get_next_available_expiry
                expiry_date_str = get_next_available_expiry(underlying)
                
                if not expiry_date_str:
                    # Final fallback to a known date
                    expiry_date_str = "2026-02-03"
            
            # Convert string to datetime object
            expiry_date = datetime.strptime(expiry_date_str, "%Y-%m-%d")
            
            st.session_state.config = {
                "index_symbol": "NSE_INDEX|Nifty 50",
                "underlying": underlying,
                "expiry_date": expiry_date,
                "max_option_keys": 200
            }
            
            # Show expiry info
            from utils.expiry_utils import is_market_open
            market_status = "open" if is_market_open() else "closed"
            
            # Get today's date
            today_str = datetime.now().strftime('%Y-%m-%d')
            
            # Show appropriate message
            if expiry_date_str == today_str:
                st.info(f"üìÖ Using TODAY's expiry: {expiry_date_str} (Market: {market_status})")
            else:
                st.info(f"üìÖ Using NEXT expiry: {expiry_date_str} (Today: {today_str}, Market: {market_status})")
        
        # Option keys cache
        if "option_keys" not in st.session_state:
            st.session_state.option_keys = []
        
        # Market regime tracking
        if "market_regime" not in st.session_state:
            st.session_state.market_regime = "UNKNOWN"
        
        # Trap detection history
        if "trap_detections" not in st.session_state:
            st.session_state.trap_detections = []
        
        st.success("‚úÖ Application state initialized with research components")

# Run initialization
initialize_app_state()

# ==============================
# AUTHENTICATION
# ==============================

st.markdown('<div class="sub-header">üîê Authentication</div>', unsafe_allow_html=True)

# Authenticate - this will show login button if not authenticated
access_token = UpstoxSession.authenticate()

# CRITICAL: Check if authenticate() actually returned a token
# If it returns None, we need to stop execution here
if not access_token:
    # authenticate() should have shown login button and stopped with st.stop()
    # But if we reach here, something went wrong
    st.error("‚ùå Authentication required. Please log in to continue.")
    
    # Show login button manually as fallback
    login_url = UpstoxSession.get_login_url()
    st.markdown(f"""
    <div style="text-align: center; margin: 20px 0;">
        <a href="{login_url}" target="_blank">
            <button style="
                background-color: #00d09c;
                color: white;
                padding: 15px 30px;
                font-size: 18px;
                font-weight: bold;
                border: none;
                border-radius: 10px;
                cursor: pointer;
                width: 60%;
                margin: 20px 0;
                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            ">
            üìà Click to Login with Upstox
            </button>
        </a>
    </div>
    """, unsafe_allow_html=True)
    
    # STOP the app here
    st.stop()

# If we reach here, we have a token
# Initialize client
try:
    client = UpstoxClient(access_token)
    st.success("‚úÖ Upstox client initialized")
        
except Exception as e:
    st.error(f"‚ùå Error initializing Upstox client: {e}")
    st.stop()

# ==============================
# SIDEBAR - SESSION STATUS
# ==============================

with st.sidebar:
    st.markdown("## üß≠ Navigation")
    
    # Display session status
    display_session_status()
    
    # Database stats
    st.markdown("---")
    st.markdown("### üìä Database")
    
    db_stats = get_database_stats()
    if db_stats:
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Features", f"{db_stats.get('market_features_count', 0):,}")
        with col2:
            st.metric("Signals", f"{db_stats.get('signals_count', 0):,}")
    
    # Market hours info
    st.markdown("---")
    st.markdown("### üïí Market Hours")

    scheduler: MarketScheduler = st.session_state.scheduler
    now = datetime.now()

    if scheduler._is_market_open(now):
        st.success("‚úÖ Market Open")
        next_run = scheduler._last_run + timedelta(seconds=scheduler.interval_seconds) if scheduler._last_run else now
        time_to_next = (next_run - now).total_seconds()
        if time_to_next > 0:
            st.info(f"Next cycle in: {int(time_to_next)}s")
    else:
        st.warning("‚è∏Ô∏è Market Closed")

    # ==============================
    # AUTO-REFRESH STATUS
    # ==============================
    st.markdown("---")
    st.markdown("### üîÑ Auto-Refresh Status")

    # Check if background thread is running
    if "background_thread_running" in st.session_state and st.session_state.background_thread_running:
        st.success("‚úÖ Background thread running")
        
        # Show last update time if available
        if "last_data_update" in st.session_state:
            time_since = (datetime.now() - st.session_state.last_data_update).total_seconds()
            st.caption(f"Last data update: {int(time_since)}s ago")
        else:
            st.caption("Waiting for first update...")
    else:
        st.warning("‚ö†Ô∏è Background thread not running")
        if st.button("‚ñ∂Ô∏è Start Auto-Refresh", width='stretch'):
            if "background_thread_running" not in st.session_state:
                start_background_manager()
                st.rerun()

    # Show execution count if available
    if "auto_refresh_manager" in st.session_state:
        manager = st.session_state.auto_refresh_manager
        st.metric("Executions", manager.execution_count)
        
        if manager.last_execution_time:
            time_since = (datetime.now() - manager.last_execution_time).total_seconds()
            st.caption(f"Last execution: {int(time_since)}s ago")

    # Quick control buttons
    col1, col2 = st.columns(2)
    with col1:
        if st.button("‚è∏Ô∏è Pause", width='stretch'):
            # Temporarily pause by changing interval
            original_interval = st.session_state.scheduler.interval_seconds
            st.session_state.scheduler.interval_seconds = 3600  # 1 hour
            st.success(f"Auto-refresh paused")
            st.rerun()

    with col2:
        if st.button("‚ñ∂Ô∏è Resume", width='stretch'):
            # Restore auto-refresh
            st.session_state.scheduler.interval_seconds = 30
            st.success("Auto-refresh resumed")
            st.rerun()
    
    # Quick actions
    st.markdown("---")
    st.markdown("### ‚ö° Quick Actions")
    
    if st.button("üîÑ Clear Cache", width='stretch'):
        for key in list(st.session_state.keys()):
            if key not in ["initialized", "instrument_master", "config", "nifty_weights"]:
                del st.session_state[key]
        st.rerun()
    
    if st.button("üìä View Performance", width='stretch'):
        st.session_state.show_performance = True
    
    if st.button("üõ†Ô∏è System Logs", width='stretch'):
        st.session_state.show_logs = True

def retry_on_failure(max_retries=3, delay=5):
    """Decorator for retrying failed executions"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    print(f"‚ö†Ô∏è Attempt {attempt + 1}/{max_retries} failed: {e}")
                    if attempt < max_retries - 1:
                        print(f"‚è∏Ô∏è Retrying in {delay}s...")
                        time.sleep(delay)
                    else:
                        print(f"‚ùå All {max_retries} attempts failed")
                        raise
            return None
        return wrapper
    return decorator

def log_execution_time(func):
    """Decorator for logging execution time"""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        print(f"‚è±Ô∏è Starting {func.__name__}...")
        
        result = func(*args, **kwargs)
        
        execution_time = time.time() - start_time
        print(f"‚è±Ô∏è {func.__name__} completed in {execution_time:.2f}s")
        
        return result
    return wrapper
# ==============================
# RESEARCH EXECUTION CYCLE
# ==============================

@log_execution_time
@retry_on_failure(max_retries=2, delay=3)
def execute_research_cycle(config=None, nifty_weights=None, option_keys=None):
    """
    Enhanced execution cycle with research analytics.
    Returns comprehensive research data.
    """
    # Use provided parameters or fall back to session state
    if config is None:
        try:
            config = st.session_state.config
        except (KeyError, AttributeError):
            print("‚ö†Ô∏è Config not found in session state")
            return None
    
    if nifty_weights is None:
        nifty_weights = st.session_state.get("nifty_weights", {})
    
    if option_keys is None:
        option_keys = st.session_state.get("option_keys", [])
    
    research_data = {}
    
    try:
        # ------------------------------
        # 1. INDEX QUOTE
        # ------------------------------
        print(f"üìä Fetching index quote for {config.get('index_symbol', 'NSE_INDEX|Nifty 50')}")
        index_data = client.fetch_index_quote(config["index_symbol"])
        
        if not index_data or index_data.get("ltp") is None:
            print("‚ùå No index data received")
            return None
        
        ltp = float(index_data["ltp"])
        open_price = float(index_data["open"])
        volume = 0.0  # index has no volume
        
        research_data["index"] = {
            "ltp": ltp,
            "open": open_price,
            "change": index_data.get("percent_change", 0),
            "timestamp": index_data.get("timestamp", datetime.now(timezone.utc).isoformat())
        }
        
        print(f"‚úÖ Index data: {ltp} (Change: {index_data.get('percent_change', 0)}%)")
        
        # ------------------------------
        # 2. UPDATE PRICE SERIES
        # ------------------------------
        # Create new series safely
        try:
            if "price_series" not in st.session_state:
                st.session_state.price_series = pd.Series(dtype=float)
            
            if "volume_series" not in st.session_state:
                st.session_state.volume_series = pd.Series(dtype=float)
            
            # Update price series
            new_price_series = pd.concat([
                st.session_state.price_series, 
                pd.Series([ltp], index=[datetime.now()])
            ]).tail(200)
            
            # Update volume series
            new_volume_series = pd.concat([
                st.session_state.volume_series, 
                pd.Series([volume], index=[datetime.now()])
            ]).tail(200)
            
            # Store back to session state
            st.session_state.price_series = new_price_series
            st.session_state.volume_series = new_volume_series
            
            print(f"üìà Price series updated: {len(st.session_state.price_series)} data points")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error updating price series: {e}")
            # Initialize fresh series
            st.session_state.price_series = pd.Series([ltp], index=[datetime.now()])
            st.session_state.volume_series = pd.Series([volume], index=[datetime.now()])
        
        # ------------------------------
        # 3. BREADTH ANALYSIS (CCC)
        # ------------------------------
        print("üìä Starting breadth analysis...")
        try:
            if not nifty_weights:
                print("‚ö†Ô∏è No Nifty weights found, skipping breadth analysis")
                ccc_value = 0.0
                constituents_df = pd.DataFrame()
            else:
                equity_quotes_df = client.fetch_equity_quotes(
                    symbols=list(nifty_weights.keys())
                )
                
                constituents_df = build_constituents_df(
                    equity_quotes_df=equity_quotes_df,
                    weights=nifty_weights
                )
                
                if not constituents_df.empty:
                    ccc_value = (
                        constituents_df["weight"] * constituents_df["price_change"]
                    ).sum()
                    print(f"‚úÖ CCC Value: {ccc_value:.4f}")
                else:
                    ccc_value = 0.0
                    print("‚ö†Ô∏è Empty constituents dataframe")
        except Exception as e:
            print(f"‚ö†Ô∏è Breadth analysis error: {e}")
            ccc_value = 0.0
            constituents_df = pd.DataFrame()
        
        # Update CCC history
        try:
            if "ccc_history" not in st.session_state:
                st.session_state.ccc_history = pd.Series(dtype=float)
            
            st.session_state.ccc_history = pd.concat([
                st.session_state.ccc_history, 
                pd.Series([ccc_value], index=[datetime.now()])
            ]).tail(200)
        except:
            st.session_state.ccc_history = pd.Series([ccc_value], index=[datetime.now()])
        
        research_data["breadth"] = {
            "ccc_value": ccc_value,
            "constituents_count": len(constituents_df),
            "positive_constituents": len(constituents_df[constituents_df["price_change"] > 0]) if not constituents_df.empty else 0
        }
        
        # ------------------------------
        # 4. OPTION INSTRUMENT DISCOVERY
        # ------------------------------
        print("üîç Fetching option instruments...")
        try:
            if not option_keys:
                expiry_str = config["expiry_date"].strftime("%Y-%m-%d")
                option_keys = get_option_keys(
                    underlying=config["underlying"],
                    expiry=expiry_str,
                    max_keys=config.get("max_option_keys", 200)
                )
                st.session_state.option_keys = option_keys
                print(f"‚úÖ Found {len(option_keys)} option keys")
        except Exception as e:
            print(f"‚ö†Ô∏è Option discovery error: {e}")
            option_keys = []
            st.session_state.option_keys = []
        
        if not option_keys:
            print("‚ö†Ô∏è No option keys found")
            return research_data  # Return what we have so far
        
        # ------------------------------
        # 5. ENHANCED OPTION CHAIN ANALYSIS
        # ------------------------------
        print("üìä Analyzing option chain...")
        option_analytics = client.fetch_option_chain_with_analytics(
            option_keys,
            ltp
        )
        
        if not option_analytics or "raw_data" not in option_analytics:
            print("‚ùå Failed to fetch option analytics")
            return research_data  # Return what we have so far
        
        option_chain_df = option_analytics["raw_data"]
        analytics = option_analytics.get("analytics", {})
        insights = option_analytics.get("market_insights", [])
        
        research_data["options"] = {
            "analytics": analytics,
            "insights": insights,
            "chain_size": len(option_chain_df),
            "put_call_ratio": analytics.get("put_call_ratio", 1.0) if isinstance(analytics.get("put_call_ratio"), (int, float)) else 1.0
        }
        
        print(f"‚úÖ Option chain analyzed: {len(option_chain_df)} strikes, PCR: {analytics.get('put_call_ratio', 'N/A')}")
        
        # Store analytics for visualization
        if analytics:
            try:
                if "research_analytics" not in st.session_state:
                    st.session_state.research_analytics = []
                
                st.session_state.research_analytics.append({
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    **analytics
                })
                
                # Keep only recent analytics
                if len(st.session_state.research_analytics) > 100:
                    st.session_state.research_analytics = st.session_state.research_analytics[-100:]
                
                # Update market regime
                st.session_state.market_regime = analytics.get("market_regime", "UNKNOWN")
                
                # Store in database
                insert_research_analytics(analytics)
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error storing analytics: {e}")
        
        # ------------------------------
        # 6. FEATURE PIPELINE
        # ------------------------------
        print("‚öôÔ∏è Running feature pipeline...")
        try:
            snapshot_ts = pd.Timestamp.utcnow().floor("ms")
            
            feature_result = build_and_store_features(
                timestamp=snapshot_ts,
                option_chain_df=option_chain_df,
                spot_price=ltp,
                expiry_datetime=config["expiry_date"],
                
                ltp=ltp,
                vwap=open_price,
                price_series=st.session_state.price_series,
                volume_series=st.session_state.volume_series,
                
                constituents_df=constituents_df,
                ccc_history=st.session_state.ccc_history,
                
                # Enhanced parameters
                client=client,
                option_keys=option_keys
            )
            
            if feature_result:
                research_data["features"] = feature_result.get("features", {})
                research_data["feature_summary"] = create_feature_summary(feature_result["features"])
                print("‚úÖ Feature pipeline completed")
            else:
                print("‚ö†Ô∏è Feature pipeline returned empty result")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Feature pipeline error: {e}")
            import traceback
            traceback.print_exc()
        
        # ------------------------------
        # 7. SIGNAL GENERATION
        # ------------------------------
        print("üéØ Checking for signals...")
        try:
            feature_row = fetch_latest_features(FEATURE_VERSION)
            if feature_row is not None and not feature_row.empty:
                # Check if signal already exists
                timestamp_str = feature_row.iloc[0]["timestamp"]
                
                if not signal_exists(timestamp_str):
                    # Generate signal
                    signal_engine: SignalStateMachine = st.session_state.signal_engine
                    signal = signal_engine.build_signal(feature_row.iloc[0])
                    
                    # Validate signal
                    is_valid, reason = SignalValidator.validate_signal(signal, feature_row.iloc[0])
                    
                    if is_valid and signal["signal_type"] != "NEUTRAL":
                        # Add research context
                        signal["research_validation"] = {
                            "is_valid": is_valid,
                            "reason": reason,
                            "timestamp": datetime.now(timezone.utc).isoformat()
                        }
                        
                        # Insert signal
                        insert_signal(signal)
                        
                        research_data["signal"] = {
                            "generated": True,
                            "type": signal["signal_type"],
                            "confidence": signal["confidence"],
                            "strength": signal.get("signal_strength", "UNKNOWN")
                        }
                        print(f"‚úÖ Signal generated: {signal['signal_type']} ({signal['confidence']:.0%})")
                    else:
                        research_data["signal"] = {
                            "generated": False,
                            "reason": reason if not is_valid else "NEUTRAL signal"
                        }
                        print(f"‚ÑπÔ∏è No valid signal: {reason if not is_valid else 'NEUTRAL'}")
            else:
                print("‚ÑπÔ∏è No features available for signal generation")
        except Exception as e:
            print(f"‚ö†Ô∏è Signal generation error: {e}")
        
        # ------------------------------
        # 8. SIGNAL LIFECYCLE MANAGEMENT
        # ------------------------------
        try:
            validate_new_signals(confidence_threshold=0.2)
            expire_old_signals()
        except Exception as e:
            print(f"‚ö†Ô∏è Signal lifecycle error: {e}")
        
        # ------------------------------
        # 9. TRAP DETECTION
        # ------------------------------
        if analytics and "potential_traps" in analytics:
            traps = analytics["potential_traps"]
            if traps:
                try:
                    if "trap_detections" not in st.session_state:
                        st.session_state.trap_detections = []
                    
                    for trap in traps:
                        if trap.get("confidence", 0) > 0.6:
                            st.session_state.trap_detections.append({
                                "timestamp": datetime.now(timezone.utc).isoformat(),
                                "trap": trap,
                                "market_regime": analytics.get("market_regime")
                            })
                    
                    # Keep only recent traps
                    if len(st.session_state.trap_detections) > 20:
                        st.session_state.trap_detections = st.session_state.trap_detections[-20:]
                    
                    research_data["traps"] = traps
                    
                    if traps:
                        print(f"üéØ Detected {len(traps)} potential traps")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Trap detection error: {e}")
        
        # ------------------------------
        # 10. SUCCESSFUL COMPLETION
        # ------------------------------
        print("‚úÖ Research cycle completed successfully")
        return research_data
        
    except Exception as e:
        print(f"‚ùå Critical error in research cycle: {e}")
        import traceback
        traceback.print_exc()
        return None

# ==============================
# ENHANCED AUTO-REFRESH SYSTEM
# ==============================

class AutoRefreshManager:
    """Robust auto-refresh manager with error handling and state persistence"""
    
    def __init__(self, scheduler, config, nifty_weights, option_keys):
        self.scheduler = scheduler
        self.config = config
        self.nifty_weights = nifty_weights
        self.option_keys = option_keys
        self.last_execution_time = None
        self.execution_count = 0
        self.errors = []
        self.max_retries = 3
        self.retry_delay = 5
        
    def should_execute(self):
        """Determine if execution should run based on time and market conditions"""
        now = datetime.now()
        
        # FIX: Add safety check for scheduler
        if not hasattr(self, 'scheduler') or self.scheduler is None:
            return False, "Scheduler not initialized"
        
        # Check market hours - FIX: Add try/except
        try:
            is_market_open = self.scheduler._is_market_open(now)
        except Exception as e:
            print(f"‚ö†Ô∏è Error checking market hours: {e}")
            is_market_open = True  # Default to true to keep running
        
        if not is_market_open:
            return False, "Market closed"
        
        # Rest of the method remains the same...
        
        # Check if first run
        if self.last_execution_time is None:
            return True, "First execution"
        
        # Check time since last execution
        time_since_last = (now - self.last_execution_time).total_seconds()
        
        # Debug log to see what's happening
        print(f"üïí Auto-refresh timing: {int(time_since_last)}s since last, interval: {self.scheduler.interval_seconds}s")
        
        if time_since_last >= self.scheduler.interval_seconds:
            return True, f"Interval reached ({int(time_since_last)}s > {self.scheduler.interval_seconds}s)"
        
        return False, f"Not due yet ({int(self.scheduler.interval_seconds - time_since_last)}s remaining)"
    
    def execute_with_retry(self):
        """Execute research cycle with retry logic"""
        for attempt in range(self.max_retries):
            try:
                print(f"üîÑ Attempt {attempt + 1}/{self.max_retries} to execute research cycle")
                
                result = execute_research_cycle(
                    config=self.config,
                    nifty_weights=self.nifty_weights,
                    option_keys=self.option_keys
                )
                
                if result:
                    self.last_execution_time = datetime.now()
                    self.execution_count += 1
                    return True, result, f"Execution successful (attempt {attempt + 1})"
                else:
                    return False, None, "Research cycle returned no data"
                    
            except Exception as e:
                error_msg = f"Attempt {attempt + 1} failed: {str(e)}"
                self.errors.append({
                    "timestamp": datetime.now(),
                    "attempt": attempt + 1,
                    "error": str(e)
                })
                
                if attempt < self.max_retries - 1:
                    print(f"‚è∏Ô∏è Retrying in {self.retry_delay}s...")
                    time.sleep(self.retry_delay)
                else:
                    return False, None, f"All retries failed. Last error: {str(e)}"
        
        return False, None, "Max retries exceeded"
    
    def get_status(self):
        """Get current status of auto-refresh manager"""
        status = {
            "last_execution": self.last_execution_time.strftime("%H:%M:%S") if self.last_execution_time else "Never",
            "execution_count": self.execution_count,
            "is_market_open": self.scheduler._is_market_open(datetime.now()),
            "interval_seconds": self.scheduler.interval_seconds,
            "error_count": len(self.errors)
        }
        
        if self.last_execution_time:
            time_since = (datetime.now() - self.last_execution_time).total_seconds()
            status["time_since_last"] = f"{int(time_since)}s"
            status["time_until_next"] = f"{max(0, int(self.scheduler.interval_seconds - time_since))}s"
        
        return status

# Run initialization
initialize_app_state()

# Initialize auto-refresh manager AFTER session state is ready
if "auto_refresh_manager" not in st.session_state:
    st.session_state.auto_refresh_manager = AutoRefreshManager(
        scheduler=st.session_state.scheduler,
        config=st.session_state.config,
        nifty_weights=st.session_state.nifty_weights,
        option_keys=st.session_state.option_keys
    )


# ==============================
# CONTEXT-AWARE BACKGROUND THREAD FIX
# ==============================

def start_background_manager():
    """Start background manager with proper Streamlit context"""
    print(f"üîç DEBUG: Attempting to start background manager...")
    print(f"üîç DEBUG: background_thread_running in session_state: {'background_thread_running' in st.session_state}")
    
    # Check if already running to prevent duplicates
    if "background_thread_running" in st.session_state and st.session_state.background_thread_running:
        print("‚ö†Ô∏è Background manager already running, skipping...")
        return
    
    try:
        # Capture the current script run context
        ctx = get_script_run_ctx()
        print(f"‚úÖ Got ScriptRunContext: {ctx}")
        
        def background_worker(context):
            """Background worker thread with attached context"""
            print("üöÄ Background worker thread STARTING...")
            
            # Attach the Streamlit context to this thread
            add_script_run_ctx(threading.current_thread(), context)
            
            print("‚úÖ Streamlit context attached to background thread")
            print(f"‚úÖ Thread ID: {threading.current_thread().ident}")
            print(f"‚úÖ Thread name: {threading.current_thread().name}")
            
            # Main background worker loop
            while True:
                try:
                    # Check if we should execute
                    if "auto_refresh_manager" in st.session_state:
                        manager = st.session_state.auto_refresh_manager
                        
                        # Use the manager's logic to decide when to run
                        should_run, reason = manager.should_execute()
                        
                        if should_run:
                            print(f"üîÑ Background executing: {reason}")
                            
                            # Execute with retry
                            success, result, msg = manager.execute_with_retry()
                            
                            if success:
                                print(f"‚úÖ Data updated: {msg}")
                                # Update timestamp for frontend to see
                                st.session_state.last_data_update = datetime.now()
                            else:
                                print(f"‚ö†Ô∏è Background execution failed: {msg}")
                    
                    # Sleep to prevent CPU spinning
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Background thread error: {e}")
                    time.sleep(5)  # Wait before retrying
        
        # Create and start the thread
        thread = threading.Thread(
            target=background_worker,
            args=(ctx,),
            daemon=True  # Thread will die when main app stops
        )
        
        # Start the thread
        thread.start()
        
        # Mark as running in session state
        st.session_state.background_thread_running = True
        st.session_state.background_thread = thread
        
        print("‚úÖ Background manager started with context-aware thread")
        
    except Exception as e:
        print(f"‚ùå Failed to start background manager: {e}")
        st.session_state.background_thread_running = False


# Execute auto-refresh
def execute_auto_refresh():
    """Main auto-refresh execution function"""
    manager = st.session_state.auto_refresh_manager
    
    try:
        # Check if we should execute
        should_execute, reason = manager.should_execute()
        
        if should_execute:
            print(f"üèÉ‚Äç‚ôÇÔ∏è Auto-execution triggered: {reason}")
            
            # Execute with retry logic
            success, result, message = manager.execute_with_retry()
            
            if success:
                print(f"‚úÖ {message}")
                
                # Update UI state if needed
                if result and "options" in result and "analytics" in result["options"]:
                    analytics = result["options"]["analytics"]
                    if analytics:
                        # Update market regime
                        st.session_state.market_regime = analytics.get("market_regime", "UNKNOWN")
                        
                        # Store analytics
                        if "research_analytics" not in st.session_state:
                            st.session_state.research_analytics = []
                        st.session_state.research_analytics.append({
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                            **analytics
                        })
                        # Keep only recent analytics
                        if len(st.session_state.research_analytics) > 100:
                            st.session_state.research_analytics = st.session_state.research_analytics[-100:]
                
                return True, message
            else:
                print(f"‚ùå Auto-execution failed: {message}")
                return False, message
        else:
            # Log status periodically
            if manager.last_execution_time:
                time_since = (datetime.now() - manager.last_execution_time).total_seconds()
                if int(time_since) % 10 == 0:  # Log every 10 seconds
                    print(f"‚è±Ô∏è Auto-refresh status: {reason}")
            return None, reason
            
    except Exception as e:
        error_msg = f"Auto-refresh system error: {str(e)}"
        print(f"üî• {error_msg}")
        manager.errors.append({
            "timestamp": datetime.now(),
            "error": error_msg
        })
        return False, error_msg

# Run auto-refresh
#execute_auto_refresh()

# ==============================
# ENHANCED BACKGROUND SCHEDULER
# ==============================

class BackgroundScheduler:
    """Robust background scheduler with error handling and monitoring"""
    
    def __init__(self, check_interval=5):
        self.check_interval = check_interval
        self.running = False
        self.thread = None
        self.errors = []
        self.last_check = None
        self.health_checks = []
        
    def health_check(self):
        """Perform system health check"""
        checks = []
        
        # Check scheduler
        try:
            scheduler = st.session_state.scheduler
            checks.append({
                "component": "Scheduler",
                "status": "HEALTHY" if scheduler else "UNKNOWN",
                "details": f"Interval: {scheduler.interval_seconds}s" if scheduler else "Not initialized"
            })
        except Exception as e:
            checks.append({
                "component": "Scheduler",
                "status": "ERROR",
                "details": str(e)
            })
        
        # Check auto-refresh manager
        try:
            manager = st.session_state.auto_refresh_manager
            checks.append({
                "component": "AutoRefresh",
                "status": "HEALTHY",
                "details": f"Executions: {manager.execution_count}"
            })
        except Exception as e:
            checks.append({
                "component": "AutoRefresh",
                "status": "ERROR",
                "details": str(e)
            })
        
        self.health_checks = checks
        return checks
    
    def start(self):
        """Start background scheduler thread"""
        if self.running:
            print("‚ö†Ô∏è Background scheduler already running")
            return
        
        def background_task():
            print("üöÄ Starting enhanced background scheduler...")
            self.running = True
            
            while self.running:
                try:
                    self.last_check = datetime.now()
                    
                    # Perform health check every minute
                    if not self.health_checks or (datetime.now() - self.last_check).seconds > 60:
                        self.health_check()
                    
                    # Log status every 30 seconds
                    current_time = datetime.now()
                    if current_time.second % 30 == 0:
                        manager = st.session_state.auto_refresh_manager
                        if manager.last_execution_time:
                            time_since = (current_time - manager.last_execution_time).total_seconds()
                            print(f"üìä Background monitor: Last execution {int(time_since)}s ago, {manager.execution_count} total runs")
                    
                    time.sleep(self.check_interval)
                    
                except Exception as e:
                    error_msg = f"Background scheduler error: {str(e)}"
                    print(f"üî• {error_msg}")
                    self.errors.append({
                        "timestamp": datetime.now(),
                        "error": error_msg
                    })
                    time.sleep(30)  # Wait longer on error
        
        self.thread = threading.Thread(target=background_task, daemon=True)
        self.thread.start()
        print("‚úÖ Enhanced background scheduler started")
    
    def stop(self):
        """Stop background scheduler"""
        self.running = False
        if self.thread:
            self.thread.join(timeout=5)
        print("üõë Background scheduler stopped")
    
    def get_status(self):
        """Get scheduler status"""
        return {
            "running": self.running,
            "check_interval": self.check_interval,
            "last_check": self.last_check.strftime("%H:%M:%S") if self.last_check else "Never",
            "error_count": len(self.errors),
            "health_checks": self.health_checks
        }

# Initialize and start background scheduler
if "background_scheduler" not in st.session_state:
    st.session_state.background_scheduler = BackgroundScheduler(check_interval=5)
    st.session_state.background_scheduler.start()


def get_scheduler_status():
    """Get comprehensive scheduler status"""
    scheduler = st.session_state.scheduler
    manager = st.session_state.auto_refresh_manager
    
    status = {
        "scheduler_active": True,
        "interval_seconds": scheduler.interval_seconds,
        "total_executions": scheduler._total_executions,
        "last_run": scheduler._last_run.strftime("%H:%M:%S") if scheduler._last_run else "Never",
        "is_market_open": scheduler._is_market_open(datetime.now()),
        "auto_refresh_status": manager.get_status()
    }
    
    # Calculate next run time
    if scheduler._last_run:
        next_run = scheduler._last_run + timedelta(seconds=scheduler.interval_seconds)
        time_until_next = (next_run - datetime.now()).total_seconds()
        status["next_run"] = next_run.strftime("%H:%M:%S") if time_until_next > 0 else "Now"
        status["time_until_next"] = f"{max(0, int(time_until_next))}s"
    else:
        status["next_run"] = "Pending"
        status["time_until_next"] = "N/A"
    
    return status

# ==============================
# CREATE TABS
# ==============================

# Create tabs for different views
tab1, tab2, tab3, tab4 = st.tabs([
    "üìà Live Analytics", 
    "üéØ Signals", 
    "üîç Research", 
    "‚öôÔ∏è Configuration"
])

# # ==============================
# # AUTO-REFRESH WORKING SOLUTION
# # ==============================

# # Create a placeholder for auto-refresh
# refresh_placeholder = st.empty()

# # Check if we need to auto-refresh
# if "next_refresh_time" not in st.session_state:
#     st.session_state.next_refresh_time = time.time() + 30  # First refresh in 30 seconds

# current_time = time.time()

# if current_time >= st.session_state.next_refresh_time:
#     # Time to refresh!
#     print(f"üîÑ AUTO-REFRESH EXECUTING at {datetime.now().strftime('%H:%M:%S')}")
    
#     # Execute research cycle
#     with st.spinner("Auto-refreshing data..."):
#         research_data = execute_research_cycle()
    
#     if research_data:
#         print(f"‚úÖ Auto-refresh completed successfully")
        
#         # Update next refresh time
#         st.session_state.next_refresh_time = current_time + 30
        
#         # Update state
#         st.session_state.scheduler._last_run = datetime.now()
#         st.session_state.scheduler._total_executions += 1
#         st.session_state.auto_refresh_manager.last_execution_time = datetime.now()
#         st.session_state.auto_refresh_manager.execution_count += 1
        
#         # Show success message
#         refresh_placeholder.success(f"‚úÖ Auto-refresh completed at {datetime.now().strftime('%H:%M:%S')}")
#         time.sleep(1)
#         refresh_placeholder.empty()
        
#         # Force a rerun to update UI
#         st.rerun()
#     else:
#         print("‚ùå Auto-refresh failed")
#         refresh_placeholder.error("Auto-refresh failed")
#         time.sleep(2)
#         refresh_placeholder.empty()
# else:
#     # Show countdown
#     time_until_next = st.session_state.next_refresh_time - current_time
#     if time_until_next > 0:
#         print(f"‚è±Ô∏è Next auto-refresh in {int(time_until_next)}s")
        
#         # Update countdown every 10 seconds
#         if int(time_until_next) % 10 == 0:
#             refresh_placeholder.info(f"Next auto-refresh in {int(time_until_next)} seconds...")


# ==============================
# TAB 1: LIVE ANALYTICS
# ==============================

with tab1:
    st.markdown('<div class="sub-header">üìä Live Market Analytics</div>', unsafe_allow_html=True)
    
    # TEST: Simple fragment test
    if hasattr(st, 'fragment'):
        @st.fragment(run_every=10)  # Test with 10 seconds first
        def test_fragment():
            st.write(f"**Fragment last updated:** {datetime.now().strftime('%H:%M:%S')}")
            
            scheduler: MarketScheduler = st.session_state.scheduler
            
            # Simple metrics
            col1, col2 = st.columns(2)
            
            with col1:
                if scheduler._last_run:
                    time_since = (datetime.now() - scheduler._last_run).total_seconds()
                    st.metric("Scheduler", "‚úÖ Active", delta=f"{int(time_since)}s ago")
                else:
                    st.metric("Scheduler", "‚è≥ Starting", "Initializing")
            
            with col2:
                if st.session_state.price_series.size > 0:
                    current_price = st.session_state.price_series.iloc[-1]
                    st.metric("Nifty 50", f"{current_price:,.0f}")
                else:
                    st.metric("Nifty 50", "Loading...")
        
        # Render the test fragment
        test_fragment()
        st.caption("üëÜ This updates every 10 seconds via fragment")
        
    else:
        st.warning("‚ùå Fragments not available. Using manual refresh.")
        # Fallback to manual refresh
        if st.button("üîÑ Refresh Now"):
            st.rerun()
    
    st.markdown("---")
    
    # Manual execution button (keep this)
    if st.button("üîÑ Execute Research Cycle Now", type="primary", width='stretch'):
        with st.spinner("Executing research cycle..."):
            research_data = execute_research_cycle()
            if research_data:
                st.success("‚úÖ Research cycle executed successfully")
                st.rerun()
            else:
                st.error("‚ùå Research cycle failed")
    
    # ... rest of your Tab 1 content (charts, etc.) ...
    
    st.markdown("---")
    
    # Rest of your Tab 1 content (charts, etc.) remains the same
    # Keep everything from line ~870 onward (your existing charts and visualizations)
    
    # Display enhanced status
    st.markdown("---")
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        status = get_scheduler_status()
        if status["scheduler_active"]:
            if status["last_run"] != "Never":
                time_since = (datetime.now() - st.session_state.scheduler._last_run).total_seconds() if st.session_state.scheduler._last_run else 999
                if time_since < 60:
                    st.metric("Scheduler", "‚úÖ Active", delta=f"{int(time_since)}s ago")
                else:
                    st.metric("Scheduler", "‚ö†Ô∏è Stale", delta=f"{int(time_since)}s ago")
            else:
                st.metric("Scheduler", "‚è≥ Initializing", delta="First run pending")
        else:
            st.metric("Scheduler", "‚ùå Inactive", delta="Check configuration")

    with col2:
        manager_status = st.session_state.auto_refresh_manager.get_status()
        st.metric("Auto Refresh", f"Run #{manager_status['execution_count']}", 
                delta=manager_status['last_execution'])

    with col3:
        if manager_status.get('time_until_next'):
            st.metric("Next Run In", manager_status['time_until_next'])

    with col4:
        error_count = manager_status.get('error_count', 0)
        if error_count == 0:
            st.metric("Errors", "‚úÖ 0", delta="Clean")
        else:
            st.metric("Errors", f"‚ö†Ô∏è {error_count}", delta="Check logs")

    # Control buttons
    col1, col2, col3 = st.columns(3)

    with col1:
        if st.button("üîÑ Force Run Now", type="primary", width='stretch'):
            # Force immediate execution
            st.session_state.auto_refresh_manager.last_execution_time = None
            st.rerun()

    with col2:
        if st.button("‚è∏Ô∏è Pause Auto-Refresh", width='stretch'):
            # Temporarily disable auto-refresh
            original_interval = st.session_state.scheduler.interval_seconds
            st.session_state.scheduler.interval_seconds = 3600  # 1 hour
            st.success(f"Auto-refresh paused. Original interval: {original_interval}s")
            st.rerun()

    with col3:
        if st.button("‚ñ∂Ô∏è Resume Auto-Refresh", width='stretch'):
            # Restore auto-refresh
            st.session_state.scheduler.interval_seconds = 30
            st.success("Auto-refresh resumed (30s interval)")
            st.rerun()
    # Display background thread info
    with st.expander("üîÑ Background Scheduler Info", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            if "scheduler_thread" in st.session_state:
                thread = st.session_state.scheduler_thread
                st.success("‚úÖ Background thread active")
                st.info(f"Thread ID: {thread.ident}")
                st.info(f"Thread alive: {thread.is_alive()}")
            else:
                st.error("‚ùå Background thread not running")
        
        with col2:
            if scheduler._last_run:
                st.metric("Last Execution", scheduler._last_run.strftime("%H:%M:%S"))
                time_since = (datetime.now() - scheduler._last_run).total_seconds()
                st.metric("Time Since", f"{int(time_since)}s")
            else:
                st.metric("Last Execution", "Never")
            
            st.metric("Total Executions", scheduler._total_executions)
    
    
    # Display latest analytics if available
    if st.session_state.research_analytics:
        latest_analytics = st.session_state.research_analytics[-1]
        
        # Create columns for analytics display
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### üìà OI & Gamma Analysis")
            
            # OI Velocity gauge
            oi_velocity = latest_analytics.get("oi_velocity", 0)
            fig_oi = go.Figure(go.Indicator(
                mode="gauge+number+delta",
                value=oi_velocity,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "OI Velocity (œÉ)"},
                delta={'reference': 0},
                gauge={
                    'axis': {'range': [-3, 3]},
                    'steps': [
                        {'range': [-3, -1.5], 'color': "lightgray"},
                        {'range': [-1.5, 1.5], 'color': "gray"},
                        {'range': [1.5, 3], 'color': "lightgray"}
                    ],
                    'threshold': {
                        'line': {'color': "red", 'width': 4},
                        'thickness': 0.75,
                        'value': oi_velocity
                    }
                }
            ))
            fig_oi.update_layout(height=250)
            st.plotly_chart(fig_oi, width='stretch')
        
        with col2:
            st.markdown("#### üß± Structure Analysis")
            
            # Structural metrics
            wall_strength = latest_analytics.get("wall_strength", 0)
            trap_prob = latest_analytics.get("trap_probability", 0)
            
            col_a, col_b = st.columns(2)
            with col_a:
                st.metric("Wall Strength", f"{wall_strength:.2f}")
            with col_b:
                st.metric("Trap Probability", f"{trap_prob:.2%}")
            
            # Display traps if any
            traps = latest_analytics.get("potential_traps", [])
            if traps:
                st.markdown("##### üéØ Detected Traps")
                for trap in traps[:2]:  # Show max 2
                    direction = trap.get("direction", "").upper()
                    confidence = trap.get("confidence", 0)
                    strike = trap.get("strike", 0)
                    
                    if confidence > 0.6:
                        st.markdown(f"""
                        <div class="trap-alert">
                            <strong>{direction} Trap</strong> at {strike:,.0f}<br>
                            Confidence: {confidence:.0%}<br>
                            Unwinding: {trap.get('unwinding_rate', 0):+.1f}%
                        </div>
                        """, unsafe_allow_html=True)
        
        # Price chart
        st.markdown("---")
        st.markdown("#### üìâ Price & Indicators")
        
        if len(st.session_state.price_series) > 1:
            # Create subplot for price and indicators
            fig = make_subplots(
                rows=3, cols=1,
                shared_xaxes=True,
                vertical_spacing=0.05,
                subplot_titles=("Nifty 50 Price", "OI Velocity", "Gamma Exposure"),
                row_heights=[0.5, 0.25, 0.25]
            )
            
            # Price
            price_data = st.session_state.price_series.reset_index(drop=True)
            fig.add_trace(
                go.Scatter(
                    y=price_data.values,
                    mode='lines',
                    name='Price',
                    line=dict(color='#3B82F6', width=2)
                ),
                row=1, col=1
            )
            
            # OI Velocity (if available in analytics history)
            if len(st.session_state.research_analytics) > 1:
                oi_velocities = [a.get("oi_velocity", 0) for a in st.session_state.research_analytics[-50:]]
                fig.add_trace(
                    go.Scatter(
                        y=oi_velocities,
                        mode='lines',
                        name='OI Velocity',
                        line=dict(color='#10B981', width=1)
                    ),
                    row=2, col=1
                )
                
                # Add zero line
                fig.add_hline(y=0, line_dash="dash", line_color="gray", row=2, col=1)
            
            # Gamma (if available)
            if len(st.session_state.research_analytics) > 1:
                gamma_values = [a.get("gamma_exposure", {}).get("net_gamma", 0) for a in st.session_state.research_analytics[-50:]]
                fig.add_trace(
                    go.Scatter(
                        y=gamma_values,
                        mode='lines',
                        name='Net Gamma',
                        line=dict(color='#8B5CF6', width=1)
                    ),
                    row=3, col=1
                )
                
                # Add zero line
                fig.add_hline(y=0, line_dash="dash", line_color="gray", row=3, col=1)
            
            fig.update_layout(height=600, showlegend=True)
            st.plotly_chart(fig, width='stretch')

# ==============================
# TAB 2: SIGNALS
# ==============================

with tab2:
    st.markdown('<div class="sub-header">üéØ Trading Signals</div>', unsafe_allow_html=True)
    
    # Fetch active signals
    active_signals = fetch_active_signals(limit=10)
    
    if not active_signals.empty:
        # Display signals in columns
        for _, signal in active_signals.iterrows():
            col1, col2, col3, col4 = st.columns([2, 1, 1, 1])
            
            with col1:
                signal_type = signal["signal_type"]
                color = "green" if signal_type == "BUY" else "red" if signal_type == "SELL" else "gray"
                st.markdown(f"### <span style='color:{color};'>{signal_type}</span>", unsafe_allow_html=True)
                
                # Rationale
                if pd.notna(signal["rationale"]):
                    st.caption(signal["rationale"])
            
            with col2:
                confidence = signal["confidence"]
                st.metric("Confidence", f"{confidence:.0%}")
            
            with col3:
                strength = signal.get("signal_strength", "UNKNOWN")
                st.metric("Strength", strength)
            
            with col4:
                status = signal["status"]
                badge_color = "blue" if status == "NEW" else "green" if status == "VALIDATED" else "orange"
                st.markdown(f"<div style='padding: 5px 10px; background-color:{badge_color}; color:white; border-radius:5px; text-align:center;'>{status}</div>", 
                           unsafe_allow_html=True)
            
            # Research context (expandable)
            with st.expander("Research Details"):
                if pd.notna(signal.get("research_context")):
                    try:
                        context = json.loads(signal["research_context"]) if isinstance(signal["research_context"], str) else signal["research_context"]
                        st.json(context)
                    except:
                        st.write("No research context available")
            
            st.markdown("---")
        
        # Performance metrics
        st.markdown("#### üìä Signal Performance")
        performance = fetch_signal_performance(days=7)
        
        if performance:
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Total Signals", performance.get("total_signals", 0))
            
            with col2:
                st.metric("Profitable", performance.get("profitable_signals", 0))
            
            with col3:
                win_rate = performance.get("win_rate", 0)
                st.metric("Win Rate", f"{win_rate:.1f}%")
            
            with col4:
                period = performance.get("period_days", 7)
                st.metric("Period", f"{period} days")
        
        # Signal distribution chart
        if not active_signals.empty:
            st.markdown("#### üìà Signal Distribution")
            
            fig = go.Figure(data=[
                go.Pie(
                    labels=active_signals["signal_type"].value_counts().index,
                    values=active_signals["signal_type"].value_counts().values,
                    hole=.3
                )
            ])
            
            fig.update_layout(
                height=300,
                showlegend=True,
                margin=dict(t=0, b=0, l=0, r=0)
            )
            
            st.plotly_chart(fig, width='stretch')
    
    else:
        st.info("üì≠ No active signals. The system will generate signals during market hours.")

# ==============================
# TAB 3: RESEARCH
# ==============================

with tab3:
    st.markdown('<div class="sub-header">üîç Research & Analytics</div>', unsafe_allow_html=True)
    
    # Research metrics
    if st.session_state.research_analytics:
        latest = st.session_state.research_analytics[-1]
        
        # Key research metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            net_gamma = latest.get("gamma_exposure", {}).get("net_gamma", 0)
            st.metric("Net Gamma", f"{net_gamma:,.0f}")
        
        with col2:
            pcr = latest.get("put_call_ratio", 1.0)
            st.metric("Put/Call Ratio", f"{pcr:.2f}")
        
        with col3:
            divergence = latest.get("divergence_score", 0)
            st.metric("Divergence Score", f"{divergence:.2f}")
        
        with col4:
            regime = latest.get("market_regime", "UNKNOWN")
            st.metric("Market Regime", regime)
        
        # Display market insights
        st.markdown("#### üí° Market Insights")
        
        insights = latest.get("market_insights", [])
        if insights:
            for insight in insights[:5]:
                if "trap" in insight.lower():
                    st.markdown(f'<div class="trap-alert">{insight}</div>', unsafe_allow_html=True)
                elif "divergence" in insight.lower():
                    st.markdown(f'<div class="divergence-alert">{insight}</div>', unsafe_allow_html=True)
                else:
                    st.markdown(f'<div class="research-insight">{insight}</div>', unsafe_allow_html=True)
        else:
            st.info("No insights available yet")
        
        # Structural walls
        st.markdown("#### üß± Structural Walls")
        
        walls = latest.get("structural_walls", [])
        if walls:
            walls_df = pd.DataFrame(walls)
            st.dataframe(walls_df, width='stretch')
        
        # Trap detection history
        st.markdown("#### üéØ Trap Detection History")
        
        if st.session_state.trap_detections:
            traps_df = pd.DataFrame([
                {
                    "Time": pd.to_datetime(t["timestamp"]).strftime("%H:%M:%S"),
                    "Type": t["trap"].get("direction", ""),
                    "Strike": t["trap"].get("strike", 0),
                    "Confidence": t["trap"].get("confidence", 0),
                    "Regime": t.get("market_regime", "")
                }
                for t in st.session_state.trap_detections
            ])
            
            if not traps_df.empty:
                st.dataframe(traps_df, width='stretch')
        
        # Research analytics over time
        st.markdown("#### üìä Analytics Timeline")
        
        if len(st.session_state.research_analytics) > 5:
            # Convert to DataFrame for plotting
            analytics_df = pd.DataFrame(st.session_state.research_analytics[-50:])
            
            if "timestamp" in analytics_df.columns:
                analytics_df["time"] = pd.to_datetime(analytics_df["timestamp"]).dt.strftime("%H:%M")
                
                # Plot OI Velocity and Gamma over time
                fig = make_subplots(
                    rows=2, cols=1,
                    shared_xaxes=True,
                    subplot_titles=("OI Velocity", "Net Gamma"),
                    vertical_spacing=0.1
                )
                
                if "oi_velocity" in analytics_df.columns:
                    fig.add_trace(
                        go.Scatter(
                            x=analytics_df["time"],
                            y=analytics_df["oi_velocity"],
                            mode='lines+markers',
                            name='OI Velocity',
                            line=dict(color='#10B981')
                        ),
                        row=1, col=1
                    )
                
                if "gamma_exposure" in analytics_df.columns:
                    # Extract net_gamma from gamma_exposure dict
                    try:
                        gamma_values = []
                        for g in analytics_df["gamma_exposure"]:
                            if isinstance(g, dict) and "net_gamma" in g:
                                gamma_values.append(g["net_gamma"])
                            else:
                                gamma_values.append(0)
                        
                        fig.add_trace(
                            go.Scatter(
                                x=analytics_df["time"],
                                y=gamma_values,
                                mode='lines+markers',
                                name='Net Gamma',
                                line=dict(color='#8B5CF6')
                            ),
                            row=2, col=1
                        )
                    except:
                        pass
                
                fig.update_layout(height=400, showlegend=True)
                st.plotly_chart(fig, width='stretch')

# ==============================
# TAB 4: CONFIGURATION
# ==============================

with tab4:
    st.markdown('<div class="sub-header">‚öôÔ∏è System Configuration</div>', unsafe_allow_html=True)
    
    # Available indices and underlyings
    available_indices = [
        "NSE_INDEX|Nifty 50",
        "NSE_INDEX|Nifty Bank", 
        "NSE_INDEX|Nifty Fin Service",
        "NSE_INDEX|Nifty Midcap 100",
        "NSE_INDEX|Nifty Next 50",
        "BSE_INDEX|S&P BSE SENSEX"  
    ]
    
    available_underlyings = [
        "NIFTY",
        "BANKNIFTY", 
        "FINNIFTY",
        "MIDCPNIFTY",
        "SENSEX"  
    ]
    
    # Get available expiries for the selected underlying
    current_underlying = st.session_state.config["underlying"]
    from data.instrument_master import get_available_expiries
    
    try:
        available_expiries = get_available_expiries(current_underlying)
        if not available_expiries:
            available_expiries = ["2026-01-27", "2026-02-03", "2026-02-10"]
    except:
        available_expiries = ["2026-01-27", "2026-02-03", "2026-02-10"]
    
    # Configuration form
    with st.form("configuration_form"):
        col1, col2 = st.columns(2)
        
        with col1:
            # Index Symbol Dropdown
            index_symbol = st.selectbox(
                "Index Symbol", 
                options=available_indices,
                index=available_indices.index(st.session_state.config["index_symbol"]) 
                if st.session_state.config["index_symbol"] in available_indices else 0
            )
            
            # Underlying Symbol Dropdown
            underlying = st.selectbox(
                "Underlying Symbol", 
                options=available_underlyings,
                index=available_underlyings.index(st.session_state.config["underlying"])
                if st.session_state.config["underlying"] in available_underlyings else 0
            )
            
            # Expiry Date Dropdown
            # Find today's date in available_expiries
            today_str = datetime.now().strftime("%Y-%m-%d")
            default_index = 0

            if today_str in available_expiries:
                default_index = available_expiries.index(today_str)
            elif today_str == "2026-01-28":  # Today's date
                # Add today to available expiries if it's not there
                available_expiries.insert(0, today_str)
                default_index = 0

            expiry_date = st.selectbox(
                "Expiry Date",
                options=available_expiries,
                index=default_index  # ‚úÖ Now selects today if available
            )
            
            # Convert selected expiry to datetime
            if expiry_date:
                try:
                    expiry_datetime = datetime.strptime(expiry_date, "%Y-%m-%d")
                    days_to_expiry = (expiry_datetime.date() - datetime.now().date()).days
                    st.info(f"Days to expiry: {days_to_expiry}")
                except:
                    st.warning("Invalid expiry date format")
        
        with col2:
            max_option_keys = st.number_input(
                "Max Option Keys", 
                min_value=50, 
                max_value=500, 
                value=st.session_state.config["max_option_keys"],
                help="Maximum number of option strikes to fetch"
            )
            
            scheduler_interval = st.slider(
                "Scheduler Interval (seconds)", 
                min_value=10, 
                max_value=300, 
                value=30,
                step=5,
                help="How often to refresh data (10-300 seconds)"
            )
            
            confidence_threshold = st.slider(
                "Signal Confidence Threshold", 
                min_value=0.0, 
                max_value=1.0, 
                value=0.2, 
                step=0.05,
                format="%.2f",
                help="Minimum confidence level for signals (0.0-1.0)"
            )
            
            # Advanced options expander
            with st.expander("Advanced Options"):
                enable_live_trading = st.checkbox(
                    "Enable Live Trading", 
                    value=False,
                    help="WARNING: Enable actual order placement"
                )
                
                risk_per_trade = st.number_input(
                    "Risk per Trade (%)",
                    min_value=0.1,
                    max_value=5.0,
                    value=1.0,
                    step=0.1,
                    format="%.1f",
                    help="Maximum risk per trade as percentage of capital"
                )
        
        # Submit button
        submit_col1, submit_col2, submit_col3 = st.columns([1, 2, 1])
        with submit_col2:
            if st.form_submit_button("üíæ Save Configuration", width='stretch'):
                # Update configuration
                st.session_state.config.update({
                    "index_symbol": index_symbol,
                    "underlying": underlying,
                    "expiry_date": datetime.strptime(expiry_date, "%Y-%m-%d") if expiry_date else datetime.now(timezone.utc) + timedelta(days=3),
                    "max_option_keys": max_option_keys,
                    "enable_live_trading": enable_live_trading,
                    "risk_per_trade": risk_per_trade
                })
                
                # Update scheduler
                st.session_state.scheduler = MarketScheduler(interval_seconds=scheduler_interval)
                
                # Update signal engine
                st.session_state.signal_engine = create_signal_engine(
                    signal_expiry_minutes=5,
                    confidence_threshold=confidence_threshold
                )
                
                # Clear option keys cache
                st.session_state.option_keys = []
                
                st.success("‚úÖ Configuration saved!")
                st.rerun()
    
    st.markdown("---")
    
    # Quick Configuration Presets
    st.markdown("#### üöÄ Quick Presets")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üìä NIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty 50",
                "underlying": "NIFTY"
            })
            st.success("Set to NIFTY Weekly")
            st.rerun()
    
    with col2:
        if st.button("üè¶ BANKNIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty Bank",
                "underlying": "BANKNIFTY"
            })
            st.success("Set to BANKNIFTY Weekly")
            st.rerun()
    
    with col3:
        if st.button("üí≥ FINNIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty Fin Service",
                "underlying": "FINNIFTY"
            })
            st.success("Set to FINNIFTY Weekly")
            st.rerun()
    
    # Current Configuration Display
    st.markdown("---")
    st.markdown("#### üñ•Ô∏è Current Configuration")
    
    config_col1, config_col2 = st.columns(2)
    
    with config_col1:
        st.markdown("**Trading Settings:**")
        st.text(f"Index: {st.session_state.config['index_symbol']}")
        st.text(f"Underlying: {st.session_state.config['underlying']}")
        st.text(f"Expiry: {st.session_state.config['expiry_date'].strftime('%Y-%m-%d')}")
        st.text(f"Max Options: {st.session_state.config['max_option_keys']}")
    
    with config_col2:
        st.markdown("**System Settings:**")
        st.text(f"Scheduler: {st.session_state.scheduler.interval_seconds}s")
        st.text(f"Signal Confidence: {confidence_threshold:.2f}")
        st.text(f"Live Trading: {'Enabled' if st.session_state.config.get('enable_live_trading', False) else 'Disabled'}")
        if st.session_state.config.get('enable_live_trading', False):
            st.text(f"Risk per Trade: {st.session_state.config.get('risk_per_trade', 1.0)}%")
    
    # Diagnostic tools (keep your existing code here)
    st.markdown("---")
    st.markdown("#### üîß Diagnostic Tools")
    # ... keep your existing diagnostic tools code ...
    
    # Diagnostic tools
    st.markdown("---")
    st.markdown("#### üîß Diagnostic Tools")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üß™ Test Upstox Connection", width='stretch'):
            # Test connection by trying to fetch profile
            try:
                profile = client.fetch_profile()
                if profile:
                    st.success(f"‚úÖ Upstox connection working")
                else:
                    st.error("‚ùå Failed to fetch profile")
            except Exception as e:
                st.error(f"‚ùå Connection test failed: {e}")
    
    with col2:
        if st.button("üìä Test Feature Pipeline", width='stretch'):
            try:
                # Get latest features
                features = fetch_latest_features(FEATURE_VERSION)
                if features is not None and not features.empty():
                    st.success(f"‚úÖ Feature pipeline working ({len(features)} features)")
                else:
                    st.warning("‚ö†Ô∏è No features found")
            except Exception as e:
                st.error(f"‚ùå Feature pipeline error: {e}")
    
    with col3:
        if st.button("üîÑ Clear Option Keys Cache", width='stretch'):
            st.session_state.option_keys = []
            st.success("‚úÖ Option keys cache cleared")

# ==============================
# PERFORMANCE MODAL
# ==============================

if st.session_state.get("show_performance", False):
    with st.expander("üìà Performance Analytics", expanded=True):
        st.markdown("### üìà Trading Performance")
        
        performance = fetch_signal_performance(days=30)
        
        if performance:
            # Overall metrics
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Total Trades", performance["total_signals"])
            
            with col2:
                st.metric("Profitable Trades", performance["profitable_signals"])
            
            with col3:
                win_rate = performance["win_rate"]
                color = "normal" if win_rate > 50 else "inverse"
                st.metric("Win Rate", f"{win_rate:.1f}%", delta_color=color)
            
            with col4:
                st.metric("Analysis Period", f"{performance['period_days']} days")
            
            # Breakdown by signal type
            st.markdown("#### Breakdown by Signal Type")
            
            if "breakdown" in performance:
                breakdown_df = pd.DataFrame(performance["breakdown"])
                st.dataframe(breakdown_df, width='stretch')
        
        st.session_state.show_performance = False

# ==============================
# LOGS MODAL
# ==============================

if st.session_state.get("show_logs", False):
    with st.expander("üìã System Logs", expanded=True):
        st.markdown("### üìã Recent System Activity")
        
        # Could fetch from system_health table here
        st.info("Logs would be displayed here from the database.")
        
        st.session_state.show_logs = False

# ==============================
# FOOTER
# ==============================

st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #6B7280; font-size: 0.9rem;'>
    Algorithmic Trading Research Platform v2.0 | 
    Research Features: OI Velocity, Gamma Exposure, Walls/Traps, Divergence Analysis
</div>
""", unsafe_allow_html=True)

# ==============================
# FINAL STARTUP CHECK
# ==============================
print(f"üéØ Final startup check:")
print(f"   - AutoRefreshManager in session_state: {'auto_refresh_manager' in st.session_state}")
print(f"   - Background thread running: {'background_thread_running' in st.session_state}")
print(f"   - Streamlit version: {st.__version__}")
print(f"   - Fragments available: {hasattr(st, 'fragment')}")

# Start the background manager
if "background_thread_running" not in st.session_state:
    print("üé¨ Starting background manager...")
    start_background_manager()
else:
    print("‚è≠Ô∏è Background manager already running, skipping...")
    



--------------------------------------------------
FILE PATH : G:\trading_app\app - Copy.py
SIZE      : 76459 bytes
--------------------------------------------------

"""
ENHANCED TRADING TOOL - Research-Based Live Feature Engine
Incorporates OI Velocity, Gamma Exposure, Walls/Traps, and Divergence analysis.
"""

import os
import sys

# Fix Conda environment issue
if 'conda' in sys.version.lower():
    try:
        # Try to fix conda path issues
        os.environ['PATH'] = os.path.join(os.environ.get('CONDA_PREFIX', ''), 'bin') + ':' + os.environ['PATH']
    except:
        pass
# ==============================
# STREAMLIT CONFIG FIX - ADD THIS
# ==============================
import os
os.environ['STREAMLIT_SERVER_FILE_WATCHER_TYPE'] = 'none'
import time 
import threading  # <-- ADD THIS LINE
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from datetime import datetime, timezone
import json
from pathlib import Path
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Enhanced core components
from core.session import UpstoxSession, initialize_session, display_session_status
from data.upstox_client import UpstoxClient, MarketAnalytics, display_market_analytics
from data.instrument_master import get_option_keys

try:
    from core.feature_pipeline import build_and_store_features, create_feature_summary
except ImportError as e:
    st.error(f"Feature pipeline import error: {e}")
    # Define dummy functions to prevent crash
    def build_and_store_features(*args, **kwargs):
        return {"error": "Feature pipeline not loaded"}
    def create_feature_summary(*args, **kwargs):
        return {"error": "Feature pipeline not loaded"}
from core.scheduler import MarketScheduler
from core.signals.state_machine import SignalStateMachine, SignalValidator, create_signal_engine
from features.breadth import build_constituents_df
from ml.feature_contract import FEATURE_VERSION

# Enhanced storage
from storage.repository import (
    fetch_latest_features,
    insert_signal,
    signal_exists,
    validate_new_signals,
    expire_old_signals,
    fetch_active_signals,
    fetch_recent_analytics,
    get_database_stats,
    fetch_signal_performance,
    insert_research_analytics
)

# ==============================
# PAGE CONFIGURATION
# ==============================

st.set_page_config(
    page_title="Algorithmic Trading Research Platform",
    page_icon="üìà",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E3A8A;
        font-weight: 700;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #374151;
        font-weight: 600;
        margin-top: 1.5rem;
        margin-bottom: 1rem;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
    }
    .research-insight {
        background: #F0F9FF;
        border-left: 4px solid #3B82F6;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
    .trap-alert {
        background: #FEF3C7;
        border-left: 4px solid #F59E0B;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
    .divergence-alert {
        background: #FEE2E2;
        border-left: 4px solid #EF4444;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
</style>
""", unsafe_allow_html=True)

# ==============================
# APP HEADER
# ==============================

st.markdown('<div class="main-header">üìä Algorithmic Trading Research Platform</div>', unsafe_allow_html=True)
st.markdown("""
<div style='color: #6B7280; margin-bottom: 2rem;'>
    Advanced derivatives analytics with OI Velocity, Gamma Exposure, Structural Walls/Traps, and Spot Divergence detection.
    Version: Research v2.0
</div>
""", unsafe_allow_html=True)

# ==============================
# INITIALIZATION
# ==============================

def initialize_app_state():
    """Initialize application state with research components."""
    
    # Initialize session
    initialize_session()
    
    # Initialize state variables
    if "initialized" not in st.session_state:
        st.session_state.initialized = True
        
        # Market data series
        for key in ["price_series", "volume_series", "ccc_history", "oi_velocity_history", "gamma_history"]:
            if key not in st.session_state:
                st.session_state[key] = pd.Series(dtype=float)
        
        # Research analytics
        if "research_analytics" not in st.session_state:
            st.session_state.research_analytics = []
        
        # Signal engine
        if "signal_engine" not in st.session_state:
            st.session_state.signal_engine = create_signal_engine(
                signal_expiry_minutes=5,
                confidence_threshold=0.2
            )
        
        # Scheduler
        if "scheduler" not in st.session_state:
            st.session_state.scheduler = MarketScheduler(interval_seconds=30)
            # Initialize scheduler properties
            st.session_state.scheduler._last_run = None
            st.session_state.scheduler._total_executions = 0
        
        # Load Nifty weights
        if "nifty_weights" not in st.session_state:
            try:
                with open("config/nifty_weights.json", "r") as f:
                    st.session_state.nifty_weights = json.load(f)
            except:
                st.session_state.nifty_weights = {}
        
        # Instrument master path
        if "instrument_master" not in st.session_state:
            st.session_state.instrument_master = Path("G:/trading_app/data/instruments.json.gz")
        
        # Configuration - SMART EXPIRY SELECTION
        if "config" not in st.session_state:
            from utils.expiry_utils import get_trading_expiry
            
            # Get smart expiry
            underlying = "NIFTY"
            expiry_date_str = get_trading_expiry(underlying)
            
            if not expiry_date_str:
                # Fallback to next available date from instrument master
                from data.instrument_master import get_next_available_expiry
                expiry_date_str = get_next_available_expiry(underlying)
                
                if not expiry_date_str:
                    # Final fallback to a known date
                    expiry_date_str = "2026-02-03"
            
            # Convert string to datetime object
            expiry_date = datetime.strptime(expiry_date_str, "%Y-%m-%d")
            
            st.session_state.config = {
                "index_symbol": "NSE_INDEX|Nifty 50",
                "underlying": underlying,
                "expiry_date": expiry_date,
                "max_option_keys": 200
            }
            
            # Show expiry info
            from utils.expiry_utils import is_market_open
            market_status = "open" if is_market_open() else "closed"
            
            # Get today's date
            today_str = datetime.now().strftime('%Y-%m-%d')
            
            # Show appropriate message
            if expiry_date_str == today_str:
                st.info(f"üìÖ Using TODAY's expiry: {expiry_date_str} (Market: {market_status})")
            else:
                st.info(f"üìÖ Using NEXT expiry: {expiry_date_str} (Today: {today_str}, Market: {market_status})")
        
        # Option keys cache
        if "option_keys" not in st.session_state:
            st.session_state.option_keys = []
        
        # Market regime tracking
        if "market_regime" not in st.session_state:
            st.session_state.market_regime = "UNKNOWN"
        
        # Trap detection history
        if "trap_detections" not in st.session_state:
            st.session_state.trap_detections = []
        
        st.success("‚úÖ Application state initialized with research components")

# Run initialization
initialize_app_state()

# ==============================
# AUTHENTICATION
# ==============================

st.markdown('<div class="sub-header">üîê Authentication</div>', unsafe_allow_html=True)

# Authenticate - this will show login button if not authenticated
access_token = UpstoxSession.authenticate()

# CRITICAL: Check if authenticate() actually returned a token
# If it returns None, we need to stop execution here
if not access_token:
    # authenticate() should have shown login button and stopped with st.stop()
    # But if we reach here, something went wrong
    st.error("‚ùå Authentication required. Please log in to continue.")
    
    # Show login button manually as fallback
    login_url = UpstoxSession.get_login_url()
    st.markdown(f"""
    <div style="text-align: center; margin: 20px 0;">
        <a href="{login_url}" target="_blank">
            <button style="
                background-color: #00d09c;
                color: white;
                padding: 15px 30px;
                font-size: 18px;
                font-weight: bold;
                border: none;
                border-radius: 10px;
                cursor: pointer;
                width: 60%;
                margin: 20px 0;
                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            ">
            üìà Click to Login with Upstox
            </button>
        </a>
    </div>
    """, unsafe_allow_html=True)
    
    # STOP the app here
    st.stop()

# If we reach here, we have a token
# Initialize client
try:
    client = UpstoxClient(access_token)
    st.success("‚úÖ Upstox client initialized")
        
except Exception as e:
    st.error(f"‚ùå Error initializing Upstox client: {e}")
    st.stop()

# ==============================
# SIDEBAR - SESSION STATUS
# ==============================

with st.sidebar:
    st.markdown("## üß≠ Navigation")
    
    # Display session status
    display_session_status()
    
    # Database stats
    st.markdown("---")
    st.markdown("### üìä Database")
    
    db_stats = get_database_stats()
    if db_stats:
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Features", f"{db_stats.get('market_features_count', 0):,}")
        with col2:
            st.metric("Signals", f"{db_stats.get('signals_count', 0):,}")
    
    # Market hours info
    st.markdown("---")
    st.markdown("### üïí Market Hours")
    
    scheduler: MarketScheduler = st.session_state.scheduler
    now = datetime.now()
    
    if scheduler._is_market_open(now):
        st.success("‚úÖ Market Open")
        next_run = scheduler._last_run + timedelta(seconds=scheduler.interval_seconds) if scheduler._last_run else now
        time_to_next = (next_run - now).total_seconds()
        if time_to_next > 0:
            st.info(f"Next cycle in: {int(time_to_next)}s")
    else:
        st.warning("‚è∏Ô∏è Market Closed")
    
    # Quick actions
    st.markdown("---")
    st.markdown("### ‚ö° Quick Actions")
    
    if st.button("üîÑ Clear Cache", width='stretch'):
        for key in list(st.session_state.keys()):
            if key not in ["initialized", "instrument_master", "config", "nifty_weights"]:
                del st.session_state[key]
        st.rerun()
    
    if st.button("üìä View Performance", width='stretch'):
        st.session_state.show_performance = True
    
    if st.button("üõ†Ô∏è System Logs", width='stretch'):
        st.session_state.show_logs = True

def retry_on_failure(max_retries=3, delay=5):
    """Decorator for retrying failed executions"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    print(f"‚ö†Ô∏è Attempt {attempt + 1}/{max_retries} failed: {e}")
                    if attempt < max_retries - 1:
                        print(f"‚è∏Ô∏è Retrying in {delay}s...")
                        time.sleep(delay)
                    else:
                        print(f"‚ùå All {max_retries} attempts failed")
                        raise
            return None
        return wrapper
    return decorator

def log_execution_time(func):
    """Decorator for logging execution time"""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        print(f"‚è±Ô∏è Starting {func.__name__}...")
        
        result = func(*args, **kwargs)
        
        execution_time = time.time() - start_time
        print(f"‚è±Ô∏è {func.__name__} completed in {execution_time:.2f}s")
        
        return result
    return wrapper
# ==============================
# RESEARCH EXECUTION CYCLE
# ==============================

@log_execution_time
@retry_on_failure(max_retries=2, delay=3)
def execute_research_cycle(config=None, nifty_weights=None, option_keys=None):
    """
    Enhanced execution cycle with research analytics.
    Returns comprehensive research data.
    """
    # Use provided parameters or fall back to session state
    if config is None:
        try:
            config = st.session_state.config
        except (KeyError, AttributeError):
            print("‚ö†Ô∏è Config not found in session state")
            return None
    
    if nifty_weights is None:
        nifty_weights = st.session_state.get("nifty_weights", {})
    
    if option_keys is None:
        option_keys = st.session_state.get("option_keys", [])
    
    research_data = {}
    
    try:
        # ------------------------------
        # 1. INDEX QUOTE
        # ------------------------------
        print(f"üìä Fetching index quote for {config.get('index_symbol', 'NSE_INDEX|Nifty 50')}")
        index_data = client.fetch_index_quote(config["index_symbol"])
        
        if not index_data or index_data.get("ltp") is None:
            print("‚ùå No index data received")
            return None
        
        ltp = float(index_data["ltp"])
        open_price = float(index_data["open"])
        volume = 0.0  # index has no volume
        
        research_data["index"] = {
            "ltp": ltp,
            "open": open_price,
            "change": index_data.get("percent_change", 0),
            "timestamp": index_data.get("timestamp", datetime.now(timezone.utc).isoformat())
        }
        
        print(f"‚úÖ Index data: {ltp} (Change: {index_data.get('percent_change', 0)}%)")
        
        # ------------------------------
        # 2. UPDATE PRICE SERIES
        # ------------------------------
        # Create new series safely
        try:
            if "price_series" not in st.session_state:
                st.session_state.price_series = pd.Series(dtype=float)
            
            if "volume_series" not in st.session_state:
                st.session_state.volume_series = pd.Series(dtype=float)
            
            # Update price series
            new_price_series = pd.concat([
                st.session_state.price_series, 
                pd.Series([ltp], index=[datetime.now()])
            ]).tail(200)
            
            # Update volume series
            new_volume_series = pd.concat([
                st.session_state.volume_series, 
                pd.Series([volume], index=[datetime.now()])
            ]).tail(200)
            
            # Store back to session state
            st.session_state.price_series = new_price_series
            st.session_state.volume_series = new_volume_series
            
            print(f"üìà Price series updated: {len(st.session_state.price_series)} data points")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error updating price series: {e}")
            # Initialize fresh series
            st.session_state.price_series = pd.Series([ltp], index=[datetime.now()])
            st.session_state.volume_series = pd.Series([volume], index=[datetime.now()])
        
        # ------------------------------
        # 3. BREADTH ANALYSIS (CCC)
        # ------------------------------
        print("üìä Starting breadth analysis...")
        try:
            if not nifty_weights:
                print("‚ö†Ô∏è No Nifty weights found, skipping breadth analysis")
                ccc_value = 0.0
                constituents_df = pd.DataFrame()
            else:
                equity_quotes_df = client.fetch_equity_quotes(
                    symbols=list(nifty_weights.keys())
                )
                
                constituents_df = build_constituents_df(
                    equity_quotes_df=equity_quotes_df,
                    weights=nifty_weights
                )
                
                if not constituents_df.empty:
                    ccc_value = (
                        constituents_df["weight"] * constituents_df["price_change"]
                    ).sum()
                    print(f"‚úÖ CCC Value: {ccc_value:.4f}")
                else:
                    ccc_value = 0.0
                    print("‚ö†Ô∏è Empty constituents dataframe")
        except Exception as e:
            print(f"‚ö†Ô∏è Breadth analysis error: {e}")
            ccc_value = 0.0
            constituents_df = pd.DataFrame()
        
        # Update CCC history
        try:
            if "ccc_history" not in st.session_state:
                st.session_state.ccc_history = pd.Series(dtype=float)
            
            st.session_state.ccc_history = pd.concat([
                st.session_state.ccc_history, 
                pd.Series([ccc_value], index=[datetime.now()])
            ]).tail(200)
        except:
            st.session_state.ccc_history = pd.Series([ccc_value], index=[datetime.now()])
        
        research_data["breadth"] = {
            "ccc_value": ccc_value,
            "constituents_count": len(constituents_df),
            "positive_constituents": len(constituents_df[constituents_df["price_change"] > 0]) if not constituents_df.empty else 0
        }
        
        # ------------------------------
        # 4. OPTION INSTRUMENT DISCOVERY
        # ------------------------------
        print("üîç Fetching option instruments...")
        try:
            if not option_keys:
                expiry_str = config["expiry_date"].strftime("%Y-%m-%d")
                option_keys = get_option_keys(
                    underlying=config["underlying"],
                    expiry=expiry_str,
                    max_keys=config.get("max_option_keys", 200)
                )
                st.session_state.option_keys = option_keys
                print(f"‚úÖ Found {len(option_keys)} option keys")
        except Exception as e:
            print(f"‚ö†Ô∏è Option discovery error: {e}")
            option_keys = []
            st.session_state.option_keys = []
        
        if not option_keys:
            print("‚ö†Ô∏è No option keys found")
            return research_data  # Return what we have so far
        
        # ------------------------------
        # 5. ENHANCED OPTION CHAIN ANALYSIS
        # ------------------------------
        print("üìä Analyzing option chain...")
        option_analytics = client.fetch_option_chain_with_analytics(
            option_keys,
            ltp
        )
        
        if not option_analytics or "raw_data" not in option_analytics:
            print("‚ùå Failed to fetch option analytics")
            return research_data  # Return what we have so far
        
        option_chain_df = option_analytics["raw_data"]
        analytics = option_analytics.get("analytics", {})
        insights = option_analytics.get("market_insights", [])
        
        research_data["options"] = {
            "analytics": analytics,
            "insights": insights,
            "chain_size": len(option_chain_df),
            "put_call_ratio": analytics.get("put_call_ratio", 1.0) if isinstance(analytics.get("put_call_ratio"), (int, float)) else 1.0
        }
        
        print(f"‚úÖ Option chain analyzed: {len(option_chain_df)} strikes, PCR: {analytics.get('put_call_ratio', 'N/A')}")
        
        # Store analytics for visualization
        if analytics:
            try:
                if "research_analytics" not in st.session_state:
                    st.session_state.research_analytics = []
                
                st.session_state.research_analytics.append({
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    **analytics
                })
                
                # Keep only recent analytics
                if len(st.session_state.research_analytics) > 100:
                    st.session_state.research_analytics = st.session_state.research_analytics[-100:]
                
                # Update market regime
                st.session_state.market_regime = analytics.get("market_regime", "UNKNOWN")
                
                # Store in database
                insert_research_analytics(analytics)
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error storing analytics: {e}")
        
        # ------------------------------
        # 6. FEATURE PIPELINE
        # ------------------------------
        print("‚öôÔ∏è Running feature pipeline...")
        try:
            snapshot_ts = pd.Timestamp.utcnow().floor("ms")
            
            feature_result = build_and_store_features(
                timestamp=snapshot_ts,
                option_chain_df=option_chain_df,
                spot_price=ltp,
                expiry_datetime=config["expiry_date"],
                
                ltp=ltp,
                vwap=open_price,
                price_series=st.session_state.price_series,
                volume_series=st.session_state.volume_series,
                
                constituents_df=constituents_df,
                ccc_history=st.session_state.ccc_history,
                
                # Enhanced parameters
                client=client,
                option_keys=option_keys
            )
            
            if feature_result:
                research_data["features"] = feature_result.get("features", {})
                research_data["feature_summary"] = create_feature_summary(feature_result["features"])
                print("‚úÖ Feature pipeline completed")
            else:
                print("‚ö†Ô∏è Feature pipeline returned empty result")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Feature pipeline error: {e}")
            import traceback
            traceback.print_exc()
        
        # ------------------------------
        # 7. SIGNAL GENERATION
        # ------------------------------
        print("üéØ Checking for signals...")
        try:
            feature_row = fetch_latest_features(FEATURE_VERSION)
            if feature_row is not None and not feature_row.empty:
                # Check if signal already exists
                timestamp_str = feature_row.iloc[0]["timestamp"]
                
                if not signal_exists(timestamp_str):
                    # Generate signal
                    signal_engine: SignalStateMachine = st.session_state.signal_engine
                    signal = signal_engine.build_signal(feature_row.iloc[0])
                    
                    # Validate signal
                    is_valid, reason = SignalValidator.validate_signal(signal, feature_row.iloc[0])
                    
                    if is_valid and signal["signal_type"] != "NEUTRAL":
                        # Add research context
                        signal["research_validation"] = {
                            "is_valid": is_valid,
                            "reason": reason,
                            "timestamp": datetime.now(timezone.utc).isoformat()
                        }
                        
                        # Insert signal
                        insert_signal(signal)
                        
                        research_data["signal"] = {
                            "generated": True,
                            "type": signal["signal_type"],
                            "confidence": signal["confidence"],
                            "strength": signal.get("signal_strength", "UNKNOWN")
                        }
                        print(f"‚úÖ Signal generated: {signal['signal_type']} ({signal['confidence']:.0%})")
                    else:
                        research_data["signal"] = {
                            "generated": False,
                            "reason": reason if not is_valid else "NEUTRAL signal"
                        }
                        print(f"‚ÑπÔ∏è No valid signal: {reason if not is_valid else 'NEUTRAL'}")
            else:
                print("‚ÑπÔ∏è No features available for signal generation")
        except Exception as e:
            print(f"‚ö†Ô∏è Signal generation error: {e}")
        
        # ------------------------------
        # 8. SIGNAL LIFECYCLE MANAGEMENT
        # ------------------------------
        try:
            validate_new_signals(confidence_threshold=0.2)
            expire_old_signals()
        except Exception as e:
            print(f"‚ö†Ô∏è Signal lifecycle error: {e}")
        
        # ------------------------------
        # 9. TRAP DETECTION
        # ------------------------------
        if analytics and "potential_traps" in analytics:
            traps = analytics["potential_traps"]
            if traps:
                try:
                    if "trap_detections" not in st.session_state:
                        st.session_state.trap_detections = []
                    
                    for trap in traps:
                        if trap.get("confidence", 0) > 0.6:
                            st.session_state.trap_detections.append({
                                "timestamp": datetime.now(timezone.utc).isoformat(),
                                "trap": trap,
                                "market_regime": analytics.get("market_regime")
                            })
                    
                    # Keep only recent traps
                    if len(st.session_state.trap_detections) > 20:
                        st.session_state.trap_detections = st.session_state.trap_detections[-20:]
                    
                    research_data["traps"] = traps
                    
                    if traps:
                        print(f"üéØ Detected {len(traps)} potential traps")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Trap detection error: {e}")
        
        # ------------------------------
        # 10. SUCCESSFUL COMPLETION
        # ------------------------------
        print("‚úÖ Research cycle completed successfully")
        return research_data
        
    except Exception as e:
        print(f"‚ùå Critical error in research cycle: {e}")
        import traceback
        traceback.print_exc()
        return None

# ==============================
# ENHANCED AUTO-REFRESH SYSTEM
# ==============================

class AutoRefreshManager:
    """Robust auto-refresh manager with error handling and state persistence"""
    
    def __init__(self, scheduler, config, nifty_weights, option_keys):
        self.scheduler = scheduler
        self.config = config
        self.nifty_weights = nifty_weights
        self.option_keys = option_keys
        self.last_execution_time = None
        self.execution_count = 0
        self.errors = []
        self.max_retries = 3
        self.retry_delay = 5
        
    def should_execute(self):
        """Determine if execution should run based on time and market conditions"""
        now = datetime.now()
        
        # Check market hours
        if not self.scheduler._is_market_open(now):
            return False, "Market closed"
        
        # Check if first run
        if self.last_execution_time is None:
            return True, "First execution"
        
        # Check time since last execution
        time_since_last = (now - self.last_execution_time).total_seconds()
        
        # Debug log to see what's happening
        print(f"üïí Auto-refresh timing: {int(time_since_last)}s since last, interval: {self.scheduler.interval_seconds}s")
        
        if time_since_last >= self.scheduler.interval_seconds:
            return True, f"Interval reached ({int(time_since_last)}s > {self.scheduler.interval_seconds}s)"
        
        return False, f"Not due yet ({int(self.scheduler.interval_seconds - time_since_last)}s remaining)"
    
    def execute_with_retry(self):
        """Execute research cycle with retry logic"""
        for attempt in range(self.max_retries):
            try:
                print(f"üîÑ Attempt {attempt + 1}/{self.max_retries} to execute research cycle")
                
                result = execute_research_cycle(
                    config=self.config,
                    nifty_weights=self.nifty_weights,
                    option_keys=self.option_keys
                )
                
                if result:
                    self.last_execution_time = datetime.now()
                    self.execution_count += 1
                    return True, result, f"Execution successful (attempt {attempt + 1})"
                else:
                    return False, None, "Research cycle returned no data"
                    
            except Exception as e:
                error_msg = f"Attempt {attempt + 1} failed: {str(e)}"
                self.errors.append({
                    "timestamp": datetime.now(),
                    "attempt": attempt + 1,
                    "error": str(e)
                })
                
                if attempt < self.max_retries - 1:
                    print(f"‚è∏Ô∏è Retrying in {self.retry_delay}s...")
                    time.sleep(self.retry_delay)
                else:
                    return False, None, f"All retries failed. Last error: {str(e)}"
        
        return False, None, "Max retries exceeded"
    
    def get_status(self):
        """Get current status of auto-refresh manager"""
        status = {
            "last_execution": self.last_execution_time.strftime("%H:%M:%S") if self.last_execution_time else "Never",
            "execution_count": self.execution_count,
            "is_market_open": self.scheduler._is_market_open(datetime.now()),
            "interval_seconds": self.scheduler.interval_seconds,
            "error_count": len(self.errors)
        }
        
        if self.last_execution_time:
            time_since = (datetime.now() - self.last_execution_time).total_seconds()
            status["time_since_last"] = f"{int(time_since)}s"
            status["time_until_next"] = f"{max(0, int(self.scheduler.interval_seconds - time_since))}s"
        
        return status

# Initialize auto-refresh manager
if "auto_refresh_manager" not in st.session_state:
    st.session_state.auto_refresh_manager = AutoRefreshManager(
        scheduler=st.session_state.scheduler,
        config=st.session_state.config,
        nifty_weights=st.session_state.nifty_weights,
        option_keys=st.session_state.option_keys
    )

# Execute auto-refresh
def execute_auto_refresh():
    """Main auto-refresh execution function"""
    manager = st.session_state.auto_refresh_manager
    
    try:
        # Check if we should execute
        should_execute, reason = manager.should_execute()
        
        if should_execute:
            print(f"üèÉ‚Äç‚ôÇÔ∏è Auto-execution triggered: {reason}")
            
            # Execute with retry logic
            success, result, message = manager.execute_with_retry()
            
            if success:
                print(f"‚úÖ {message}")
                
                # Update UI state if needed
                if result and "options" in result and "analytics" in result["options"]:
                    analytics = result["options"]["analytics"]
                    if analytics:
                        # Update market regime
                        st.session_state.market_regime = analytics.get("market_regime", "UNKNOWN")
                        
                        # Store analytics
                        if "research_analytics" not in st.session_state:
                            st.session_state.research_analytics = []
                        st.session_state.research_analytics.append({
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                            **analytics
                        })
                        # Keep only recent analytics
                        if len(st.session_state.research_analytics) > 100:
                            st.session_state.research_analytics = st.session_state.research_analytics[-100:]
                
                return True, message
            else:
                print(f"‚ùå Auto-execution failed: {message}")
                return False, message
        else:
            # Log status periodically
            if manager.last_execution_time:
                time_since = (datetime.now() - manager.last_execution_time).total_seconds()
                if int(time_since) % 10 == 0:  # Log every 10 seconds
                    print(f"‚è±Ô∏è Auto-refresh status: {reason}")
            return None, reason
            
    except Exception as e:
        error_msg = f"Auto-refresh system error: {str(e)}"
        print(f"üî• {error_msg}")
        manager.errors.append({
            "timestamp": datetime.now(),
            "error": error_msg
        })
        return False, error_msg

# Run auto-refresh
execute_auto_refresh()

# ==============================
# ENHANCED BACKGROUND SCHEDULER
# ==============================

class BackgroundScheduler:
    """Robust background scheduler with error handling and monitoring"""
    
    def __init__(self, check_interval=5):
        self.check_interval = check_interval
        self.running = False
        self.thread = None
        self.errors = []
        self.last_check = None
        self.health_checks = []
        
    def health_check(self):
        """Perform system health check"""
        checks = []
        
        # Check scheduler
        try:
            scheduler = st.session_state.scheduler
            checks.append({
                "component": "Scheduler",
                "status": "HEALTHY" if scheduler else "UNKNOWN",
                "details": f"Interval: {scheduler.interval_seconds}s" if scheduler else "Not initialized"
            })
        except Exception as e:
            checks.append({
                "component": "Scheduler",
                "status": "ERROR",
                "details": str(e)
            })
        
        # Check auto-refresh manager
        try:
            manager = st.session_state.auto_refresh_manager
            checks.append({
                "component": "AutoRefresh",
                "status": "HEALTHY",
                "details": f"Executions: {manager.execution_count}"
            })
        except Exception as e:
            checks.append({
                "component": "AutoRefresh",
                "status": "ERROR",
                "details": str(e)
            })
        
        self.health_checks = checks
        return checks
    
    def start(self):
        """Start background scheduler thread"""
        if self.running:
            print("‚ö†Ô∏è Background scheduler already running")
            return
        
        def background_task():
            print("üöÄ Starting enhanced background scheduler...")
            self.running = True
            
            while self.running:
                try:
                    self.last_check = datetime.now()
                    
                    # Perform health check every minute
                    if not self.health_checks or (datetime.now() - self.last_check).seconds > 60:
                        self.health_check()
                    
                    # Log status every 30 seconds
                    current_time = datetime.now()
                    if current_time.second % 30 == 0:
                        manager = st.session_state.auto_refresh_manager
                        if manager.last_execution_time:
                            time_since = (current_time - manager.last_execution_time).total_seconds()
                            print(f"üìä Background monitor: Last execution {int(time_since)}s ago, {manager.execution_count} total runs")
                    
                    time.sleep(self.check_interval)
                    
                except Exception as e:
                    error_msg = f"Background scheduler error: {str(e)}"
                    print(f"üî• {error_msg}")
                    self.errors.append({
                        "timestamp": datetime.now(),
                        "error": error_msg
                    })
                    time.sleep(30)  # Wait longer on error
        
        self.thread = threading.Thread(target=background_task, daemon=True)
        self.thread.start()
        print("‚úÖ Enhanced background scheduler started")
    
    def stop(self):
        """Stop background scheduler"""
        self.running = False
        if self.thread:
            self.thread.join(timeout=5)
        print("üõë Background scheduler stopped")
    
    def get_status(self):
        """Get scheduler status"""
        return {
            "running": self.running,
            "check_interval": self.check_interval,
            "last_check": self.last_check.strftime("%H:%M:%S") if self.last_check else "Never",
            "error_count": len(self.errors),
            "health_checks": self.health_checks
        }

# Initialize and start background scheduler
if "background_scheduler" not in st.session_state:
    st.session_state.background_scheduler = BackgroundScheduler(check_interval=5)
    st.session_state.background_scheduler.start()


def get_scheduler_status():
    """Get comprehensive scheduler status"""
    scheduler = st.session_state.scheduler
    manager = st.session_state.auto_refresh_manager
    
    status = {
        "scheduler_active": True,
        "interval_seconds": scheduler.interval_seconds,
        "total_executions": scheduler._total_executions,
        "last_run": scheduler._last_run.strftime("%H:%M:%S") if scheduler._last_run else "Never",
        "is_market_open": scheduler._is_market_open(datetime.now()),
        "auto_refresh_status": manager.get_status()
    }
    
    # Calculate next run time
    if scheduler._last_run:
        next_run = scheduler._last_run + timedelta(seconds=scheduler.interval_seconds)
        time_until_next = (next_run - datetime.now()).total_seconds()
        status["next_run"] = next_run.strftime("%H:%M:%S") if time_until_next > 0 else "Now"
        status["time_until_next"] = f"{max(0, int(time_until_next))}s"
    else:
        status["next_run"] = "Pending"
        status["time_until_next"] = "N/A"
    
    return status

# ==============================
# CREATE TABS
# ==============================

# Create tabs for different views
tab1, tab2, tab3, tab4 = st.tabs([
    "üìà Live Analytics", 
    "üéØ Signals", 
    "üîç Research", 
    "‚öôÔ∏è Configuration"
])

# ==============================
# AUTO-REFRESH WORKING SOLUTION
# ==============================

# Create a placeholder for auto-refresh
refresh_placeholder = st.empty()

# Check if we need to auto-refresh
if "next_refresh_time" not in st.session_state:
    st.session_state.next_refresh_time = time.time() + 30  # First refresh in 30 seconds

current_time = time.time()

if current_time >= st.session_state.next_refresh_time:
    # Time to refresh!
    print(f"üîÑ AUTO-REFRESH EXECUTING at {datetime.now().strftime('%H:%M:%S')}")
    
    # Execute research cycle
    with st.spinner("Auto-refreshing data..."):
        research_data = execute_research_cycle()
    
    if research_data:
        print(f"‚úÖ Auto-refresh completed successfully")
        
        # Update next refresh time
        st.session_state.next_refresh_time = current_time + 30
        
        # Update state
        st.session_state.scheduler._last_run = datetime.now()
        st.session_state.scheduler._total_executions += 1
        st.session_state.auto_refresh_manager.last_execution_time = datetime.now()
        st.session_state.auto_refresh_manager.execution_count += 1
        
        # Show success message
        refresh_placeholder.success(f"‚úÖ Auto-refresh completed at {datetime.now().strftime('%H:%M:%S')}")
        time.sleep(1)
        refresh_placeholder.empty()
        
        # Force a rerun to update UI
        st.rerun()
    else:
        print("‚ùå Auto-refresh failed")
        refresh_placeholder.error("Auto-refresh failed")
        time.sleep(2)
        refresh_placeholder.empty()
else:
    # Show countdown
    time_until_next = st.session_state.next_refresh_time - current_time
    if time_until_next > 0:
        print(f"‚è±Ô∏è Next auto-refresh in {int(time_until_next)}s")
        
        # Update countdown every 10 seconds
        if int(time_until_next) % 10 == 0:
            refresh_placeholder.info(f"Next auto-refresh in {int(time_until_next)} seconds...")


# ==============================
# TAB 1: LIVE ANALYTICS
# ==============================

with tab1:
    # Add JavaScript auto-refresh
    auto_refresh_js = """
    <script>
    // Auto-refresh every 30 seconds
    setTimeout(function() {
        window.location.reload();
    }, 30000);
    </script>
    """
    st.components.v1.html(auto_refresh_js, height=0)
    
    st.markdown('<div class="sub-header">üìä Live Market Analytics</div>', unsafe_allow_html=True)
   
    
    # Execute cycle - but this is just for UI display now
    scheduler: MarketScheduler = st.session_state.scheduler
    
    # Track last execution time
    if "last_execution_time" not in st.session_state:
        st.session_state.last_execution_time = None
    
    # Display background scheduler status
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if scheduler._last_run:
            time_since_last = (datetime.now() - scheduler._last_run).total_seconds()
            status_text = "‚úÖ Running" if time_since_last < 60 else "‚ö†Ô∏è Stale"
            st.metric("Scheduler", status_text, delta=f"{int(time_since_last)}s ago")
        else:
            st.metric("Scheduler", "‚è≥ Starting", delta="Initializing")
    
    with col2:
        if st.session_state.price_series.size > 0:
            current_price = st.session_state.price_series.iloc[-1]
            prev_price = st.session_state.price_series.iloc[-2] if st.session_state.price_series.size > 1 else current_price
            change_pct = ((current_price - prev_price) / prev_price * 100) if prev_price > 0 else 0
            st.metric("Nifty 50", f"{current_price:,.0f}", f"{change_pct:+.2f}%")
    
    with col3:
        if st.session_state.research_analytics:
            latest = st.session_state.research_analytics[-1]
            regime = latest.get("market_regime", "UNKNOWN")
            st.metric("Market Regime", regime)
    
    with col4:
        if st.session_state.research_analytics:
            latest = st.session_state.research_analytics[-1]
            oi_vel = latest.get("oi_velocity", 0)
            st.metric("OI Velocity", f"{oi_vel:+.2f}œÉ")
    
    # Manual execution button - still works
    if st.button("üîÑ Execute Research Cycle Now", type="primary", width='stretch'):
        with st.spinner("Executing research cycle..."):
            research_data = execute_research_cycle()
            if research_data:
                st.success("‚úÖ Research cycle executed successfully")
                
                # Display insights if available
                if "options" in research_data and "insights" in research_data["options"]:
                    for insight in research_data["options"]["insights"][:3]:
                        st.info(insight)
            else:
                st.error("‚ùå Research cycle failed")
    
    st.markdown("---")
    
    # Display enhanced status
    st.markdown("---")
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        status = get_scheduler_status()
        if status["scheduler_active"]:
            if status["last_run"] != "Never":
                time_since = (datetime.now() - st.session_state.scheduler._last_run).total_seconds() if st.session_state.scheduler._last_run else 999
                if time_since < 60:
                    st.metric("Scheduler", "‚úÖ Active", delta=f"{int(time_since)}s ago")
                else:
                    st.metric("Scheduler", "‚ö†Ô∏è Stale", delta=f"{int(time_since)}s ago")
            else:
                st.metric("Scheduler", "‚è≥ Initializing", delta="First run pending")
        else:
            st.metric("Scheduler", "‚ùå Inactive", delta="Check configuration")

    with col2:
        manager_status = st.session_state.auto_refresh_manager.get_status()
        st.metric("Auto Refresh", f"Run #{manager_status['execution_count']}", 
                delta=manager_status['last_execution'])

    with col3:
        if manager_status.get('time_until_next'):
            st.metric("Next Run In", manager_status['time_until_next'])

    with col4:
        error_count = manager_status.get('error_count', 0)
        if error_count == 0:
            st.metric("Errors", "‚úÖ 0", delta="Clean")
        else:
            st.metric("Errors", f"‚ö†Ô∏è {error_count}", delta="Check logs")

    # Control buttons
    col1, col2, col3 = st.columns(3)

    with col1:
        if st.button("üîÑ Force Run Now", type="primary", width='stretch'):
            # Force immediate execution
            st.session_state.auto_refresh_manager.last_execution_time = None
            st.rerun()

    with col2:
        if st.button("‚è∏Ô∏è Pause Auto-Refresh", width='stretch'):
            # Temporarily disable auto-refresh
            original_interval = st.session_state.scheduler.interval_seconds
            st.session_state.scheduler.interval_seconds = 3600  # 1 hour
            st.success(f"Auto-refresh paused. Original interval: {original_interval}s")
            st.rerun()

    with col3:
        if st.button("‚ñ∂Ô∏è Resume Auto-Refresh", width='stretch'):
            # Restore auto-refresh
            st.session_state.scheduler.interval_seconds = 30
            st.success("Auto-refresh resumed (30s interval)")
            st.rerun()
    # Display background thread info
    with st.expander("üîÑ Background Scheduler Info", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            if "scheduler_thread" in st.session_state:
                thread = st.session_state.scheduler_thread
                st.success("‚úÖ Background thread active")
                st.info(f"Thread ID: {thread.ident}")
                st.info(f"Thread alive: {thread.is_alive()}")
            else:
                st.error("‚ùå Background thread not running")
        
        with col2:
            if scheduler._last_run:
                st.metric("Last Execution", scheduler._last_run.strftime("%H:%M:%S"))
                time_since = (datetime.now() - scheduler._last_run).total_seconds()
                st.metric("Time Since", f"{int(time_since)}s")
            else:
                st.metric("Last Execution", "Never")
            
            st.metric("Total Executions", scheduler._total_executions)
    
    
    # Display latest analytics if available
    if st.session_state.research_analytics:
        latest_analytics = st.session_state.research_analytics[-1]
        
        # Create columns for analytics display
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### üìà OI & Gamma Analysis")
            
            # OI Velocity gauge
            oi_velocity = latest_analytics.get("oi_velocity", 0)
            fig_oi = go.Figure(go.Indicator(
                mode="gauge+number+delta",
                value=oi_velocity,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "OI Velocity (œÉ)"},
                delta={'reference': 0},
                gauge={
                    'axis': {'range': [-3, 3]},
                    'steps': [
                        {'range': [-3, -1.5], 'color': "lightgray"},
                        {'range': [-1.5, 1.5], 'color': "gray"},
                        {'range': [1.5, 3], 'color': "lightgray"}
                    ],
                    'threshold': {
                        'line': {'color': "red", 'width': 4},
                        'thickness': 0.75,
                        'value': oi_velocity
                    }
                }
            ))
            fig_oi.update_layout(height=250)
            st.plotly_chart(fig_oi, width='stretch')
        
        with col2:
            st.markdown("#### üß± Structure Analysis")
            
            # Structural metrics
            wall_strength = latest_analytics.get("wall_strength", 0)
            trap_prob = latest_analytics.get("trap_probability", 0)
            
            col_a, col_b = st.columns(2)
            with col_a:
                st.metric("Wall Strength", f"{wall_strength:.2f}")
            with col_b:
                st.metric("Trap Probability", f"{trap_prob:.2%}")
            
            # Display traps if any
            traps = latest_analytics.get("potential_traps", [])
            if traps:
                st.markdown("##### üéØ Detected Traps")
                for trap in traps[:2]:  # Show max 2
                    direction = trap.get("direction", "").upper()
                    confidence = trap.get("confidence", 0)
                    strike = trap.get("strike", 0)
                    
                    if confidence > 0.6:
                        st.markdown(f"""
                        <div class="trap-alert">
                            <strong>{direction} Trap</strong> at {strike:,.0f}<br>
                            Confidence: {confidence:.0%}<br>
                            Unwinding: {trap.get('unwinding_rate', 0):+.1f}%
                        </div>
                        """, unsafe_allow_html=True)
        
        # Price chart
        st.markdown("---")
        st.markdown("#### üìâ Price & Indicators")
        
        if len(st.session_state.price_series) > 1:
            # Create subplot for price and indicators
            fig = make_subplots(
                rows=3, cols=1,
                shared_xaxes=True,
                vertical_spacing=0.05,
                subplot_titles=("Nifty 50 Price", "OI Velocity", "Gamma Exposure"),
                row_heights=[0.5, 0.25, 0.25]
            )
            
            # Price
            price_data = st.session_state.price_series.reset_index(drop=True)
            fig.add_trace(
                go.Scatter(
                    y=price_data.values,
                    mode='lines',
                    name='Price',
                    line=dict(color='#3B82F6', width=2)
                ),
                row=1, col=1
            )
            
            # OI Velocity (if available in analytics history)
            if len(st.session_state.research_analytics) > 1:
                oi_velocities = [a.get("oi_velocity", 0) for a in st.session_state.research_analytics[-50:]]
                fig.add_trace(
                    go.Scatter(
                        y=oi_velocities,
                        mode='lines',
                        name='OI Velocity',
                        line=dict(color='#10B981', width=1)
                    ),
                    row=2, col=1
                )
                
                # Add zero line
                fig.add_hline(y=0, line_dash="dash", line_color="gray", row=2, col=1)
            
            # Gamma (if available)
            if len(st.session_state.research_analytics) > 1:
                gamma_values = [a.get("gamma_exposure", {}).get("net_gamma", 0) for a in st.session_state.research_analytics[-50:]]
                fig.add_trace(
                    go.Scatter(
                        y=gamma_values,
                        mode='lines',
                        name='Net Gamma',
                        line=dict(color='#8B5CF6', width=1)
                    ),
                    row=3, col=1
                )
                
                # Add zero line
                fig.add_hline(y=0, line_dash="dash", line_color="gray", row=3, col=1)
            
            fig.update_layout(height=600, showlegend=True)
            st.plotly_chart(fig, width='stretch')

# ==============================
# TAB 2: SIGNALS
# ==============================

with tab2:
    st.markdown('<div class="sub-header">üéØ Trading Signals</div>', unsafe_allow_html=True)
    
    # Fetch active signals
    active_signals = fetch_active_signals(limit=10)
    
    if not active_signals.empty:
        # Display signals in columns
        for _, signal in active_signals.iterrows():
            col1, col2, col3, col4 = st.columns([2, 1, 1, 1])
            
            with col1:
                signal_type = signal["signal_type"]
                color = "green" if signal_type == "BUY" else "red" if signal_type == "SELL" else "gray"
                st.markdown(f"### <span style='color:{color};'>{signal_type}</span>", unsafe_allow_html=True)
                
                # Rationale
                if pd.notna(signal["rationale"]):
                    st.caption(signal["rationale"])
            
            with col2:
                confidence = signal["confidence"]
                st.metric("Confidence", f"{confidence:.0%}")
            
            with col3:
                strength = signal.get("signal_strength", "UNKNOWN")
                st.metric("Strength", strength)
            
            with col4:
                status = signal["status"]
                badge_color = "blue" if status == "NEW" else "green" if status == "VALIDATED" else "orange"
                st.markdown(f"<div style='padding: 5px 10px; background-color:{badge_color}; color:white; border-radius:5px; text-align:center;'>{status}</div>", 
                           unsafe_allow_html=True)
            
            # Research context (expandable)
            with st.expander("Research Details"):
                if pd.notna(signal.get("research_context")):
                    try:
                        context = json.loads(signal["research_context"]) if isinstance(signal["research_context"], str) else signal["research_context"]
                        st.json(context)
                    except:
                        st.write("No research context available")
            
            st.markdown("---")
        
        # Performance metrics
        st.markdown("#### üìä Signal Performance")
        performance = fetch_signal_performance(days=7)
        
        if performance:
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Total Signals", performance.get("total_signals", 0))
            
            with col2:
                st.metric("Profitable", performance.get("profitable_signals", 0))
            
            with col3:
                win_rate = performance.get("win_rate", 0)
                st.metric("Win Rate", f"{win_rate:.1f}%")
            
            with col4:
                period = performance.get("period_days", 7)
                st.metric("Period", f"{period} days")
        
        # Signal distribution chart
        if not active_signals.empty:
            st.markdown("#### üìà Signal Distribution")
            
            fig = go.Figure(data=[
                go.Pie(
                    labels=active_signals["signal_type"].value_counts().index,
                    values=active_signals["signal_type"].value_counts().values,
                    hole=.3
                )
            ])
            
            fig.update_layout(
                height=300,
                showlegend=True,
                margin=dict(t=0, b=0, l=0, r=0)
            )
            
            st.plotly_chart(fig, width='stretch')
    
    else:
        st.info("üì≠ No active signals. The system will generate signals during market hours.")

# ==============================
# TAB 3: RESEARCH
# ==============================

with tab3:
    st.markdown('<div class="sub-header">üîç Research & Analytics</div>', unsafe_allow_html=True)
    
    # Research metrics
    if st.session_state.research_analytics:
        latest = st.session_state.research_analytics[-1]
        
        # Key research metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            net_gamma = latest.get("gamma_exposure", {}).get("net_gamma", 0)
            st.metric("Net Gamma", f"{net_gamma:,.0f}")
        
        with col2:
            pcr = latest.get("put_call_ratio", 1.0)
            st.metric("Put/Call Ratio", f"{pcr:.2f}")
        
        with col3:
            divergence = latest.get("divergence_score", 0)
            st.metric("Divergence Score", f"{divergence:.2f}")
        
        with col4:
            regime = latest.get("market_regime", "UNKNOWN")
            st.metric("Market Regime", regime)
        
        # Display market insights
        st.markdown("#### üí° Market Insights")
        
        insights = latest.get("market_insights", [])
        if insights:
            for insight in insights[:5]:
                if "trap" in insight.lower():
                    st.markdown(f'<div class="trap-alert">{insight}</div>', unsafe_allow_html=True)
                elif "divergence" in insight.lower():
                    st.markdown(f'<div class="divergence-alert">{insight}</div>', unsafe_allow_html=True)
                else:
                    st.markdown(f'<div class="research-insight">{insight}</div>', unsafe_allow_html=True)
        else:
            st.info("No insights available yet")
        
        # Structural walls
        st.markdown("#### üß± Structural Walls")
        
        walls = latest.get("structural_walls", [])
        if walls:
            walls_df = pd.DataFrame(walls)
            st.dataframe(walls_df, width='stretch')
        
        # Trap detection history
        st.markdown("#### üéØ Trap Detection History")
        
        if st.session_state.trap_detections:
            traps_df = pd.DataFrame([
                {
                    "Time": pd.to_datetime(t["timestamp"]).strftime("%H:%M:%S"),
                    "Type": t["trap"].get("direction", ""),
                    "Strike": t["trap"].get("strike", 0),
                    "Confidence": t["trap"].get("confidence", 0),
                    "Regime": t.get("market_regime", "")
                }
                for t in st.session_state.trap_detections
            ])
            
            if not traps_df.empty:
                st.dataframe(traps_df, width='stretch')
        
        # Research analytics over time
        st.markdown("#### üìä Analytics Timeline")
        
        if len(st.session_state.research_analytics) > 5:
            # Convert to DataFrame for plotting
            analytics_df = pd.DataFrame(st.session_state.research_analytics[-50:])
            
            if "timestamp" in analytics_df.columns:
                analytics_df["time"] = pd.to_datetime(analytics_df["timestamp"]).dt.strftime("%H:%M")
                
                # Plot OI Velocity and Gamma over time
                fig = make_subplots(
                    rows=2, cols=1,
                    shared_xaxes=True,
                    subplot_titles=("OI Velocity", "Net Gamma"),
                    vertical_spacing=0.1
                )
                
                if "oi_velocity" in analytics_df.columns:
                    fig.add_trace(
                        go.Scatter(
                            x=analytics_df["time"],
                            y=analytics_df["oi_velocity"],
                            mode='lines+markers',
                            name='OI Velocity',
                            line=dict(color='#10B981')
                        ),
                        row=1, col=1
                    )
                
                if "gamma_exposure" in analytics_df.columns:
                    # Extract net_gamma from gamma_exposure dict
                    try:
                        gamma_values = []
                        for g in analytics_df["gamma_exposure"]:
                            if isinstance(g, dict) and "net_gamma" in g:
                                gamma_values.append(g["net_gamma"])
                            else:
                                gamma_values.append(0)
                        
                        fig.add_trace(
                            go.Scatter(
                                x=analytics_df["time"],
                                y=gamma_values,
                                mode='lines+markers',
                                name='Net Gamma',
                                line=dict(color='#8B5CF6')
                            ),
                            row=2, col=1
                        )
                    except:
                        pass
                
                fig.update_layout(height=400, showlegend=True)
                st.plotly_chart(fig, width='stretch')

# ==============================
# TAB 4: CONFIGURATION
# ==============================

with tab4:
    st.markdown('<div class="sub-header">‚öôÔ∏è System Configuration</div>', unsafe_allow_html=True)
    
    # Available indices and underlyings
    available_indices = [
        "NSE_INDEX|Nifty 50",
        "NSE_INDEX|Nifty Bank", 
        "NSE_INDEX|Nifty Fin Service",
        "NSE_INDEX|Nifty Midcap 100",
        "NSE_INDEX|Nifty Next 50",
        "BSE_INDEX|S&P BSE SENSEX"  
    ]
    
    available_underlyings = [
        "NIFTY",
        "BANKNIFTY", 
        "FINNIFTY",
        "MIDCPNIFTY",
        "SENSEX"  
    ]
    
    # Get available expiries for the selected underlying
    current_underlying = st.session_state.config["underlying"]
    from data.instrument_master import get_available_expiries
    
    try:
        available_expiries = get_available_expiries(current_underlying)
        if not available_expiries:
            available_expiries = ["2026-01-27", "2026-02-03", "2026-02-10"]
    except:
        available_expiries = ["2026-01-27", "2026-02-03", "2026-02-10"]
    
    # Configuration form
    with st.form("configuration_form"):
        col1, col2 = st.columns(2)
        
        with col1:
            # Index Symbol Dropdown
            index_symbol = st.selectbox(
                "Index Symbol", 
                options=available_indices,
                index=available_indices.index(st.session_state.config["index_symbol"]) 
                if st.session_state.config["index_symbol"] in available_indices else 0
            )
            
            # Underlying Symbol Dropdown
            underlying = st.selectbox(
                "Underlying Symbol", 
                options=available_underlyings,
                index=available_underlyings.index(st.session_state.config["underlying"])
                if st.session_state.config["underlying"] in available_underlyings else 0
            )
            
            # Expiry Date Dropdown
            # Find today's date in available_expiries
            today_str = datetime.now().strftime("%Y-%m-%d")
            default_index = 0

            if today_str in available_expiries:
                default_index = available_expiries.index(today_str)
            elif today_str == "2026-01-28":  # Today's date
                # Add today to available expiries if it's not there
                available_expiries.insert(0, today_str)
                default_index = 0

            expiry_date = st.selectbox(
                "Expiry Date",
                options=available_expiries,
                index=default_index  # ‚úÖ Now selects today if available
            )
            
            # Convert selected expiry to datetime
            if expiry_date:
                try:
                    expiry_datetime = datetime.strptime(expiry_date, "%Y-%m-%d")
                    days_to_expiry = (expiry_datetime.date() - datetime.now().date()).days
                    st.info(f"Days to expiry: {days_to_expiry}")
                except:
                    st.warning("Invalid expiry date format")
        
        with col2:
            max_option_keys = st.number_input(
                "Max Option Keys", 
                min_value=50, 
                max_value=500, 
                value=st.session_state.config["max_option_keys"],
                help="Maximum number of option strikes to fetch"
            )
            
            scheduler_interval = st.slider(
                "Scheduler Interval (seconds)", 
                min_value=10, 
                max_value=300, 
                value=30,
                step=5,
                help="How often to refresh data (10-300 seconds)"
            )
            
            confidence_threshold = st.slider(
                "Signal Confidence Threshold", 
                min_value=0.0, 
                max_value=1.0, 
                value=0.2, 
                step=0.05,
                format="%.2f",
                help="Minimum confidence level for signals (0.0-1.0)"
            )
            
            # Advanced options expander
            with st.expander("Advanced Options"):
                enable_live_trading = st.checkbox(
                    "Enable Live Trading", 
                    value=False,
                    help="WARNING: Enable actual order placement"
                )
                
                risk_per_trade = st.number_input(
                    "Risk per Trade (%)",
                    min_value=0.1,
                    max_value=5.0,
                    value=1.0,
                    step=0.1,
                    format="%.1f",
                    help="Maximum risk per trade as percentage of capital"
                )
        
        # Submit button
        submit_col1, submit_col2, submit_col3 = st.columns([1, 2, 1])
        with submit_col2:
            if st.form_submit_button("üíæ Save Configuration", width='stretch'):
                # Update configuration
                st.session_state.config.update({
                    "index_symbol": index_symbol,
                    "underlying": underlying,
                    "expiry_date": datetime.strptime(expiry_date, "%Y-%m-%d") if expiry_date else datetime.now(timezone.utc) + timedelta(days=3),
                    "max_option_keys": max_option_keys,
                    "enable_live_trading": enable_live_trading,
                    "risk_per_trade": risk_per_trade
                })
                
                # Update scheduler
                st.session_state.scheduler = MarketScheduler(interval_seconds=scheduler_interval)
                
                # Update signal engine
                st.session_state.signal_engine = create_signal_engine(
                    signal_expiry_minutes=5,
                    confidence_threshold=confidence_threshold
                )
                
                # Clear option keys cache
                st.session_state.option_keys = []
                
                st.success("‚úÖ Configuration saved!")
                st.rerun()
    
    st.markdown("---")
    
    # Quick Configuration Presets
    st.markdown("#### üöÄ Quick Presets")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üìä NIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty 50",
                "underlying": "NIFTY"
            })
            st.success("Set to NIFTY Weekly")
            st.rerun()
    
    with col2:
        if st.button("üè¶ BANKNIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty Bank",
                "underlying": "BANKNIFTY"
            })
            st.success("Set to BANKNIFTY Weekly")
            st.rerun()
    
    with col3:
        if st.button("üí≥ FINNIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty Fin Service",
                "underlying": "FINNIFTY"
            })
            st.success("Set to FINNIFTY Weekly")
            st.rerun()
    
    # Current Configuration Display
    st.markdown("---")
    st.markdown("#### üñ•Ô∏è Current Configuration")
    
    config_col1, config_col2 = st.columns(2)
    
    with config_col1:
        st.markdown("**Trading Settings:**")
        st.text(f"Index: {st.session_state.config['index_symbol']}")
        st.text(f"Underlying: {st.session_state.config['underlying']}")
        st.text(f"Expiry: {st.session_state.config['expiry_date'].strftime('%Y-%m-%d')}")
        st.text(f"Max Options: {st.session_state.config['max_option_keys']}")
    
    with config_col2:
        st.markdown("**System Settings:**")
        st.text(f"Scheduler: {st.session_state.scheduler.interval_seconds}s")
        st.text(f"Signal Confidence: {confidence_threshold:.2f}")
        st.text(f"Live Trading: {'Enabled' if st.session_state.config.get('enable_live_trading', False) else 'Disabled'}")
        if st.session_state.config.get('enable_live_trading', False):
            st.text(f"Risk per Trade: {st.session_state.config.get('risk_per_trade', 1.0)}%")
    
    # Diagnostic tools (keep your existing code here)
    st.markdown("---")
    st.markdown("#### üîß Diagnostic Tools")
    # ... keep your existing diagnostic tools code ...
    
    # Diagnostic tools
    st.markdown("---")
    st.markdown("#### üîß Diagnostic Tools")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üß™ Test Upstox Connection", width='stretch'):
            # Test connection by trying to fetch profile
            try:
                profile = client.fetch_profile()
                if profile:
                    st.success(f"‚úÖ Upstox connection working")
                else:
                    st.error("‚ùå Failed to fetch profile")
            except Exception as e:
                st.error(f"‚ùå Connection test failed: {e}")
    
    with col2:
        if st.button("üìä Test Feature Pipeline", width='stretch'):
            try:
                # Get latest features
                features = fetch_latest_features(FEATURE_VERSION)
                if features is not None and not features.empty():
                    st.success(f"‚úÖ Feature pipeline working ({len(features)} features)")
                else:
                    st.warning("‚ö†Ô∏è No features found")
            except Exception as e:
                st.error(f"‚ùå Feature pipeline error: {e}")
    
    with col3:
        if st.button("üîÑ Clear Option Keys Cache", width='stretch'):
            st.session_state.option_keys = []
            st.success("‚úÖ Option keys cache cleared")

# ==============================
# PERFORMANCE MODAL
# ==============================

if st.session_state.get("show_performance", False):
    with st.expander("üìà Performance Analytics", expanded=True):
        st.markdown("### üìà Trading Performance")
        
        performance = fetch_signal_performance(days=30)
        
        if performance:
            # Overall metrics
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Total Trades", performance["total_signals"])
            
            with col2:
                st.metric("Profitable Trades", performance["profitable_signals"])
            
            with col3:
                win_rate = performance["win_rate"]
                color = "normal" if win_rate > 50 else "inverse"
                st.metric("Win Rate", f"{win_rate:.1f}%", delta_color=color)
            
            with col4:
                st.metric("Analysis Period", f"{performance['period_days']} days")
            
            # Breakdown by signal type
            st.markdown("#### Breakdown by Signal Type")
            
            if "breakdown" in performance:
                breakdown_df = pd.DataFrame(performance["breakdown"])
                st.dataframe(breakdown_df, width='stretch')
        
        st.session_state.show_performance = False

# ==============================
# LOGS MODAL
# ==============================

if st.session_state.get("show_logs", False):
    with st.expander("üìã System Logs", expanded=True):
        st.markdown("### üìã Recent System Activity")
        
        # Could fetch from system_health table here
        st.info("Logs would be displayed here from the database.")
        
        st.session_state.show_logs = False

# ==============================
# FOOTER
# ==============================

st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #6B7280; font-size: 0.9rem;'>
    Algorithmic Trading Research Platform v2.0 | 
    Research Features: OI Velocity, Gamma Exposure, Walls/Traps, Divergence Analysis
</div>
""", unsafe_allow_html=True)

# Simple auto-refresh timer
import time

if "last_auto_run" not in st.session_state:
    st.session_state.last_auto_run = time.time()

current_time = time.time()
time_since_last = current_time - st.session_state.last_auto_run

if time_since_last > 30:  # 30 seconds
    print(f"üîÑ Simple timer triggered after {int(time_since_last)}s")
    
    # Execute research cycle
    research_data = execute_research_cycle()
    
    if research_data:
        st.session_state.last_auto_run = current_time
        print(f"‚úÖ Auto-refresh executed successfully")
        
        # Update scheduler's last run time
        st.session_state.scheduler._last_run = datetime.now()
        st.session_state.scheduler._total_executions += 1
        
        # Update auto-refresh manager
        st.session_state.auto_refresh_manager.last_execution_time = datetime.now()
        st.session_state.auto_refresh_manager.execution_count += 1
        
        # Force UI update
        st.rerun()
    else:
        print("‚ùå Auto-refresh failed")
else:
    print(f"‚è±Ô∏è Next auto-refresh in {int(30 - time_since_last)}s")


--------------------------------------------------
FILE PATH : G:\trading_app\app.py
SIZE      : 69349 bytes
--------------------------------------------------

"""
ENHANCED TRADING TOOL - Research-Based Live Feature Engine
Incorporates OI Velocity, Gamma Exposure, Walls/Traps, and Divergence analysis.
"""
# Add this near the top of your imports
import streamlit as st
print(f"Streamlit version: {st.__version__}")

# Check if fragments are available
if hasattr(st, 'fragment'):
    print("‚úÖ Fragments are available in this Streamlit version")
else:
    print("‚ùå Fragments are NOT available. You need Streamlit >= 1.37.0")
    
    
import os
import sys
import time
from datetime import datetime


# Fix Conda environment issue
if 'conda' in sys.version.lower():
    try:
        # Try to fix conda path issues
        os.environ['PATH'] = os.path.join(os.environ.get('CONDA_PREFIX', ''), 'bin') + ':' + os.environ['PATH']
    except:
        pass
# ==============================
# STREAMLIT CONFIG FIX - ADD THIS
# ==============================
import os
os.environ['STREAMLIT_SERVER_FILE_WATCHER_TYPE'] = 'none'
import time 
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from datetime import datetime, timezone
import json
from pathlib import Path
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Enhanced core components
from core.session import UpstoxSession, initialize_session, display_session_status
from data.upstox_client import UpstoxClient, MarketAnalytics, display_market_analytics
from data.instrument_master import get_option_keys

try:
    from core.feature_pipeline import build_and_store_features, create_feature_summary
except ImportError as e:
    st.error(f"Feature pipeline import error: {e}")
    # Define dummy functions to prevent crash
    def build_and_store_features(*args, **kwargs):
        return {"error": "Feature pipeline not loaded"}
    def create_feature_summary(*args, **kwargs):
        return {"error": "Feature pipeline not loaded"}
from core.scheduler import MarketScheduler
from core.signals.state_machine import SignalStateMachine, SignalValidator, create_signal_engine
from features.breadth import build_constituents_df
from ml.feature_contract import FEATURE_VERSION

# Enhanced storage
from storage.repository import (
    fetch_latest_features,
    insert_signal,
    signal_exists,
    validate_new_signals,
    expire_old_signals,
    fetch_active_signals,
    fetch_recent_analytics,
    get_database_stats,
    fetch_signal_performance,
    insert_research_analytics
)

# ==============================
# PAGE CONFIGURATION
# ==============================

st.set_page_config(
    page_title="Algorithmic Trading Research Platform",
    page_icon="üìà",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E3A8A;
        font-weight: 700;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #374151;
        font-weight: 600;
        margin-top: 1.5rem;
        margin-bottom: 1rem;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
    }
    .research-insight {
        background: #F0F9FF;
        border-left: 4px solid #3B82F6;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
    .trap-alert {
        background: #FEF3C7;
        border-left: 4px solid #F59E0B;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
    .divergence-alert {
        background: #FEE2E2;
        border-left: 4px solid #EF4444;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
</style>
""", unsafe_allow_html=True)

# ==============================
# APP HEADER
# ==============================

st.markdown('<div class="main-header">üìä Algorithmic Trading Research Platform</div>', unsafe_allow_html=True)
st.markdown("""
<div style='color: #6B7280; margin-bottom: 2rem;'>
    Advanced derivatives analytics with OI Velocity, Gamma Exposure, Structural Walls/Traps, and Spot Divergence detection.
    Version: Research v2.0
</div>
""", unsafe_allow_html=True)


# ==============================
# INDEX / EXPIRY HELPERS
# ==============================

def get_index_display_name(index_symbol: str) -> str:
    return {
        "NSE_INDEX|Nifty 50": "NIFTY 50",
        "NSE_INDEX|Nifty Bank": "BANKNIFTY",
        "NSE_INDEX|Nifty Fin Service": "FINNIFTY",
        "NSE_INDEX|Nifty Midcap 100": "MIDCAP NIFTY",
        "NSE_INDEX|Nifty Next 50": "NIFTY NEXT 50",
        "BSE_INDEX|S&P BSE SENSEX": "SENSEX",
    }.get(index_symbol, index_symbol)


def resolve_expiry_for_underlying(underlying: str) -> datetime:
    """
    Central expiry resolver.
    Uses your existing expiry logic.
    """
    from utils.expiry_utils import get_trading_expiry

    expiry_str = get_trading_expiry(underlying)
    if not expiry_str:
        raise ValueError(f"No valid expiry for {underlying}")

    return datetime.strptime(expiry_str, "%Y-%m-%d")

# ==============================
# INITIALIZATION
# ==============================

def initialize_app_state():
    """Initialize application state with research components."""
    
    # Initialize session
    initialize_session()
    
    # Initialize state variables
    if "initialized" not in st.session_state:
        st.session_state.initialized = True
        
        # Market data series
        for key in ["price_series", "volume_series", "ccc_history", "oi_velocity_history", "gamma_history"]:
            if key not in st.session_state:
                st.session_state[key] = pd.Series(dtype=float)
        
        # Research analytics
        if "research_analytics" not in st.session_state:
            st.session_state.research_analytics = []
        
        # Signal engine
        if "signal_engine" not in st.session_state:
            st.session_state.signal_engine = create_signal_engine(
                signal_expiry_minutes=5,
                confidence_threshold=0.2
            )
        
        # Scheduler
        if "scheduler" not in st.session_state:
            st.session_state.scheduler = MarketScheduler(interval_seconds=30)
            # Initialize scheduler properties
            st.session_state.scheduler._last_run = None
            st.session_state.scheduler._total_executions = 0
        
        # Load Nifty weights (STRICT Upstox normalization)
        if "nifty_weights" not in st.session_state:
            try:
                from pathlib import Path

                weights_path = Path("G:/trading_app/config/nifty_weights.json")

                with open(weights_path, "r") as f:
                    raw_weights = json.load(f)

                # üîë STRICT normalization for Upstox equity symbols
                def normalize_equity_symbol(sym: str) -> str:
                    return (
                        sym.upper()
                        .replace("-", "_")   # BAJAJ-AUTO ‚Üí BAJAJ_AUTO
                        .replace(" ", "")    # SBI LIFE ‚Üí SBILIFE
                    )

                st.session_state.nifty_weights = {
                    f"NSE_EQ|{normalize_equity_symbol(symbol)}": weight
                    for symbol, weight in raw_weights.items()
                }

                # üîí HARD VALIDATION (fail fast)
                assert st.session_state.nifty_weights, "Nifty weights empty after load"
                assert all(
                    sym.startswith("NSE_EQ|") and "-" not in sym and " " not in sym
                    for sym in st.session_state.nifty_weights
                ), "Invalid equity symbols detected after normalization"

                print(f"‚úÖ Loaded {len(st.session_state.nifty_weights)} Nifty weights (normalized)")

            except Exception as e:
                print(f"‚ùå Failed to load Nifty weights: {e}")
                st.session_state.nifty_weights = {}


            # üîí HARD VALIDATION (RUNS ALWAYS)
            assert st.session_state.nifty_weights, "Nifty weights missing or empty"

            assert all(
                sym.count("|") == 1 and sym.startswith("NSE_EQ|")
                for sym in st.session_state.nifty_weights
            ), f"Invalid Upstox symbols detected: {list(st.session_state.nifty_weights)[:5]}"

        # Instrument master path
        if "instrument_master" not in st.session_state:
            st.session_state.instrument_master = Path("G:/trading_app/data/instruments.json.gz")
        
        # Configuration - SMART EXPIRY SELECTION
        if "config" not in st.session_state:
            from utils.expiry_utils import get_trading_expiry
            
            # Get smart expiry
            underlying = "NIFTY"
            expiry_date_str = get_trading_expiry(underlying)
            
            if not expiry_date_str:
                # Fallback to next available date from instrument master
                from data.instrument_master import get_next_available_expiry
                expiry_date_str = get_next_available_expiry(underlying)
                
                if not expiry_date_str:
                    # Final fallback to a known date
                    expiry_date_str = "2026-02-03"
            
            # Convert string to datetime object
            expiry_date = datetime.strptime(expiry_date_str, "%Y-%m-%d")
            
            st.session_state.config = {
                "index_symbol": "NSE_INDEX|Nifty 50",
                "underlying": underlying,
                "expiry_date": expiry_date,
                "max_option_keys": 200
            }
            
            # Show expiry info
            from utils.expiry_utils import is_market_open
            market_status = "open" if is_market_open() else "closed"
            
            # Get today's date
            today_str = datetime.now().strftime('%Y-%m-%d')
            
            # Show appropriate message
            if expiry_date_str == today_str:
                st.info(f"üìÖ Using TODAY's expiry: {expiry_date_str} (Market: {market_status})")
            else:
                st.info(f"üìÖ Using NEXT expiry: {expiry_date_str} (Today: {today_str}, Market: {market_status})")
        
        # Option keys cache
        if "option_keys" not in st.session_state:
            st.session_state.option_keys = []
        
        # Market regime tracking
        if "market_regime" not in st.session_state:
            st.session_state.market_regime = "UNKNOWN"
        
        # Trap detection history
        if "trap_detections" not in st.session_state:
            st.session_state.trap_detections = []
        
        st.success("‚úÖ Application state initialized with research components")

# Run initialization
initialize_app_state()

# ==============================
# AUTHENTICATION
# ==============================

st.markdown('<div class="sub-header">üîê Authentication</div>', unsafe_allow_html=True)

# Authenticate - this will show login button if not authenticated
access_token = UpstoxSession.authenticate()

# CRITICAL: Check if authenticate() actually returned a token
# If it returns None, we need to stop execution here
if not access_token:
    # authenticate() should have shown login button and stopped with st.stop()
    # But if we reach here, something went wrong
    st.error("‚ùå Authentication required. Please log in to continue.")
    
    # Show login button manually as fallback
    login_url = UpstoxSession.get_login_url()
    st.markdown(f"""
    <div style="text-align: center; margin: 20px 0;">
        <a href="{login_url}" target="_blank">
            <button style="
                background-color: #00d09c;
                color: white;
                padding: 15px 30px;
                font-size: 18px;
                font-weight: bold;
                border: none;
                border-radius: 10px;
                cursor: pointer;
                width: 60%;
                margin: 20px 0;
                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            ">
            üìà Click to Login with Upstox
            </button>
        </a>
    </div>
    """, unsafe_allow_html=True)
    
    # STOP the app here
    st.stop()

# If we reach here, we have a token
# Initialize client
try:
    client = UpstoxClient(access_token)
    st.success("‚úÖ Upstox client initialized")
        
except Exception as e:
    st.error(f"‚ùå Error initializing Upstox client: {e}")
    st.stop()


# ==============================
# AUTO-REFRESH DEBUG OVERRIDE
# ==============================

if "force_auto_refresh" not in st.session_state:
    st.session_state.force_auto_refresh = False


# ==============================
# STREAMLIT-NATIVE AUTO REFRESH ENGINE (FRAGMENT SAFE)
# ==============================

# ---- CONFIGURATION (change here anytime) ----
DEFAULT_AUTO_REFRESH_SECONDS = 5  # ‚¨ÖÔ∏è set to 5 seconds now, increase later if needed

# Persist refresh interval in session_state (future-proof)
if "auto_refresh_seconds" not in st.session_state:
    st.session_state.auto_refresh_seconds = DEFAULT_AUTO_REFRESH_SECONDS

# Track last successful execution (duplicate guard)
if "last_cycle_ts" not in st.session_state:
    st.session_state.last_cycle_ts = None

# üîÑ UI refresh heartbeat (forces Streamlit rerun)
if "refresh_heartbeat" not in st.session_state:
    st.session_state.refresh_heartbeat = 0


@st.fragment(run_every=st.session_state.auto_refresh_seconds)
def auto_refresh_engine():
    """
    Streamlit-safe auto refresh engine.
    Executes only during market hours.
    """

    now_utc = datetime.now(timezone.utc)

    # ------------------------------
    # DUPLICATE EXECUTION GUARD
    # ------------------------------
    last_ts = st.session_state.last_cycle_ts
    if last_ts is not None:
        elapsed = (now_utc - last_ts).total_seconds()
        if elapsed < st.session_state.auto_refresh_seconds - 1:
            return

    scheduler: MarketScheduler = st.session_state.scheduler

    # ------------------------------
    # MARKET HOURS / DEBUG GATE
    # ------------------------------
    market_open = scheduler._is_market_open(datetime.now())

    # Allow auto-refresh outside market hours ONLY if debug override is enabled
    if not market_open and not st.session_state.force_auto_refresh:
        return


    # ------------------------------
    # EXECUTION
    # ------------------------------
    with st.spinner("üîÑ Auto-refreshing market data..."):
        result = execute_research_cycle(
            config=st.session_state.config,
            nifty_weights=st.session_state.nifty_weights,
            option_keys=st.session_state.option_keys,
        )

        if result:
            now_local = datetime.now()

            st.session_state.last_cycle_ts = now_utc
            scheduler._last_run = now_local
            scheduler._total_executions += 1
            st.session_state.last_data_update = now_local

            # üî• FORCE UI REFRESH (CRITICAL)
            st.session_state.refresh_heartbeat += 1


# ==============================
# SIDEBAR - SESSION STATUS
# ==============================

with st.sidebar:
    st.markdown("## üß≠ Navigation")
    
    # Display session status
    display_session_status()
    
    # Database stats
    st.markdown("---")
    st.markdown("### üìä Database")
    
    db_stats = get_database_stats()
    if db_stats:
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Features", f"{db_stats.get('market_features_count', 0):,}")
        with col2:
            st.metric("Signals", f"{db_stats.get('signals_count', 0):,}")
    
    # Market hours info
    st.markdown("---")
    st.markdown("### üïí Market Hours")

    scheduler: MarketScheduler = st.session_state.scheduler
    now = datetime.now()

    if scheduler._is_market_open(now):
        st.success("‚úÖ Market Open")
        next_run = scheduler._last_run + timedelta(seconds=st.session_state.auto_refresh_seconds) if scheduler._last_run else now
        time_to_next = (next_run - now).total_seconds()
        if time_to_next > 0:
            st.info(f"Next cycle in: {int(time_to_next)}s")
    else:
        st.warning("‚è∏Ô∏è Market Closed")

def retry_on_failure(max_retries=3, delay=5):
    """Decorator for retrying failed executions"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    print(f"‚ö†Ô∏è Attempt {attempt + 1}/{max_retries} failed: {e}")
                    if attempt < max_retries - 1:
                        print(f"‚è∏Ô∏è Retrying in {delay}s...")
                        time.sleep(delay)
                    else:
                        print(f"‚ùå All {max_retries} attempts failed")
                        raise
            return None
        return wrapper
    return decorator

def log_execution_time(func):
    """Decorator for logging execution time"""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        print(f"‚è±Ô∏è Starting {func.__name__}...")
        
        result = func(*args, **kwargs)
        
        execution_time = time.time() - start_time
        print(f"‚è±Ô∏è {func.__name__} completed in {execution_time:.2f}s")
        
        return result
    return wrapper
# ==============================
# RESEARCH EXECUTION CYCLE
# ==============================

@log_execution_time
@retry_on_failure(max_retries=2, delay=3)
def execute_research_cycle(config=None, nifty_weights=None, option_keys=None):
    """
    Enhanced execution cycle with research analytics.
    Returns comprehensive research data.
    """
    # Use provided parameters or fall back to session state
    if config is None:
        try:
            config = st.session_state.config
        except (KeyError, AttributeError):
            print("‚ö†Ô∏è Config not found in session state")
            return None
    
    if nifty_weights is None:
        nifty_weights = st.session_state.get("nifty_weights", {})
    
    if option_keys is None:
        option_keys = st.session_state.get("option_keys", [])
    
    research_data = {}
    
    try:
        # ------------------------------
        # 1. INDEX QUOTE
        # ------------------------------
        print(f"üìä Fetching index quote for {config.get('index_symbol', 'NSE_INDEX|Nifty 50')}")
        index_data = client.fetch_index_quote(config["index_symbol"])
        
        if not index_data or index_data.get("ltp") is None:
            print("‚ùå No index data received")
            return None
        
        ltp = float(index_data["ltp"])
        open_price = float(index_data["open"])
        volume = 0.0  # index has no volume
        
        research_data["index"] = {
            "ltp": ltp,
            "open": open_price,
            "change": index_data.get("percent_change", 0),
            "timestamp": index_data.get("timestamp", datetime.now(timezone.utc).isoformat())
        }
        
        print(f"‚úÖ Index data: {ltp} (Change: {index_data.get('percent_change', 0)}%)")
        
        # ------------------------------
        # 2. UPDATE PRICE SERIES
        # ------------------------------
        try:
            # Ensure series exist
            if "price_series" not in st.session_state or not isinstance(st.session_state.price_series, pd.Series):
                st.session_state.price_series = pd.Series(dtype=float)

            if "volume_series" not in st.session_state or not isinstance(st.session_state.volume_series, pd.Series):
                st.session_state.volume_series = pd.Series(dtype=float)

            now = datetime.now()

            # Assign directly (NO concat, NO FutureWarning)
            st.session_state.price_series.loc[now] = float(ltp)
            st.session_state.volume_series.loc[now] = float(volume)

            # Hard cap history
            st.session_state.price_series = st.session_state.price_series.tail(200)
            st.session_state.volume_series = st.session_state.volume_series.tail(200)

            print(f"üìà Price series updated: {len(st.session_state.price_series)} data points")

        except Exception as e:
            print(f"‚ö†Ô∏è Error updating price series: {e}")
            now = datetime.now()
            st.session_state.price_series = pd.Series([float(ltp)], index=[now])
            st.session_state.volume_series = pd.Series([float(volume)], index=[now])

        
        # ------------------------------
        # 3. BREADTH ANALYSIS (CCC)
        # ------------------------------
        print("üìä Starting breadth analysis...")
        try:
            if not nifty_weights or not any(    sym.startswith("NSE_EQ|") for sym in nifty_weights):
                print("‚ö†Ô∏è No Nifty weights found, skipping breadth analysis")
                ccc_value = 0.0
                constituents_df = pd.DataFrame()
            else:
                # Convert NSE_EQ|SYMBOL ‚Üí SYMBOL for quote fetch
                breadth_symbols = [
                                        sym for sym in nifty_weights.keys()
                                        if sym.startswith("NSE_EQ|") and sym.count("|") == 1
                                    ]


                equity_quotes_df = client.fetch_equity_quotes(
                    symbols=breadth_symbols
                )

                if equity_quotes_df.empty:
                    raise RuntimeError("Equity quote fetch returned empty dataframe")
            
                constituents_df = build_constituents_df(
                    equity_quotes_df=equity_quotes_df,
                    weights=nifty_weights
                )
                
                if not constituents_df.empty:
                    ccc_value = (
                        constituents_df["weight"] * constituents_df["price_change"]
                    ).sum()
                    print(f"‚úÖ CCC Value: {ccc_value:.4f}")
                else:
                    ccc_value = 0.0
                    print("‚ö†Ô∏è Empty constituents dataframe")
        except Exception as e:
            print(f"‚ö†Ô∏è Breadth analysis error: {e}")
            ccc_value = 0.0
            constituents_df = pd.DataFrame()
        
        # Update CCC history
        try:
            if "ccc_history" not in st.session_state:
                st.session_state.ccc_history = pd.Series(dtype=float)
            
            st.session_state.ccc_history = pd.concat([
                st.session_state.ccc_history, 
                pd.Series([ccc_value], index=[datetime.now()])
            ]).tail(200)
        except:
            st.session_state.ccc_history = pd.Series([ccc_value], index=[datetime.now()])
        
        research_data["breadth"] = {
            "ccc_value": ccc_value,
            "constituents_count": len(constituents_df),
            "positive_constituents": len(constituents_df[constituents_df["price_change"] > 0]) if not constituents_df.empty else 0
        }
        
        # ------------------------------
        # 4. OPTION INSTRUMENT DISCOVERY
        # ------------------------------
        print("üîç Fetching option instruments...")
        try:
            if not option_keys:
                expiry_str = config["expiry_date"].strftime("%Y-%m-%d")
                option_keys = get_option_keys(
                    underlying=config["underlying"],
                    expiry=expiry_str,
                    max_keys=config.get("max_option_keys", 200)
                )
                st.session_state.option_keys = option_keys
                print(f"‚úÖ Found {len(option_keys)} option keys")
        except Exception as e:
            print(f"‚ö†Ô∏è Option discovery error: {e}")
            option_keys = []
            st.session_state.option_keys = []
        
        if not option_keys:
            print("‚ö†Ô∏è No option keys found")
            return research_data  # Return what we have so far
        
        # ------------------------------
        # 5. ENHANCED OPTION CHAIN ANALYSIS
        # ------------------------------
        print("üìä Analyzing option chain...")
        option_analytics = client.fetch_option_chain_with_analytics(
            option_keys,
            ltp
        )
        
        if not option_analytics or "raw_data" not in option_analytics:
            print("‚ùå Failed to fetch option analytics")
            return research_data  # Return what we have so far
        
        option_chain_df = option_analytics["raw_data"]
        analytics = option_analytics.get("analytics", {})
        insights = option_analytics.get("market_insights", [])
        
        research_data["options"] = {
            "analytics": analytics,
            "insights": insights,
            "chain_size": len(option_chain_df),
            "put_call_ratio": analytics.get("put_call_ratio", 1.0) if isinstance(analytics.get("put_call_ratio"), (int, float)) else 1.0
        }
        
        print(f"‚úÖ Option chain analyzed: {len(option_chain_df)} strikes, PCR: {analytics.get('put_call_ratio', 'N/A')}")
        
        # Store analytics for visualization
        if analytics:
            try:
                if "research_analytics" not in st.session_state:
                    st.session_state.research_analytics = []
                
                st.session_state.research_analytics.append({
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    **analytics
                })
                
                # Keep only recent analytics
                if len(st.session_state.research_analytics) > 100:
                    st.session_state.research_analytics = st.session_state.research_analytics[-100:]
                
                # Update market regime
                st.session_state.market_regime = analytics.get("market_regime", "UNKNOWN")
                
                # Store in database
                insert_research_analytics(analytics)
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error storing analytics: {e}")
        
        # ------------------------------
        # 6. FEATURE PIPELINE
        # ------------------------------
        print("‚öôÔ∏è Running feature pipeline...")
        try:
            snapshot_ts = pd.Timestamp.utcnow().floor("ms")
            
            feature_result = build_and_store_features(
                timestamp=snapshot_ts,
                option_chain_df=option_chain_df,
                spot_price=ltp,
                expiry_datetime=config["expiry_date"],
                
                ltp=ltp,
                vwap=open_price,
                price_series=st.session_state.price_series,
                volume_series=st.session_state.volume_series,
                
                constituents_df=constituents_df,
                ccc_history=st.session_state.ccc_history,
                
                # Enhanced parameters
                client=client,
                option_keys=option_keys
            )
            
            if feature_result:
                research_data["features"] = feature_result.get("features", {})
                research_data["feature_summary"] = create_feature_summary(feature_result["features"])
                print("‚úÖ Feature pipeline completed")
            else:
                print("‚ö†Ô∏è Feature pipeline returned empty result")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Feature pipeline error: {e}")
            import traceback
            traceback.print_exc()
        
        # ------------------------------
        # 7. SIGNAL GENERATION
        # ------------------------------
        print("üéØ Checking for signals...")
        try:
            feature_row = fetch_latest_features(FEATURE_VERSION)
            if feature_row is not None and not feature_row.empty:
                # Check if signal already exists
                timestamp_str = feature_row.iloc[0]["timestamp"]
                
                if not signal_exists(timestamp_str):
                    # Generate signal
                    signal_engine: SignalStateMachine = st.session_state.signal_engine
                    signal = signal_engine.build_signal(feature_row.iloc[0])
                    
                    # Validate signal
                    is_valid, reason = SignalValidator.validate_signal(signal, feature_row.iloc[0])
                    
                    if is_valid and signal["signal_type"] != "NEUTRAL":
                        # Add research context
                        signal["research_validation"] = {
                            "is_valid": is_valid,
                            "reason": reason,
                            "timestamp": datetime.now(timezone.utc).isoformat()
                        }
                        
                        # Insert signal
                        insert_signal(signal)
                        
                        research_data["signal"] = {
                            "generated": True,
                            "type": signal["signal_type"],
                            "confidence": signal["confidence"],
                            "strength": signal.get("signal_strength", "UNKNOWN")
                        }
                        print(f"‚úÖ Signal generated: {signal['signal_type']} ({signal['confidence']:.0%})")
                    else:
                        research_data["signal"] = {
                            "generated": False,
                            "reason": reason if not is_valid else "NEUTRAL signal"
                        }
                        print(f"‚ÑπÔ∏è No valid signal: {reason if not is_valid else 'NEUTRAL'}")
            else:
                print("‚ÑπÔ∏è No features available for signal generation")
        except Exception as e:
            print(f"‚ö†Ô∏è Signal generation error: {e}")
        
        # ------------------------------
        # 8. SIGNAL LIFECYCLE MANAGEMENT
        # ------------------------------
        try:
            validate_new_signals(confidence_threshold=0.2)
            expire_old_signals()
        except Exception as e:
            print(f"‚ö†Ô∏è Signal lifecycle error: {e}")
        
        # ------------------------------
        # 9. TRAP DETECTION
        # ------------------------------
        if analytics and "potential_traps" in analytics:
            traps = analytics["potential_traps"]
            if traps:
                try:
                    if "trap_detections" not in st.session_state:
                        st.session_state.trap_detections = []
                    
                    for trap in traps:
                        if trap.get("confidence", 0) > 0.6:
                            st.session_state.trap_detections.append({
                                "timestamp": datetime.now(timezone.utc).isoformat(),
                                "trap": trap,
                                "market_regime": analytics.get("market_regime")
                            })
                    
                    # Keep only recent traps
                    if len(st.session_state.trap_detections) > 20:
                        st.session_state.trap_detections = st.session_state.trap_detections[-20:]
                    
                    research_data["traps"] = traps
                    
                    if traps:
                        print(f"üéØ Detected {len(traps)} potential traps")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Trap detection error: {e}")
        
        # ------------------------------
        # 10. SUCCESSFUL COMPLETION
        # ------------------------------
        print("‚úÖ Research cycle completed successfully")
        return research_data
        
    except Exception as e:
        print(f"‚ùå Critical error in research cycle: {e}")
        import traceback
        traceback.print_exc()
        return None


def get_scheduler_status():
    scheduler = st.session_state.scheduler

    status = {
        "scheduler_active": True,
        "interval_seconds": scheduler.interval_seconds,
        "total_executions": scheduler._total_executions,
        "last_run": scheduler._last_run.strftime("%H:%M:%S") if scheduler._last_run else "Never",
        "is_market_open": scheduler._is_market_open(datetime.now()),
    }

    if scheduler._last_run:
        next_run = scheduler._last_run + timedelta(seconds=st.session_state.auto_refresh_seconds)

        status["time_until_next"] = max(
            0, int((next_run - datetime.now()).total_seconds())
        )

    return status


auto_refresh_engine()

# ==============================
# CREATE TABS
# ==============================

# Create tabs for different views
tab1, tab2, tab3, tab4 = st.tabs([
    "üìà Live Analytics", 
    "üéØ Signals", 
    "üîç Research", 
    "‚öôÔ∏è Configuration"
])

# # ==============================
# # AUTO-REFRESH WORKING SOLUTION
# # ==============================

# # Create a placeholder for auto-refresh
# refresh_placeholder = st.empty()

# # Check if we need to auto-refresh
# if "next_refresh_time" not in st.session_state:
#     st.session_state.next_refresh_time = time.time() + 30  # First refresh in 30 seconds

# current_time = time.time()

# if current_time >= st.session_state.next_refresh_time:
#     # Time to refresh!
#     print(f"üîÑ AUTO-REFRESH EXECUTING at {datetime.now().strftime('%H:%M:%S')}")
    
#     # Execute research cycle
#     with st.spinner("Auto-refreshing data..."):
#         research_data = execute_research_cycle()
    
#     if research_data:
#         print(f"‚úÖ Auto-refresh completed successfully")
        
#         # Update next refresh time
#         st.session_state.next_refresh_time = current_time + 30
        
#         # Update state
#         st.session_state.scheduler._last_run = datetime.now()
#         st.session_state.scheduler._total_executions += 1
#         st.session_state.auto_refresh_manager.last_execution_time = datetime.now()
#         st.session_state.auto_refresh_manager.execution_count += 1
        
#         # Show success message
#         refresh_placeholder.success(f"‚úÖ Auto-refresh completed at {datetime.now().strftime('%H:%M:%S')}")
#         time.sleep(1)
#         refresh_placeholder.empty()
        
#         # Force a rerun to update UI
#         st.rerun()
#     else:
#         print("‚ùå Auto-refresh failed")
#         refresh_placeholder.error("Auto-refresh failed")
#         time.sleep(2)
#         refresh_placeholder.empty()
# else:
#     # Show countdown
#     time_until_next = st.session_state.next_refresh_time - current_time
#     if time_until_next > 0:
#         print(f"‚è±Ô∏è Next auto-refresh in {int(time_until_next)}s")
        
#         # Update countdown every 10 seconds
#         if int(time_until_next) % 10 == 0:
#             refresh_placeholder.info(f"Next auto-refresh in {int(time_until_next)} seconds...")


# ==============================
# TAB 1: LIVE ANALYTICS
# ==============================

with tab1:
    st.caption(f"üîÑ Auto-refresh heartbeat: {st.session_state.refresh_heartbeat}")

    
    # TEST: Simple fragment test
    if hasattr(st, 'fragment'):
        @st.fragment(run_every=10)  # Test with 10 seconds first
        def test_fragment():
            st.write(f"**Fragment last updated:** {datetime.now().strftime('%H:%M:%S')}")
            
            scheduler: MarketScheduler = st.session_state.scheduler
            
            # Simple metrics
            col1, col2 = st.columns(2)
            
            with col1:
                if scheduler._last_run:
                    time_since = (datetime.now() - scheduler._last_run).total_seconds()
                    st.metric("Scheduler", "‚úÖ Active", delta=f"{int(time_since)}s ago")
                else:
                    st.metric("Scheduler", "‚è≥ Starting", "Initializing")
            
            with col2:
                if st.session_state.price_series.size > 0:
                    current_price = st.session_state.price_series.iloc[-1]
                    index_name = get_index_display_name(st.session_state.config["index_symbol"])
                    st.metric(index_name, f"{current_price:,.0f}")

                else:
                    index_name = get_index_display_name(st.session_state.config["index_symbol"])
                    st.metric(index_name, "Loading...")

        
        # Render the test fragment
        test_fragment()
        st.caption("üëÜ This updates every 10 seconds via fragment")
        
    else:
        st.warning("‚ùå Fragments not available. Using manual refresh.")
        # Fallback to manual refresh
        if st.button("üîÑ Refresh Now"):
            st.rerun()
    
    st.markdown("---")
    
    # Manual execution button (keep this)
    if st.button("üîÑ Execute Research Cycle Now", type="primary", width='stretch'):
        with st.spinner("Executing research cycle..."):
            research_data = execute_research_cycle()
            if research_data:
                st.success("‚úÖ Research cycle executed successfully")
                st.rerun()
            else:
                st.error("‚ùå Research cycle failed")
    
       
    st.markdown("---")

    # ==============================
    # AUTO-REFRESH STATUS (FRAGMENT SAFE)
    # ==============================
    if st.session_state.force_auto_refresh:
        st.caption("üß™ Debug Mode: Auto-refresh forced ON (ignoring market hours)")
    else:
        st.caption("üìà Production Mode: Auto-refresh runs during market hours only")
    status = get_scheduler_status()

    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric(
            "Scheduler",
            "‚úÖ Active" if status["scheduler_active"] else "‚ùå Inactive"
        )

    with col2:
        st.metric(
            "Executions",
            status["total_executions"],
            delta=f"Last: {status['last_run']}"
        )

    with col3:
        if "time_until_next" in status:
            st.metric("Next Run In", f"{status['time_until_next']}s")
        else:
            st.metric("Next Run In", "Pending")


    # Display latest analytics if available
    if st.session_state.research_analytics:
        latest_analytics = st.session_state.research_analytics[-1]
        
        # Create columns for analytics display
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### üìà OI & Gamma Analysis")
            
            # OI Velocity gauge
            oi_velocity = latest_analytics.get("oi_velocity", 0)
            fig_oi = go.Figure(go.Indicator(
                mode="gauge+number+delta",
                value=oi_velocity,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "OI Velocity (œÉ)"},
                delta={'reference': 0},
                gauge={
                    'axis': {'range': [-3, 3]},
                    'steps': [
                        {'range': [-3, -1.5], 'color': "lightgray"},
                        {'range': [-1.5, 1.5], 'color': "gray"},
                        {'range': [1.5, 3], 'color': "lightgray"}
                    ],
                    'threshold': {
                        'line': {'color': "red", 'width': 4},
                        'thickness': 0.75,
                        'value': oi_velocity
                    }
                }
            ))
            fig_oi.update_layout(height=250)
            st.plotly_chart(fig_oi, width='stretch')
        
        with col2:
            st.markdown("#### üß± Structure Analysis")
            
            # Structural metrics
            wall_strength = latest_analytics.get("wall_strength", 0)
            trap_prob = latest_analytics.get("trap_probability", 0)
            
            col_a, col_b = st.columns(2)
            with col_a:
                st.metric("Wall Strength", f"{wall_strength:.2f}")
            with col_b:
                st.metric("Trap Probability", f"{trap_prob:.2%}")
            
            # Display traps if any
            traps = latest_analytics.get("potential_traps", [])
            if traps:
                st.markdown("##### üéØ Detected Traps")
                for trap in traps[:2]:  # Show max 2
                    direction = trap.get("direction", "").upper()
                    confidence = trap.get("confidence", 0)
                    strike = trap.get("strike", 0)
                    
                    if confidence > 0.6:
                        st.markdown(f"""
                        <div class="trap-alert">
                            <strong>{direction} Trap</strong> at {strike:,.0f}<br>
                            Confidence: {confidence:.0%}<br>
                            Unwinding: {trap.get('unwinding_rate', 0):+.1f}%
                        </div>
                        """, unsafe_allow_html=True)
        
        # Price chart
        st.markdown("---")
        st.markdown("#### üìâ Price & Indicators")
        
        if len(st.session_state.price_series) > 1:
            # Create subplot for price and indicators
            index_name = get_index_display_name(st.session_state.config["index_symbol"])

            fig = make_subplots(
                rows=3,
                cols=1,
                shared_xaxes=True,
                vertical_spacing=0.05,
                subplot_titles=(
                    f"{index_name} Price",
                    "OI Velocity",
                    "Gamma Exposure"
                ),
                row_heights=[0.5, 0.25, 0.25]
            )

            
            # Price
            price_data = st.session_state.price_series.reset_index(drop=True)
            fig.add_trace(
                go.Scatter(
                    y=price_data.values,
                    mode='lines',
                    name='Price',
                    line=dict(color='#3B82F6', width=2)
                ),
                row=1, col=1
            )
            
            # OI Velocity (if available in analytics history)
            if len(st.session_state.research_analytics) > 1:
                oi_velocities = [a.get("oi_velocity", 0) for a in st.session_state.research_analytics[-50:]]
                fig.add_trace(
                    go.Scatter(
                        y=oi_velocities,
                        mode='lines',
                        name='OI Velocity',
                        line=dict(color='#10B981', width=1)
                    ),
                    row=2, col=1
                )
                
                # Add zero line
                fig.add_hline(y=0, line_dash="dash", line_color="gray", row=2, col=1)
            
            # Gamma (if available)
            if len(st.session_state.research_analytics) > 1:
                gamma_values = [a.get("gamma_exposure", {}).get("net_gamma", 0) for a in st.session_state.research_analytics[-50:]]
                fig.add_trace(
                    go.Scatter(
                        y=gamma_values,
                        mode='lines',
                        name='Net Gamma',
                        line=dict(color='#8B5CF6', width=1)
                    ),
                    row=3, col=1
                )
                
                # Add zero line
                fig.add_hline(y=0, line_dash="dash", line_color="gray", row=3, col=1)
            
            fig.update_layout(height=600, showlegend=True)
            st.plotly_chart(fig, width='stretch')

# ==============================
# TAB 2: SIGNALS
# ==============================

with tab2:
    st.markdown('<div class="sub-header">üéØ Trading Signals</div>', unsafe_allow_html=True)
    
    # Fetch active signals
    active_signals = fetch_active_signals(limit=10)
    
    if not active_signals.empty:
        # Display signals in columns
        for _, signal in active_signals.iterrows():
            col1, col2, col3, col4 = st.columns([2, 1, 1, 1])
            
            with col1:
                signal_type = signal["signal_type"]
                color = "green" if signal_type == "BUY" else "red" if signal_type == "SELL" else "gray"
                st.markdown(f"### <span style='color:{color};'>{signal_type}</span>", unsafe_allow_html=True)
                
                # Rationale
                if pd.notna(signal["rationale"]):
                    st.caption(signal["rationale"])
            
            with col2:
                confidence = signal["confidence"]
                st.metric("Confidence", f"{confidence:.0%}")
            
            with col3:
                strength = signal.get("signal_strength", "UNKNOWN")
                st.metric("Strength", strength)
            
            with col4:
                status = signal["status"]
                badge_color = "blue" if status == "NEW" else "green" if status == "VALIDATED" else "orange"
                st.markdown(f"<div style='padding: 5px 10px; background-color:{badge_color}; color:white; border-radius:5px; text-align:center;'>{status}</div>", 
                           unsafe_allow_html=True)
            
            # Research context (expandable)
            with st.expander("Research Details"):
                if pd.notna(signal.get("research_context")):
                    try:
                        context = json.loads(signal["research_context"]) if isinstance(signal["research_context"], str) else signal["research_context"]
                        st.json(context)
                    except:
                        st.write("No research context available")
            
            st.markdown("---")
        
        # Performance metrics
        st.markdown("#### üìä Signal Performance")
        performance = fetch_signal_performance(days=7)
        
        if performance:
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Total Signals", performance.get("total_signals", 0))
            
            with col2:
                st.metric("Profitable", performance.get("profitable_signals", 0))
            
            with col3:
                win_rate = performance.get("win_rate", 0)
                st.metric("Win Rate", f"{win_rate:.1f}%")
            
            with col4:
                period = performance.get("period_days", 7)
                st.metric("Period", f"{period} days")
        
        # Signal distribution chart
        if not active_signals.empty:
            st.markdown("#### üìà Signal Distribution")
            
            fig = go.Figure(data=[
                go.Pie(
                    labels=active_signals["signal_type"].value_counts().index,
                    values=active_signals["signal_type"].value_counts().values,
                    hole=.3
                )
            ])
            
            fig.update_layout(
                height=300,
                showlegend=True,
                margin=dict(t=0, b=0, l=0, r=0)
            )
            
            st.plotly_chart(fig, width='stretch')
    
    else:
        st.info("üì≠ No active signals. The system will generate signals during market hours.")

# ==============================
# TAB 3: RESEARCH
# ==============================

with tab3:
    st.markdown('<div class="sub-header">üîç Research & Analytics</div>', unsafe_allow_html=True)
    
    # Research metrics
    if st.session_state.research_analytics:
        latest = st.session_state.research_analytics[-1]
        
        # Key research metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            net_gamma = latest.get("gamma_exposure", {}).get("net_gamma", 0)
            st.metric("Net Gamma", f"{net_gamma:,.0f}")
        
        with col2:
            pcr = latest.get("put_call_ratio", 1.0)
            st.metric("Put/Call Ratio", f"{pcr:.2f}")
        
        with col3:
            divergence = latest.get("divergence_score", 0)
            st.metric("Divergence Score", f"{divergence:.2f}")
        
        with col4:
            regime = latest.get("market_regime", "UNKNOWN")
            st.metric("Market Regime", regime)
        
        # Display market insights
        st.markdown("#### üí° Market Insights")
        
        insights = latest.get("market_insights", [])
        if insights:
            for insight in insights[:5]:
                if "trap" in insight.lower():
                    st.markdown(f'<div class="trap-alert">{insight}</div>', unsafe_allow_html=True)
                elif "divergence" in insight.lower():
                    st.markdown(f'<div class="divergence-alert">{insight}</div>', unsafe_allow_html=True)
                else:
                    st.markdown(f'<div class="research-insight">{insight}</div>', unsafe_allow_html=True)
        else:
            st.info("No insights available yet")
        
        # Structural walls
        st.markdown("#### üß± Structural Walls")
        
        walls = latest.get("structural_walls", [])
        if walls:
            walls_df = pd.DataFrame(walls)
            st.dataframe(walls_df, width='stretch')
        
        # Trap detection history
        st.markdown("#### üéØ Trap Detection History")
        
        if st.session_state.trap_detections:
            traps_df = pd.DataFrame([
                {
                    "Time": pd.to_datetime(t["timestamp"]).strftime("%H:%M:%S"),
                    "Type": t["trap"].get("direction", ""),
                    "Strike": t["trap"].get("strike", 0),
                    "Confidence": t["trap"].get("confidence", 0),
                    "Regime": t.get("market_regime", "")
                }
                for t in st.session_state.trap_detections
            ])
            
            if not traps_df.empty:
                st.dataframe(traps_df, width='stretch')
        
        # Research analytics over time
        st.markdown("#### üìä Analytics Timeline")
        
        if len(st.session_state.research_analytics) > 5:
            # Convert to DataFrame for plotting
            analytics_df = pd.DataFrame(st.session_state.research_analytics[-50:])
            
            if "timestamp" in analytics_df.columns:
                analytics_df["time"] = pd.to_datetime(analytics_df["timestamp"]).dt.strftime("%H:%M")
                
                # Plot OI Velocity and Gamma over time
                fig = make_subplots(
                    rows=2, cols=1,
                    shared_xaxes=True,
                    subplot_titles=("OI Velocity", "Net Gamma"),
                    vertical_spacing=0.1
                )
                
                if "oi_velocity" in analytics_df.columns:
                    fig.add_trace(
                        go.Scatter(
                            x=analytics_df["time"],
                            y=analytics_df["oi_velocity"],
                            mode='lines+markers',
                            name='OI Velocity',
                            line=dict(color='#10B981')
                        ),
                        row=1, col=1
                    )
                
                if "gamma_exposure" in analytics_df.columns:
                    # Extract net_gamma from gamma_exposure dict
                    try:
                        gamma_values = []
                        for g in analytics_df["gamma_exposure"]:
                            if isinstance(g, dict) and "net_gamma" in g:
                                gamma_values.append(g["net_gamma"])
                            else:
                                gamma_values.append(0)
                        
                        fig.add_trace(
                            go.Scatter(
                                x=analytics_df["time"],
                                y=gamma_values,
                                mode='lines+markers',
                                name='Net Gamma',
                                line=dict(color='#8B5CF6')
                            ),
                            row=2, col=1
                        )
                    except:
                        pass
                
                fig.update_layout(height=400, showlegend=True)
                st.plotly_chart(fig, width='stretch')

# ==============================
# TAB 4: CONFIGURATION
# ==============================

with tab4:
    st.markdown('<div class="sub-header">‚öôÔ∏è System Configuration</div>', unsafe_allow_html=True)
    
    # Available indices and underlyings
    available_indices = [
        "NSE_INDEX|Nifty 50",
        "NSE_INDEX|Nifty Bank", 
        "NSE_INDEX|Nifty Fin Service",
        "NSE_INDEX|Nifty Midcap 100",
        "NSE_INDEX|Nifty Next 50",
        "BSE_INDEX|S&P BSE SENSEX"  
    ]
    
    available_underlyings = [
        "NIFTY",
        "BANKNIFTY", 
        "FINNIFTY",
        "MIDCPNIFTY",
        "SENSEX"  
    ]
    
    # Get available expiries for the selected underlying
    current_underlying = st.session_state.config["underlying"]
    from data.instrument_master import get_available_expiries
    
    try:
        available_expiries = get_available_expiries(current_underlying)
        if not available_expiries:
            available_expiries = ["2026-01-27", "2026-02-03", "2026-02-10"]
    except:
        available_expiries = ["2026-01-27", "2026-02-03", "2026-02-10"]
    
    # Configuration form
    with st.form("configuration_form"):
        col1, col2 = st.columns(2)
        
        with col1:
            # Index Symbol Dropdown
            index_symbol = st.selectbox(
                "Index Symbol", 
                options=available_indices,
                index=available_indices.index(st.session_state.config["index_symbol"]) 
                if st.session_state.config["index_symbol"] in available_indices else 0
            )
            
            # Underlying Symbol Dropdown
            underlying = st.selectbox(
                "Underlying Symbol", 
                options=available_underlyings,
                index=available_underlyings.index(st.session_state.config["underlying"])
                if st.session_state.config["underlying"] in available_underlyings else 0
            )
            
            # Expiry Date Dropdown
            # Find today's date in available_expiries
            today_str = datetime.now().strftime("%Y-%m-%d")
            default_index = 0

            if today_str in available_expiries:
                default_index = available_expiries.index(today_str)
            elif today_str == "2026-01-28":  # Today's date
                # Add today to available expiries if it's not there
                available_expiries.insert(0, today_str)
                default_index = 0

            expiry_date = st.selectbox(
                "Expiry Date",
                options=available_expiries,
                index=default_index  # ‚úÖ Now selects today if available
            )
            
            # Convert selected expiry to datetime
            if expiry_date:
                try:
                    expiry_datetime = datetime.strptime(expiry_date, "%Y-%m-%d")
                    days_to_expiry = (expiry_datetime.date() - datetime.now().date()).days
                    st.info(f"Days to expiry: {days_to_expiry}")
                except:
                    st.warning("Invalid expiry date format")
        
        with col2:
            max_option_keys = st.number_input(
                "Max Option Keys", 
                min_value=50, 
                max_value=500, 
                value=st.session_state.config["max_option_keys"],
                help="Maximum number of option strikes to fetch"
            )
            
            scheduler_interval = st.slider(
                "Scheduler Interval (seconds)", 
                min_value=10, 
                max_value=300, 
                value=30,
                step=5,
                help="How often to refresh data (10-300 seconds)"
            )
            
            confidence_threshold = st.slider(
                "Signal Confidence Threshold", 
                min_value=0.0, 
                max_value=1.0, 
                value=0.2, 
                step=0.05,
                format="%.2f",
                help="Minimum confidence level for signals (0.0-1.0)"
            )
            
            # Advanced options expander
            with st.expander("Advanced Options"):
                enable_live_trading = st.checkbox(
                    "Enable Live Trading", 
                    value=False,
                    help="WARNING: Enable actual order placement"
                )
                
                risk_per_trade = st.number_input(
                    "Risk per Trade (%)",
                    min_value=0.1,
                    max_value=5.0,
                    value=1.0,
                    step=0.1,
                    format="%.1f",
                    help="Maximum risk per trade as percentage of capital"
                )


        # ==============================
        # SUBMIT CONFIGURATION (FULL REPLACEMENT)
        # ==============================

        submit_col1, submit_col2, submit_col3 = st.columns([1, 2, 1])

        with submit_col2:
            if st.form_submit_button("üíæ Save Configuration", width='stretch'):

                # --------------------------------------------------
                # 1. Resolve expiry (manual OR smart auto)
                # --------------------------------------------------
                if expiry_date:
                    # User explicitly selected an expiry
                    resolved_expiry = datetime.strptime(expiry_date, "%Y-%m-%d")
                else:
                    # Auto-resolve based on underlying (weekly/monthly logic)
                    from utils.expiry_utils import get_trading_expiry

                    expiry_str = get_trading_expiry(underlying)
                    if not expiry_str:
                        st.error(f"‚ùå Could not resolve expiry for {underlying}")
                        st.stop()

                    resolved_expiry = datetime.strptime(expiry_str, "%Y-%m-%d")

                # --------------------------------------------------
                # 2. Update core configuration
                # --------------------------------------------------
                st.session_state.config.update({
                    "index_symbol": index_symbol,
                    "underlying": underlying,
                    "expiry_date": resolved_expiry,
                    "max_option_keys": max_option_keys,
                    "enable_live_trading": enable_live_trading,
                    "risk_per_trade": risk_per_trade,
                })

                # --------------------------------------------------
                # 3. HARD RESET dependent runtime state
                #    (CRITICAL when index / expiry changes)
                # --------------------------------------------------
                st.session_state.option_keys = []
                st.session_state.price_series = pd.Series(dtype=float)
                st.session_state.volume_series = pd.Series(dtype=float)
                st.session_state.ccc_history = pd.Series(dtype=float)

                # --------------------------------------------------
                # 4. Recreate scheduler with new interval
                # --------------------------------------------------
                st.session_state.scheduler = MarketScheduler(
                    interval_seconds=scheduler_interval
                )
                st.session_state.scheduler._last_run = None
                st.session_state.scheduler._total_executions = 0

                # --------------------------------------------------
                # 5. Recreate signal engine with new confidence
                # --------------------------------------------------
                st.session_state.signal_engine = create_signal_engine(
                    signal_expiry_minutes=5,
                    confidence_threshold=confidence_threshold
                )

                # --------------------------------------------------
                # 6. Feedback + rerun
                # --------------------------------------------------
                st.success(
                    f"‚úÖ Configuration saved | "
                    f"{underlying} | Expiry: {resolved_expiry.strftime('%Y-%m-%d')}"
                )
                st.rerun()

    
    st.markdown("---")
    st.markdown("### üß™ Debug / Testing Controls")

    force_refresh = st.checkbox(
        "Force Auto-Refresh Outside Market Hours",
        value=st.session_state.force_auto_refresh,
        help="Enable auto-refresh even when the market is closed (testing/debugging only)"
    )

    st.session_state.force_auto_refresh = force_refresh

    if force_refresh:
        st.warning("‚ö†Ô∏è Debug mode enabled: auto-refresh will run outside market hours")

    
    
    
    
    # Quick Configuration Presets
    st.markdown("#### üöÄ Quick Presets")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üìä NIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty 50",
                "underlying": "NIFTY"
            })
            st.success("Set to NIFTY Weekly")
            st.rerun()
    
    with col2:
        if st.button("üè¶ BANKNIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty Bank",
                "underlying": "BANKNIFTY"
            })
            st.success("Set to BANKNIFTY Weekly")
            st.rerun()
    
    with col3:
        if st.button("üí≥ FINNIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty Fin Service",
                "underlying": "FINNIFTY"
            })
            st.success("Set to FINNIFTY Weekly")
            st.rerun()
    
    # Current Configuration Display
    st.markdown("---")
    st.markdown("#### üñ•Ô∏è Current Configuration")
    
    config_col1, config_col2 = st.columns(2)
    
    with config_col1:
        st.markdown("**Trading Settings:**")
        st.text(f"Index: {st.session_state.config['index_symbol']}")
        st.text(f"Underlying: {st.session_state.config['underlying']}")
        st.text(f"Expiry: {st.session_state.config['expiry_date'].strftime('%Y-%m-%d')}")
        st.text(f"Max Options: {st.session_state.config['max_option_keys']}")
    
    with config_col2:
        st.markdown("**System Settings:**")
        st.text(f"Scheduler: {st.session_state.scheduler.interval_seconds}s")
        st.text(f"Signal Confidence: {confidence_threshold:.2f}")
        st.text(f"Live Trading: {'Enabled' if st.session_state.config.get('enable_live_trading', False) else 'Disabled'}")
        if st.session_state.config.get('enable_live_trading', False):
            st.text(f"Risk per Trade: {st.session_state.config.get('risk_per_trade', 1.0)}%")
    
    # Diagnostic tools (keep your existing code here)
    st.markdown("---")
    st.markdown("#### üîß Diagnostic Tools")
    # ... keep your existing diagnostic tools code ...
    
    # Diagnostic tools
    st.markdown("---")
    st.markdown("#### üîß Diagnostic Tools")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üß™ Test Upstox Connection", width='stretch'):
            # Test connection by trying to fetch profile
            try:
                profile = client.fetch_profile()
                if profile:
                    st.success(f"‚úÖ Upstox connection working")
                else:
                    st.error("‚ùå Failed to fetch profile")
            except Exception as e:
                st.error(f"‚ùå Connection test failed: {e}")
    
    with col2:
        if st.button("üìä Test Feature Pipeline", width='stretch'):
            try:
                # Get latest features
                features = fetch_latest_features(FEATURE_VERSION)
                if features is not None and not features.empty():
                    st.success(f"‚úÖ Feature pipeline working ({len(features)} features)")
                else:
                    st.warning("‚ö†Ô∏è No features found")
            except Exception as e:
                st.error(f"‚ùå Feature pipeline error: {e}")
    
    with col3:
        if st.button("üîÑ Clear Option Keys Cache", width='stretch'):
            st.session_state.option_keys = []
            st.success("‚úÖ Option keys cache cleared")

# ==============================
# PERFORMANCE MODAL
# ==============================

if st.session_state.get("show_performance", False):
    with st.expander("üìà Performance Analytics", expanded=True):
        st.markdown("### üìà Trading Performance")
        
        performance = fetch_signal_performance(days=30)
        
        if performance:
            # Overall metrics
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Total Trades", performance["total_signals"])
            
            with col2:
                st.metric("Profitable Trades", performance["profitable_signals"])
            
            with col3:
                win_rate = performance["win_rate"]
                color = "normal" if win_rate > 50 else "inverse"
                st.metric("Win Rate", f"{win_rate:.1f}%", delta_color=color)
            
            with col4:
                st.metric("Analysis Period", f"{performance['period_days']} days")
            
            # Breakdown by signal type
            st.markdown("#### Breakdown by Signal Type")
            
            if "breakdown" in performance:
                breakdown_df = pd.DataFrame(performance["breakdown"])
                st.dataframe(breakdown_df, width='stretch')
        
        st.session_state.show_performance = False

# ==============================
# LOGS MODAL
# ==============================

if st.session_state.get("show_logs", False):
    with st.expander("üìã System Logs", expanded=True):
        st.markdown("### üìã Recent System Activity")
        
        # Could fetch from system_health table here
        st.info("Logs would be displayed here from the database.")
        
        st.session_state.show_logs = False

# ==============================
# FOOTER
# ==============================

st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #6B7280; font-size: 0.9rem;'>
    Algorithmic Trading Research Platform v2.0 | 
    Research Features: OI Velocity, Gamma Exposure, Walls/Traps, Divergence Analysis
</div>
""", unsafe_allow_html=True)

# ==============================
# FINAL STARTUP CHECK
# ==============================
print(f"üéØ Final startup check:")
print("üéØ Final startup check:")
print(f"   - Scheduler executions: {st.session_state.scheduler._total_executions}")
print(f"   - Last run: {st.session_state.scheduler._last_run}")
print(f"   - Streamlit version: {st.__version__}")
print(f"   - Fragments available: {hasattr(st, 'fragment')}")



--------------------------------------------------
FILE PATH : G:\trading_app\backtest\__init__.py
SIZE      : 0 bytes
--------------------------------------------------



--------------------------------------------------
FILE PATH : G:\trading_app\backtest\engine.py
SIZE      : 3383 bytes
--------------------------------------------------

import sqlite3
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path

from core.signals.state_machine import SignalStateMachine


DB_PATH = Path("G:/trading_app/storage/trading.db")


class BacktestEngine:
    """
    Replays historical features to simulate signals and trades.
    """

    def __init__(
        self,
        feature_version: str,
        holding_minutes: int = 5,
        quantity: int = 1
    ):
        self.feature_version = feature_version
        self.holding_minutes = holding_minutes
        self.quantity = quantity
        self.signal_engine = SignalStateMachine(signal_expiry_minutes=holding_minutes)

    # ==============================
    # LOAD HISTORICAL FEATURES
    # ==============================

    def load_features(self) -> pd.DataFrame:
        with sqlite3.connect(DB_PATH) as conn:
            df = pd.read_sql(
                """
                SELECT *
                FROM market_features
                WHERE feature_version = ?
                ORDER BY timestamp
                """,
                conn,
                params=(self.feature_version,)
            )

        df["timestamp"] = pd.to_datetime(df["timestamp"])
        return df

    # ==============================
    # RUN BACKTEST
    # ==============================

    def run(self) -> pd.DataFrame:
        df = self.load_features()
        if df.empty:
            raise RuntimeError("No features found for backtest")

        trades = []
        open_trade = None

        for i in range(len(df) - 1):
            row = df.iloc[i]
            next_row = df.iloc[i + 1]

            # If no open trade ‚Üí check for signal
            if open_trade is None:
                signal = self.signal_engine.build_signal(row)

                if signal["signal_type"] in ("BUY", "SELL"):
                    open_trade = {
                        "signal_id": signal["signal_id"],
                        "direction": signal["signal_type"],
                        "entry_time": next_row["timestamp"],
                        "entry_price": next_row["spot_price"],
                        "expiry_time": next_row["timestamp"]
                                       + timedelta(minutes=self.holding_minutes)
                    }

            # If trade open ‚Üí check exit
            if open_trade is not None:
                if next_row["timestamp"] >= open_trade["expiry_time"]:
                    exit_price = next_row["spot_price"]

                    pnl = (
                        (exit_price - open_trade["entry_price"])
                        if open_trade["direction"] == "BUY"
                        else (open_trade["entry_price"] - exit_price)
                    ) * self.quantity

                    trades.append({
                        "signal_id": open_trade["signal_id"],
                        "entry_time": open_trade["entry_time"],
                        "exit_time": next_row["timestamp"],
                        "entry_price": open_trade["entry_price"],
                        "exit_price": exit_price,
                        "quantity": self.quantity,
                        "pnl": pnl
                    })

                    open_trade = None

        return pd.DataFrame(trades)



--------------------------------------------------
FILE PATH : G:\trading_app\backtest\metrics.py
SIZE      : 1547 bytes
--------------------------------------------------

import pandas as pd
import numpy as np


def compute_backtest_metrics(trades_df: pd.DataFrame) -> dict:
    """
    Compute performance metrics from backtest trades.
    Safe for empty DataFrames.
    """

    if trades_df is None or trades_df.empty:
        return {
            "total_trades": 0,
            "win_rate": 0.0,
            "total_pnl": 0.0,
            "avg_pnl": 0.0,
            "max_drawdown": 0.0,
            "sharpe": 0.0,
            "expectancy": 0.0
        }

    pnl = trades_df["pnl"]

    total_trades = len(pnl)
    wins = pnl[pnl > 0]
    losses = pnl[pnl <= 0]

    win_rate = len(wins) / total_trades if total_trades else 0.0
    total_pnl = pnl.sum()
    avg_pnl = pnl.mean()

    # Equity curve
    equity = pnl.cumsum()
    running_max = equity.cummax()
    drawdown = equity - running_max
    max_drawdown = drawdown.min()

    # Sharpe (PnL-based)
    sharpe = 0.0
    if pnl.std() != 0:
        sharpe = (pnl.mean() / pnl.std()) * np.sqrt(len(pnl))

    # Expectancy
    avg_win = wins.mean() if not wins.empty else 0.0
    avg_loss = losses.mean() if not losses.empty else 0.0
    expectancy = (win_rate * avg_win) + ((1 - win_rate) * avg_loss)

    return {
        "total_trades": total_trades,
        "win_rate": round(win_rate, 3),
        "total_pnl": round(total_pnl, 2),
        "avg_pnl": round(avg_pnl, 2),
        "max_drawdown": round(max_drawdown, 2),
        "sharpe": round(sharpe, 3),
        "expectancy": round(expectancy, 2)
    }



--------------------------------------------------
FILE PATH : G:\trading_app\backtest\run_backtest.py
SIZE      : 460 bytes
--------------------------------------------------

from backtest.engine import BacktestEngine
from backtest.metrics import compute_backtest_metrics
from ml.feature_contract import FEATURE_VERSION

engine = BacktestEngine(
    feature_version=FEATURE_VERSION,
    holding_minutes=5,
    quantity=1
)

results = engine.run()
metrics = compute_backtest_metrics(results)

print("Backtest Results:")
print(results)

print("\nBacktest Metrics:")
for k, v in metrics.items():
    print(f"{k}: {v}")



--------------------------------------------------
FILE PATH : G:\trading_app\codebase_export.txt
SIZE      : 2453372 bytes
--------------------------------------------------


PROJECT OVERVIEW
Generated: 2026-01-28 15:51:25
Base Path: G:\trading_app


FILE STATISTICS
Total Files: 42
.py: 34 files
.json: 2 files
.txt: 1 files
.bat: 1 files
.key: 1 files
.enc: 1 files
.sql: 1 files
.db: 1 files

----------------------------------------------------------------------------------------------------


****************************************************************************************************
CATEGORY: Source Code Files (34 files)
****************************************************************************************************


====================================================================================================
FILE: .\app - Copy.py
====================================================================================================


File Name: app - Copy.py
Full Path: G:\trading_app\app - Copy.py
Size: 74.67 KB
Last Modified: 01/28/2026 14:12:50
Extension: .py

"""
ENHANCED TRADING TOOL - Research-Based Live Feature Engine
Incorporates OI Velocity, Gamma Exposure, Walls/Traps, and Divergence analysis.
"""

import os
import sys

# Fix Conda environment issue
if 'conda' in sys.version.lower():
    try:
        # Try to fix conda path issues
        os.environ['PATH'] = os.path.join(os.environ.get('CONDA_PREFIX', ''), 'bin') + ':' + os.environ['PATH']
    except:
        pass
# ==============================
# STREAMLIT CONFIG FIX - ADD THIS
# ==============================
import os
os.environ['STREAMLIT_SERVER_FILE_WATCHER_TYPE'] = 'none'
import time 
import threading  # <-- ADD THIS LINE
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from datetime import datetime, timezone
import json
from pathlib import Path
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Enhanced core components
from core.session import UpstoxSession, initialize_session, display_session_status
from data.upstox_client import UpstoxClient, MarketAnalytics, display_market_analytics
from data.instrument_master import get_option_keys

try:
    from core.feature_pipeline import build_and_store_features, create_feature_summary
except ImportError as e:
    st.error(f"Feature pipeline import error: {e}")
    # Define dummy functions to prevent crash
    def build_and_store_features(*args, **kwargs):
        return {"error": "Feature pipeline not loaded"}
    def create_feature_summary(*args, **kwargs):
        return {"error": "Feature pipeline not loaded"}
from core.scheduler import MarketScheduler
from core.signals.state_machine import SignalStateMachine, SignalValidator, create_signal_engine
from features.breadth import build_constituents_df
from ml.feature_contract import FEATURE_VERSION

# Enhanced storage
from storage.repository import (
    fetch_latest_features,
    insert_signal,
    signal_exists,
    validate_new_signals,
    expire_old_signals,
    fetch_active_signals,
    fetch_recent_analytics,
    get_database_stats,
    fetch_signal_performance,
    insert_research_analytics
)

# ==============================
# PAGE CONFIGURATION
# ==============================

st.set_page_config(
    page_title="Algorithmic Trading Research Platform",
    page_icon="üìà",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E3A8A;
        font-weight: 700;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #374151;
        font-weight: 600;
        margin-top: 1.5rem;
        margin-bottom: 1rem;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
    }
    .research-insight {
        background: #F0F9FF;
        border-left: 4px solid #3B82F6;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
    .trap-alert {
        background: #FEF3C7;
        border-left: 4px solid #F59E0B;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
    .divergence-alert {
        background: #FEE2E2;
        border-left: 4px solid #EF4444;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
</style>
""", unsafe_allow_html=True)

# ==============================
# APP HEADER
# ==============================

st.markdown('<div class="main-header">üìä Algorithmic Trading Research Platform</div>', unsafe_allow_html=True)
st.markdown("""
<div style='color: #6B7280; margin-bottom: 2rem;'>
    Advanced derivatives analytics with OI Velocity, Gamma Exposure, Structural Walls/Traps, and Spot Divergence detection.
    Version: Research v2.0
</div>
""", unsafe_allow_html=True)

# ==============================
# INITIALIZATION
# ==============================

def initialize_app_state():
    """Initialize application state with research components."""
    
    # Initialize session
    initialize_session()
    
    # Initialize state variables
    if "initialized" not in st.session_state:
        st.session_state.initialized = True
        
        # Market data series
        for key in ["price_series", "volume_series", "ccc_history", "oi_velocity_history", "gamma_history"]:
            if key not in st.session_state:
                st.session_state[key] = pd.Series(dtype=float)
        
        # Research analytics
        if "research_analytics" not in st.session_state:
            st.session_state.research_analytics = []
        
        # Signal engine
        if "signal_engine" not in st.session_state:
            st.session_state.signal_engine = create_signal_engine(
                signal_expiry_minutes=5,
                confidence_threshold=0.2
            )
        
        # Scheduler
        if "scheduler" not in st.session_state:
            st.session_state.scheduler = MarketScheduler(interval_seconds=30)
            # Initialize scheduler properties
            st.session_state.scheduler._last_run = None
            st.session_state.scheduler._total_executions = 0
        
        # Load Nifty weights
        if "nifty_weights" not in st.session_state:
            try:
                with open("config/nifty_weights.json", "r") as f:
                    st.session_state.nifty_weights = json.load(f)
            except:
                st.session_state.nifty_weights = {}
        
        # Instrument master path
        if "instrument_master" not in st.session_state:
            st.session_state.instrument_master = Path("G:/trading_app/data/instruments.json.gz")
        
        # Configuration - SMART EXPIRY SELECTION
        if "config" not in st.session_state:
            from utils.expiry_utils import get_trading_expiry
            
            # Get smart expiry
            underlying = "NIFTY"
            expiry_date_str = get_trading_expiry(underlying)
            
            if not expiry_date_str:
                # Fallback to next available date from instrument master
                from data.instrument_master import get_next_available_expiry
                expiry_date_str = get_next_available_expiry(underlying)
                
                if not expiry_date_str:
                    # Final fallback to a known date
                    expiry_date_str = "2026-02-03"
            
            # Convert string to datetime object
            expiry_date = datetime.strptime(expiry_date_str, "%Y-%m-%d")
            
            st.session_state.config = {
                "index_symbol": "NSE_INDEX|Nifty 50",
                "underlying": underlying,
                "expiry_date": expiry_date,
                "max_option_keys": 200
            }
            
            # Show expiry info
            from utils.expiry_utils import is_market_open
            market_status = "open" if is_market_open() else "closed"
            
            # Get today's date
            today_str = datetime.now().strftime('%Y-%m-%d')
            
            # Show appropriate message
            if expiry_date_str == today_str:
                st.info(f"üìÖ Using TODAY's expiry: {expiry_date_str} (Market: {market_status})")
            else:
                st.info(f"üìÖ Using NEXT expiry: {expiry_date_str} (Today: {today_str}, Market: {market_status})")
        
        # Option keys cache
        if "option_keys" not in st.session_state:
            st.session_state.option_keys = []
        
        # Market regime tracking
        if "market_regime" not in st.session_state:
            st.session_state.market_regime = "UNKNOWN"
        
        # Trap detection history
        if "trap_detections" not in st.session_state:
            st.session_state.trap_detections = []
        
        st.success("‚úÖ Application state initialized with research components")

# Run initialization
initialize_app_state()

# ==============================
# AUTHENTICATION
# ==============================

st.markdown('<div class="sub-header">üîê Authentication</div>', unsafe_allow_html=True)

# Authenticate - this will show login button if not authenticated
access_token = UpstoxSession.authenticate()

# CRITICAL: Check if authenticate() actually returned a token
# If it returns None, we need to stop execution here
if not access_token:
    # authenticate() should have shown login button and stopped with st.stop()
    # But if we reach here, something went wrong
    st.error("‚ùå Authentication required. Please log in to continue.")
    
    # Show login button manually as fallback
    login_url = UpstoxSession.get_login_url()
    st.markdown(f"""
    <div style="text-align: center; margin: 20px 0;">
        <a href="{login_url}" target="_blank">
            <button style="
                background-color: #00d09c;
                color: white;
                padding: 15px 30px;
                font-size: 18px;
                font-weight: bold;
                border: none;
                border-radius: 10px;
                cursor: pointer;
                width: 60%;
                margin: 20px 0;
                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            ">
            üìà Click to Login with Upstox
            </button>
        </a>
    </div>
    """, unsafe_allow_html=True)
    
    # STOP the app here
    st.stop()

# If we reach here, we have a token
# Initialize client
try:
    client = UpstoxClient(access_token)
    st.success("‚úÖ Upstox client initialized")
        
except Exception as e:
    st.error(f"‚ùå Error initializing Upstox client: {e}")
    st.stop()

# ==============================
# SIDEBAR - SESSION STATUS
# ==============================

with st.sidebar:
    st.markdown("## üß≠ Navigation")
    
    # Display session status
    display_session_status()
    
    # Database stats
    st.markdown("---")
    st.markdown("### üìä Database")
    
    db_stats = get_database_stats()
    if db_stats:
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Features", f"{db_stats.get('market_features_count', 0):,}")
        with col2:
            st.metric("Signals", f"{db_stats.get('signals_count', 0):,}")
    
    # Market hours info
    st.markdown("---")
    st.markdown("### üïí Market Hours")
    
    scheduler: MarketScheduler = st.session_state.scheduler
    now = datetime.now()
    
    if scheduler._is_market_open(now):
        st.success("‚úÖ Market Open")
        next_run = scheduler._last_run + timedelta(seconds=scheduler.interval_seconds) if scheduler._last_run else now
        time_to_next = (next_run - now).total_seconds()
        if time_to_next > 0:
            st.info(f"Next cycle in: {int(time_to_next)}s")
    else:
        st.warning("‚è∏Ô∏è Market Closed")
    
    # Quick actions
    st.markdown("---")
    st.markdown("### ‚ö° Quick Actions")
    
    if st.button("üîÑ Clear Cache", width='stretch'):
        for key in list(st.session_state.keys()):
            if key not in ["initialized", "instrument_master", "config", "nifty_weights"]:
                del st.session_state[key]
        st.rerun()
    
    if st.button("üìä View Performance", width='stretch'):
        st.session_state.show_performance = True
    
    if st.button("üõ†Ô∏è System Logs", width='stretch'):
        st.session_state.show_logs = True

def retry_on_failure(max_retries=3, delay=5):
    """Decorator for retrying failed executions"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    print(f"‚ö†Ô∏è Attempt {attempt + 1}/{max_retries} failed: {e}")
                    if attempt < max_retries - 1:
                        print(f"‚è∏Ô∏è Retrying in {delay}s...")
                        time.sleep(delay)
                    else:
                        print(f"‚ùå All {max_retries} attempts failed")
                        raise
            return None
        return wrapper
    return decorator

def log_execution_time(func):
    """Decorator for logging execution time"""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        print(f"‚è±Ô∏è Starting {func.__name__}...")
        
        result = func(*args, **kwargs)
        
        execution_time = time.time() - start_time
        print(f"‚è±Ô∏è {func.__name__} completed in {execution_time:.2f}s")
        
        return result
    return wrapper
# ==============================
# RESEARCH EXECUTION CYCLE
# ==============================

@log_execution_time
@retry_on_failure(max_retries=2, delay=3)
def execute_research_cycle(config=None, nifty_weights=None, option_keys=None):
    """
    Enhanced execution cycle with research analytics.
    Returns comprehensive research data.
    """
    # Use provided parameters or fall back to session state
    if config is None:
        try:
            config = st.session_state.config
        except (KeyError, AttributeError):
            print("‚ö†Ô∏è Config not found in session state")
            return None
    
    if nifty_weights is None:
        nifty_weights = st.session_state.get("nifty_weights", {})
    
    if option_keys is None:
        option_keys = st.session_state.get("option_keys", [])
    
    research_data = {}
    
    try:
        # ------------------------------
        # 1. INDEX QUOTE
        # ------------------------------
        print(f"üìä Fetching index quote for {config.get('index_symbol', 'NSE_INDEX|Nifty 50')}")
        index_data = client.fetch_index_quote(config["index_symbol"])
        
        if not index_data or index_data.get("ltp") is None:
            print("‚ùå No index data received")
            return None
        
        ltp = float(index_data["ltp"])
        open_price = float(index_data["open"])
        volume = 0.0  # index has no volume
        
        research_data["index"] = {
            "ltp": ltp,
            "open": open_price,
            "change": index_data.get("percent_change", 0),
            "timestamp": index_data.get("timestamp", datetime.now(timezone.utc).isoformat())
        }
        
        print(f"‚úÖ Index data: {ltp} (Change: {index_data.get('percent_change', 0)}%)")
        
        # ------------------------------
        # 2. UPDATE PRICE SERIES
        # ------------------------------
        # Create new series safely
        try:
            if "price_series" not in st.session_state:
                st.session_state.price_series = pd.Series(dtype=float)
            
            if "volume_series" not in st.session_state:
                st.session_state.volume_series = pd.Series(dtype=float)
            
            # Update price series
            new_price_series = pd.concat([
                st.session_state.price_series, 
                pd.Series([ltp], index=[datetime.now()])
            ]).tail(200)
            
            # Update volume series
            new_volume_series = pd.concat([
                st.session_state.volume_series, 
                pd.Series([volume], index=[datetime.now()])
            ]).tail(200)
            
            # Store back to session state
            st.session_state.price_series = new_price_series
            st.session_state.volume_series = new_volume_series
            
            print(f"üìà Price series updated: {len(st.session_state.price_series)} data points")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error updating price series: {e}")
            # Initialize fresh series
            st.session_state.price_series = pd.Series([ltp], index=[datetime.now()])
            st.session_state.volume_series = pd.Series([volume], index=[datetime.now()])
        
        # ------------------------------
        # 3. BREADTH ANALYSIS (CCC)
        # ------------------------------
        print("üìä Starting breadth analysis...")
        try:
            if not nifty_weights:
                print("‚ö†Ô∏è No Nifty weights found, skipping breadth analysis")
                ccc_value = 0.0
                constituents_df = pd.DataFrame()
            else:
                equity_quotes_df = client.fetch_equity_quotes(
                    symbols=list(nifty_weights.keys())
                )
                
                constituents_df = build_constituents_df(
                    equity_quotes_df=equity_quotes_df,
                    weights=nifty_weights
                )
                
                if not constituents_df.empty:
                    ccc_value = (
                        constituents_df["weight"] * constituents_df["price_change"]
                    ).sum()
                    print(f"‚úÖ CCC Value: {ccc_value:.4f}")
                else:
                    ccc_value = 0.0
                    print("‚ö†Ô∏è Empty constituents dataframe")
        except Exception as e:
            print(f"‚ö†Ô∏è Breadth analysis error: {e}")
            ccc_value = 0.0
            constituents_df = pd.DataFrame()
        
        # Update CCC history
        try:
            if "ccc_history" not in st.session_state:
                st.session_state.ccc_history = pd.Series(dtype=float)
            
            st.session_state.ccc_history = pd.concat([
                st.session_state.ccc_history, 
                pd.Series([ccc_value], index=[datetime.now()])
            ]).tail(200)
        except:
            st.session_state.ccc_history = pd.Series([ccc_value], index=[datetime.now()])
        
        research_data["breadth"] = {
            "ccc_value": ccc_value,
            "constituents_count": len(constituents_df),
            "positive_constituents": len(constituents_df[constituents_df["price_change"] > 0]) if not constituents_df.empty else 0
        }
        
        # ------------------------------
        # 4. OPTION INSTRUMENT DISCOVERY
        # ------------------------------
        print("üîç Fetching option instruments...")
        try:
            if not option_keys:
                expiry_str = config["expiry_date"].strftime("%Y-%m-%d")
                option_keys = get_option_keys(
                    underlying=config["underlying"],
                    expiry=expiry_str,
                    max_keys=config.get("max_option_keys", 200)
                )
                st.session_state.option_keys = option_keys
                print(f"‚úÖ Found {len(option_keys)} option keys")
        except Exception as e:
            print(f"‚ö†Ô∏è Option discovery error: {e}")
            option_keys = []
            st.session_state.option_keys = []
        
        if not option_keys:
            print("‚ö†Ô∏è No option keys found")
            return research_data  # Return what we have so far
        
        # ------------------------------
        # 5. ENHANCED OPTION CHAIN ANALYSIS
        # ------------------------------
        print("üìä Analyzing option chain...")
        option_analytics = client.fetch_option_chain_with_analytics(
            option_keys,
            ltp
        )
        
        if not option_analytics or "raw_data" not in option_analytics:
            print("‚ùå Failed to fetch option analytics")
            return research_data  # Return what we have so far
        
        option_chain_df = option_analytics["raw_data"]
        analytics = option_analytics.get("analytics", {})
        insights = option_analytics.get("market_insights", [])
        
        research_data["options"] = {
            "analytics": analytics,
            "insights": insights,
            "chain_size": len(option_chain_df),
            "put_call_ratio": analytics.get("put_call_ratio", 1.0) if isinstance(analytics.get("put_call_ratio"), (int, float)) else 1.0
        }
        
        print(f"‚úÖ Option chain analyzed: {len(option_chain_df)} strikes, PCR: {analytics.get('put_call_ratio', 'N/A')}")
        
        # Store analytics for visualization
        if analytics:
            try:
                if "research_analytics" not in st.session_state:
                    st.session_state.research_analytics = []
                
                st.session_state.research_analytics.append({
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    **analytics
                })
                
                # Keep only recent analytics
                if len(st.session_state.research_analytics) > 100:
                    st.session_state.research_analytics = st.session_state.research_analytics[-100:]
                
                # Update market regime
                st.session_state.market_regime = analytics.get("market_regime", "UNKNOWN")
                
                # Store in database
                insert_research_analytics(analytics)
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error storing analytics: {e}")
        
        # ------------------------------
        # 6. FEATURE PIPELINE
        # ------------------------------
        print("‚öôÔ∏è Running feature pipeline...")
        try:
            snapshot_ts = pd.Timestamp.utcnow().floor("ms")
            
            feature_result = build_and_store_features(
                timestamp=snapshot_ts,
                option_chain_df=option_chain_df,
                spot_price=ltp,
                expiry_datetime=config["expiry_date"],
                
                ltp=ltp,
                vwap=open_price,
                price_series=st.session_state.price_series,
                volume_series=st.session_state.volume_series,
                
                constituents_df=constituents_df,
                ccc_history=st.session_state.ccc_history,
                
                # Enhanced parameters
                client=client,
                option_keys=option_keys
            )
            
            if feature_result:
                research_data["features"] = feature_result.get("features", {})
                research_data["feature_summary"] = create_feature_summary(feature_result["features"])
                print("‚úÖ Feature pipeline completed")
            else:
                print("‚ö†Ô∏è Feature pipeline returned empty result")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Feature pipeline error: {e}")
            import traceback
            traceback.print_exc()
        
        # ------------------------------
        # 7. SIGNAL GENERATION
        # ------------------------------
        print("üéØ Checking for signals...")
        try:
            feature_row = fetch_latest_features(FEATURE_VERSION)
            if feature_row is not None and not feature_row.empty:
                # Check if signal already exists
                timestamp_str = feature_row.iloc[0]["timestamp"]
                
                if not signal_exists(timestamp_str):
                    # Generate signal
                    signal_engine: SignalStateMachine = st.session_state.signal_engine
                    signal = signal_engine.build_signal(feature_row.iloc[0])
                    
                    # Validate signal
                    is_valid, reason = SignalValidator.validate_signal(signal, feature_row.iloc[0])
                    
                    if is_valid and signal["signal_type"] != "NEUTRAL":
                        # Add research context
                        signal["research_validation"] = {
                            "is_valid": is_valid,
                            "reason": reason,
                            "timestamp": datetime.now(timezone.utc).isoformat()
                        }
                        
                        # Insert signal
                        insert_signal(signal)
                        
                        research_data["signal"] = {
                            "generated": True,
                            "type": signal["signal_type"],
                            "confidence": signal["confidence"],
                            "strength": signal.get("signal_strength", "UNKNOWN")
                        }
                        print(f"‚úÖ Signal generated: {signal['signal_type']} ({signal['confidence']:.0%})")
                    else:
                        research_data["signal"] = {
                            "generated": False,
                            "reason": reason if not is_valid else "NEUTRAL signal"
                        }
                        print(f"‚ÑπÔ∏è No valid signal: {reason if not is_valid else 'NEUTRAL'}")
            else:
                print("‚ÑπÔ∏è No features available for signal generation")
        except Exception as e:
            print(f"‚ö†Ô∏è Signal generation error: {e}")
        
        # ------------------------------
        # 8. SIGNAL LIFECYCLE MANAGEMENT
        # ------------------------------
        try:
            validate_new_signals(confidence_threshold=0.2)
            expire_old_signals()
        except Exception as e:
            print(f"‚ö†Ô∏è Signal lifecycle error: {e}")
        
        # ------------------------------
        # 9. TRAP DETECTION
        # ------------------------------
        if analytics and "potential_traps" in analytics:
            traps = analytics["potential_traps"]
            if traps:
                try:
                    if "trap_detections" not in st.session_state:
                        st.session_state.trap_detections = []
                    
                    for trap in traps:
                        if trap.get("confidence", 0) > 0.6:
                            st.session_state.trap_detections.append({
                                "timestamp": datetime.now(timezone.utc).isoformat(),
                                "trap": trap,
                                "market_regime": analytics.get("market_regime")
                            })
                    
                    # Keep only recent traps
                    if len(st.session_state.trap_detections) > 20:
                        st.session_state.trap_detections = st.session_state.trap_detections[-20:]
                    
                    research_data["traps"] = traps
                    
                    if traps:
                        print(f"üéØ Detected {len(traps)} potential traps")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Trap detection error: {e}")
        
        # ------------------------------
        # 10. SUCCESSFUL COMPLETION
        # ------------------------------
        print("‚úÖ Research cycle completed successfully")
        return research_data
        
    except Exception as e:
        print(f"‚ùå Critical error in research cycle: {e}")
        import traceback
        traceback.print_exc()
        return None

# ==============================
# ENHANCED AUTO-REFRESH SYSTEM
# ==============================

class AutoRefreshManager:
    """Robust auto-refresh manager with error handling and state persistence"""
    
    def __init__(self, scheduler, config, nifty_weights, option_keys):
        self.scheduler = scheduler
        self.config = config
        self.nifty_weights = nifty_weights
        self.option_keys = option_keys
        self.last_execution_time = None
        self.execution_count = 0
        self.errors = []
        self.max_retries = 3
        self.retry_delay = 5
        
    def should_execute(self):
        """Determine if execution should run based on time and market conditions"""
        now = datetime.now()
        
        # Check market hours
        if not self.scheduler._is_market_open(now):
            return False, "Market closed"
        
        # Check if first run
        if self.last_execution_time is None:
            return True, "First execution"
        
        # Check time since last execution
        time_since_last = (now - self.last_execution_time).total_seconds()
        
        # Debug log to see what's happening
        print(f"üïí Auto-refresh timing: {int(time_since_last)}s since last, interval: {self.scheduler.interval_seconds}s")
        
        if time_since_last >= self.scheduler.interval_seconds:
            return True, f"Interval reached ({int(time_since_last)}s > {self.scheduler.interval_seconds}s)"
        
        return False, f"Not due yet ({int(self.scheduler.interval_seconds - time_since_last)}s remaining)"
    
    def execute_with_retry(self):
        """Execute research cycle with retry logic"""
        for attempt in range(self.max_retries):
            try:
                print(f"üîÑ Attempt {attempt + 1}/{self.max_retries} to execute research cycle")
                
                result = execute_research_cycle(
                    config=self.config,
                    nifty_weights=self.nifty_weights,
                    option_keys=self.option_keys
                )
                
                if result:
                    self.last_execution_time = datetime.now()
                    self.execution_count += 1
                    return True, result, f"Execution successful (attempt {attempt + 1})"
                else:
                    return False, None, "Research cycle returned no data"
                    
            except Exception as e:
                error_msg = f"Attempt {attempt + 1} failed: {str(e)}"
                self.errors.append({
                    "timestamp": datetime.now(),
                    "attempt": attempt + 1,
                    "error": str(e)
                })
                
                if attempt < self.max_retries - 1:
                    print(f"‚è∏Ô∏è Retrying in {self.retry_delay}s...")
                    time.sleep(self.retry_delay)
                else:
                    return False, None, f"All retries failed. Last error: {str(e)}"
        
        return False, None, "Max retries exceeded"
    
    def get_status(self):
        """Get current status of auto-refresh manager"""
        status = {
            "last_execution": self.last_execution_time.strftime("%H:%M:%S") if self.last_execution_time else "Never",
            "execution_count": self.execution_count,
            "is_market_open": self.scheduler._is_market_open(datetime.now()),
            "interval_seconds": self.scheduler.interval_seconds,
            "error_count": len(self.errors)
        }
        
        if self.last_execution_time:
            time_since = (datetime.now() - self.last_execution_time).total_seconds()
            status["time_since_last"] = f"{int(time_since)}s"
            status["time_until_next"] = f"{max(0, int(self.scheduler.interval_seconds - time_since))}s"
        
        return status

# Initialize auto-refresh manager
if "auto_refresh_manager" not in st.session_state:
    st.session_state.auto_refresh_manager = AutoRefreshManager(
        scheduler=st.session_state.scheduler,
        config=st.session_state.config,
        nifty_weights=st.session_state.nifty_weights,
        option_keys=st.session_state.option_keys
    )

# Execute auto-refresh
def execute_auto_refresh():
    """Main auto-refresh execution function"""
    manager = st.session_state.auto_refresh_manager
    
    try:
        # Check if we should execute
        should_execute, reason = manager.should_execute()
        
        if should_execute:
            print(f"üèÉ‚Äç‚ôÇÔ∏è Auto-execution triggered: {reason}")
            
            # Execute with retry logic
            success, result, message = manager.execute_with_retry()
            
            if success:
                print(f"‚úÖ {message}")
                
                # Update UI state if needed
                if result and "options" in result and "analytics" in result["options"]:
                    analytics = result["options"]["analytics"]
                    if analytics:
                        # Update market regime
                        st.session_state.market_regime = analytics.get("market_regime", "UNKNOWN")
                        
                        # Store analytics
                        if "research_analytics" not in st.session_state:
                            st.session_state.research_analytics = []
                        st.session_state.research_analytics.append({
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                            **analytics
                        })
                        # Keep only recent analytics
                        if len(st.session_state.research_analytics) > 100:
                            st.session_state.research_analytics = st.session_state.research_analytics[-100:]
                
                return True, message
            else:
                print(f"‚ùå Auto-execution failed: {message}")
                return False, message
        else:
            # Log status periodically
            if manager.last_execution_time:
                time_since = (datetime.now() - manager.last_execution_time).total_seconds()
                if int(time_since) % 10 == 0:  # Log every 10 seconds
                    print(f"‚è±Ô∏è Auto-refresh status: {reason}")
            return None, reason
            
    except Exception as e:
        error_msg = f"Auto-refresh system error: {str(e)}"
        print(f"üî• {error_msg}")
        manager.errors.append({
            "timestamp": datetime.now(),
            "error": error_msg
        })
        return False, error_msg

# Run auto-refresh
execute_auto_refresh()

# ==============================
# ENHANCED BACKGROUND SCHEDULER
# ==============================

class BackgroundScheduler:
    """Robust background scheduler with error handling and monitoring"""
    
    def __init__(self, check_interval=5):
        self.check_interval = check_interval
        self.running = False
        self.thread = None
        self.errors = []
        self.last_check = None
        self.health_checks = []
        
    def health_check(self):
        """Perform system health check"""
        checks = []
        
        # Check scheduler
        try:
            scheduler = st.session_state.scheduler
            checks.append({
                "component": "Scheduler",
                "status": "HEALTHY" if scheduler else "UNKNOWN",
                "details": f"Interval: {scheduler.interval_seconds}s" if scheduler else "Not initialized"
            })
        except Exception as e:
            checks.append({
                "component": "Scheduler",
                "status": "ERROR",
                "details": str(e)
            })
        
        # Check auto-refresh manager
        try:
            manager = st.session_state.auto_refresh_manager
            checks.append({
                "component": "AutoRefresh",
                "status": "HEALTHY",
                "details": f"Executions: {manager.execution_count}"
            })
        except Exception as e:
            checks.append({
                "component": "AutoRefresh",
                "status": "ERROR",
                "details": str(e)
            })
        
        self.health_checks = checks
        return checks
    
    def start(self):
        """Start background scheduler thread"""
        if self.running:
            print("‚ö†Ô∏è Background scheduler already running")
            return
        
        def background_task():
            print("üöÄ Starting enhanced background scheduler...")
            self.running = True
            
            while self.running:
                try:
                    self.last_check = datetime.now()
                    
                    # Perform health check every minute
                    if not self.health_checks or (datetime.now() - self.last_check).seconds > 60:
                        self.health_check()
                    
                    # Log status every 30 seconds
                    current_time = datetime.now()
                    if current_time.second % 30 == 0:
                        manager = st.session_state.auto_refresh_manager
                        if manager.last_execution_time:
                            time_since = (current_time - manager.last_execution_time).total_seconds()
                            print(f"üìä Background monitor: Last execution {int(time_since)}s ago, {manager.execution_count} total runs")
                    
                    time.sleep(self.check_interval)
                    
                except Exception as e:
                    error_msg = f"Background scheduler error: {str(e)}"
                    print(f"üî• {error_msg}")
                    self.errors.append({
                        "timestamp": datetime.now(),
                        "error": error_msg
                    })
                    time.sleep(30)  # Wait longer on error
        
        self.thread = threading.Thread(target=background_task, daemon=True)
        self.thread.start()
        print("‚úÖ Enhanced background scheduler started")
    
    def stop(self):
        """Stop background scheduler"""
        self.running = False
        if self.thread:
            self.thread.join(timeout=5)
        print("üõë Background scheduler stopped")
    
    def get_status(self):
        """Get scheduler status"""
        return {
            "running": self.running,
            "check_interval": self.check_interval,
            "last_check": self.last_check.strftime("%H:%M:%S") if self.last_check else "Never",
            "error_count": len(self.errors),
            "health_checks": self.health_checks
        }

# Initialize and start background scheduler
if "background_scheduler" not in st.session_state:
    st.session_state.background_scheduler = BackgroundScheduler(check_interval=5)
    st.session_state.background_scheduler.start()


def get_scheduler_status():
    """Get comprehensive scheduler status"""
    scheduler = st.session_state.scheduler
    manager = st.session_state.auto_refresh_manager
    
    status = {
        "scheduler_active": True,
        "interval_seconds": scheduler.interval_seconds,
        "total_executions": scheduler._total_executions,
        "last_run": scheduler._last_run.strftime("%H:%M:%S") if scheduler._last_run else "Never",
        "is_market_open": scheduler._is_market_open(datetime.now()),
        "auto_refresh_status": manager.get_status()
    }
    
    # Calculate next run time
    if scheduler._last_run:
        next_run = scheduler._last_run + timedelta(seconds=scheduler.interval_seconds)
        time_until_next = (next_run - datetime.now()).total_seconds()
        status["next_run"] = next_run.strftime("%H:%M:%S") if time_until_next > 0 else "Now"
        status["time_until_next"] = f"{max(0, int(time_until_next))}s"
    else:
        status["next_run"] = "Pending"
        status["time_until_next"] = "N/A"
    
    return status

# ==============================
# CREATE TABS
# ==============================

# Create tabs for different views
tab1, tab2, tab3, tab4 = st.tabs([
    "üìà Live Analytics", 
    "üéØ Signals", 
    "üîç Research", 
    "‚öôÔ∏è Configuration"
])

# ==============================
# AUTO-REFRESH WORKING SOLUTION
# ==============================

# Create a placeholder for auto-refresh
refresh_placeholder = st.empty()

# Check if we need to auto-refresh
if "next_refresh_time" not in st.session_state:
    st.session_state.next_refresh_time = time.time() + 30  # First refresh in 30 seconds

current_time = time.time()

if current_time >= st.session_state.next_refresh_time:
    # Time to refresh!
    print(f"üîÑ AUTO-REFRESH EXECUTING at {datetime.now().strftime('%H:%M:%S')}")
    
    # Execute research cycle
    with st.spinner("Auto-refreshing data..."):
        research_data = execute_research_cycle()
    
    if research_data:
        print(f"‚úÖ Auto-refresh completed successfully")
        
        # Update next refresh time
        st.session_state.next_refresh_time = current_time + 30
        
        # Update state
        st.session_state.scheduler._last_run = datetime.now()
        st.session_state.scheduler._total_executions += 1
        st.session_state.auto_refresh_manager.last_execution_time = datetime.now()
        st.session_state.auto_refresh_manager.execution_count += 1
        
        # Show success message
        refresh_placeholder.success(f"‚úÖ Auto-refresh completed at {datetime.now().strftime('%H:%M:%S')}")
        time.sleep(1)
        refresh_placeholder.empty()
        
        # Force a rerun to update UI
        st.rerun()
    else:
        print("‚ùå Auto-refresh failed")
        refresh_placeholder.error("Auto-refresh failed")
        time.sleep(2)
        refresh_placeholder.empty()
else:
    # Show countdown
    time_until_next = st.session_state.next_refresh_time - current_time
    if time_until_next > 0:
        print(f"‚è±Ô∏è Next auto-refresh in {int(time_until_next)}s")
        
        # Update countdown every 10 seconds
        if int(time_until_next) % 10 == 0:
            refresh_placeholder.info(f"Next auto-refresh in {int(time_until_next)} seconds...")


# ==============================
# TAB 1: LIVE ANALYTICS
# ==============================

with tab1:
    # Add JavaScript auto-refresh
    auto_refresh_js = """
    <script>
    // Auto-refresh every 30 seconds
    setTimeout(function() {
        window.location.reload();
    }, 30000);
    </script>
    """
    st.components.v1.html(auto_refresh_js, height=0)
    
    st.markdown('<div class="sub-header">üìä Live Market Analytics</div>', unsafe_allow_html=True)
   
    
    # Execute cycle - but this is just for UI display now
    scheduler: MarketScheduler = st.session_state.scheduler
    
    # Track last execution time
    if "last_execution_time" not in st.session_state:
        st.session_state.last_execution_time = None
    
    # Display background scheduler status
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if scheduler._last_run:
            time_since_last = (datetime.now() - scheduler._last_run).total_seconds()
            status_text = "‚úÖ Running" if time_since_last < 60 else "‚ö†Ô∏è Stale"
            st.metric("Scheduler", status_text, delta=f"{int(time_since_last)}s ago")
        else:
            st.metric("Scheduler", "‚è≥ Starting", delta="Initializing")
    
    with col2:
        if st.session_state.price_series.size > 0:
            current_price = st.session_state.price_series.iloc[-1]
            prev_price = st.session_state.price_series.iloc[-2] if st.session_state.price_series.size > 1 else current_price
            change_pct = ((current_price - prev_price) / prev_price * 100) if prev_price > 0 else 0
            st.metric("Nifty 50", f"{current_price:,.0f}", f"{change_pct:+.2f}%")
    
    with col3:
        if st.session_state.research_analytics:
            latest = st.session_state.research_analytics[-1]
            regime = latest.get("market_regime", "UNKNOWN")
            st.metric("Market Regime", regime)
    
    with col4:
        if st.session_state.research_analytics:
            latest = st.session_state.research_analytics[-1]
            oi_vel = latest.get("oi_velocity", 0)
            st.metric("OI Velocity", f"{oi_vel:+.2f}œÉ")
    
    # Manual execution button - still works
    if st.button("üîÑ Execute Research Cycle Now", type="primary", width='stretch'):
        with st.spinner("Executing research cycle..."):
            research_data = execute_research_cycle()
            if research_data:
                st.success("‚úÖ Research cycle executed successfully")
                
                # Display insights if available
                if "options" in research_data and "insights" in research_data["options"]:
                    for insight in research_data["options"]["insights"][:3]:
                        st.info(insight)
            else:
                st.error("‚ùå Research cycle failed")
    
    st.markdown("---")
    
    # Display enhanced status
    st.markdown("---")
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        status = get_scheduler_status()
        if status["scheduler_active"]:
            if status["last_run"] != "Never":
                time_since = (datetime.now() - st.session_state.scheduler._last_run).total_seconds() if st.session_state.scheduler._last_run else 999
                if time_since < 60:
                    st.metric("Scheduler", "‚úÖ Active", delta=f"{int(time_since)}s ago")
                else:
                    st.metric("Scheduler", "‚ö†Ô∏è Stale", delta=f"{int(time_since)}s ago")
            else:
                st.metric("Scheduler", "‚è≥ Initializing", delta="First run pending")
        else:
            st.metric("Scheduler", "‚ùå Inactive", delta="Check configuration")

    with col2:
        manager_status = st.session_state.auto_refresh_manager.get_status()
        st.metric("Auto Refresh", f"Run #{manager_status['execution_count']}", 
                delta=manager_status['last_execution'])

    with col3:
        if manager_status.get('time_until_next'):
            st.metric("Next Run In", manager_status['time_until_next'])

    with col4:
        error_count = manager_status.get('error_count', 0)
        if error_count == 0:
            st.metric("Errors", "‚úÖ 0", delta="Clean")
        else:
            st.metric("Errors", f"‚ö†Ô∏è {error_count}", delta="Check logs")

    # Control buttons
    col1, col2, col3 = st.columns(3)

    with col1:
        if st.button("üîÑ Force Run Now", type="primary", width='stretch'):
            # Force immediate execution
            st.session_state.auto_refresh_manager.last_execution_time = None
            st.rerun()

    with col2:
        if st.button("‚è∏Ô∏è Pause Auto-Refresh", width='stretch'):
            # Temporarily disable auto-refresh
            original_interval = st.session_state.scheduler.interval_seconds
            st.session_state.scheduler.interval_seconds = 3600  # 1 hour
            st.success(f"Auto-refresh paused. Original interval: {original_interval}s")
            st.rerun()

    with col3:
        if st.button("‚ñ∂Ô∏è Resume Auto-Refresh", width='stretch'):
            # Restore auto-refresh
            st.session_state.scheduler.interval_seconds = 30
            st.success("Auto-refresh resumed (30s interval)")
            st.rerun()
    # Display background thread info
    with st.expander("üîÑ Background Scheduler Info", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            if "scheduler_thread" in st.session_state:
                thread = st.session_state.scheduler_thread
                st.success("‚úÖ Background thread active")
                st.info(f"Thread ID: {thread.ident}")
                st.info(f"Thread alive: {thread.is_alive()}")
            else:
                st.error("‚ùå Background thread not running")
        
        with col2:
            if scheduler._last_run:
                st.metric("Last Execution", scheduler._last_run.strftime("%H:%M:%S"))
                time_since = (datetime.now() - scheduler._last_run).total_seconds()
                st.metric("Time Since", f"{int(time_since)}s")
            else:
                st.metric("Last Execution", "Never")
            
            st.metric("Total Executions", scheduler._total_executions)
    
    
    # Display latest analytics if available
    if st.session_state.research_analytics:
        latest_analytics = st.session_state.research_analytics[-1]
        
        # Create columns for analytics display
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### üìà OI & Gamma Analysis")
            
            # OI Velocity gauge
            oi_velocity = latest_analytics.get("oi_velocity", 0)
            fig_oi = go.Figure(go.Indicator(
                mode="gauge+number+delta",
                value=oi_velocity,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "OI Velocity (œÉ)"},
                delta={'reference': 0},
                gauge={
                    'axis': {'range': [-3, 3]},
                    'steps': [
                        {'range': [-3, -1.5], 'color': "lightgray"},
                        {'range': [-1.5, 1.5], 'color': "gray"},
                        {'range': [1.5, 3], 'color': "lightgray"}
                    ],
                    'threshold': {
                        'line': {'color': "red", 'width': 4},
                        'thickness': 0.75,
                        'value': oi_velocity
                    }
                }
            ))
            fig_oi.update_layout(height=250)
            st.plotly_chart(fig_oi, width='stretch')
        
        with col2:
            st.markdown("#### üß± Structure Analysis")
            
            # Structural metrics
            wall_strength = latest_analytics.get("wall_strength", 0)
            trap_prob = latest_analytics.get("trap_probability", 0)
            
            col_a, col_b = st.columns(2)
            with col_a:
                st.metric("Wall Strength", f"{wall_strength:.2f}")
            with col_b:
                st.metric("Trap Probability", f"{trap_prob:.2%}")
            
            # Display traps if any
            traps = latest_analytics.get("potential_traps", [])
            if traps:
                st.markdown("##### üéØ Detected Traps")
                for trap in traps[:2]:  # Show max 2
                    direction = trap.get("direction", "").upper()
                    confidence = trap.get("confidence", 0)
                    strike = trap.get("strike", 0)
                    
                    if confidence > 0.6:
                        st.markdown(f"""
                        <div class="trap-alert">
                            <strong>{direction} Trap</strong> at {strike:,.0f}<br>
                            Confidence: {confidence:.0%}<br>
                            Unwinding: {trap.get('unwinding_rate', 0):+.1f}%
                        </div>
                        """, unsafe_allow_html=True)
        
        # Price chart
        st.markdown("---")
        st.markdown("#### üìâ Price & Indicators")
        
        if len(st.session_state.price_series) > 1:
            # Create subplot for price and indicators
            fig = make_subplots(
                rows=3, cols=1,
                shared_xaxes=True,
                vertical_spacing=0.05,
                subplot_titles=("Nifty 50 Price", "OI Velocity", "Gamma Exposure"),
                row_heights=[0.5, 0.25, 0.25]
            )
            
            # Price
            price_data = st.session_state.price_series.reset_index(drop=True)
            fig.add_trace(
                go.Scatter(
                    y=price_data.values,
                    mode='lines',
                    name='Price',
                    line=dict(color='#3B82F6', width=2)
                ),
                row=1, col=1
            )
            
            # OI Velocity (if available in analytics history)
            if len(st.session_state.research_analytics) > 1:
                oi_velocities = [a.get("oi_velocity", 0) for a in st.session_state.research_analytics[-50:]]
                fig.add_trace(
                    go.Scatter(
                        y=oi_velocities,
                        mode='lines',
                        name='OI Velocity',
                        line=dict(color='#10B981', width=1)
                    ),
                    row=2, col=1
                )
                
                # Add zero line
                fig.add_hline(y=0, line_dash="dash", line_color="gray", row=2, col=1)
            
            # Gamma (if available)
            if len(st.session_state.research_analytics) > 1:
                gamma_values = [a.get("gamma_exposure", {}).get("net_gamma", 0) for a in st.session_state.research_analytics[-50:]]
                fig.add_trace(
                    go.Scatter(
                        y=gamma_values,
                        mode='lines',
                        name='Net Gamma',
                        line=dict(color='#8B5CF6', width=1)
                    ),
                    row=3, col=1
                )
                
                # Add zero line
                fig.add_hline(y=0, line_dash="dash", line_color="gray", row=3, col=1)
            
            fig.update_layout(height=600, showlegend=True)
            st.plotly_chart(fig, width='stretch')

# ==============================
# TAB 2: SIGNALS
# ==============================

with tab2:
    st.markdown('<div class="sub-header">üéØ Trading Signals</div>', unsafe_allow_html=True)
    
    # Fetch active signals
    active_signals = fetch_active_signals(limit=10)
    
    if not active_signals.empty:
        # Display signals in columns
        for _, signal in active_signals.iterrows():
            col1, col2, col3, col4 = st.columns([2, 1, 1, 1])
            
            with col1:
                signal_type = signal["signal_type"]
                color = "green" if signal_type == "BUY" else "red" if signal_type == "SELL" else "gray"
                st.markdown(f"### <span style='color:{color};'>{signal_type}</span>", unsafe_allow_html=True)
                
                # Rationale
                if pd.notna(signal["rationale"]):
                    st.caption(signal["rationale"])
            
            with col2:
                confidence = signal["confidence"]
                st.metric("Confidence", f"{confidence:.0%}")
            
            with col3:
                strength = signal.get("signal_strength", "UNKNOWN")
                st.metric("Strength", strength)
            
            with col4:
                status = signal["status"]
                badge_color = "blue" if status == "NEW" else "green" if status == "VALIDATED" else "orange"
                st.markdown(f"<div style='padding: 5px 10px; background-color:{badge_color}; color:white; border-radius:5px; text-align:center;'>{status}</div>", 
                           unsafe_allow_html=True)
            
            # Research context (expandable)
            with st.expander("Research Details"):
                if pd.notna(signal.get("research_context")):
                    try:
                        context = json.loads(signal["research_context"]) if isinstance(signal["research_context"], str) else signal["research_context"]
                        st.json(context)
                    except:
                        st.write("No research context available")
            
            st.markdown("---")
        
        # Performance metrics
        st.markdown("#### üìä Signal Performance")
        performance = fetch_signal_performance(days=7)
        
        if performance:
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Total Signals", performance.get("total_signals", 0))
            
            with col2:
                st.metric("Profitable", performance.get("profitable_signals", 0))
            
            with col3:
                win_rate = performance.get("win_rate", 0)
                st.metric("Win Rate", f"{win_rate:.1f}%")
            
            with col4:
                period = performance.get("period_days", 7)
                st.metric("Period", f"{period} days")
        
        # Signal distribution chart
        if not active_signals.empty:
            st.markdown("#### üìà Signal Distribution")
            
            fig = go.Figure(data=[
                go.Pie(
                    labels=active_signals["signal_type"].value_counts().index,
                    values=active_signals["signal_type"].value_counts().values,
                    hole=.3
                )
            ])
            
            fig.update_layout(
                height=300,
                showlegend=True,
                margin=dict(t=0, b=0, l=0, r=0)
            )
            
            st.plotly_chart(fig, width='stretch')
    
    else:
        st.info("üì≠ No active signals. The system will generate signals during market hours.")

# ==============================
# TAB 3: RESEARCH
# ==============================

with tab3:
    st.markdown('<div class="sub-header">üîç Research & Analytics</div>', unsafe_allow_html=True)
    
    # Research metrics
    if st.session_state.research_analytics:
        latest = st.session_state.research_analytics[-1]
        
        # Key research metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            net_gamma = latest.get("gamma_exposure", {}).get("net_gamma", 0)
            st.metric("Net Gamma", f"{net_gamma:,.0f}")
        
        with col2:
            pcr = latest.get("put_call_ratio", 1.0)
            st.metric("Put/Call Ratio", f"{pcr:.2f}")
        
        with col3:
            divergence = latest.get("divergence_score", 0)
            st.metric("Divergence Score", f"{divergence:.2f}")
        
        with col4:
            regime = latest.get("market_regime", "UNKNOWN")
            st.metric("Market Regime", regime)
        
        # Display market insights
        st.markdown("#### üí° Market Insights")
        
        insights = latest.get("market_insights", [])
        if insights:
            for insight in insights[:5]:
                if "trap" in insight.lower():
                    st.markdown(f'<div class="trap-alert">{insight}</div>', unsafe_allow_html=True)
                elif "divergence" in insight.lower():
                    st.markdown(f'<div class="divergence-alert">{insight}</div>', unsafe_allow_html=True)
                else:
                    st.markdown(f'<div class="research-insight">{insight}</div>', unsafe_allow_html=True)
        else:
            st.info("No insights available yet")
        
        # Structural walls
        st.markdown("#### üß± Structural Walls")
        
        walls = latest.get("structural_walls", [])
        if walls:
            walls_df = pd.DataFrame(walls)
            st.dataframe(walls_df, width='stretch')
        
        # Trap detection history
        st.markdown("#### üéØ Trap Detection History")
        
        if st.session_state.trap_detections:
            traps_df = pd.DataFrame([
                {
                    "Time": pd.to_datetime(t["timestamp"]).strftime("%H:%M:%S"),
                    "Type": t["trap"].get("direction", ""),
                    "Strike": t["trap"].get("strike", 0),
                    "Confidence": t["trap"].get("confidence", 0),
                    "Regime": t.get("market_regime", "")
                }
                for t in st.session_state.trap_detections
            ])
            
            if not traps_df.empty:
                st.dataframe(traps_df, width='stretch')
        
        # Research analytics over time
        st.markdown("#### üìä Analytics Timeline")
        
        if len(st.session_state.research_analytics) > 5:
            # Convert to DataFrame for plotting
            analytics_df = pd.DataFrame(st.session_state.research_analytics[-50:])
            
            if "timestamp" in analytics_df.columns:
                analytics_df["time"] = pd.to_datetime(analytics_df["timestamp"]).dt.strftime("%H:%M")
                
                # Plot OI Velocity and Gamma over time
                fig = make_subplots(
                    rows=2, cols=1,
                    shared_xaxes=True,
                    subplot_titles=("OI Velocity", "Net Gamma"),
                    vertical_spacing=0.1
                )
                
                if "oi_velocity" in analytics_df.columns:
                    fig.add_trace(
                        go.Scatter(
                            x=analytics_df["time"],
                            y=analytics_df["oi_velocity"],
                            mode='lines+markers',
                            name='OI Velocity',
                            line=dict(color='#10B981')
                        ),
                        row=1, col=1
                    )
                
                if "gamma_exposure" in analytics_df.columns:
                    # Extract net_gamma from gamma_exposure dict
                    try:
                        gamma_values = []
                        for g in analytics_df["gamma_exposure"]:
                            if isinstance(g, dict) and "net_gamma" in g:
                                gamma_values.append(g["net_gamma"])
                            else:
                                gamma_values.append(0)
                        
                        fig.add_trace(
                            go.Scatter(
                                x=analytics_df["time"],
                                y=gamma_values,
                                mode='lines+markers',
                                name='Net Gamma',
                                line=dict(color='#8B5CF6')
                            ),
                            row=2, col=1
                        )
                    except:
                        pass
                
                fig.update_layout(height=400, showlegend=True)
                st.plotly_chart(fig, width='stretch')

# ==============================
# TAB 4: CONFIGURATION
# ==============================

with tab4:
    st.markdown('<div class="sub-header">‚öôÔ∏è System Configuration</div>', unsafe_allow_html=True)
    
    # Available indices and underlyings
    available_indices = [
        "NSE_INDEX|Nifty 50",
        "NSE_INDEX|Nifty Bank", 
        "NSE_INDEX|Nifty Fin Service",
        "NSE_INDEX|Nifty Midcap 100",
        "NSE_INDEX|Nifty Next 50",
        "BSE_INDEX|S&P BSE SENSEX"  
    ]
    
    available_underlyings = [
        "NIFTY",
        "BANKNIFTY", 
        "FINNIFTY",
        "MIDCPNIFTY",
        "SENSEX"  
    ]
    
    # Get available expiries for the selected underlying
    current_underlying = st.session_state.config["underlying"]
    from data.instrument_master import get_available_expiries
    
    try:
        available_expiries = get_available_expiries(current_underlying)
        if not available_expiries:
            available_expiries = ["2026-01-27", "2026-02-03", "2026-02-10"]
    except:
        available_expiries = ["2026-01-27", "2026-02-03", "2026-02-10"]
    
    # Configuration form
    with st.form("configuration_form"):
        col1, col2 = st.columns(2)
        
        with col1:
            # Index Symbol Dropdown
            index_symbol = st.selectbox(
                "Index Symbol", 
                options=available_indices,
                index=available_indices.index(st.session_state.config["index_symbol"]) 
                if st.session_state.config["index_symbol"] in available_indices else 0
            )
            
            # Underlying Symbol Dropdown
            underlying = st.selectbox(
                "Underlying Symbol", 
                options=available_underlyings,
                index=available_underlyings.index(st.session_state.config["underlying"])
                if st.session_state.config["underlying"] in available_underlyings else 0
            )
            
            # Expiry Date Dropdown
            # Find today's date in available_expiries
            today_str = datetime.now().strftime("%Y-%m-%d")
            default_index = 0

            if today_str in available_expiries:
                default_index = available_expiries.index(today_str)
            elif today_str == "2026-01-28":  # Today's date
                # Add today to available expiries if it's not there
                available_expiries.insert(0, today_str)
                default_index = 0

            expiry_date = st.selectbox(
                "Expiry Date",
                options=available_expiries,
                index=default_index  # ‚úÖ Now selects today if available
            )
            
            # Convert selected expiry to datetime
            if expiry_date:
                try:
                    expiry_datetime = datetime.strptime(expiry_date, "%Y-%m-%d")
                    days_to_expiry = (expiry_datetime.date() - datetime.now().date()).days
                    st.info(f"Days to expiry: {days_to_expiry}")
                except:
                    st.warning("Invalid expiry date format")
        
        with col2:
            max_option_keys = st.number_input(
                "Max Option Keys", 
                min_value=50, 
                max_value=500, 
                value=st.session_state.config["max_option_keys"],
                help="Maximum number of option strikes to fetch"
            )
            
            scheduler_interval = st.slider(
                "Scheduler Interval (seconds)", 
                min_value=10, 
                max_value=300, 
                value=30,
                step=5,
                help="How often to refresh data (10-300 seconds)"
            )
            
            confidence_threshold = st.slider(
                "Signal Confidence Threshold", 
                min_value=0.0, 
                max_value=1.0, 
                value=0.2, 
                step=0.05,
                format="%.2f",
                help="Minimum confidence level for signals (0.0-1.0)"
            )
            
            # Advanced options expander
            with st.expander("Advanced Options"):
                enable_live_trading = st.checkbox(
                    "Enable Live Trading", 
                    value=False,
                    help="WARNING: Enable actual order placement"
                )
                
                risk_per_trade = st.number_input(
                    "Risk per Trade (%)",
                    min_value=0.1,
                    max_value=5.0,
                    value=1.0,
                    step=0.1,
                    format="%.1f",
                    help="Maximum risk per trade as percentage of capital"
                )
        
        # Submit button
        submit_col1, submit_col2, submit_col3 = st.columns([1, 2, 1])
        with submit_col2:
            if st.form_submit_button("üíæ Save Configuration", width='stretch'):
                # Update configuration
                st.session_state.config.update({
                    "index_symbol": index_symbol,
                    "underlying": underlying,
                    "expiry_date": datetime.strptime(expiry_date, "%Y-%m-%d") if expiry_date else datetime.now(timezone.utc) + timedelta(days=3),
                    "max_option_keys": max_option_keys,
                    "enable_live_trading": enable_live_trading,
                    "risk_per_trade": risk_per_trade
                })
                
                # Update scheduler
                st.session_state.scheduler = MarketScheduler(interval_seconds=scheduler_interval)
                
                # Update signal engine
                st.session_state.signal_engine = create_signal_engine(
                    signal_expiry_minutes=5,
                    confidence_threshold=confidence_threshold
                )
                
                # Clear option keys cache
                st.session_state.option_keys = []
                
                st.success("‚úÖ Configuration saved!")
                st.rerun()
    
    st.markdown("---")
    
    # Quick Configuration Presets
    st.markdown("#### üöÄ Quick Presets")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üìä NIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty 50",
                "underlying": "NIFTY"
            })
            st.success("Set to NIFTY Weekly")
            st.rerun()
    
    with col2:
        if st.button("üè¶ BANKNIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty Bank",
                "underlying": "BANKNIFTY"
            })
            st.success("Set to BANKNIFTY Weekly")
            st.rerun()
    
    with col3:
        if st.button("üí≥ FINNIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty Fin Service",
                "underlying": "FINNIFTY"
            })
            st.success("Set to FINNIFTY Weekly")
            st.rerun()
    
    # Current Configuration Display
    st.markdown("---")
    st.markdown("#### üñ•Ô∏è Current Configuration")
    
    config_col1, config_col2 = st.columns(2)
    
    with config_col1:
        st.markdown("**Trading Settings:**")
        st.text(f"Index: {st.session_state.config['index_symbol']}")
        st.text(f"Underlying: {st.session_state.config['underlying']}")
        st.text(f"Expiry: {st.session_state.config['expiry_date'].strftime('%Y-%m-%d')}")
        st.text(f"Max Options: {st.session_state.config['max_option_keys']}")
    
    with config_col2:
        st.markdown("**System Settings:**")
        st.text(f"Scheduler: {st.session_state.scheduler.interval_seconds}s")
        st.text(f"Signal Confidence: {confidence_threshold:.2f}")
        st.text(f"Live Trading: {'Enabled' if st.session_state.config.get('enable_live_trading', False) else 'Disabled'}")
        if st.session_state.config.get('enable_live_trading', False):
            st.text(f"Risk per Trade: {st.session_state.config.get('risk_per_trade', 1.0)}%")
    
    # Diagnostic tools (keep your existing code here)
    st.markdown("---")
    st.markdown("#### üîß Diagnostic Tools")
    # ... keep your existing diagnostic tools code ...
    
    # Diagnostic tools
    st.markdown("---")
    st.markdown("#### üîß Diagnostic Tools")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üß™ Test Upstox Connection", width='stretch'):
            # Test connection by trying to fetch profile
            try:
                profile = client.fetch_profile()
                if profile:
                    st.success(f"‚úÖ Upstox connection working")
                else:
                    st.error("‚ùå Failed to fetch profile")
            except Exception as e:
                st.error(f"‚ùå Connection test failed: {e}")
    
    with col2:
        if st.button("üìä Test Feature Pipeline", width='stretch'):
            try:
                # Get latest features
                features = fetch_latest_features(FEATURE_VERSION)
                if features is not None and not features.empty():
                    st.success(f"‚úÖ Feature pipeline working ({len(features)} features)")
                else:
                    st.warning("‚ö†Ô∏è No features found")
            except Exception as e:
                st.error(f"‚ùå Feature pipeline error: {e}")
    
    with col3:
        if st.button("üîÑ Clear Option Keys Cache", width='stretch'):
            st.session_state.option_keys = []
            st.success("‚úÖ Option keys cache cleared")

# ==============================
# PERFORMANCE MODAL
# ==============================

if st.session_state.get("show_performance", False):
    with st.expander("üìà Performance Analytics", expanded=True):
        st.markdown("### üìà Trading Performance")
        
        performance = fetch_signal_performance(days=30)
        
        if performance:
            # Overall metrics
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Total Trades", performance["total_signals"])
            
            with col2:
                st.metric("Profitable Trades", performance["profitable_signals"])
            
            with col3:
                win_rate = performance["win_rate"]
                color = "normal" if win_rate > 50 else "inverse"
                st.metric("Win Rate", f"{win_rate:.1f}%", delta_color=color)
            
            with col4:
                st.metric("Analysis Period", f"{performance['period_days']} days")
            
            # Breakdown by signal type
            st.markdown("#### Breakdown by Signal Type")
            
            if "breakdown" in performance:
                breakdown_df = pd.DataFrame(performance["breakdown"])
                st.dataframe(breakdown_df, width='stretch')
        
        st.session_state.show_performance = False

# ==============================
# LOGS MODAL
# ==============================

if st.session_state.get("show_logs", False):
    with st.expander("üìã System Logs", expanded=True):
        st.markdown("### üìã Recent System Activity")
        
        # Could fetch from system_health table here
        st.info("Logs would be displayed here from the database.")
        
        st.session_state.show_logs = False

# ==============================
# FOOTER
# ==============================

st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #6B7280; font-size: 0.9rem;'>
    Algorithmic Trading Research Platform v2.0 | 
    Research Features: OI Velocity, Gamma Exposure, Walls/Traps, Divergence Analysis
</div>
""", unsafe_allow_html=True)

# Simple auto-refresh timer
import time

if "last_auto_run" not in st.session_state:
    st.session_state.last_auto_run = time.time()

current_time = time.time()
time_since_last = current_time - st.session_state.last_auto_run

if time_since_last > 30:  # 30 seconds
    print(f"üîÑ Simple timer triggered after {int(time_since_last)}s")
    
    # Execute research cycle
    research_data = execute_research_cycle()
    
    if research_data:
        st.session_state.last_auto_run = current_time
        print(f"‚úÖ Auto-refresh executed successfully")
        
        # Update scheduler's last run time
        st.session_state.scheduler._last_run = datetime.now()
        st.session_state.scheduler._total_executions += 1
        
        # Update auto-refresh manager
        st.session_state.auto_refresh_manager.last_execution_time = datetime.now()
        st.session_state.auto_refresh_manager.execution_count += 1
        
        # Force UI update
        st.rerun()
    else:
        print("‚ùå Auto-refresh failed")
else:
    print(f"‚è±Ô∏è Next auto-refresh in {int(30 - time_since_last)}s")



====================================================================================================
FILE: .\app.py
====================================================================================================


File Name: app.py
Full Path: G:\trading_app\app.py
Size: 74.67 KB
Last Modified: 01/28/2026 15:40:44
Extension: .py

"""
ENHANCED TRADING TOOL - Research-Based Live Feature Engine
Incorporates OI Velocity, Gamma Exposure, Walls/Traps, and Divergence analysis.
"""

import os
import sys

# Fix Conda environment issue
if 'conda' in sys.version.lower():
    try:
        # Try to fix conda path issues
        os.environ['PATH'] = os.path.join(os.environ.get('CONDA_PREFIX', ''), 'bin') + ':' + os.environ['PATH']
    except:
        pass
# ==============================
# STREAMLIT CONFIG FIX - ADD THIS
# ==============================
import os
os.environ['STREAMLIT_SERVER_FILE_WATCHER_TYPE'] = 'none'
import time 
import threading  # <-- ADD THIS LINE
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from datetime import datetime, timezone
import json
from pathlib import Path
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Enhanced core components
from core.session import UpstoxSession, initialize_session, display_session_status
from data.upstox_client import UpstoxClient, MarketAnalytics, display_market_analytics
from data.instrument_master import get_option_keys

try:
    from core.feature_pipeline import build_and_store_features, create_feature_summary
except ImportError as e:
    st.error(f"Feature pipeline import error: {e}")
    # Define dummy functions to prevent crash
    def build_and_store_features(*args, **kwargs):
        return {"error": "Feature pipeline not loaded"}
    def create_feature_summary(*args, **kwargs):
        return {"error": "Feature pipeline not loaded"}
from core.scheduler import MarketScheduler
from core.signals.state_machine import SignalStateMachine, SignalValidator, create_signal_engine
from features.breadth import build_constituents_df
from ml.feature_contract import FEATURE_VERSION

# Enhanced storage
from storage.repository import (
    fetch_latest_features,
    insert_signal,
    signal_exists,
    validate_new_signals,
    expire_old_signals,
    fetch_active_signals,
    fetch_recent_analytics,
    get_database_stats,
    fetch_signal_performance,
    insert_research_analytics
)

# ==============================
# PAGE CONFIGURATION
# ==============================

st.set_page_config(
    page_title="Algorithmic Trading Research Platform",
    page_icon="üìà",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E3A8A;
        font-weight: 700;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #374151;
        font-weight: 600;
        margin-top: 1.5rem;
        margin-bottom: 1rem;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
    }
    .research-insight {
        background: #F0F9FF;
        border-left: 4px solid #3B82F6;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
    .trap-alert {
        background: #FEF3C7;
        border-left: 4px solid #F59E0B;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
    .divergence-alert {
        background: #FEE2E2;
        border-left: 4px solid #EF4444;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
</style>
""", unsafe_allow_html=True)

# ==============================
# APP HEADER
# ==============================

st.markdown('<div class="main-header">üìä Algorithmic Trading Research Platform</div>', unsafe_allow_html=True)
st.markdown("""
<div style='color: #6B7280; margin-bottom: 2rem;'>
    Advanced derivatives analytics with OI Velocity, Gamma Exposure, Structural Walls/Traps, and Spot Divergence detection.
    Version: Research v2.0
</div>
""", unsafe_allow_html=True)

# ==============================
# INITIALIZATION
# ==============================

def initialize_app_state():
    """Initialize application state with research components."""
    
    # Initialize session
    initialize_session()
    
    # Initialize state variables
    if "initialized" not in st.session_state:
        st.session_state.initialized = True
        
        # Market data series
        for key in ["price_series", "volume_series", "ccc_history", "oi_velocity_history", "gamma_history"]:
            if key not in st.session_state:
                st.session_state[key] = pd.Series(dtype=float)
        
        # Research analytics
        if "research_analytics" not in st.session_state:
            st.session_state.research_analytics = []
        
        # Signal engine
        if "signal_engine" not in st.session_state:
            st.session_state.signal_engine = create_signal_engine(
                signal_expiry_minutes=5,
                confidence_threshold=0.2
            )
        
        # Scheduler
        if "scheduler" not in st.session_state:
            st.session_state.scheduler = MarketScheduler(interval_seconds=30)
            # Initialize scheduler properties
            st.session_state.scheduler._last_run = None
            st.session_state.scheduler._total_executions = 0
        
        # Load Nifty weights
        if "nifty_weights" not in st.session_state:
            try:
                with open("config/nifty_weights.json", "r") as f:
                    st.session_state.nifty_weights = json.load(f)
            except:
                st.session_state.nifty_weights = {}
        
        # Instrument master path
        if "instrument_master" not in st.session_state:
            st.session_state.instrument_master = Path("G:/trading_app/data/instruments.json.gz")
        
        # Configuration - SMART EXPIRY SELECTION
        if "config" not in st.session_state:
            from utils.expiry_utils import get_trading_expiry
            
            # Get smart expiry
            underlying = "NIFTY"
            expiry_date_str = get_trading_expiry(underlying)
            
            if not expiry_date_str:
                # Fallback to next available date from instrument master
                from data.instrument_master import get_next_available_expiry
                expiry_date_str = get_next_available_expiry(underlying)
                
                if not expiry_date_str:
                    # Final fallback to a known date
                    expiry_date_str = "2026-02-03"
            
            # Convert string to datetime object
            expiry_date = datetime.strptime(expiry_date_str, "%Y-%m-%d")
            
            st.session_state.config = {
                "index_symbol": "NSE_INDEX|Nifty 50",
                "underlying": underlying,
                "expiry_date": expiry_date,
                "max_option_keys": 200
            }
            
            # Show expiry info
            from utils.expiry_utils import is_market_open
            market_status = "open" if is_market_open() else "closed"
            
            # Get today's date
            today_str = datetime.now().strftime('%Y-%m-%d')
            
            # Show appropriate message
            if expiry_date_str == today_str:
                st.info(f"üìÖ Using TODAY's expiry: {expiry_date_str} (Market: {market_status})")
            else:
                st.info(f"üìÖ Using NEXT expiry: {expiry_date_str} (Today: {today_str}, Market: {market_status})")
        
        # Option keys cache
        if "option_keys" not in st.session_state:
            st.session_state.option_keys = []
        
        # Market regime tracking
        if "market_regime" not in st.session_state:
            st.session_state.market_regime = "UNKNOWN"
        
        # Trap detection history
        if "trap_detections" not in st.session_state:
            st.session_state.trap_detections = []
        
        st.success("‚úÖ Application state initialized with research components")

# Run initialization
initialize_app_state()

# ==============================
# AUTHENTICATION
# ==============================

st.markdown('<div class="sub-header">üîê Authentication</div>', unsafe_allow_html=True)

# Authenticate - this will show login button if not authenticated
access_token = UpstoxSession.authenticate()

# CRITICAL: Check if authenticate() actually returned a token
# If it returns None, we need to stop execution here
if not access_token:
    # authenticate() should have shown login button and stopped with st.stop()
    # But if we reach here, something went wrong
    st.error("‚ùå Authentication required. Please log in to continue.")
    
    # Show login button manually as fallback
    login_url = UpstoxSession.get_login_url()
    st.markdown(f"""
    <div style="text-align: center; margin: 20px 0;">
        <a href="{login_url}" target="_blank">
            <button style="
                background-color: #00d09c;
                color: white;
                padding: 15px 30px;
                font-size: 18px;
                font-weight: bold;
                border: none;
                border-radius: 10px;
                cursor: pointer;
                width: 60%;
                margin: 20px 0;
                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            ">
            üìà Click to Login with Upstox
            </button>
        </a>
    </div>
    """, unsafe_allow_html=True)
    
    # STOP the app here
    st.stop()

# If we reach here, we have a token
# Initialize client
try:
    client = UpstoxClient(access_token)
    st.success("‚úÖ Upstox client initialized")
        
except Exception as e:
    st.error(f"‚ùå Error initializing Upstox client: {e}")
    st.stop()

# ==============================
# SIDEBAR - SESSION STATUS
# ==============================

with st.sidebar:
    st.markdown("## üß≠ Navigation")
    
    # Display session status
    display_session_status()
    
    # Database stats
    st.markdown("---")
    st.markdown("### üìä Database")
    
    db_stats = get_database_stats()
    if db_stats:
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Features", f"{db_stats.get('market_features_count', 0):,}")
        with col2:
            st.metric("Signals", f"{db_stats.get('signals_count', 0):,}")
    
    # Market hours info
    st.markdown("---")
    st.markdown("### üïí Market Hours")
    
    scheduler: MarketScheduler = st.session_state.scheduler
    now = datetime.now()
    
    if scheduler._is_market_open(now):
        st.success("‚úÖ Market Open")
        next_run = scheduler._last_run + timedelta(seconds=scheduler.interval_seconds) if scheduler._last_run else now
        time_to_next = (next_run - now).total_seconds()
        if time_to_next > 0:
            st.info(f"Next cycle in: {int(time_to_next)}s")
    else:
        st.warning("‚è∏Ô∏è Market Closed")
    
    # Quick actions
    st.markdown("---")
    st.markdown("### ‚ö° Quick Actions")
    
    if st.button("üîÑ Clear Cache", width='stretch'):
        for key in list(st.session_state.keys()):
            if key not in ["initialized", "instrument_master", "config", "nifty_weights"]:
                del st.session_state[key]
        st.rerun()
    
    if st.button("üìä View Performance", width='stretch'):
        st.session_state.show_performance = True
    
    if st.button("üõ†Ô∏è System Logs", width='stretch'):
        st.session_state.show_logs = True

def retry_on_failure(max_retries=3, delay=5):
    """Decorator for retrying failed executions"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    print(f"‚ö†Ô∏è Attempt {attempt + 1}/{max_retries} failed: {e}")
                    if attempt < max_retries - 1:
                        print(f"‚è∏Ô∏è Retrying in {delay}s...")
                        time.sleep(delay)
                    else:
                        print(f"‚ùå All {max_retries} attempts failed")
                        raise
            return None
        return wrapper
    return decorator

def log_execution_time(func):
    """Decorator for logging execution time"""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        print(f"‚è±Ô∏è Starting {func.__name__}...")
        
        result = func(*args, **kwargs)
        
        execution_time = time.time() - start_time
        print(f"‚è±Ô∏è {func.__name__} completed in {execution_time:.2f}s")
        
        return result
    return wrapper
# ==============================
# RESEARCH EXECUTION CYCLE
# ==============================

@log_execution_time
@retry_on_failure(max_retries=2, delay=3)
def execute_research_cycle(config=None, nifty_weights=None, option_keys=None):
    """
    Enhanced execution cycle with research analytics.
    Returns comprehensive research data.
    """
    # Use provided parameters or fall back to session state
    if config is None:
        try:
            config = st.session_state.config
        except (KeyError, AttributeError):
            print("‚ö†Ô∏è Config not found in session state")
            return None
    
    if nifty_weights is None:
        nifty_weights = st.session_state.get("nifty_weights", {})
    
    if option_keys is None:
        option_keys = st.session_state.get("option_keys", [])
    
    research_data = {}
    
    try:
        # ------------------------------
        # 1. INDEX QUOTE
        # ------------------------------
        print(f"üìä Fetching index quote for {config.get('index_symbol', 'NSE_INDEX|Nifty 50')}")
        index_data = client.fetch_index_quote(config["index_symbol"])
        
        if not index_data or index_data.get("ltp") is None:
            print("‚ùå No index data received")
            return None
        
        ltp = float(index_data["ltp"])
        open_price = float(index_data["open"])
        volume = 0.0  # index has no volume
        
        research_data["index"] = {
            "ltp": ltp,
            "open": open_price,
            "change": index_data.get("percent_change", 0),
            "timestamp": index_data.get("timestamp", datetime.now(timezone.utc).isoformat())
        }
        
        print(f"‚úÖ Index data: {ltp} (Change: {index_data.get('percent_change', 0)}%)")
        
        # ------------------------------
        # 2. UPDATE PRICE SERIES
        # ------------------------------
        # Create new series safely
        try:
            if "price_series" not in st.session_state:
                st.session_state.price_series = pd.Series(dtype=float)
            
            if "volume_series" not in st.session_state:
                st.session_state.volume_series = pd.Series(dtype=float)
            
            # Update price series
            new_price_series = pd.concat([
                st.session_state.price_series, 
                pd.Series([ltp], index=[datetime.now()])
            ]).tail(200)
            
            # Update volume series
            new_volume_series = pd.concat([
                st.session_state.volume_series, 
                pd.Series([volume], index=[datetime.now()])
            ]).tail(200)
            
            # Store back to session state
            st.session_state.price_series = new_price_series
            st.session_state.volume_series = new_volume_series
            
            print(f"üìà Price series updated: {len(st.session_state.price_series)} data points")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error updating price series: {e}")
            # Initialize fresh series
            st.session_state.price_series = pd.Series([ltp], index=[datetime.now()])
            st.session_state.volume_series = pd.Series([volume], index=[datetime.now()])
        
        # ------------------------------
        # 3. BREADTH ANALYSIS (CCC)
        # ------------------------------
        print("üìä Starting breadth analysis...")
        try:
            if not nifty_weights:
                print("‚ö†Ô∏è No Nifty weights found, skipping breadth analysis")
                ccc_value = 0.0
                constituents_df = pd.DataFrame()
            else:
                equity_quotes_df = client.fetch_equity_quotes(
                    symbols=list(nifty_weights.keys())
                )
                
                constituents_df = build_constituents_df(
                    equity_quotes_df=equity_quotes_df,
                    weights=nifty_weights
                )
                
                if not constituents_df.empty:
                    ccc_value = (
                        constituents_df["weight"] * constituents_df["price_change"]
                    ).sum()
                    print(f"‚úÖ CCC Value: {ccc_value:.4f}")
                else:
                    ccc_value = 0.0
                    print("‚ö†Ô∏è Empty constituents dataframe")
        except Exception as e:
            print(f"‚ö†Ô∏è Breadth analysis error: {e}")
            ccc_value = 0.0
            constituents_df = pd.DataFrame()
        
        # Update CCC history
        try:
            if "ccc_history" not in st.session_state:
                st.session_state.ccc_history = pd.Series(dtype=float)
            
            st.session_state.ccc_history = pd.concat([
                st.session_state.ccc_history, 
                pd.Series([ccc_value], index=[datetime.now()])
            ]).tail(200)
        except:
            st.session_state.ccc_history = pd.Series([ccc_value], index=[datetime.now()])
        
        research_data["breadth"] = {
            "ccc_value": ccc_value,
            "constituents_count": len(constituents_df),
            "positive_constituents": len(constituents_df[constituents_df["price_change"] > 0]) if not constituents_df.empty else 0
        }
        
        # ------------------------------
        # 4. OPTION INSTRUMENT DISCOVERY
        # ------------------------------
        print("üîç Fetching option instruments...")
        try:
            if not option_keys:
                expiry_str = config["expiry_date"].strftime("%Y-%m-%d")
                option_keys = get_option_keys(
                    underlying=config["underlying"],
                    expiry=expiry_str,
                    max_keys=config.get("max_option_keys", 200)
                )
                st.session_state.option_keys = option_keys
                print(f"‚úÖ Found {len(option_keys)} option keys")
        except Exception as e:
            print(f"‚ö†Ô∏è Option discovery error: {e}")
            option_keys = []
            st.session_state.option_keys = []
        
        if not option_keys:
            print("‚ö†Ô∏è No option keys found")
            return research_data  # Return what we have so far
        
        # ------------------------------
        # 5. ENHANCED OPTION CHAIN ANALYSIS
        # ------------------------------
        print("üìä Analyzing option chain...")
        option_analytics = client.fetch_option_chain_with_analytics(
            option_keys,
            ltp
        )
        
        if not option_analytics or "raw_data" not in option_analytics:
            print("‚ùå Failed to fetch option analytics")
            return research_data  # Return what we have so far
        
        option_chain_df = option_analytics["raw_data"]
        analytics = option_analytics.get("analytics", {})
        insights = option_analytics.get("market_insights", [])
        
        research_data["options"] = {
            "analytics": analytics,
            "insights": insights,
            "chain_size": len(option_chain_df),
            "put_call_ratio": analytics.get("put_call_ratio", 1.0) if isinstance(analytics.get("put_call_ratio"), (int, float)) else 1.0
        }
        
        print(f"‚úÖ Option chain analyzed: {len(option_chain_df)} strikes, PCR: {analytics.get('put_call_ratio', 'N/A')}")
        
        # Store analytics for visualization
        if analytics:
            try:
                if "research_analytics" not in st.session_state:
                    st.session_state.research_analytics = []
                
                st.session_state.research_analytics.append({
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    **analytics
                })
                
                # Keep only recent analytics
                if len(st.session_state.research_analytics) > 100:
                    st.session_state.research_analytics = st.session_state.research_analytics[-100:]
                
                # Update market regime
                st.session_state.market_regime = analytics.get("market_regime", "UNKNOWN")
                
                # Store in database
                insert_research_analytics(analytics)
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error storing analytics: {e}")
        
        # ------------------------------
        # 6. FEATURE PIPELINE
        # ------------------------------
        print("‚öôÔ∏è Running feature pipeline...")
        try:
            snapshot_ts = pd.Timestamp.utcnow().floor("ms")
            
            feature_result = build_and_store_features(
                timestamp=snapshot_ts,
                option_chain_df=option_chain_df,
                spot_price=ltp,
                expiry_datetime=config["expiry_date"],
                
                ltp=ltp,
                vwap=open_price,
                price_series=st.session_state.price_series,
                volume_series=st.session_state.volume_series,
                
                constituents_df=constituents_df,
                ccc_history=st.session_state.ccc_history,
                
                # Enhanced parameters
                client=client,
                option_keys=option_keys
            )
            
            if feature_result:
                research_data["features"] = feature_result.get("features", {})
                research_data["feature_summary"] = create_feature_summary(feature_result["features"])
                print("‚úÖ Feature pipeline completed")
            else:
                print("‚ö†Ô∏è Feature pipeline returned empty result")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Feature pipeline error: {e}")
            import traceback
            traceback.print_exc()
        
        # ------------------------------
        # 7. SIGNAL GENERATION
        # ------------------------------
        print("üéØ Checking for signals...")
        try:
            feature_row = fetch_latest_features(FEATURE_VERSION)
            if feature_row is not None and not feature_row.empty:
                # Check if signal already exists
                timestamp_str = feature_row.iloc[0]["timestamp"]
                
                if not signal_exists(timestamp_str):
                    # Generate signal
                    signal_engine: SignalStateMachine = st.session_state.signal_engine
                    signal = signal_engine.build_signal(feature_row.iloc[0])
                    
                    # Validate signal
                    is_valid, reason = SignalValidator.validate_signal(signal, feature_row.iloc[0])
                    
                    if is_valid and signal["signal_type"] != "NEUTRAL":
                        # Add research context
                        signal["research_validation"] = {
                            "is_valid": is_valid,
                            "reason": reason,
                            "timestamp": datetime.now(timezone.utc).isoformat()
                        }
                        
                        # Insert signal
                        insert_signal(signal)
                        
                        research_data["signal"] = {
                            "generated": True,
                            "type": signal["signal_type"],
                            "confidence": signal["confidence"],
                            "strength": signal.get("signal_strength", "UNKNOWN")
                        }
                        print(f"‚úÖ Signal generated: {signal['signal_type']} ({signal['confidence']:.0%})")
                    else:
                        research_data["signal"] = {
                            "generated": False,
                            "reason": reason if not is_valid else "NEUTRAL signal"
                        }
                        print(f"‚ÑπÔ∏è No valid signal: {reason if not is_valid else 'NEUTRAL'}")
            else:
                print("‚ÑπÔ∏è No features available for signal generation")
        except Exception as e:
            print(f"‚ö†Ô∏è Signal generation error: {e}")
        
        # ------------------------------
        # 8. SIGNAL LIFECYCLE MANAGEMENT
        # ------------------------------
        try:
            validate_new_signals(confidence_threshold=0.2)
            expire_old_signals()
        except Exception as e:
            print(f"‚ö†Ô∏è Signal lifecycle error: {e}")
        
        # ------------------------------
        # 9. TRAP DETECTION
        # ------------------------------
        if analytics and "potential_traps" in analytics:
            traps = analytics["potential_traps"]
            if traps:
                try:
                    if "trap_detections" not in st.session_state:
                        st.session_state.trap_detections = []
                    
                    for trap in traps:
                        if trap.get("confidence", 0) > 0.6:
                            st.session_state.trap_detections.append({
                                "timestamp": datetime.now(timezone.utc).isoformat(),
                                "trap": trap,
                                "market_regime": analytics.get("market_regime")
                            })
                    
                    # Keep only recent traps
                    if len(st.session_state.trap_detections) > 20:
                        st.session_state.trap_detections = st.session_state.trap_detections[-20:]
                    
                    research_data["traps"] = traps
                    
                    if traps:
                        print(f"üéØ Detected {len(traps)} potential traps")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Trap detection error: {e}")
        
        # ------------------------------
        # 10. SUCCESSFUL COMPLETION
        # ------------------------------
        print("‚úÖ Research cycle completed successfully")
        return research_data
        
    except Exception as e:
        print(f"‚ùå Critical error in research cycle: {e}")
        import traceback
        traceback.print_exc()
        return None

# ==============================
# ENHANCED AUTO-REFRESH SYSTEM
# ==============================

class AutoRefreshManager:
    """Robust auto-refresh manager with error handling and state persistence"""
    
    def __init__(self, scheduler, config, nifty_weights, option_keys):
        self.scheduler = scheduler
        self.config = config
        self.nifty_weights = nifty_weights
        self.option_keys = option_keys
        self.last_execution_time = None
        self.execution_count = 0
        self.errors = []
        self.max_retries = 3
        self.retry_delay = 5
        
    def should_execute(self):
        """Determine if execution should run based on time and market conditions"""
        now = datetime.now()
        
        # Check market hours
        if not self.scheduler._is_market_open(now):
            return False, "Market closed"
        
        # Check if first run
        if self.last_execution_time is None:
            return True, "First execution"
        
        # Check time since last execution
        time_since_last = (now - self.last_execution_time).total_seconds()
        
        # Debug log to see what's happening
        print(f"üïí Auto-refresh timing: {int(time_since_last)}s since last, interval: {self.scheduler.interval_seconds}s")
        
        if time_since_last >= self.scheduler.interval_seconds:
            return True, f"Interval reached ({int(time_since_last)}s > {self.scheduler.interval_seconds}s)"
        
        return False, f"Not due yet ({int(self.scheduler.interval_seconds - time_since_last)}s remaining)"
    
    def execute_with_retry(self):
        """Execute research cycle with retry logic"""
        for attempt in range(self.max_retries):
            try:
                print(f"üîÑ Attempt {attempt + 1}/{self.max_retries} to execute research cycle")
                
                result = execute_research_cycle(
                    config=self.config,
                    nifty_weights=self.nifty_weights,
                    option_keys=self.option_keys
                )
                
                if result:
                    self.last_execution_time = datetime.now()
                    self.execution_count += 1
                    return True, result, f"Execution successful (attempt {attempt + 1})"
                else:
                    return False, None, "Research cycle returned no data"
                    
            except Exception as e:
                error_msg = f"Attempt {attempt + 1} failed: {str(e)}"
                self.errors.append({
                    "timestamp": datetime.now(),
                    "attempt": attempt + 1,
                    "error": str(e)
                })
                
                if attempt < self.max_retries - 1:
                    print(f"‚è∏Ô∏è Retrying in {self.retry_delay}s...")
                    time.sleep(self.retry_delay)
                else:
                    return False, None, f"All retries failed. Last error: {str(e)}"
        
        return False, None, "Max retries exceeded"
    
    def get_status(self):
        """Get current status of auto-refresh manager"""
        status = {
            "last_execution": self.last_execution_time.strftime("%H:%M:%S") if self.last_execution_time else "Never",
            "execution_count": self.execution_count,
            "is_market_open": self.scheduler._is_market_open(datetime.now()),
            "interval_seconds": self.scheduler.interval_seconds,
            "error_count": len(self.errors)
        }
        
        if self.last_execution_time:
            time_since = (datetime.now() - self.last_execution_time).total_seconds()
            status["time_since_last"] = f"{int(time_since)}s"
            status["time_until_next"] = f"{max(0, int(self.scheduler.interval_seconds - time_since))}s"
        
        return status

# Initialize auto-refresh manager
if "auto_refresh_manager" not in st.session_state:
    st.session_state.auto_refresh_manager = AutoRefreshManager(
        scheduler=st.session_state.scheduler,
        config=st.session_state.config,
        nifty_weights=st.session_state.nifty_weights,
        option_keys=st.session_state.option_keys
    )

# Execute auto-refresh
def execute_auto_refresh():
    """Main auto-refresh execution function"""
    manager = st.session_state.auto_refresh_manager
    
    try:
        # Check if we should execute
        should_execute, reason = manager.should_execute()
        
        if should_execute:
            print(f"üèÉ‚Äç‚ôÇÔ∏è Auto-execution triggered: {reason}")
            
            # Execute with retry logic
            success, result, message = manager.execute_with_retry()
            
            if success:
                print(f"‚úÖ {message}")
                
                # Update UI state if needed
                if result and "options" in result and "analytics" in result["options"]:
                    analytics = result["options"]["analytics"]
                    if analytics:
                        # Update market regime
                        st.session_state.market_regime = analytics.get("market_regime", "UNKNOWN")
                        
                        # Store analytics
                        if "research_analytics" not in st.session_state:
                            st.session_state.research_analytics = []
                        st.session_state.research_analytics.append({
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                            **analytics
                        })
                        # Keep only recent analytics
                        if len(st.session_state.research_analytics) > 100:
                            st.session_state.research_analytics = st.session_state.research_analytics[-100:]
                
                return True, message
            else:
                print(f"‚ùå Auto-execution failed: {message}")
                return False, message
        else:
            # Log status periodically
            if manager.last_execution_time:
                time_since = (datetime.now() - manager.last_execution_time).total_seconds()
                if int(time_since) % 10 == 0:  # Log every 10 seconds
                    print(f"‚è±Ô∏è Auto-refresh status: {reason}")
            return None, reason
            
    except Exception as e:
        error_msg = f"Auto-refresh system error: {str(e)}"
        print(f"üî• {error_msg}")
        manager.errors.append({
            "timestamp": datetime.now(),
            "error": error_msg
        })
        return False, error_msg

# Run auto-refresh
execute_auto_refresh()

# ==============================
# ENHANCED BACKGROUND SCHEDULER
# ==============================

class BackgroundScheduler:
    """Robust background scheduler with error handling and monitoring"""
    
    def __init__(self, check_interval=5):
        self.check_interval = check_interval
        self.running = False
        self.thread = None
        self.errors = []
        self.last_check = None
        self.health_checks = []
        
    def health_check(self):
        """Perform system health check"""
        checks = []
        
        # Check scheduler
        try:
            scheduler = st.session_state.scheduler
            checks.append({
                "component": "Scheduler",
                "status": "HEALTHY" if scheduler else "UNKNOWN",
                "details": f"Interval: {scheduler.interval_seconds}s" if scheduler else "Not initialized"
            })
        except Exception as e:
            checks.append({
                "component": "Scheduler",
                "status": "ERROR",
                "details": str(e)
            })
        
        # Check auto-refresh manager
        try:
            manager = st.session_state.auto_refresh_manager
            checks.append({
                "component": "AutoRefresh",
                "status": "HEALTHY",
                "details": f"Executions: {manager.execution_count}"
            })
        except Exception as e:
            checks.append({
                "component": "AutoRefresh",
                "status": "ERROR",
                "details": str(e)
            })
        
        self.health_checks = checks
        return checks
    
    def start(self):
        """Start background scheduler thread"""
        if self.running:
            print("‚ö†Ô∏è Background scheduler already running")
            return
        
        def background_task():
            print("üöÄ Starting enhanced background scheduler...")
            self.running = True
            
            while self.running:
                try:
                    self.last_check = datetime.now()
                    
                    # Perform health check every minute
                    if not self.health_checks or (datetime.now() - self.last_check).seconds > 60:
                        self.health_check()
                    
                    # Log status every 30 seconds
                    current_time = datetime.now()
                    if current_time.second % 30 == 0:
                        manager = st.session_state.auto_refresh_manager
                        if manager.last_execution_time:
                            time_since = (current_time - manager.last_execution_time).total_seconds()
                            print(f"üìä Background monitor: Last execution {int(time_since)}s ago, {manager.execution_count} total runs")
                    
                    time.sleep(self.check_interval)
                    
                except Exception as e:
                    error_msg = f"Background scheduler error: {str(e)}"
                    print(f"üî• {error_msg}")
                    self.errors.append({
                        "timestamp": datetime.now(),
                        "error": error_msg
                    })
                    time.sleep(30)  # Wait longer on error
        
        self.thread = threading.Thread(target=background_task, daemon=True)
        self.thread.start()
        print("‚úÖ Enhanced background scheduler started")
    
    def stop(self):
        """Stop background scheduler"""
        self.running = False
        if self.thread:
            self.thread.join(timeout=5)
        print("üõë Background scheduler stopped")
    
    def get_status(self):
        """Get scheduler status"""
        return {
            "running": self.running,
            "check_interval": self.check_interval,
            "last_check": self.last_check.strftime("%H:%M:%S") if self.last_check else "Never",
            "error_count": len(self.errors),
            "health_checks": self.health_checks
        }

# Initialize and start background scheduler
if "background_scheduler" not in st.session_state:
    st.session_state.background_scheduler = BackgroundScheduler(check_interval=5)
    st.session_state.background_scheduler.start()


def get_scheduler_status():
    """Get comprehensive scheduler status"""
    scheduler = st.session_state.scheduler
    manager = st.session_state.auto_refresh_manager
    
    status = {
        "scheduler_active": True,
        "interval_seconds": scheduler.interval_seconds,
        "total_executions": scheduler._total_executions,
        "last_run": scheduler._last_run.strftime("%H:%M:%S") if scheduler._last_run else "Never",
        "is_market_open": scheduler._is_market_open(datetime.now()),
        "auto_refresh_status": manager.get_status()
    }
    
    # Calculate next run time
    if scheduler._last_run:
        next_run = scheduler._last_run + timedelta(seconds=scheduler.interval_seconds)
        time_until_next = (next_run - datetime.now()).total_seconds()
        status["next_run"] = next_run.strftime("%H:%M:%S") if time_until_next > 0 else "Now"
        status["time_until_next"] = f"{max(0, int(time_until_next))}s"
    else:
        status["next_run"] = "Pending"
        status["time_until_next"] = "N/A"
    
    return status

# ==============================
# CREATE TABS
# ==============================

# Create tabs for different views
tab1, tab2, tab3, tab4 = st.tabs([
    "üìà Live Analytics", 
    "üéØ Signals", 
    "üîç Research", 
    "‚öôÔ∏è Configuration"
])

# ==============================
# AUTO-REFRESH WORKING SOLUTION
# ==============================

# Create a placeholder for auto-refresh
refresh_placeholder = st.empty()

# Check if we need to auto-refresh
if "next_refresh_time" not in st.session_state:
    st.session_state.next_refresh_time = time.time() + 30  # First refresh in 30 seconds

current_time = time.time()

if current_time >= st.session_state.next_refresh_time:
    # Time to refresh!
    print(f"üîÑ AUTO-REFRESH EXECUTING at {datetime.now().strftime('%H:%M:%S')}")
    
    # Execute research cycle
    with st.spinner("Auto-refreshing data..."):
        research_data = execute_research_cycle()
    
    if research_data:
        print(f"‚úÖ Auto-refresh completed successfully")
        
        # Update next refresh time
        st.session_state.next_refresh_time = current_time + 30
        
        # Update state
        st.session_state.scheduler._last_run = datetime.now()
        st.session_state.scheduler._total_executions += 1
        st.session_state.auto_refresh_manager.last_execution_time = datetime.now()
        st.session_state.auto_refresh_manager.execution_count += 1
        
        # Show success message
        refresh_placeholder.success(f"‚úÖ Auto-refresh completed at {datetime.now().strftime('%H:%M:%S')}")
        time.sleep(1)
        refresh_placeholder.empty()
        
        # Force a rerun to update UI
        st.rerun()
    else:
        print("‚ùå Auto-refresh failed")
        refresh_placeholder.error("Auto-refresh failed")
        time.sleep(2)
        refresh_placeholder.empty()
else:
    # Show countdown
    time_until_next = st.session_state.next_refresh_time - current_time
    if time_until_next > 0:
        print(f"‚è±Ô∏è Next auto-refresh in {int(time_until_next)}s")
        
        # Update countdown every 10 seconds
        if int(time_until_next) % 10 == 0:
            refresh_placeholder.info(f"Next auto-refresh in {int(time_until_next)} seconds...")


# ==============================
# TAB 1: LIVE ANALYTICS
# ==============================

with tab1:
    # Add JavaScript auto-refresh
    auto_refresh_js = """
    <script>
    // Auto-refresh every 30 seconds
    setTimeout(function() {
        window.location.reload();
    }, 30000);
    </script>
    """
    st.components.v1.html(auto_refresh_js, height=0)
    
    st.markdown('<div class="sub-header">üìä Live Market Analytics</div>', unsafe_allow_html=True)
   
    
    # Execute cycle - but this is just for UI display now
    scheduler: MarketScheduler = st.session_state.scheduler
    
    # Track last execution time
    if "last_execution_time" not in st.session_state:
        st.session_state.last_execution_time = None
    
    # Display background scheduler status
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if scheduler._last_run:
            time_since_last = (datetime.now() - scheduler._last_run).total_seconds()
            status_text = "‚úÖ Running" if time_since_last < 60 else "‚ö†Ô∏è Stale"
            st.metric("Scheduler", status_text, delta=f"{int(time_since_last)}s ago")
        else:
            st.metric("Scheduler", "‚è≥ Starting", delta="Initializing")
    
    with col2:
        if st.session_state.price_series.size > 0:
            current_price = st.session_state.price_series.iloc[-1]
            prev_price = st.session_state.price_series.iloc[-2] if st.session_state.price_series.size > 1 else current_price
            change_pct = ((current_price - prev_price) / prev_price * 100) if prev_price > 0 else 0
            st.metric("Nifty 50", f"{current_price:,.0f}", f"{change_pct:+.2f}%")
    
    with col3:
        if st.session_state.research_analytics:
            latest = st.session_state.research_analytics[-1]
            regime = latest.get("market_regime", "UNKNOWN")
            st.metric("Market Regime", regime)
    
    with col4:
        if st.session_state.research_analytics:
            latest = st.session_state.research_analytics[-1]
            oi_vel = latest.get("oi_velocity", 0)
            st.metric("OI Velocity", f"{oi_vel:+.2f}œÉ")
    
    # Manual execution button - still works
    if st.button("üîÑ Execute Research Cycle Now", type="primary", width='stretch'):
        with st.spinner("Executing research cycle..."):
            research_data = execute_research_cycle()
            if research_data:
                st.success("‚úÖ Research cycle executed successfully")
                
                # Display insights if available
                if "options" in research_data and "insights" in research_data["options"]:
                    for insight in research_data["options"]["insights"][:3]:
                        st.info(insight)
            else:
                st.error("‚ùå Research cycle failed")
    
    st.markdown("---")
    
    # Display enhanced status
    st.markdown("---")
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        status = get_scheduler_status()
        if status["scheduler_active"]:
            if status["last_run"] != "Never":
                time_since = (datetime.now() - st.session_state.scheduler._last_run).total_seconds() if st.session_state.scheduler._last_run else 999
                if time_since < 60:
                    st.metric("Scheduler", "‚úÖ Active", delta=f"{int(time_since)}s ago")
                else:
                    st.metric("Scheduler", "‚ö†Ô∏è Stale", delta=f"{int(time_since)}s ago")
            else:
                st.metric("Scheduler", "‚è≥ Initializing", delta="First run pending")
        else:
            st.metric("Scheduler", "‚ùå Inactive", delta="Check configuration")

    with col2:
        manager_status = st.session_state.auto_refresh_manager.get_status()
        st.metric("Auto Refresh", f"Run #{manager_status['execution_count']}", 
                delta=manager_status['last_execution'])

    with col3:
        if manager_status.get('time_until_next'):
            st.metric("Next Run In", manager_status['time_until_next'])

    with col4:
        error_count = manager_status.get('error_count', 0)
        if error_count == 0:
            st.metric("Errors", "‚úÖ 0", delta="Clean")
        else:
            st.metric("Errors", f"‚ö†Ô∏è {error_count}", delta="Check logs")

    # Control buttons
    col1, col2, col3 = st.columns(3)

    with col1:
        if st.button("üîÑ Force Run Now", type="primary", width='stretch'):
            # Force immediate execution
            st.session_state.auto_refresh_manager.last_execution_time = None
            st.rerun()

    with col2:
        if st.button("‚è∏Ô∏è Pause Auto-Refresh", width='stretch'):
            # Temporarily disable auto-refresh
            original_interval = st.session_state.scheduler.interval_seconds
            st.session_state.scheduler.interval_seconds = 3600  # 1 hour
            st.success(f"Auto-refresh paused. Original interval: {original_interval}s")
            st.rerun()

    with col3:
        if st.button("‚ñ∂Ô∏è Resume Auto-Refresh", width='stretch'):
            # Restore auto-refresh
            st.session_state.scheduler.interval_seconds = 30
            st.success("Auto-refresh resumed (30s interval)")
            st.rerun()
    # Display background thread info
    with st.expander("üîÑ Background Scheduler Info", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            if "scheduler_thread" in st.session_state:
                thread = st.session_state.scheduler_thread
                st.success("‚úÖ Background thread active")
                st.info(f"Thread ID: {thread.ident}")
                st.info(f"Thread alive: {thread.is_alive()}")
            else:
                st.error("‚ùå Background thread not running")
        
        with col2:
            if scheduler._last_run:
                st.metric("Last Execution", scheduler._last_run.strftime("%H:%M:%S"))
                time_since = (datetime.now() - scheduler._last_run).total_seconds()
                st.metric("Time Since", f"{int(time_since)}s")
            else:
                st.metric("Last Execution", "Never")
            
            st.metric("Total Executions", scheduler._total_executions)
    
    
    # Display latest analytics if available
    if st.session_state.research_analytics:
        latest_analytics = st.session_state.research_analytics[-1]
        
        # Create columns for analytics display
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### üìà OI & Gamma Analysis")
            
            # OI Velocity gauge
            oi_velocity = latest_analytics.get("oi_velocity", 0)
            fig_oi = go.Figure(go.Indicator(
                mode="gauge+number+delta",
                value=oi_velocity,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "OI Velocity (œÉ)"},
                delta={'reference': 0},
                gauge={
                    'axis': {'range': [-3, 3]},
                    'steps': [
                        {'range': [-3, -1.5], 'color': "lightgray"},
                        {'range': [-1.5, 1.5], 'color': "gray"},
                        {'range': [1.5, 3], 'color': "lightgray"}
                    ],
                    'threshold': {
                        'line': {'color': "red", 'width': 4},
                        'thickness': 0.75,
                        'value': oi_velocity
                    }
                }
            ))
            fig_oi.update_layout(height=250)
            st.plotly_chart(fig_oi, width='stretch')
        
        with col2:
            st.markdown("#### üß± Structure Analysis")
            
            # Structural metrics
            wall_strength = latest_analytics.get("wall_strength", 0)
            trap_prob = latest_analytics.get("trap_probability", 0)
            
            col_a, col_b = st.columns(2)
            with col_a:
                st.metric("Wall Strength", f"{wall_strength:.2f}")
            with col_b:
                st.metric("Trap Probability", f"{trap_prob:.2%}")
            
            # Display traps if any
            traps = latest_analytics.get("potential_traps", [])
            if traps:
                st.markdown("##### üéØ Detected Traps")
                for trap in traps[:2]:  # Show max 2
                    direction = trap.get("direction", "").upper()
                    confidence = trap.get("confidence", 0)
                    strike = trap.get("strike", 0)
                    
                    if confidence > 0.6:
                        st.markdown(f"""
                        <div class="trap-alert">
                            <strong>{direction} Trap</strong> at {strike:,.0f}<br>
                            Confidence: {confidence:.0%}<br>
                            Unwinding: {trap.get('unwinding_rate', 0):+.1f}%
                        </div>
                        """, unsafe_allow_html=True)
        
        # Price chart
        st.markdown("---")
        st.markdown("#### üìâ Price & Indicators")
        
        if len(st.session_state.price_series) > 1:
            # Create subplot for price and indicators
            fig = make_subplots(
                rows=3, cols=1,
                shared_xaxes=True,
                vertical_spacing=0.05,
                subplot_titles=("Nifty 50 Price", "OI Velocity", "Gamma Exposure"),
                row_heights=[0.5, 0.25, 0.25]
            )
            
            # Price
            price_data = st.session_state.price_series.reset_index(drop=True)
            fig.add_trace(
                go.Scatter(
                    y=price_data.values,
                    mode='lines',
                    name='Price',
                    line=dict(color='#3B82F6', width=2)
                ),
                row=1, col=1
            )
            
            # OI Velocity (if available in analytics history)
            if len(st.session_state.research_analytics) > 1:
                oi_velocities = [a.get("oi_velocity", 0) for a in st.session_state.research_analytics[-50:]]
                fig.add_trace(
                    go.Scatter(
                        y=oi_velocities,
                        mode='lines',
                        name='OI Velocity',
                        line=dict(color='#10B981', width=1)
                    ),
                    row=2, col=1
                )
                
                # Add zero line
                fig.add_hline(y=0, line_dash="dash", line_color="gray", row=2, col=1)
            
            # Gamma (if available)
            if len(st.session_state.research_analytics) > 1:
                gamma_values = [a.get("gamma_exposure", {}).get("net_gamma", 0) for a in st.session_state.research_analytics[-50:]]
                fig.add_trace(
                    go.Scatter(
                        y=gamma_values,
                        mode='lines',
                        name='Net Gamma',
                        line=dict(color='#8B5CF6', width=1)
                    ),
                    row=3, col=1
                )
                
                # Add zero line
                fig.add_hline(y=0, line_dash="dash", line_color="gray", row=3, col=1)
            
            fig.update_layout(height=600, showlegend=True)
            st.plotly_chart(fig, width='stretch')

# ==============================
# TAB 2: SIGNALS
# ==============================

with tab2:
    st.markdown('<div class="sub-header">üéØ Trading Signals</div>', unsafe_allow_html=True)
    
    # Fetch active signals
    active_signals = fetch_active_signals(limit=10)
    
    if not active_signals.empty:
        # Display signals in columns
        for _, signal in active_signals.iterrows():
            col1, col2, col3, col4 = st.columns([2, 1, 1, 1])
            
            with col1:
                signal_type = signal["signal_type"]
                color = "green" if signal_type == "BUY" else "red" if signal_type == "SELL" else "gray"
                st.markdown(f"### <span style='color:{color};'>{signal_type}</span>", unsafe_allow_html=True)
                
                # Rationale
                if pd.notna(signal["rationale"]):
                    st.caption(signal["rationale"])
            
            with col2:
                confidence = signal["confidence"]
                st.metric("Confidence", f"{confidence:.0%}")
            
            with col3:
                strength = signal.get("signal_strength", "UNKNOWN")
                st.metric("Strength", strength)
            
            with col4:
                status = signal["status"]
                badge_color = "blue" if status == "NEW" else "green" if status == "VALIDATED" else "orange"
                st.markdown(f"<div style='padding: 5px 10px; background-color:{badge_color}; color:white; border-radius:5px; text-align:center;'>{status}</div>", 
                           unsafe_allow_html=True)
            
            # Research context (expandable)
            with st.expander("Research Details"):
                if pd.notna(signal.get("research_context")):
                    try:
                        context = json.loads(signal["research_context"]) if isinstance(signal["research_context"], str) else signal["research_context"]
                        st.json(context)
                    except:
                        st.write("No research context available")
            
            st.markdown("---")
        
        # Performance metrics
        st.markdown("#### üìä Signal Performance")
        performance = fetch_signal_performance(days=7)
        
        if performance:
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Total Signals", performance.get("total_signals", 0))
            
            with col2:
                st.metric("Profitable", performance.get("profitable_signals", 0))
            
            with col3:
                win_rate = performance.get("win_rate", 0)
                st.metric("Win Rate", f"{win_rate:.1f}%")
            
            with col4:
                period = performance.get("period_days", 7)
                st.metric("Period", f"{period} days")
        
        # Signal distribution chart
        if not active_signals.empty:
            st.markdown("#### üìà Signal Distribution")
            
            fig = go.Figure(data=[
                go.Pie(
                    labels=active_signals["signal_type"].value_counts().index,
                    values=active_signals["signal_type"].value_counts().values,
                    hole=.3
                )
            ])
            
            fig.update_layout(
                height=300,
                showlegend=True,
                margin=dict(t=0, b=0, l=0, r=0)
            )
            
            st.plotly_chart(fig, width='stretch')
    
    else:
        st.info("üì≠ No active signals. The system will generate signals during market hours.")

# ==============================
# TAB 3: RESEARCH
# ==============================

with tab3:
    st.markdown('<div class="sub-header">üîç Research & Analytics</div>', unsafe_allow_html=True)
    
    # Research metrics
    if st.session_state.research_analytics:
        latest = st.session_state.research_analytics[-1]
        
        # Key research metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            net_gamma = latest.get("gamma_exposure", {}).get("net_gamma", 0)
            st.metric("Net Gamma", f"{net_gamma:,.0f}")
        
        with col2:
            pcr = latest.get("put_call_ratio", 1.0)
            st.metric("Put/Call Ratio", f"{pcr:.2f}")
        
        with col3:
            divergence = latest.get("divergence_score", 0)
            st.metric("Divergence Score", f"{divergence:.2f}")
        
        with col4:
            regime = latest.get("market_regime", "UNKNOWN")
            st.metric("Market Regime", regime)
        
        # Display market insights
        st.markdown("#### üí° Market Insights")
        
        insights = latest.get("market_insights", [])
        if insights:
            for insight in insights[:5]:
                if "trap" in insight.lower():
                    st.markdown(f'<div class="trap-alert">{insight}</div>', unsafe_allow_html=True)
                elif "divergence" in insight.lower():
                    st.markdown(f'<div class="divergence-alert">{insight}</div>', unsafe_allow_html=True)
                else:
                    st.markdown(f'<div class="research-insight">{insight}</div>', unsafe_allow_html=True)
        else:
            st.info("No insights available yet")
        
        # Structural walls
        st.markdown("#### üß± Structural Walls")
        
        walls = latest.get("structural_walls", [])
        if walls:
            walls_df = pd.DataFrame(walls)
            st.dataframe(walls_df, width='stretch')
        
        # Trap detection history
        st.markdown("#### üéØ Trap Detection History")
        
        if st.session_state.trap_detections:
            traps_df = pd.DataFrame([
                {
                    "Time": pd.to_datetime(t["timestamp"]).strftime("%H:%M:%S"),
                    "Type": t["trap"].get("direction", ""),
                    "Strike": t["trap"].get("strike", 0),
                    "Confidence": t["trap"].get("confidence", 0),
                    "Regime": t.get("market_regime", "")
                }
                for t in st.session_state.trap_detections
            ])
            
            if not traps_df.empty:
                st.dataframe(traps_df, width='stretch')
        
        # Research analytics over time
        st.markdown("#### üìä Analytics Timeline")
        
        if len(st.session_state.research_analytics) > 5:
            # Convert to DataFrame for plotting
            analytics_df = pd.DataFrame(st.session_state.research_analytics[-50:])
            
            if "timestamp" in analytics_df.columns:
                analytics_df["time"] = pd.to_datetime(analytics_df["timestamp"]).dt.strftime("%H:%M")
                
                # Plot OI Velocity and Gamma over time
                fig = make_subplots(
                    rows=2, cols=1,
                    shared_xaxes=True,
                    subplot_titles=("OI Velocity", "Net Gamma"),
                    vertical_spacing=0.1
                )
                
                if "oi_velocity" in analytics_df.columns:
                    fig.add_trace(
                        go.Scatter(
                            x=analytics_df["time"],
                            y=analytics_df["oi_velocity"],
                            mode='lines+markers',
                            name='OI Velocity',
                            line=dict(color='#10B981')
                        ),
                        row=1, col=1
                    )
                
                if "gamma_exposure" in analytics_df.columns:
                    # Extract net_gamma from gamma_exposure dict
                    try:
                        gamma_values = []
                        for g in analytics_df["gamma_exposure"]:
                            if isinstance(g, dict) and "net_gamma" in g:
                                gamma_values.append(g["net_gamma"])
                            else:
                                gamma_values.append(0)
                        
                        fig.add_trace(
                            go.Scatter(
                                x=analytics_df["time"],
                                y=gamma_values,
                                mode='lines+markers',
                                name='Net Gamma',
                                line=dict(color='#8B5CF6')
                            ),
                            row=2, col=1
                        )
                    except:
                        pass
                
                fig.update_layout(height=400, showlegend=True)
                st.plotly_chart(fig, width='stretch')

# ==============================
# TAB 4: CONFIGURATION
# ==============================

with tab4:
    st.markdown('<div class="sub-header">‚öôÔ∏è System Configuration</div>', unsafe_allow_html=True)
    
    # Available indices and underlyings
    available_indices = [
        "NSE_INDEX|Nifty 50",
        "NSE_INDEX|Nifty Bank", 
        "NSE_INDEX|Nifty Fin Service",
        "NSE_INDEX|Nifty Midcap 100",
        "NSE_INDEX|Nifty Next 50",
        "BSE_INDEX|S&P BSE SENSEX"  
    ]
    
    available_underlyings = [
        "NIFTY",
        "BANKNIFTY", 
        "FINNIFTY",
        "MIDCPNIFTY",
        "SENSEX"  
    ]
    
    # Get available expiries for the selected underlying
    current_underlying = st.session_state.config["underlying"]
    from data.instrument_master import get_available_expiries
    
    try:
        available_expiries = get_available_expiries(current_underlying)
        if not available_expiries:
            available_expiries = ["2026-01-27", "2026-02-03", "2026-02-10"]
    except:
        available_expiries = ["2026-01-27", "2026-02-03", "2026-02-10"]
    
    # Configuration form
    with st.form("configuration_form"):
        col1, col2 = st.columns(2)
        
        with col1:
            # Index Symbol Dropdown
            index_symbol = st.selectbox(
                "Index Symbol", 
                options=available_indices,
                index=available_indices.index(st.session_state.config["index_symbol"]) 
                if st.session_state.config["index_symbol"] in available_indices else 0
            )
            
            # Underlying Symbol Dropdown
            underlying = st.selectbox(
                "Underlying Symbol", 
                options=available_underlyings,
                index=available_underlyings.index(st.session_state.config["underlying"])
                if st.session_state.config["underlying"] in available_underlyings else 0
            )
            
            # Expiry Date Dropdown
            # Find today's date in available_expiries
            today_str = datetime.now().strftime("%Y-%m-%d")
            default_index = 0

            if today_str in available_expiries:
                default_index = available_expiries.index(today_str)
            elif today_str == "2026-01-28":  # Today's date
                # Add today to available expiries if it's not there
                available_expiries.insert(0, today_str)
                default_index = 0

            expiry_date = st.selectbox(
                "Expiry Date",
                options=available_expiries,
                index=default_index  # ‚úÖ Now selects today if available
            )
            
            # Convert selected expiry to datetime
            if expiry_date:
                try:
                    expiry_datetime = datetime.strptime(expiry_date, "%Y-%m-%d")
                    days_to_expiry = (expiry_datetime.date() - datetime.now().date()).days
                    st.info(f"Days to expiry: {days_to_expiry}")
                except:
                    st.warning("Invalid expiry date format")
        
        with col2:
            max_option_keys = st.number_input(
                "Max Option Keys", 
                min_value=50, 
                max_value=500, 
                value=st.session_state.config["max_option_keys"],
                help="Maximum number of option strikes to fetch"
            )
            
            scheduler_interval = st.slider(
                "Scheduler Interval (seconds)", 
                min_value=10, 
                max_value=300, 
                value=30,
                step=5,
                help="How often to refresh data (10-300 seconds)"
            )
            
            confidence_threshold = st.slider(
                "Signal Confidence Threshold", 
                min_value=0.0, 
                max_value=1.0, 
                value=0.2, 
                step=0.05,
                format="%.2f",
                help="Minimum confidence level for signals (0.0-1.0)"
            )
            
            # Advanced options expander
            with st.expander("Advanced Options"):
                enable_live_trading = st.checkbox(
                    "Enable Live Trading", 
                    value=False,
                    help="WARNING: Enable actual order placement"
                )
                
                risk_per_trade = st.number_input(
                    "Risk per Trade (%)",
                    min_value=0.1,
                    max_value=5.0,
                    value=1.0,
                    step=0.1,
                    format="%.1f",
                    help="Maximum risk per trade as percentage of capital"
                )
        
        # Submit button
        submit_col1, submit_col2, submit_col3 = st.columns([1, 2, 1])
        with submit_col2:
            if st.form_submit_button("üíæ Save Configuration", width='stretch'):
                # Update configuration
                st.session_state.config.update({
                    "index_symbol": index_symbol,
                    "underlying": underlying,
                    "expiry_date": datetime.strptime(expiry_date, "%Y-%m-%d") if expiry_date else datetime.now(timezone.utc) + timedelta(days=3),
                    "max_option_keys": max_option_keys,
                    "enable_live_trading": enable_live_trading,
                    "risk_per_trade": risk_per_trade
                })
                
                # Update scheduler
                st.session_state.scheduler = MarketScheduler(interval_seconds=scheduler_interval)
                
                # Update signal engine
                st.session_state.signal_engine = create_signal_engine(
                    signal_expiry_minutes=5,
                    confidence_threshold=confidence_threshold
                )
                
                # Clear option keys cache
                st.session_state.option_keys = []
                
                st.success("‚úÖ Configuration saved!")
                st.rerun()
    
    st.markdown("---")
    
    # Quick Configuration Presets
    st.markdown("#### üöÄ Quick Presets")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üìä NIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty 50",
                "underlying": "NIFTY"
            })
            st.success("Set to NIFTY Weekly")
            st.rerun()
    
    with col2:
        if st.button("üè¶ BANKNIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty Bank",
                "underlying": "BANKNIFTY"
            })
            st.success("Set to BANKNIFTY Weekly")
            st.rerun()
    
    with col3:
        if st.button("üí≥ FINNIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty Fin Service",
                "underlying": "FINNIFTY"
            })
            st.success("Set to FINNIFTY Weekly")
            st.rerun()
    
    # Current Configuration Display
    st.markdown("---")
    st.markdown("#### üñ•Ô∏è Current Configuration")
    
    config_col1, config_col2 = st.columns(2)
    
    with config_col1:
        st.markdown("**Trading Settings:**")
        st.text(f"Index: {st.session_state.config['index_symbol']}")
        st.text(f"Underlying: {st.session_state.config['underlying']}")
        st.text(f"Expiry: {st.session_state.config['expiry_date'].strftime('%Y-%m-%d')}")
        st.text(f"Max Options: {st.session_state.config['max_option_keys']}")
    
    with config_col2:
        st.markdown("**System Settings:**")
        st.text(f"Scheduler: {st.session_state.scheduler.interval_seconds}s")
        st.text(f"Signal Confidence: {confidence_threshold:.2f}")
        st.text(f"Live Trading: {'Enabled' if st.session_state.config.get('enable_live_trading', False) else 'Disabled'}")
        if st.session_state.config.get('enable_live_trading', False):
            st.text(f"Risk per Trade: {st.session_state.config.get('risk_per_trade', 1.0)}%")
    
    # Diagnostic tools (keep your existing code here)
    st.markdown("---")
    st.markdown("#### üîß Diagnostic Tools")
    # ... keep your existing diagnostic tools code ...
    
    # Diagnostic tools
    st.markdown("---")
    st.markdown("#### üîß Diagnostic Tools")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üß™ Test Upstox Connection", width='stretch'):
            # Test connection by trying to fetch profile
            try:
                profile = client.fetch_profile()
                if profile:
                    st.success(f"‚úÖ Upstox connection working")
                else:
                    st.error("‚ùå Failed to fetch profile")
            except Exception as e:
                st.error(f"‚ùå Connection test failed: {e}")
    
    with col2:
        if st.button("üìä Test Feature Pipeline", width='stretch'):
            try:
                # Get latest features
                features = fetch_latest_features(FEATURE_VERSION)
                if features is not None and not features.empty():
                    st.success(f"‚úÖ Feature pipeline working ({len(features)} features)")
                else:
                    st.warning("‚ö†Ô∏è No features found")
            except Exception as e:
                st.error(f"‚ùå Feature pipeline error: {e}")
    
    with col3:
        if st.button("üîÑ Clear Option Keys Cache", width='stretch'):
            st.session_state.option_keys = []
            st.success("‚úÖ Option keys cache cleared")

# ==============================
# PERFORMANCE MODAL
# ==============================

if st.session_state.get("show_performance", False):
    with st.expander("üìà Performance Analytics", expanded=True):
        st.markdown("### üìà Trading Performance")
        
        performance = fetch_signal_performance(days=30)
        
        if performance:
            # Overall metrics
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Total Trades", performance["total_signals"])
            
            with col2:
                st.metric("Profitable Trades", performance["profitable_signals"])
            
            with col3:
                win_rate = performance["win_rate"]
                color = "normal" if win_rate > 50 else "inverse"
                st.metric("Win Rate", f"{win_rate:.1f}%", delta_color=color)
            
            with col4:
                st.metric("Analysis Period", f"{performance['period_days']} days")
            
            # Breakdown by signal type
            st.markdown("#### Breakdown by Signal Type")
            
            if "breakdown" in performance:
                breakdown_df = pd.DataFrame(performance["breakdown"])
                st.dataframe(breakdown_df, width='stretch')
        
        st.session_state.show_performance = False

# ==============================
# LOGS MODAL
# ==============================

if st.session_state.get("show_logs", False):
    with st.expander("üìã System Logs", expanded=True):
        st.markdown("### üìã Recent System Activity")
        
        # Could fetch from system_health table here
        st.info("Logs would be displayed here from the database.")
        
        st.session_state.show_logs = False

# ==============================
# FOOTER
# ==============================

st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #6B7280; font-size: 0.9rem;'>
    Algorithmic Trading Research Platform v2.0 | 
    Research Features: OI Velocity, Gamma Exposure, Walls/Traps, Divergence Analysis
</div>
""", unsafe_allow_html=True)

# Simple auto-refresh timer
import time

if "last_auto_run" not in st.session_state:
    st.session_state.last_auto_run = time.time()

current_time = time.time()
time_since_last = current_time - st.session_state.last_auto_run

if time_since_last > 30:  # 30 seconds
    print(f"üîÑ Simple timer triggered after {int(time_since_last)}s")
    
    # Execute research cycle
    research_data = execute_research_cycle()
    
    if research_data:
        st.session_state.last_auto_run = current_time
        print(f"‚úÖ Auto-refresh executed successfully")
        
        # Update scheduler's last run time
        st.session_state.scheduler._last_run = datetime.now()
        st.session_state.scheduler._total_executions += 1
        
        # Update auto-refresh manager
        st.session_state.auto_refresh_manager.last_execution_time = datetime.now()
        st.session_state.auto_refresh_manager.execution_count += 1
        
        # Force UI update
        st.rerun()
    else:
        print("‚ùå Auto-refresh failed")
else:
    print(f"‚è±Ô∏è Next auto-refresh in {int(30 - time_since_last)}s")



====================================================================================================
FILE: .\backtest\__init__.py
====================================================================================================


File Name: __init__.py
Full Path: G:\trading_app\backtest\__init__.py
Size: 0 KB
Last Modified: 01/25/2026 18:09:44
Extension: .py




====================================================================================================
FILE: .\backtest\engine.py
====================================================================================================


File Name: engine.py
Full Path: G:\trading_app\backtest\engine.py
Size: 3.3 KB
Last Modified: 01/25/2026 18:07:12
Extension: .py

import sqlite3
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path

from core.signals.state_machine import SignalStateMachine


DB_PATH = Path("G:/trading_app/storage/trading.db")


class BacktestEngine:
    """
    Replays historical features to simulate signals and trades.
    """

    def __init__(
        self,
        feature_version: str,
        holding_minutes: int = 5,
        quantity: int = 1
    ):
        self.feature_version = feature_version
        self.holding_minutes = holding_minutes
        self.quantity = quantity
        self.signal_engine = SignalStateMachine(signal_expiry_minutes=holding_minutes)

    # ==============================
    # LOAD HISTORICAL FEATURES
    # ==============================

    def load_features(self) -> pd.DataFrame:
        with sqlite3.connect(DB_PATH) as conn:
            df = pd.read_sql(
                """
                SELECT *
                FROM market_features
                WHERE feature_version = ?
                ORDER BY timestamp
                """,
                conn,
                params=(self.feature_version,)
            )

        df["timestamp"] = pd.to_datetime(df["timestamp"])
        return df

    # ==============================
    # RUN BACKTEST
    # ==============================

    def run(self) -> pd.DataFrame:
        df = self.load_features()
        if df.empty:
            raise RuntimeError("No features found for backtest")

        trades = []
        open_trade = None

        for i in range(len(df) - 1):
            row = df.iloc[i]
            next_row = df.iloc[i + 1]

            # If no open trade ‚Üí check for signal
            if open_trade is None:
                signal = self.signal_engine.build_signal(row)

                if signal["signal_type"] in ("BUY", "SELL"):
                    open_trade = {
                        "signal_id": signal["signal_id"],
                        "direction": signal["signal_type"],
                        "entry_time": next_row["timestamp"],
                        "entry_price": next_row["spot_price"],
                        "expiry_time": next_row["timestamp"]
                                       + timedelta(minutes=self.holding_minutes)
                    }

            # If trade open ‚Üí check exit
            if open_trade is not None:
                if next_row["timestamp"] >= open_trade["expiry_time"]:
                    exit_price = next_row["spot_price"]

                    pnl = (
                        (exit_price - open_trade["entry_price"])
                        if open_trade["direction"] == "BUY"
                        else (open_trade["entry_price"] - exit_price)
                    ) * self.quantity

                    trades.append({
                        "signal_id": open_trade["signal_id"],
                        "entry_time": open_trade["entry_time"],
                        "exit_time": next_row["timestamp"],
                        "entry_price": open_trade["entry_price"],
                        "exit_price": exit_price,
                        "quantity": self.quantity,
                        "pnl": pnl
                    })

                    open_trade = None

        return pd.DataFrame(trades)




====================================================================================================
FILE: .\backtest\metrics.py
====================================================================================================


File Name: metrics.py
Full Path: G:\trading_app\backtest\metrics.py
Size: 1.51 KB
Last Modified: 01/25/2026 18:15:15
Extension: .py

import pandas as pd
import numpy as np


def compute_backtest_metrics(trades_df: pd.DataFrame) -> dict:
    """
    Compute performance metrics from backtest trades.
    Safe for empty DataFrames.
    """

    if trades_df is None or trades_df.empty:
        return {
            "total_trades": 0,
            "win_rate": 0.0,
            "total_pnl": 0.0,
            "avg_pnl": 0.0,
            "max_drawdown": 0.0,
            "sharpe": 0.0,
            "expectancy": 0.0
        }

    pnl = trades_df["pnl"]

    total_trades = len(pnl)
    wins = pnl[pnl > 0]
    losses = pnl[pnl <= 0]

    win_rate = len(wins) / total_trades if total_trades else 0.0
    total_pnl = pnl.sum()
    avg_pnl = pnl.mean()

    # Equity curve
    equity = pnl.cumsum()
    running_max = equity.cummax()
    drawdown = equity - running_max
    max_drawdown = drawdown.min()

    # Sharpe (PnL-based)
    sharpe = 0.0
    if pnl.std() != 0:
        sharpe = (pnl.mean() / pnl.std()) * np.sqrt(len(pnl))

    # Expectancy
    avg_win = wins.mean() if not wins.empty else 0.0
    avg_loss = losses.mean() if not losses.empty else 0.0
    expectancy = (win_rate * avg_win) + ((1 - win_rate) * avg_loss)

    return {
        "total_trades": total_trades,
        "win_rate": round(win_rate, 3),
        "total_pnl": round(total_pnl, 2),
        "avg_pnl": round(avg_pnl, 2),
        "max_drawdown": round(max_drawdown, 2),
        "sharpe": round(sharpe, 3),
        "expectancy": round(expectancy, 2)
    }




====================================================================================================
FILE: .\backtest\run_backtest.py
====================================================================================================


File Name: run_backtest.py
Full Path: G:\trading_app\backtest\run_backtest.py
Size: 0.45 KB
Last Modified: 01/25/2026 18:15:27
Extension: .py

from backtest.engine import BacktestEngine
from backtest.metrics import compute_backtest_metrics
from ml.feature_contract import FEATURE_VERSION

engine = BacktestEngine(
    feature_version=FEATURE_VERSION,
    holding_minutes=5,
    quantity=1
)

results = engine.run()
metrics = compute_backtest_metrics(results)

print("Backtest Results:")
print(results)

print("\nBacktest Metrics:")
for k, v in metrics.items():
    print(f"{k}: {v}")




====================================================================================================
FILE: .\core\debug_session.py
====================================================================================================


File Name: debug_session.py
Full Path: G:\trading_app\core\debug_session.py
Size: 2.08 KB
Last Modified: 01/27/2026 22:44:52
Extension: .py

import streamlit as st
from pathlib import Path

st.set_page_config(layout="wide", page_title="Debug Auth")

st.title("üîç Authentication Debug")

# Check token file
token_path = Path("data/tokens/upstox_tokens.enc")
st.write(f"Token path: {token_path}")
st.write(f"Token exists: {token_path.exists()}")

# Simulate what authenticate() should do
if not token_path.exists():
    st.success("‚úÖ Perfect! Token file doesn't exist - should show login button")
    
    # Show what the login button should look like
    st.markdown("---")
    st.subheader("This is what should appear:")
    
    login_url = "https://api.upstox.com/v2/login/authorization/dialog?response_type=code&client_id=e9bc6b14-447a-4d2f-aa32-2a4aecbafe56&redirect_uri=http://127.0.0.1:8501/callback&scope=order placement portfolio"
    
    st.markdown(f"""
    <div style="text-align: center;">
        <a href="{login_url}" target="_blank">
            <button style="
                background-color: #00d09c;
                color: white;
                padding: 15px 30px;
                font-size: 18px;
                font-weight: bold;
                border: none;
                border-radius: 10px;
                cursor: pointer;
                width: 80%;
                margin: 20px 0;
                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            ">
            üìà Login with Upstox
            </button>
        </a>
    </div>
    """, unsafe_allow_html=True)
    
    st.warning("If you don't see this in the main app, the issue is in how authenticate() is being called.")
else:
    st.warning("Token file exists - try deleting it with: `rm data/tokens/upstox_tokens.enc`")

# Check how authenticate is called in app.py
st.markdown("---")
st.subheader("How authenticate() is called in app.py:")
st.code("""
# This should be in your app.py:
from core.session import UpstoxSession

# Early in the app initialization:
token = UpstoxSession.authenticate()

# If token is None, authenticate() should have shown login button
# and stopped execution with st.stop()
""")



====================================================================================================
FILE: .\core\feature_pipeline.py
====================================================================================================


File Name: feature_pipeline.py
Full Path: G:\trading_app\core\feature_pipeline.py
Size: 27.89 KB
Last Modified: 01/28/2026 11:09:52
Extension: .py

"""
Enhanced Feature Pipeline with Research-Based Feature Engineering.
Implements all research concepts:
1. OI Velocity & Gamma Exposure features
2. Structural Walls & Traps features
3. Spot Divergence features
4. Market microstructure features
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, Optional, Tuple
from dataclasses import dataclass
from scipy import stats
from datetime import timezone

from ml.feature_contract import FEATURE_VERSION, FEATURE_COLUMNS, TARGET_COLUMN
from storage.repository import insert_market_features
from data.upstox_client import UpstoxClient, MarketAnalytics

# Import feature computation modules
from features.option_features import compute_option_features
from features.price_features import compute_price_features
from features.breadth import compute_breadth_features

# ==============================
# RESEARCH-BASED FEATURE ENGINEERING
# ==============================

@dataclass
class ResearchFeatures:
    """Container for research-based features"""
    # OI Velocity features
    oi_velocity: float
    oi_velocity_ma: float  # Moving average
    oi_velocity_std: float  # Standard deviation
    oi_regime: str  # EXPANSIVE/NORMAL/CONSTRICTED
    
    # Gamma Exposure features
    net_gamma: float
    gamma_regime: str  # POSITIVE/NEGATIVE/NEUTRAL
    gamma_flip_distance: float  # Distance to nearest gamma flip
    max_gamma_strike: float
    
    # Structural features
    wall_strength: float  # Combined strength of top walls
    wall_defense_score: float  # Defense score (0-1)
    trap_probability: float  # Probability of trap formation
    
    # Divergence features
    price_oi_divergence: float  # Divergence between price and OI
    price_gamma_divergence: float  # Divergence between price and gamma
    divergence_score: float  # Combined divergence score
    
    # Market microstructure
    put_call_ratio: float
    max_pain_distance: float  # Distance to max pain
    vix_smile: float  # Volatility smile curvature
    skewness: float  # Option skew
    
    # Wyckoff-inspired features
    spring_detection: float  # Bear trap probability
    upthrust_detection: float  # Bull trap probability
    accumulation_score: float  # Accumulation phase score

class EnhancedFeatureEngine:
    """
    Enhanced feature engineering engine incorporating research concepts.
    Transforms raw market data into research-based predictive features.
    """
    
    def __init__(self, lookback_periods: int = 20):
        self.lookback = lookback_periods
        self.feature_history = []
        self.oi_velocity_history = []
        self.gamma_history = []
        
    def extract_research_features(self, 
                                option_chain_analytics: Dict,
                                spot_price: float,
                                price_series: pd.Series,
                                volume_series: pd.Series,
                                constituents_df: pd.DataFrame) -> ResearchFeatures:
        """
        Extract research-based features from market analytics.
        """
        analytics = option_chain_analytics.get("analytics", {})
        
        # Add null checks
        if not analytics:
            # Return default/empty features
            return ResearchFeatures(
                oi_velocity=0.0,
                oi_velocity_ma=0.0,
                oi_velocity_std=0.0,
                oi_regime="NORMAL",
                net_gamma=0.0,
                gamma_regime="NEUTRAL",
                gamma_flip_distance=0.0,
                max_gamma_strike=0.0,
                wall_strength=0.0,
                wall_defense_score=0.0,
                trap_probability=0.0,
                price_oi_divergence=0.0,
                price_gamma_divergence=0.0,
                divergence_score=0.0,
                put_call_ratio=1.0,
                max_pain_distance=0.0,
                vix_smile=0.0,
                skewness=0.0,
                spring_detection=0.0,
                upthrust_detection=0.0,
                accumulation_score=0.0
            )
        
        # OI Velocity features
        oi_velocity = analytics.get("oi_velocity", 0.0)
        oi_regime = analytics.get("oi_regime", "NORMAL")
        
        # Update history and calculate statistics
        self.oi_velocity_history.append(oi_velocity)
        if len(self.oi_velocity_history) > self.lookback:
            self.oi_velocity_history = self.oi_velocity_history[-self.lookback:]
        
        oi_velocity_ma = np.mean(self.oi_velocity_history) if self.oi_velocity_history else 0.0
        oi_velocity_std = np.std(self.oi_velocity_history) if len(self.oi_velocity_history) > 1 else 1.0
        
        # Gamma Exposure features
        gamma_data = analytics.get("gamma_exposure", {})
        net_gamma = gamma_data.get("net_gamma", 0.0)
        gamma_regime = gamma_data.get("regime", "NEUTRAL")
        flip_levels = gamma_data.get("flip_levels", [])
        max_gamma_strike = gamma_data.get("max_impact_strike", 0.0)
        
        # Calculate distance to nearest gamma flip
        gamma_flip_distance = 0.0
        if flip_levels:
            distances = [abs(level - spot_price) / spot_price for level in flip_levels]
            gamma_flip_distance = min(distances) if distances else 0.0
        
        # Update gamma history
        self.gamma_history.append(net_gamma)
        if len(self.gamma_history) > self.lookback:
            self.gamma_history = self.gamma_history[-self.lookback:]
        
        # Structural features
        walls = analytics.get("structural_walls", [])
        traps = analytics.get("potential_traps", [])
        
        # Wall strength (weighted by concentration and defense)
        wall_strength = 0.0
        wall_defense_score = 0.0
        if walls:
            for wall in walls:
                concentration = wall.get("concentration", 0.0)
                defended = 1.0 if wall.get("defended", False) else 0.0
                distance = wall.get("distance_pct", 100) / 100  # Normalize
                
                # Walls closer to spot have more impact
                proximity_factor = 1.0 / (1.0 + distance)
                wall_strength += concentration * proximity_factor
                wall_defense_score += defended * concentration
        
        # Normalize wall features
        wall_strength = min(wall_strength, 1.0)
        wall_defense_score = min(wall_defense_score, 1.0)
        
        # Trap probability
        trap_probability = 0.0
        if traps:
            trap_confidences = [trap.get("confidence", 0.0) for trap in traps]
            trap_probability = max(trap_confidences) if trap_confidences else 0.0
        
        # Divergence features
        divergence_data = analytics.get("spot_divergence", {})
        has_divergence = divergence_data.get("has_divergence", False)
        divergence_type = divergence_data.get("type", "")
        divergence_confidence = divergence_data.get("confidence", 0.0)
        
        # Calculate price-OI divergence
        price_change = 0.0
        if len(price_series) >= 2:
            price_change = (price_series.iloc[-1] - price_series.iloc[-2]) / price_series.iloc[-2] * 100
        
        price_oi_divergence = abs(price_change - oi_velocity) if oi_velocity != 0 else 0.0
        
        # Calculate price-gamma divergence
        price_gamma_divergence = 0.0
        if self.gamma_history and len(self.gamma_history) >= 2:
            gamma_change = self.gamma_history[-1] - self.gamma_history[-2]
            price_gamma_divergence = abs(price_change - gamma_change * 100)
        
        divergence_score = divergence_confidence if has_divergence else 0.0
        
        # Market microstructure features
        pcr_data = MarketAnalytics.calculate_put_call_ratio(option_chain_analytics.get("raw_data", pd.DataFrame()))
        put_call_ratio = pcr_data.get("pcr_oi", 0.0)
        
        max_pain_data = MarketAnalytics.detect_max_pain(option_chain_analytics.get("raw_data", pd.DataFrame()))
        max_pain_strike = max_pain_data.get("max_pain_strike", 0.0)
        max_pain_distance = abs(spot_price - max_pain_strike) / spot_price if spot_price > 0 else 0.0
        
        # Calculate VIX smile (simplified)
        vix_smile = self._calculate_vix_smile(option_chain_analytics.get("raw_data", pd.DataFrame()), spot_price)
        
        # Calculate skewness
        skewness = self._calculate_option_skew(option_chain_analytics.get("raw_data", pd.DataFrame()))
        
        # Wyckoff-inspired features
        spring_detection = self._detect_spring_pattern(price_series, volume_series, constituents_df)
        upthrust_detection = self._detect_upthrust_pattern(price_series, volume_series, constituents_df)
        accumulation_score = self._calculate_accumulation_score(price_series, volume_series, oi_velocity)
        
        return ResearchFeatures(
            oi_velocity=oi_velocity,
            oi_velocity_ma=oi_velocity_ma,
            oi_velocity_std=oi_velocity_std,
            oi_regime=oi_regime,
            
            net_gamma=net_gamma,
            gamma_regime=gamma_regime,
            gamma_flip_distance=gamma_flip_distance,
            max_gamma_strike=max_gamma_strike,
            
            wall_strength=wall_strength,
            wall_defense_score=wall_defense_score,
            trap_probability=trap_probability,
            
            price_oi_divergence=price_oi_divergence,
            price_gamma_divergence=price_gamma_divergence,
            divergence_score=divergence_score,
            
            put_call_ratio=put_call_ratio,
            max_pain_distance=max_pain_distance,
            vix_smile=vix_smile,
            skewness=skewness,
            
            spring_detection=spring_detection,
            upthrust_detection=upthrust_detection,
            accumulation_score=accumulation_score
        )
    
    def _calculate_vix_smile(self, option_chain_df: pd.DataFrame, spot_price: float) -> float:
        """Calculate volatility smile curvature."""
        if option_chain_df.empty or spot_price <= 0:
            return 0.0
        
        # Group by distance from spot
        option_chain_df = option_chain_df.copy()
        option_chain_df['distance_pct'] = abs(option_chain_df['strike'] - spot_price) / spot_price * 100
        
        # Bin by distance
        bins = [0, 2, 5, 10, 20]
        smiles = []
        
        for i in range(len(bins) - 1):
            lower = bins[i]
            upper = bins[i + 1]
            
            mask = (option_chain_df['distance_pct'] >= lower) & (option_chain_df['distance_pct'] < upper)
            bin_iv = option_chain_df.loc[mask, 'iv'].mean()
            
            if not np.isnan(bin_iv):
                smiles.append(bin_iv)
        
        # Calculate smile curvature (higher = steeper smile)
        if len(smiles) >= 3:
            return smiles[0] - smiles[-1]  # ATM vs far OTM
        return 0.0
    
    def _calculate_option_skew(self, option_chain_df: pd.DataFrame) -> float:
        """Calculate option skew (put IV - call IV)."""
        if option_chain_df.empty:
            return 0.0
        
        put_iv = option_chain_df[option_chain_df['option_type'] == 'PE']['iv'].mean()
        call_iv = option_chain_df[option_chain_df['option_type'] == 'CE']['iv'].mean()
        
        if np.isnan(put_iv) or np.isnan(call_iv):
            return 0.0
        
        return put_iv - call_iv  # Positive = put skew (bearish), Negative = call skew (bullish)
    
    def _detect_spring_pattern(self, price_series: pd.Series, 
                              volume_series: pd.Series,
                              constituents_df: pd.DataFrame) -> float:
        """Detect Wyckoff spring pattern (bear trap)."""
        if len(price_series) < 10 or len(volume_series) < 10:
            return 0.0
        
        # Simplified spring detection
        recent_prices = price_series.iloc[-5:].values
        recent_volumes = volume_series.iloc[-5:].values
        
        # Spring: price makes lower low but closes above previous low
        if len(recent_prices) >= 5:
            low1 = np.min(recent_prices[:3])  # First low
            low2 = np.min(recent_prices[2:])  # Second low (potential spring)
            close = recent_prices[-1]
            
            # Volume spike on second low
            volume_spike = recent_volumes[-2] > np.mean(recent_volumes[:-1]) * 1.5
            
            if low2 < low1 * 0.995 and close > low1 and volume_spike:
                # Calculate spring probability
                price_recovery = (close - low2) / low2
                return min(price_recovery * 10, 1.0)
        
        return 0.0
    
    def _detect_upthrust_pattern(self, price_series: pd.Series,
                                volume_series: pd.Series,
                                constituents_df: pd.DataFrame) -> float:
        """Detect Wyckoff upthrust pattern (bull trap)."""
        if len(price_series) < 10 or len(volume_series) < 10:
            return 0.0
        
        recent_prices = price_series.iloc[-5:].values
        recent_volumes = volume_series.iloc[-5:].values
        
        # Upthrust: price makes higher high but closes below previous high
        if len(recent_prices) >= 5:
            high1 = np.max(recent_prices[:3])  # First high
            high2 = np.max(recent_prices[2:])  # Second high (potential upthrust)
            close = recent_prices[-1]
            
            # Volume spike on second high
            volume_spike = recent_volumes[-2] > np.mean(recent_volumes[:-1]) * 1.5
            
            if high2 > high1 * 1.005 and close < high1 and volume_spike:
                # Calculate upthrust probability
                price_rejection = (high2 - close) / high2
                return min(price_rejection * 10, 1.0)
        
        return 0.0
    
    def _calculate_accumulation_score(self, price_series: pd.Series,
                                     volume_series: pd.Series,
                                     oi_velocity: float) -> float:
        """Calculate accumulation/distribution score."""
        if len(price_series) < 20 or len(volume_series) < 20:
            return 0.0
        
        # Price in trading range
        price_range = price_series.iloc[-20:]
        range_high = price_range.max()
        range_low = price_range.min()
        range_width = (range_high - range_low) / range_low
        
        # Low volatility in range (accumulation)
        volatility = price_range.pct_change().std()
        
        # Volume analysis
        volume_trend = np.polyfit(range(len(volume_series.iloc[-20:])), 
                                 volume_series.iloc[-20:].values, 1)[0]
        
        # OI building during range (accumulation)
        oi_building = oi_velocity > 0.5
        
        # Calculate accumulation score
        score = 0.0
        
        # Narrow range with low volatility
        if range_width < 0.02 and volatility < 0.005:
            score += 0.3
        
        # Volume declining or stable (not distribution)
        if volume_trend <= 0:
            score += 0.2
        
        # OI building
        if oi_building:
            score += 0.3
        
        # Price near range lows (better accumulation)
        current_price = price_series.iloc[-1]
        if current_price < range_low * 1.02:
            score += 0.2
        
        return min(score, 1.0)

# ==============================
# ENHANCED FEATURE PIPELINE
# ==============================

# Global feature engine
_feature_engine = EnhancedFeatureEngine()

def build_and_store_features(
    *,
    timestamp: pd.Timestamp,
    option_chain_df: pd.DataFrame,
    spot_price: float,
    expiry_datetime,
    ltp: float,
    vwap: float,
    price_series: pd.Series,
    volume_series: pd.Series,
    constituents_df: pd.DataFrame,
    ccc_history: pd.Series,
    client: Optional[UpstoxClient] = None,
    option_keys: Optional[list] = None
) -> Optional[Dict]:
    """
    Enhanced feature pipeline with research-based feature engineering.
    
    Args:
        client: UpstoxClient instance (required for advanced analytics)
        option_keys: Option keys for fetching enhanced analytics
    
    Returns:
        Dictionary with features and research analytics
    """
    
    # ------------------------------
    # TIMESTAMP VALIDATION
    # ------------------------------
    if not isinstance(timestamp, pd.Timestamp):
        raise TypeError("timestamp must be pandas.Timestamp")
    
    timestamp_str = timestamp.strftime("%Y-%m-%dT%H:%M:%S.%f")
    
    # ------------------------------
    # FETCH ENHANCED ANALYTICS (if client provided)
    # ------------------------------
    research_analytics = None
    if client and option_keys and len(option_keys) > 0:
        try:
            # Get comprehensive analytics
            research_analytics = client.fetch_option_chain_with_analytics(
                option_keys, spot_price
            )
        except Exception as e:
            print(f"Warning: Could not fetch enhanced analytics: {e}")
            research_analytics = None
    
    # ------------------------------
    # COMPUTE BASE FEATURES
    # ------------------------------
    option_feats = compute_option_features(option_chain_df, spot_price, expiry_datetime)
    price_feats = compute_price_features(ltp, vwap, price_series, volume_series)
    breadth_feats = compute_breadth_features(constituents_df, ccc_history)
    
    # ------------------------------
    # COMPUTE RESEARCH FEATURES
    # ------------------------------
    research_feats_dict = {}
    if research_analytics:
        try:
            research_feats = _feature_engine.extract_research_features(
                research_analytics,
                spot_price,
                price_series,
                volume_series,
                constituents_df
            )
            
            # Convert research features to dictionary
            research_feats_dict = {
                # OI Velocity features
                "oi_velocity": research_feats.oi_velocity,
                "oi_velocity_ma": research_feats.oi_velocity_ma,
                "oi_velocity_std": research_feats.oi_velocity_std,
                "oi_regime_expansive": 1.0 if research_feats.oi_regime == "EXPANSIVE" else 0.0,
                "oi_regime_constricted": 1.0 if research_feats.oi_regime == "CONSTRICTED" else 0.0,
                
                # Gamma Exposure features
                "net_gamma": research_feats.net_gamma,
                "gamma_regime_positive": 1.0 if "POSITIVE" in research_feats.gamma_regime else 0.0,
                "gamma_regime_negative": 1.0 if "NEGATIVE" in research_feats.gamma_regime else 0.0,
                "gamma_flip_distance": research_feats.gamma_flip_distance,
                "max_gamma_strike_distance": abs(spot_price - research_feats.max_gamma_strike) / spot_price,
                
                # Structural features
                "wall_strength": research_feats.wall_strength,
                "wall_defense_score": research_feats.wall_defense_score,
                "trap_probability": research_feats.trap_probability,
                
                # Divergence features
                "price_oi_divergence": research_feats.price_oi_divergence,
                "price_gamma_divergence": research_feats.price_gamma_divergence,
                "divergence_score": research_feats.divergence_score,
                "has_divergence": 1.0 if research_feats.divergence_score > 0.3 else 0.0,
                
                # Market microstructure
                "put_call_ratio": research_feats.put_call_ratio,
                "max_pain_distance": research_feats.max_pain_distance,
                "vix_smile": research_feats.vix_smile,
                "skewness": research_feats.skewness,
                
                # Wyckoff features
                "spring_detection": research_feats.spring_detection,
                "upthrust_detection": research_feats.upthrust_detection,
                "accumulation_score": research_feats.accumulation_score,
                
                # Derived features
                "gamma_wall_interaction": research_feats.wall_strength * abs(research_feats.net_gamma),
                "velocity_divergence_composite": research_feats.oi_velocity * research_feats.divergence_score,
                "trap_gamma_composite": research_feats.trap_probability * (1.0 if research_feats.gamma_regime == "NEGATIVE" else 0.0)
            }
        except Exception as e:
            print(f"Warning: Research feature extraction failed: {e}")
            research_feats_dict = {}
    
    # ------------------------------
    # BUILD FEATURE ROW
    # ------------------------------
    feature_row = {
        "timestamp": timestamp_str,
        "feature_version": FEATURE_VERSION,
        "future_return_5m": None
    }
    
    # Fill ALL required feature columns explicitly
    for col in FEATURE_COLUMNS:
        if col == "timestamp":
            continue
        feature_row[col] = (
            option_feats.get(col) or
            price_feats.get(col) or
            breadth_feats.get(col) or
            0.0
        )
    
    # Add time to expiry with proper timezone handling
    def normalize_datetime(dt):
        """Normalize datetime to UTC timezone-aware."""
        if dt is None:
            return None
        
        # If it's already timezone-aware, convert to UTC
        if hasattr(dt, 'tzinfo') and dt.tzinfo is not None:
            return dt.astimezone(timezone.utc)
        
        # If it's timezone-naive, assume UTC and make it aware
        if isinstance(dt, pd.Timestamp):
            if dt.tz is None:
                return dt.tz_localize('UTC')
            else:
                return dt.tz_convert('UTC')
        elif isinstance(dt, datetime):
            return dt.replace(tzinfo=timezone.utc)
        
        return dt
    
    # Normalize both timestamps to UTC
    if expiry_datetime and timestamp:
        expiry_utc = normalize_datetime(expiry_datetime)
        timestamp_utc = normalize_datetime(timestamp)
        
        if expiry_utc and timestamp_utc:
            time_diff = (expiry_utc - timestamp_utc).total_seconds() / 60
            feature_row["time_to_expiry_minutes"] = max(int(time_diff), 0)
        else:
            feature_row["time_to_expiry_minutes"] = 0
    else:
        feature_row["time_to_expiry_minutes"] = 0
    
    # Merge research features
    feature_row.update(research_feats_dict)
    
    # ------------------------------
    # VALIDATE AND PERSIST
    # ------------------------------
    # Create DataFrame with all columns (base + research)
    all_columns = list(feature_row.keys())
    df = pd.DataFrame([feature_row], columns=all_columns)
    
    # Debug info
    print(f"FEATURE PIPELINE: Generated {len(feature_row)} features")
    print(f"Timestamp: {timestamp_str}")
    print(f"Spot Price: {spot_price}")
    
    if research_analytics:
        print(f"Research Analytics: {len(research_feats_dict)} research features added")
        # Log key metrics
        analytics = research_analytics.get("analytics", {})
        print(f"OI Velocity: {analytics.get('oi_velocity', 'N/A')}")
        print(f"Gamma Regime: {analytics.get('gamma_exposure', {}).get('regime', 'N/A')}")
        print(f"Market Regime: {analytics.get('market_regime', 'N/A')}")
    
    # Check for NULLs
    null_cols = df.columns[df.isnull().any()].tolist()
    if null_cols:
        print(f"WARNING: NULL values in columns: {null_cols}")
        # Fill NULLs with 0 for numeric columns
        for col in null_cols:
            if col in df.select_dtypes(include=[np.number]).columns:
                df[col] = df[col].fillna(0.0)
    
    assert not df.empty, "Feature DataFrame is empty"
    
    # Persist to database
    insert_market_features(df)
    
    print(f"‚úì Features stored successfully at {timestamp_str}")
    
    # Return comprehensive result
    return {
        "features": feature_row,
        "research_analytics": research_analytics.get("analytics", {}) if research_analytics else {},
        "market_insights": research_analytics.get("market_insights", []) if research_analytics else [],
        "timestamp": timestamp_str,
        "spot_price": spot_price
    }

# ==============================
# UTILITY FUNCTIONS
# ==============================

def create_feature_summary(feature_row: Dict) -> Dict:
    """Create a summary of key features for display."""
    summary = {
        "timestamp": feature_row.get("timestamp", "N/A"),
        "base_features": {},
        "research_features": {},
        "signals": {}
    }
    
    # Base features
    base_keys = ["put_call_ratio", "vwap_distance", "price_momentum", 
                 "ccc_value", "ccc_slope", "time_to_expiry_minutes"]
    for key in base_keys:
        if key in feature_row:
            summary["base_features"][key] = feature_row[key]
    
    # Research features (top level)
    research_keys = ["oi_velocity", "net_gamma", "trap_probability", 
                    "divergence_score", "wall_strength"]
    for key in research_keys:
        if key in feature_row:
            summary["research_features"][key] = feature_row[key]
    
    # Generate signals
    signals = []
    
    # OI Velocity signal
    oi_vel = feature_row.get("oi_velocity", 0)
    if oi_vel > 1.5:
        signals.append("üìà Strong OI Buildup")
    elif oi_vel < -1.5:
        signals.append("üìâ OI Unwinding")
    
    # Gamma signal
    gamma_regime_pos = feature_row.get("gamma_regime_positive", 0)
    gamma_regime_neg = feature_row.get("gamma_regime_negative", 0)
    if gamma_regime_pos > 0.5:
        signals.append("üìå Positive Gamma (Stabilizing)")
    elif gamma_regime_neg > 0.5:
        signals.append("üöÄ Negative Gamma (Accelerating)")
    
    # Trap signal
    trap_prob = feature_row.get("trap_probability", 0)
    if trap_prob > 0.7:
        signals.append("üéØ High Trap Probability")
    elif trap_prob > 0.5:
        signals.append("‚ö†Ô∏è Moderate Trap Risk")
    
    # Divergence signal
    has_div = feature_row.get("has_divergence", 0)
    if has_div > 0.5:
        signals.append("üîç Divergence Detected")
    
    summary["signals"] = signals
    
    return summary

def validate_feature_contract(feature_row: Dict) -> bool:
    """Validate feature row against contract."""
    # Check required columns
    required_base = ["timestamp", "feature_version"]
    for col in required_base:
        if col not in feature_row:
            print(f"Missing required column: {col}")
            return False
    
    # Check data types
    for col, value in feature_row.items():
        if col == "timestamp":
            continue
        if col == "feature_version":
            if not isinstance(value, str):
                print(f"Invalid type for {col}: expected str, got {type(value)}")
                return False
        elif col in ["oi_regime_expansive", "oi_regime_constricted", 
                    "gamma_regime_positive", "gamma_regime_negative", "has_divergence"]:
            # Binary features
            if not isinstance(value, (int, float)):
                print(f"Invalid type for {col}: expected numeric, got {type(value)}")
                return False
        elif isinstance(value, (int, float)):
            # Numeric features - check for extreme values
            if abs(value) > 1e6:  # Unreasonable large value
                print(f"Extreme value for {col}: {value}")
                return False
            if pd.isna(value):
                print(f"NaN value for {col}")
                return False
    
    return True



====================================================================================================
FILE: .\core\scheduler.py
====================================================================================================


File Name: scheduler.py
Full Path: G:\trading_app\core\scheduler.py
Size: 14.82 KB
Last Modified: 01/28/2026 12:17:22
Extension: .py

from datetime import datetime, time, timedelta, timezone
from typing import Callable, Optional, Dict
import threading
import time as time_module  # Rename to avoid conflict with datetime.time
import streamlit as st
import numpy as np  # Add numpy import for metrics

class MarketScheduler:
    """
    Enhanced scheduler with research-aware execution control.
    
    Features:
    - Market hours detection
    - Intelligent interval adjustment
    - Execution throttling
    - Performance monitoring
    - Error recovery
    """
    
    def __init__(
        self,
        interval_seconds: int = 30,
        market_open: time = time(9, 15),  # CORRECT: use time objects
        market_close: time = time(15, 30),  # CORRECT: use time objects
        pre_market_minutes: int = 15,
        post_market_minutes: int = 15
    ):
        self.interval_seconds = interval_seconds
        self.market_open = market_open
        self.market_close = market_close
        self.pre_market_minutes = pre_market_minutes
        self.post_market_minutes = post_market_minutes
        
        # Execution tracking
        self._last_run: Optional[datetime] = None
        self._last_success: Optional[datetime] = None
        self._consecutive_failures: int = 0
        self._total_executions: int = 0
        self._lock = threading.Lock()
        
        # Performance metrics
        self.execution_times = []
        self.error_log = []
        
        # Adaptive interval (can adjust based on market conditions)
        self.min_interval = 10  # seconds
        self.max_interval = 300  # seconds
        self._current_interval = interval_seconds
        
    # ==============================
    # TIME MANAGEMENT
    # ==============================
    
    def _is_market_open(self, now: datetime) -> bool:
        """
        Check if market is open, including pre/post market.
        """
        t = now.time()
        
        # Pre-market period
        pre_market_start = time(
            self.market_open.hour,
            max(0, self.market_open.minute - self.pre_market_minutes)  # Ensure minutes don't go negative
        )
        
        # Post-market period
        post_market_end_minute = self.market_close.minute + self.post_market_minutes
        post_market_end_hour = self.market_close.hour + (post_market_end_minute // 60)
        post_market_end_minute = post_market_end_minute % 60
        post_market_end = time(
            post_market_end_hour,
            post_market_end_minute
        )
        
        return pre_market_start <= t <= post_market_end
    
    def _is_core_market_hours(self, now: datetime) -> bool:
        """Check if it's core trading hours."""
        t = now.time()
        return self.market_open <= t <= self.market_close
    
    def _get_time_to_market_open(self, now: datetime) -> Optional[float]:
        """Get seconds until market opens."""
        market_open_today = datetime.combine(now.date(), self.market_open)
        
        if now < market_open_today:
            return (market_open_today - now).total_seconds()
        
        # Market already open today, check tomorrow
        tomorrow = now.date() + timedelta(days=1)
        market_open_tomorrow = datetime.combine(tomorrow, self.market_open)
        return (market_open_tomorrow - now).total_seconds()
    
    def _get_market_status(self, now: datetime) -> Dict[str, any]:
        """Get detailed market status."""
        status = {
            "is_market_open": self._is_market_open(now),
            "is_core_hours": self._is_core_market_hours(now),
            "current_time": now,
            "market_open": self.market_open,
            "market_close": self.market_close
        }
        
        if not status["is_market_open"]:
            time_to_open = self._get_time_to_market_open(now)
            if time_to_open:
                status["time_to_open_hours"] = time_to_open / 3600
                status["next_open"] = now + timedelta(seconds=time_to_open)
        
        return status
    
    # ==============================
    # EXECUTION MANAGEMENT
    # ==============================
    
    def _is_due(self, now: datetime) -> bool:
        """Check if execution is due."""
        if self._last_run is None:
            return True
        
        delta = (now - self._last_run).total_seconds()
        return delta >= self._current_interval
    
    def _adjust_interval(self, execution_time: float, success: bool):
        """
        Adaptively adjust execution interval based on performance.
        """
        if not success:
            self._consecutive_failures += 1
            
            # Increase interval on consecutive failures
            if self._consecutive_failures >= 3:
                self._current_interval = min(
                    self._current_interval * 1.5,
                    self.max_interval
                )
                self._consecutive_failures = 0
        else:
            self._consecutive_failures = 0
            self._last_success = datetime.now()
            
            # Adjust interval based on execution time
            if execution_time > self._current_interval * 0.8:
                # Execution taking too long, increase interval
                self._current_interval = min(
                    self._current_interval * 1.2,
                    self.max_interval
                )
            elif execution_time < self._current_interval * 0.3:
                # Execution fast, can decrease interval (but not below min)
                self._current_interval = max(
                    self._current_interval * 0.9,
                    self.min_interval
                )
    
    def _should_execute(self, now: datetime, market_regime: Optional[str] = None) -> bool:
        """
        Determine if execution should proceed based on multiple factors.
        """
        # Basic checks
        if not self._is_market_open(now):
            return False
        
        if not self._is_due(now):
            return False
        
        # Check for too many recent failures
        if self._consecutive_failures >= 5:
            if self._last_success:
                time_since_success = (now - self._last_success).total_seconds()
                if time_since_success < 300:  # 5 minutes
                    return False  # Wait after multiple failures
        
        # Market regime-based adjustments
        if market_regime:
            # Execute more frequently during volatile regimes
            if market_regime in ["SQUEEZE", "BREAKOUT", "ACCELERATING"]:
                self._current_interval = max(self.min_interval, self.interval_seconds * 0.7)
            # Execute less frequently during stable regimes
            elif market_regime in ["RANGING", "STABILIZING"]:
                self._current_interval = min(self.max_interval, self.interval_seconds * 1.3)
        
        return True
    
    # ==============================
    # EXECUTION ENTRYPOINT
    # ==============================
    
    def run_if_due(self, run_cycle: Callable[[], any], market_regime: Optional[str] = None) -> Dict[str, any]:
        """
        Execute run_cycle() if due, with enhanced monitoring.
        
        Returns:
            Execution result dictionary
        """
        now = datetime.now()
        
        # Check if should execute
        if not self._should_execute(now, market_regime):
            return {
                "executed": False,
                "reason": "Not due or market closed",
                "next_execution_in": self._get_next_execution_time(now),
                "market_status": self._get_market_status(now)
            }
        
        # Thread-safe execution
        with self._lock:
            # Double-check inside lock
            if not self._is_due(now):
                return {
                    "executed": False,
                    "reason": "Already executed by another thread",
                    "next_execution_in": self._get_next_execution_time(now)
                }
            
            execution_start = datetime.now()
            result = {
                "executed": True,
                "start_time": execution_start.isoformat(),
                "success": False,
                "error": None,
                "execution_time": 0.0
            }
            
            try:
                # Execute the cycle
                cycle_result = run_cycle()
                execution_end = datetime.now()
                execution_time = (execution_end - execution_start).total_seconds()
                
                # Update tracking
                self._last_run = execution_start
                self._total_executions += 1
                self.execution_times.append(execution_time)
                
                # Keep only recent execution times
                if len(self.execution_times) > 100:
                    self.execution_times = self.execution_times[-100:]
                
                # Update result
                result.update({
                    "success": True,
                    "execution_time": execution_time,
                    "end_time": execution_end.isoformat(),
                    "cycle_result": cycle_result,
                    "current_interval": self._current_interval
                })
                
                # Adjust interval based on performance
                self._adjust_interval(execution_time, success=True)
                
            except Exception as e:
                execution_end = datetime.now()
                execution_time = (execution_end - execution_start).total_seconds()
                
                # Log error
                self.error_log.append({
                    "timestamp": execution_start.isoformat(),
                    "error": str(e),
                    "execution_time": execution_time
                })
                
                # Keep error log manageable
                if len(self.error_log) > 50:
                    self.error_log = self.error_log[-50:]
                
                # Update result
                result.update({
                    "success": False,
                    "error": str(e),
                    "execution_time": execution_time,
                    "end_time": execution_end.isoformat()
                })
                
                # Adjust interval on failure
                self._adjust_interval(execution_time, success=False)
                
                # Update last run time even on failure (to prevent rapid retry)
                self._last_run = execution_start
                self._total_executions += 1
            
            return result
    
    # ==============================
    # MONITORING & METRICS
    # ==============================
    
    def get_metrics(self) -> Dict[str, any]:
        """Get scheduler performance metrics."""
        metrics = {
            "total_executions": self._total_executions,
            "current_interval": self._current_interval,
            "last_run": self._last_run.isoformat() if self._last_run else None,
            "last_success": self._last_success.isoformat() if self._last_success else None,
            "consecutive_failures": self._consecutive_failures,
            "recent_error_count": len(self.error_log[-10:]),
            "is_market_open_now": self._is_market_open(datetime.now())
        }
        
        # Add execution time statistics
        if self.execution_times:
            metrics.update({
                "avg_execution_time": np.mean(self.execution_times),
                "max_execution_time": np.max(self.execution_times),
                "min_execution_time": np.min(self.execution_times),
                "recent_avg_execution_time": np.mean(self.execution_times[-10:]) if len(self.execution_times) >= 10 else None
            })
        
        return metrics
    
    def _get_next_execution_time(self, now: datetime) -> float:
        """Get seconds until next execution."""
        if self._last_run is None:
            return 0.0
        
        next_run = self._last_run + timedelta(seconds=self._current_interval)
        return max((next_run - now).total_seconds(), 0.0)
    
    def reset(self):
        """Reset scheduler state."""
        with self._lock:
            self._last_run = None
            self._last_success = None
            self._consecutive_failures = 0
            self._total_executions = 0
            self.execution_times = []
            self.error_log = []
            self._current_interval = self.interval_seconds
    
    def set_interval(self, interval_seconds: int):
        """Dynamically set execution interval."""
        with self._lock:
            self.interval_seconds = interval_seconds
            self._current_interval = max(
                min(interval_seconds, self.max_interval),
                self.min_interval
            )

# ==============================
# UTILITY FUNCTIONS
# ==============================

def create_market_scheduler(
    interval_seconds: int = 30,
    market_open: str = "09:15",
    market_close: str = "15:30"
) -> MarketScheduler:
    """
    Factory function to create market scheduler.
    
    Args:
        interval_seconds: Execution interval in seconds
        market_open: Market open time (HH:MM)
        market_close: Market close time (HH:MM)
    """
    # Parse time strings
    open_hour, open_minute = map(int, market_open.split(":"))
    close_hour, close_minute = map(int, market_close.split(":"))
    
    return MarketScheduler(
        interval_seconds=interval_seconds,
        market_open=time(open_hour, open_minute),  # CORRECT
        market_close=time(close_hour, close_minute)  # CORRECT
    )

def display_scheduler_status(scheduler: MarketScheduler):
    """Display scheduler status in Streamlit."""
    if not scheduler:
        return
    
    metrics = scheduler.get_metrics()
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if metrics["is_market_open_now"]:
            st.success("üü¢ Market Open")
        else:
            st.warning("üî¥ Market Closed")
        
        next_exec = scheduler._get_next_execution_time(datetime.now())
        if next_exec > 0:
            st.metric("Next Execution", f"{int(next_exec)}s")
    
    with col2:
        st.metric("Interval", f"{metrics['current_interval']}s")
        
        if metrics.get("avg_execution_time"):
            st.metric("Avg Exec Time", f"{metrics['avg_execution_time']:.1f}s")
    
    with col3:
        st.metric("Total Executions", metrics["total_executions"])
        
        if metrics["recent_error_count"] > 0:
            st.error(f"Recent Errors: {metrics['recent_error_count']}")



====================================================================================================
FILE: .\core\session - Copy.py
====================================================================================================


File Name: session - Copy.py
Full Path: G:\trading_app\core\session - Copy.py
Size: 22.76 KB
Last Modified: 01/27/2026 21:39:47
Extension: .py

"""
Enhanced Upstox Session Manager with persistent token storage,
refresh mechanism, and market state tracking.
Incorporates research insights: funding liquidity monitoring via OI velocity
and market microstructure analysis.
"""

import os
import json
import requests
import streamlit as st
from urllib.parse import urlencode
from typing import Optional, Dict, Tuple
from datetime import datetime, timedelta
import pickle
from pathlib import Path
from cryptography.fernet import Fernet
import hashlib

# ==============================
# CONFIG (EDIT THESE)
# ==============================

UPSTOX_AUTH_URL = "https://api.upstox.com/v2/login/authorization/dialog"
UPSTOX_TOKEN_URL = "https://api.upstox.com/v2/login/authorization/token"
UPSTOX_PROFILE_URL = "https://api.upstox.com/v2/user/profile"

# Get from Streamlit secrets or environment
try:
    CLIENT_ID = st.secrets["UPSTOX_CLIENT_ID"]
    CLIENT_SECRET = st.secrets["UPSTOX_CLIENT_SECRET"]
    REDIRECT_URI = st.secrets["UPSTOX_REDIRECT_URI"]
except:
    CLIENT_ID = os.getenv("UPSTOX_CLIENT_ID")
    CLIENT_SECRET = os.getenv("UPSTOX_CLIENT_SECRET")
    REDIRECT_URI = os.getenv("UPSTOX_REDIRECT_URI")

# ==============================
# SECURE TOKEN STORAGE
# ==============================

class TokenStorage:
    """
    Secure token storage with encryption and persistence.
    Stores tokens locally with automatic refresh capability.
    """
    
    def __init__(self, storage_path: str = "data/tokens"):
        self.storage_dir = Path(storage_path)
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate or load encryption key
        self.key_file = self.storage_dir / ".key"
        if self.key_file.exists():
            with open(self.key_file, 'rb') as f:
                self.encryption_key = f.read()
        else:
            self.encryption_key = Fernet.generate_key()
            with open(self.key_file, 'wb') as f:
                f.write(self.encryption_key)
        
        self.cipher = Fernet(self.encryption_key)
        self.token_file = self.storage_dir / "upstox_tokens.enc"
        
        # Track funding liquidity state (research concept)
        self.funding_state = {
            "last_oi_velocity": 0.0,
            "liquidity_regime": "NORMAL",  # NORMAL, CONSTRICTED, EXPANSIVE
            "last_update": datetime.utcnow()
        }
    
    def _generate_user_hash(self) -> str:
        """Generate unique user identifier for multi-user support"""
        # In production, use actual user ID. Here we use client ID as proxy
        return hashlib.sha256(f"{CLIENT_ID}_{REDIRECT_URI}".encode()).hexdigest()[:16]
    
    def save_tokens(self, access_token: str, refresh_token: str, expires_in: int):
        """Securely save tokens with expiration"""
        token_data = {
            "access_token": access_token,
            "refresh_token": refresh_token,
            "expires_at": (datetime.utcnow() + timedelta(seconds=expires_in)).isoformat(),
            "client_id": CLIENT_ID,
            "user_hash": self._generate_user_hash(),
            "created_at": datetime.utcnow().isoformat()
        }
        
        # Encrypt and save
        encrypted = self.cipher.encrypt(pickle.dumps(token_data))
        with open(self.token_file, 'wb') as f:
            f.write(encrypted)
        
        st.success("‚úì Authentication tokens saved securely")
    
    def load_tokens(self) -> Optional[Dict]:
        """Load and decrypt tokens, check expiration"""
        if not self.token_file.exists():
            return None
        
        try:
            with open(self.token_file, 'rb') as f:
                encrypted = f.read()
            
            token_data = pickle.loads(self.cipher.decrypt(encrypted))
            
            # Check expiration
            expires_at = datetime.fromisoformat(token_data["expires_at"])
            if datetime.utcnow() > expires_at - timedelta(minutes=5):  # 5 min buffer
                st.warning("‚ö†Ô∏è Access token expired or near expiry")
                return None
            
            # Verify client matches
            if token_data.get("client_id") != CLIENT_ID:
                st.error("Client ID mismatch - reauthentication required")
                return None
            
            return token_data
        except Exception as e:
            st.error(f"Token load error: {e}")
            return None
    
    def update_funding_state(self, oi_velocity: float):
        """
        Update funding liquidity state based on OI velocity.
        Research concept: Monitor capital flow intensity.
        """
        self.funding_state["last_oi_velocity"] = oi_velocity
        
        # Determine liquidity regime (research concept)
        if oi_velocity > 1.5:
            regime = "EXPANSIVE"  # High capital inflow
        elif oi_velocity < -1.5:
            regime = "CONSTRICTED"  # Capital outflow
        else:
            regime = "NORMAL"
        
        if self.funding_state["liquidity_regime"] != regime:
            st.info(f"üí∞ Liquidity regime changed: {regime} (OI Velocity: {oi_velocity:.2f})")
        
        self.funding_state["liquidity_regime"] = regime
        self.funding_state["last_update"] = datetime.utcnow()
    
    def get_funding_state(self) -> Dict:
        """Get current funding liquidity state"""
        return self.funding_state.copy()

# ==============================
# ENHANCED SESSION MANAGER
# ==============================

class UpstoxSession:
    """
    Enhanced session manager with:
    1. Persistent token storage
    2. Automatic token refresh
    3. Funding liquidity tracking
    4. Market state awareness
    """
    
    SESSION_KEY = "upstox_access_token"
    SESSION_REFRESH_KEY = "upstox_refresh_token"
    SESSION_PROFILE_KEY = "upstox_profile"
    
    _token_storage = TokenStorage()
    _market_state = {
        "current_regime": None,
        "last_velocity": 0.0,
        "gamma_regime": None,  # POSITIVE/NEGATIVE from research
        "wall_levels": {"call": None, "put": None},
        "trap_zones": []
    }
    
    @staticmethod
    def is_authenticated() -> bool:
        """Check if user has valid authentication"""
        if UpstoxSession.SESSION_KEY in st.session_state:
            return True
        
        # Check persistent storage
        tokens = UpstoxSession._token_storage.load_tokens()
        if tokens:
            # Load into session state
            st.session_state[UpstoxSession.SESSION_KEY] = tokens["access_token"]
            st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = tokens["refresh_token"]
            return True
        
        return False
    
    @staticmethod
    def get_access_token() -> Optional[str]:
        """Get current access token, refresh if needed"""
        if not UpstoxSession.is_authenticated():
            return None
        
        # Check if token is in session state
        if UpstoxSession.SESSION_KEY in st.session_state:
            return st.session_state[UpstoxSession.SESSION_KEY]
        
        return None
    
    @staticmethod
    def logout() -> None:
        """Secure logout - clear all session and storage"""
        for key in [UpstoxSession.SESSION_KEY, 
                   UpstoxSession.SESSION_REFRESH_KEY,
                   UpstoxSession.SESSION_PROFILE_KEY]:
            if key in st.session_state:
                del st.session_state[key]
        
        # Clear persistent storage
        storage_file = Path("data/tokens/upstox_tokens.enc")
        if storage_file.exists():
            storage_file.unlink()
        
        # Clear query params
        st.query_params.clear()
        st.success("‚úì Logged out successfully")
        st.rerun()
    
    # ==========================
    # TOKEN MANAGEMENT
    # ==========================
    
    @staticmethod
    def refresh_access_token(refresh_token: str) -> Optional[Tuple[str, str]]:
        """
        Refresh expired access token using refresh token.
        Returns (new_access_token, new_refresh_token) or None
        """
        headers = {
            "Content-Type": "application/x-www-form-urlencoded",
            "Accept": "application/json"
        }
        
        data = {
            "grant_type": "refresh_token",
            "refresh_token": refresh_token,
            "client_id": CLIENT_ID,
            "client_secret": CLIENT_SECRET
        }
        
        try:
            response = requests.post(
                UPSTOX_TOKEN_URL,
                headers=headers,
                data=data,
                timeout=10
            )
            
            if response.status_code == 200:
                token_data = response.json()
                return token_data.get("access_token"), token_data.get("refresh_token")
            else:
                st.error(f"Token refresh failed: {response.text}")
                return None
                
        except Exception as e:
            st.error(f"Token refresh error: {e}")
            return None
    
    @staticmethod
    def exchange_code_for_token(code: str) -> Optional[Dict]:
        """Exchange authorization code for tokens with full token data"""
        headers = {
            "Content-Type": "application/x-www-form-urlencoded",
            "Accept": "application/json"
        }
        
        data = {
            "code": code,
            "client_id": CLIENT_ID,
            "client_secret": CLIENT_SECRET,
            "redirect_uri": REDIRECT_URI,
            "grant_type": "authorization_code"
        }
        
        try:
            response = requests.post(
                UPSTOX_TOKEN_URL,
                headers=headers,
                data=data,
                timeout=10
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                st.error(f"Token exchange failed: {response.text}")
                return None
                
        except Exception as e:
            st.error(f"Token exchange error: {e}")
            return None
    
    # ==========================
    # USER PROFILE
    # ==========================
    
    @staticmethod
    def get_user_profile(access_token: str) -> Optional[Dict]:
        """Fetch user profile for session context"""
        headers = {
            "Accept": "application/json",
            "Authorization": f"Bearer {access_token}"
        }
        
        try:
            response = requests.get(UPSTOX_PROFILE_URL, headers=headers, timeout=10)
            if response.status_code == 200:
                return response.json().get("data", {})
            return None
        except Exception as e:
            st.warning(f"Profile fetch warning: {e}")
            return None
    
    # ==========================
    # MARKET STATE TRACKING (Research Integration)
    # ==========================
    
    @staticmethod
    def update_market_state(oi_velocity: float, gamma_exposure: float, 
                           wall_levels: Dict, spot_divergence: float):
        """
        Update comprehensive market state based on research concepts.
        
        Args:
            oi_velocity: Rate of change of Open Interest
            gamma_exposure: Net Gamma Exposure (positive/negative)
            wall_levels: Dict with 'call' and 'put' wall strike levels
            spot_divergence: Divergence between spot and derivative metrics
        """
        # Update funding liquidity state
        UpstoxSession._token_storage.update_funding_state(oi_velocity)
        
        # Determine gamma regime (research concept)
        if gamma_exposure > 0:
            gamma_regime = "POSITIVE"  # Stabilizing, pinning expected
        elif gamma_exposure < 0:
            gamma_regime = "NEGATIVE"  # Accelerating, squeezes possible
        else:
            gamma_regime = "NEUTRAL"
        
        # Detect potential traps (research concept)
        trap_zones = []
        current_price = 0  # Would be passed in production
        
        # Check if price is near walls with high OI unwinding
        for side, level in wall_levels.items():
            if level and abs(current_price - level) / level < 0.005:  # Within 0.5%
                # High probability of trap if OI velocity is negative (unwinding)
                if oi_velocity < -1.0:
                    trap_zones.append({
                        "side": side,
                        "level": level,
                        "type": "GAMMA_TRAP" if abs(gamma_exposure) > 0.5 else "OI_TRAP",
                        "confidence": min(abs(oi_velocity) / 3, 1.0)
                    })
        
        # Update market state
        UpstoxSession._market_state.update({
            "last_velocity": oi_velocity,
            "gamma_regime": gamma_regime,
            "wall_levels": wall_levels,
            "trap_zones": trap_zones,
            "spot_divergence": spot_divergence,
            "funding_regime": UpstoxSession._token_storage.get_funding_state()["liquidity_regime"],
            "timestamp": datetime.utcnow().isoformat()
        })
    
    @staticmethod
    def get_market_state() -> Dict:
        """Get current market state analysis"""
        return UpstoxSession._market_state.copy()
    
    # ==========================
    # STREAMLIT ENTRYPOINT (ENHANCED)
    # ==========================
    
    @staticmethod
    def authenticate() -> str:
        """
        Enhanced authentication flow with:
        1. Persistent token storage
        2. Automatic refresh
        3. User profile loading
        4. Market state initialization
        """
        
        # Already authenticated in session
        if UpstoxSession.is_authenticated():
            token = UpstoxSession.get_access_token()
            
            # Load profile if not loaded
            if UpstoxSession.SESSION_PROFILE_KEY not in st.session_state:
                profile = UpstoxSession.get_user_profile(token)
                if profile:
                    st.session_state[UpstoxSession.SESSION_PROFILE_KEY] = profile
            
            return token
        
        # Check persistent storage
        stored_tokens = UpstoxSession._token_storage.load_tokens()
        if stored_tokens:
            # Check if token needs refresh
            expires_at = datetime.fromisoformat(stored_tokens["expires_at"])
            if datetime.utcnow() > expires_at - timedelta(minutes=10):
                # Attempt refresh
                new_tokens = UpstoxSession.refresh_access_token(
                    stored_tokens["refresh_token"]
                )
                if new_tokens:
                    access_token, refresh_token = new_tokens
                    UpstoxSession._token_storage.save_tokens(
                        access_token, refresh_token, 86400
                    )
                    st.session_state[UpstoxSession.SESSION_KEY] = access_token
                    st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = refresh_token
                    st.success("‚úì Token refreshed automatically")
                else:
                    # Refresh failed, need re-auth
                    st.warning("Session expired, please login again")
            else:
                # Use stored token
                st.session_state[UpstoxSession.SESSION_KEY] = stored_tokens["access_token"]
                st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = stored_tokens["refresh_token"]
            
            # Load profile
            token = UpstoxSession.get_access_token()
            profile = UpstoxSession.get_user_profile(token)
            if profile:
                st.session_state[UpstoxSession.SESSION_PROFILE_KEY] = profile
            
            return token
        
        # Check for OAuth callback
        query_params = st.query_params
        if "code" in query_params:
            try:
                # Exchange code for tokens
                token_data = UpstoxSession.exchange_code_for_token(query_params["code"])
                if token_data:
                    # Save tokens
                    UpstoxSession._token_storage.save_tokens(
                        token_data["access_token"],
                        token_data.get("refresh_token", ""),
                        token_data.get("expires_in", 86400)
                    )
                    
                    # Store in session
                    st.session_state[UpstoxSession.SESSION_KEY] = token_data["access_token"]
                    if "refresh_token" in token_data:
                        st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = token_data["refresh_token"]
                    
                    # Load profile
                    profile = UpstoxSession.get_user_profile(token_data["access_token"])
                    if profile:
                        st.session_state[UpstoxSession.SESSION_PROFILE_KEY] = profile
                    
                    # Clean URL
                    st.query_params.clear()
                    st.rerun()
                    
            except Exception as e:
                st.error(f"Authentication failed: {str(e)}")
                st.stop()
        
        # Show login interface with market context
        st.markdown("""
        <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                    padding: 30px; border-radius: 15px; color: white;'>
            <h2 style='color: white;'>üîê Algorithmic Trading Platform Login</h2>
            <p style='font-size: 16px;'>
                Access real-time derivatives analytics including:<br>
                ‚Ä¢ OI Velocity & Gamma Exposure (GEX)<br>
                ‚Ä¢ Structural Walls & Trap Detection<br>
                ‚Ä¢ Spot Divergence Analysis<br>
                ‚Ä¢ Market Microstructure Insights
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("---")
        
        # Login button using Streamlit's native button
        login_url = UpstoxSession.get_login_url()
        
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            if st.button("üìà Login with Upstox", type="primary", use_container_width=True):
                # Redirect to Upstox login
                js = f'window.open("{login_url}", "_blank")'
                st.components.v1.html(f"""
                    <script>
                        {js}
                    </script>
                """, height=0)
            
            st.markdown("""
            <div style="margin-top: 20px; padding: 15px; background: #f8f9fa; border-radius: 10px;">
                <small>üîê <strong>Secure:</strong> Tokens are encrypted and stored locally</small><br>
                <small>üîÑ <strong>Persistent:</strong> Session persists across restarts</small><br>
                <small>üìä <strong>Enhanced:</strong> Market state tracking enabled</small>
            </div>
            """, unsafe_allow_html=True)
            
            st.markdown("""
            <div style="margin-top: 20px; padding: 15px; background: #f8f9fa; border-radius: 10px;">
                <small>üîê <strong>Secure:</strong> Tokens are encrypted and stored locally</small><br>
                <small>üîÑ <strong>Persistent:</strong> Session persists across restarts</small><br>
                <small>üìä <strong>Enhanced:</strong> Market state tracking enabled</small>
            </div>
            """, unsafe_allow_html=True)
        
        st.stop()
    
    @staticmethod
    def get_login_url() -> str:
        """Generate OAuth login URL"""
        params = {
            "response_type": "code",
            "client_id": CLIENT_ID,
            "redirect_uri": REDIRECT_URI,
            "scope": "order placement portfolio"  # Add required scopes
        }
        return f"{UPSTOX_AUTH_URL}?{urlencode(params)}"
    
    @staticmethod
    def get_session_info() -> Dict:
        """Get comprehensive session information"""
        return {
            "authenticated": UpstoxSession.is_authenticated(),
            "has_profile": UpstoxSession.SESSION_PROFILE_KEY in st.session_state,
            "profile": st.session_state.get(UpstoxSession.SESSION_PROFILE_KEY, {}),
            "funding_state": UpstoxSession._token_storage.get_funding_state(),
            "market_state": UpstoxSession.get_market_state(),
            "storage_path": str(UpstoxSession._token_storage.storage_dir)
        }

# ==============================
# SESSION UTILITIES
# ==============================

def display_session_status():
    """Display session status in Streamlit sidebar"""
    if UpstoxSession.is_authenticated():
        with st.sidebar:
            st.markdown("---")
            st.markdown("### üß† Session Status")
            
            # User info
            profile = st.session_state.get(UpstoxSession.SESSION_PROFILE_KEY, {})
            if profile:
                st.markdown(f"**User:** {profile.get('user_name', 'N/A')}")
                st.markdown(f"**Email:** {profile.get('email', 'N/A')}")
            
            # Market state
            market_state = UpstoxSession.get_market_state()
            if market_state.get("funding_regime"):
                regime = market_state["funding_regime"]
                color = {
                    "EXPANSIVE": "üü¢",
                    "NORMAL": "üü°", 
                    "CONSTRICTED": "üî¥"
                }.get(regime, "‚ö™")
                st.markdown(f"**Funding:** {color} {regime}")
            
            if market_state.get("gamma_regime"):
                gamma = market_state["gamma_regime"]
                icon = "üìå" if gamma == "POSITIVE" else "üöÄ" if gamma == "NEGATIVE" else "‚öñÔ∏è"
                st.markdown(f"**Gamma:** {icon} {gamma}")
            
            # Logout button
            if st.button("üö™ Logout", type="secondary", use_container_width=True):
                UpstoxSession.logout()

# ==============================
# INITIALIZATION
# ==============================

def initialize_session():
    """
    Initialize session with enhanced capabilities.
    Call this at the start of your app.
    """
    # Ensure token storage directory exists
    Path("data/tokens").mkdir(parents=True, exist_ok=True)
    
    # Initialize session state keys
    session_keys = [
        UpstoxSession.SESSION_KEY,
        UpstoxSession.SESSION_REFRESH_KEY,
        UpstoxSession.SESSION_PROFILE_KEY,
        "market_state",
        "funding_analysis"
    ]
    
    for key in session_keys:
        if key not in st.session_state:
            st.session_state[key] = None if "state" in key else {}



====================================================================================================
FILE: .\core\session.py
====================================================================================================


File Name: session.py
Full Path: G:\trading_app\core\session.py
Size: 22.02 KB
Last Modified: 01/27/2026 23:00:51
Extension: .py

"""
Enhanced Upstox Session Manager with persistent token storage,
refresh mechanism, and market state tracking.
Incorporates research insights: funding liquidity monitoring via OI velocity
and market microstructure analysis.
"""

import os
import json
import requests
import streamlit as st
from urllib.parse import urlencode
from typing import Optional, Dict, Tuple
from datetime import datetime, timedelta
import pickle
from pathlib import Path
from cryptography.fernet import Fernet
import hashlib

# ==============================
# CONFIG (EDIT THESE)
# ==============================

# Upstox API v3 endpoints
UPSTOX_AUTH_URL = "https://api.upstox.com/v2/login/authorization/dialog"
UPSTOX_TOKEN_URL = "https://api.upstox.com/v2/login/authorization/token"
UPSTOX_PROFILE_URL = "https://api.upstox.com/v2/user/profile"

# Get from Streamlit secrets or environment
def get_config():
    """Get configuration from secrets.toml or environment variables."""
    # Try Streamlit secrets first
    try:
        if hasattr(st, 'secrets'):
            return {
                'client_id': st.secrets.get("UPSTOX_CLIENT_ID"),
                'client_secret': st.secrets.get("UPSTOX_CLIENT_SECRET"),
                'redirect_uri': st.secrets.get("UPSTOX_REDIRECT_URI", "http://127.0.0.1:8501/callback")
            }
    except:
        pass
    
    # Fallback to environment variables
    return {
        'client_id': os.getenv("UPSTOX_CLIENT_ID"),
        'client_secret': os.getenv("UPSTOX_CLIENT_SECRET"),
        'redirect_uri': os.getenv("UPSTOX_REDIRECT_URI", "http://127.0.0.1:8501/callback")
    }

config = get_config()
CLIENT_ID = config['client_id']
CLIENT_SECRET = config['client_secret']
REDIRECT_URI = config['redirect_uri']

# ==============================
# SECURE TOKEN STORAGE
# ==============================

class TokenStorage:
    """
    Secure token storage with encryption and persistence.
    Stores tokens locally with automatic refresh capability.
    """
    
    def __init__(self, storage_path: str = "data/tokens"):
        self.storage_dir = Path(storage_path)
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate or load encryption key
        self.key_file = self.storage_dir / ".key"
        if self.key_file.exists():
            with open(self.key_file, 'rb') as f:
                self.encryption_key = f.read()
        else:
            self.encryption_key = Fernet.generate_key()
            with open(self.key_file, 'wb') as f:
                f.write(self.encryption_key)
        
        self.cipher = Fernet(self.encryption_key)
        self.token_file = self.storage_dir / "upstox_tokens.enc"
        
        # Track funding liquidity state (research concept)
        self.funding_state = {
            "last_oi_velocity": 0.0,
            "liquidity_regime": "NORMAL",  # NORMAL, CONSTRICTED, EXPANSIVE
            "last_update": datetime.utcnow()
        }
    
    def _generate_user_hash(self) -> str:
        """Generate unique user identifier for multi-user support"""
        # In production, use actual user ID. Here we use client ID as proxy
        return hashlib.sha256(f"{CLIENT_ID}_{REDIRECT_URI}".encode()).hexdigest()[:16]
    
    def save_tokens(self, access_token: str, refresh_token: str, expires_in: int):
        """Securely save tokens with expiration"""
        token_data = {
            "access_token": access_token,
            "refresh_token": refresh_token,
            "expires_at": (datetime.utcnow() + timedelta(seconds=expires_in)).isoformat(),
            "client_id": CLIENT_ID,
            "user_hash": self._generate_user_hash(),
            "created_at": datetime.utcnow().isoformat()
        }
        
        # Encrypt and save
        encrypted = self.cipher.encrypt(pickle.dumps(token_data))
        with open(self.token_file, 'wb') as f:
            f.write(encrypted)
        
        st.success("‚úì Authentication tokens saved securely")
    
    def load_tokens(self) -> Optional[Dict]:
        """Load and decrypt tokens, check expiration"""
        if not self.token_file.exists():
            return None
        
        try:
            with open(self.token_file, 'rb') as f:
                encrypted = f.read()
            
            token_data = pickle.loads(self.cipher.decrypt(encrypted))
            
            # Check expiration
            expires_at = datetime.fromisoformat(token_data["expires_at"])
            if datetime.utcnow() > expires_at - timedelta(minutes=5):  # 5 min buffer
                st.warning("‚ö†Ô∏è Access token expired or near expiry")
                return None
            
            # Verify client matches
            if token_data.get("client_id") != CLIENT_ID:
                st.error("Client ID mismatch - reauthentication required")
                return None
            
            return token_data
        except Exception as e:
            st.error(f"Token load error: {e}")
            return None
    
    def update_funding_state(self, oi_velocity: float):
        """
        Update funding liquidity state based on OI velocity.
        Research concept: Monitor capital flow intensity.
        """
        self.funding_state["last_oi_velocity"] = oi_velocity
        
        # Determine liquidity regime (research concept)
        if oi_velocity > 1.5:
            regime = "EXPANSIVE"  # High capital inflow
        elif oi_velocity < -1.5:
            regime = "CONSTRICTED"  # Capital outflow
        else:
            regime = "NORMAL"
        
        if self.funding_state["liquidity_regime"] != regime:
            st.info(f"üí∞ Liquidity regime changed: {regime} (OI Velocity: {oi_velocity:.2f})")
        
        self.funding_state["liquidity_regime"] = regime
        self.funding_state["last_update"] = datetime.utcnow()
    
    def get_funding_state(self) -> Dict:
        """Get current funding liquidity state"""
        return self.funding_state.copy()

# ==============================
# ENHANCED SESSION MANAGER
# ==============================

class UpstoxSession:
    """
    Enhanced session manager with:
    1. Persistent token storage
    2. Automatic token refresh
    3. Funding liquidity tracking
    4. Market state awareness
    """
    
    SESSION_KEY = "upstox_access_token"
    SESSION_REFRESH_KEY = "upstox_refresh_token"
    SESSION_PROFILE_KEY = "upstox_profile"
    
    _token_storage = TokenStorage()
    _market_state = {
        "current_regime": None,
        "last_velocity": 0.0,
        "gamma_regime": None,  # POSITIVE/NEGATIVE from research
        "wall_levels": {"call": None, "put": None},
        "trap_zones": []
    }
    
    @staticmethod
    def is_authenticated() -> bool:
        """Check if user has valid authentication"""
        if UpstoxSession.SESSION_KEY in st.session_state:
            return True
        
        # Check persistent storage
        tokens = UpstoxSession._token_storage.load_tokens()
        if tokens:
            # Load into session state
            st.session_state[UpstoxSession.SESSION_KEY] = tokens["access_token"]
            st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = tokens["refresh_token"]
            return True
        
        return False
    
    @staticmethod
    def get_access_token() -> Optional[str]:
        """Get current access token, refresh if needed"""
        if not UpstoxSession.is_authenticated():
            return None
        
        # Check if token is in session state
        if UpstoxSession.SESSION_KEY in st.session_state:
            return st.session_state[UpstoxSession.SESSION_KEY]
        
        return None
    
    @staticmethod
    def logout() -> None:
        """Secure logout - clear all session and storage"""
        for key in [UpstoxSession.SESSION_KEY, 
                   UpstoxSession.SESSION_REFRESH_KEY,
                   UpstoxSession.SESSION_PROFILE_KEY]:
            if key in st.session_state:
                del st.session_state[key]
        
        # Clear persistent storage
        storage_file = Path("data/tokens/upstox_tokens.enc")
        if storage_file.exists():
            storage_file.unlink()
        
        # Clear query params
        st.query_params.clear()
        st.success("‚úì Logged out successfully")
        st.rerun()
    
    # ==========================
    # TOKEN MANAGEMENT
    # ==========================
    
    @staticmethod
    def refresh_access_token(refresh_token: str) -> Optional[Tuple[str, str]]:
        """
        Refresh expired access token using refresh token.
        Returns (new_access_token, new_refresh_token) or None
        """
        headers = {
            "Content-Type": "application/x-www-form-urlencoded",
            "Accept": "application/json"
        }
        
        data = {
            "grant_type": "refresh_token",
            "refresh_token": refresh_token,
            "client_id": CLIENT_ID,
            "client_secret": CLIENT_SECRET
        }
        
        try:
            response = requests.post(
                UPSTOX_TOKEN_URL,
                headers=headers,
                data=data,
                timeout=10
            )
            
            if response.status_code == 200:
                token_data = response.json()
                return token_data.get("access_token"), token_data.get("refresh_token")
            else:
                st.error(f"Token refresh failed: {response.text}")
                return None
                
        except Exception as e:
            st.error(f"Token refresh error: {e}")
            return None
    
    @staticmethod
    def exchange_code_for_token(code: str) -> Optional[Dict]:
        """Exchange authorization code for tokens with full token data"""
        headers = {
            "Content-Type": "application/x-www-form-urlencoded",
            "Accept": "application/json"
        }
        
        data = {
            "code": code,
            "client_id": CLIENT_ID,
            "client_secret": CLIENT_SECRET,
            "redirect_uri": REDIRECT_URI,
            "grant_type": "authorization_code"
        }
        
        try:
            response = requests.post(
                UPSTOX_TOKEN_URL,
                headers=headers,
                data=data,
                timeout=10
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                st.error(f"Token exchange failed: {response.text}")
                return None
                
        except Exception as e:
            st.error(f"Token exchange error: {e}")
            return None
    
    # ==========================
    # USER PROFILE
    # ==========================
    
    @staticmethod
    def get_user_profile(access_token: str) -> Optional[Dict]:
        """Fetch user profile for session context"""
        headers = {
            "Accept": "application/json",
            "Authorization": f"Bearer {access_token}"
        }
        
        try:
            response = requests.get(UPSTOX_PROFILE_URL, headers=headers, timeout=10)
            if response.status_code == 200:
                return response.json().get("data", {})
            return None
        except Exception as e:
            st.warning(f"Profile fetch warning: {e}")
            return None
    
    # ==========================
    # MARKET STATE TRACKING (Research Integration)
    # ==========================
    
    @staticmethod
    def update_market_state(oi_velocity: float, gamma_exposure: float, 
                           wall_levels: Dict, spot_divergence: float):
        """
        Update comprehensive market state based on research concepts.
        
        Args:
            oi_velocity: Rate of change of Open Interest
            gamma_exposure: Net Gamma Exposure (positive/negative)
            wall_levels: Dict with 'call' and 'put' wall strike levels
            spot_divergence: Divergence between spot and derivative metrics
        """
        # Update funding liquidity state
        UpstoxSession._token_storage.update_funding_state(oi_velocity)
        
        # Determine gamma regime (research concept)
        if gamma_exposure > 0:
            gamma_regime = "POSITIVE"  # Stabilizing, pinning expected
        elif gamma_exposure < 0:
            gamma_regime = "NEGATIVE"  # Accelerating, squeezes possible
        else:
            gamma_regime = "NEUTRAL"
        
        # Detect potential traps (research concept)
        trap_zones = []
        current_price = 0  # Would be passed in production
        
        # Check if price is near walls with high OI unwinding
        for side, level in wall_levels.items():
            if level and abs(current_price - level) / level < 0.005:  # Within 0.5%
                # High probability of trap if OI velocity is negative (unwinding)
                if oi_velocity < -1.0:
                    trap_zones.append({
                        "side": side,
                        "level": level,
                        "type": "GAMMA_TRAP" if abs(gamma_exposure) > 0.5 else "OI_TRAP",
                        "confidence": min(abs(oi_velocity) / 3, 1.0)
                    })
        
        # Update market state
        UpstoxSession._market_state.update({
            "last_velocity": oi_velocity,
            "gamma_regime": gamma_regime,
            "wall_levels": wall_levels,
            "trap_zones": trap_zones,
            "spot_divergence": spot_divergence,
            "funding_regime": UpstoxSession._token_storage.get_funding_state()["liquidity_regime"],
            "timestamp": datetime.utcnow().isoformat()
        })
    
    @staticmethod
    def get_market_state() -> Dict:
        """Get current market state analysis"""
        return UpstoxSession._market_state.copy()
    
    # ==========================
    # STREAMLIT ENTRYPOINT (ENHANCED)
    # ==========================
    

    @staticmethod
    def authenticate() -> str:
        """
        Simplified authentication flow that ALWAYS shows login button when not authenticated
        """
        
        # DEBUG: Show what's in query params
        query_params = st.query_params
        st.sidebar.write(f"üîç Query params: {dict(query_params)}")
        
        # Check for OAuth callback FIRST (before checking anything else)
        if "code" in query_params:
            try:
                code = query_params["code"]
                st.sidebar.write(f"üîç Got OAuth code: {code[:20]}...")
                
                # Exchange code for tokens
                token_data = UpstoxSession.exchange_code_for_token(code)
                if token_data:
                    st.sidebar.success("‚úÖ Token exchange successful")
                    
                    # Save tokens
                    UpstoxSession._token_storage.save_tokens(
                        token_data["access_token"],
                        token_data.get("refresh_token", ""),
                        token_data.get("expires_in", 86400)
                    )
                    
                    # Store in session
                    st.session_state[UpstoxSession.SESSION_KEY] = token_data["access_token"]
                    if "refresh_token" in token_data:
                        st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = token_data["refresh_token"]
                    
                    # Load profile
                    profile = UpstoxSession.get_user_profile(token_data["access_token"])
                    if profile:
                        st.session_state[UpstoxSession.SESSION_PROFILE_KEY] = profile
                    
                    # Clean URL and rerun
                    st.query_params.clear()
                    st.rerun()
                else:
                    st.error("‚ùå Failed to exchange code for tokens")
                    
            except Exception as e:
                st.error(f"‚ùå Authentication failed: {str(e)}")
                import traceback
                st.error(traceback.format_exc())
        
        # Check if already authenticated
        if UpstoxSession.is_authenticated():
            token = UpstoxSession.get_access_token()
            return token
        
        # Check persistent storage
        stored_tokens = UpstoxSession._token_storage.load_tokens()
        if stored_tokens:
            # Use stored token
            st.session_state[UpstoxSession.SESSION_KEY] = stored_tokens["access_token"]
            st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = stored_tokens["refresh_token"]
            
            token = UpstoxSession.get_access_token()
            profile = UpstoxSession.get_user_profile(token)
            if profile:
                st.session_state[UpstoxSession.SESSION_PROFILE_KEY] = profile
            
            return token
        
        # ==============================================
        # SHOW LOGIN INTERFACE (only if not authenticated)
        # ==============================================
        st.markdown("""
        <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                    padding: 30px; border-radius: 15px; color: white;'>
            <h2 style='color: white;'>üîê Algorithmic Trading Platform Login</h2>
            <p style='font-size: 16px;'>
                Access real-time derivatives analytics
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("---")
        
        # Login button
        login_url = UpstoxSession.get_login_url()
        
        st.markdown(f"""
        <div style="text-align: center;">
            <a href="{login_url}">
                <button style="
                    background-color: #00d09c;
                    color: white;
                    padding: 15px 30px;
                    font-size: 18px;
                    font-weight: bold;
                    border: none;
                    border-radius: 10px;
                    cursor: pointer;
                    width: 80%;
                    margin: 20px 0;
                    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
                    transition: all 0.3s ease;
                ">
                üìà Login with Upstox
                </button>
            </a>
        </div>
        """, unsafe_allow_html=True)
        
        st.info("""
        **Login Instructions:**
        1. Click the "Login with Upstox" button above
        2. You'll be redirected to Upstox authorization page
        3. Log in with your Upstox credentials
        4. Authorize the application
        5. You'll be redirected back to this app
        """)
        
        # CRITICAL: Stop execution here
        st.stop()
        return None
    
    @staticmethod
    def get_login_url() -> str:
        """Generate OAuth login URL"""
        params = {
            "response_type": "code",
            "client_id": CLIENT_ID,
            "redirect_uri": REDIRECT_URI,
            "scope": "order placement portfolio"
        }
        return f"{UPSTOX_AUTH_URL}?{urlencode(params)}"
    
    @staticmethod
    def get_session_info() -> Dict:
        """Get comprehensive session information"""
        return {
            "authenticated": UpstoxSession.is_authenticated(),
            "has_profile": UpstoxSession.SESSION_PROFILE_KEY in st.session_state,
            "profile": st.session_state.get(UpstoxSession.SESSION_PROFILE_KEY, {}),
            "funding_state": UpstoxSession._token_storage.get_funding_state(),
            "market_state": UpstoxSession.get_market_state(),
            "storage_path": str(UpstoxSession._token_storage.storage_dir)
        }

# ==============================
# SESSION UTILITIES
# ==============================

def display_session_status():
    """Display session status in Streamlit sidebar"""
    if UpstoxSession.is_authenticated():
        with st.sidebar:
            st.markdown("---")
            st.markdown("### üß† Session Status")
            
            # User info
            profile = st.session_state.get(UpstoxSession.SESSION_PROFILE_KEY, {})
            if profile:
                st.markdown(f"**User:** {profile.get('user_name', 'N/A')}")
                st.markdown(f"**Email:** {profile.get('email', 'N/A')}")
            
            # Market state
            market_state = UpstoxSession.get_market_state()
            if market_state.get("funding_regime"):
                regime = market_state["funding_regime"]
                color = {
                    "EXPANSIVE": "üü¢",
                    "NORMAL": "üü°", 
                    "CONSTRICTED": "üî¥"
                }.get(regime, "‚ö™")
                st.markdown(f"**Funding:** {color} {regime}")
            
            if market_state.get("gamma_regime"):
                gamma = market_state["gamma_regime"]
                icon = "üìå" if gamma == "POSITIVE" else "üöÄ" if gamma == "NEGATIVE" else "‚öñÔ∏è"
                st.markdown(f"**Gamma:** {icon} {gamma}")
            
            # Logout button
            if st.button("üö™ Logout", type="secondary", use_container_width=True):
                UpstoxSession.logout()

# ==============================
# INITIALIZATION
# ==============================

def initialize_session():
    """
    Initialize session with enhanced capabilities.
    Call this at the start of your app.
    """
    # Ensure token storage directory exists
    Path("data/tokens").mkdir(parents=True, exist_ok=True)
    
    # Initialize session state keys
    session_keys = [
        UpstoxSession.SESSION_KEY,
        UpstoxSession.SESSION_REFRESH_KEY,
        UpstoxSession.SESSION_PROFILE_KEY,
        "market_state",
        "funding_analysis"
    ]
    
    for key in session_keys:
        if key not in st.session_state:
            st.session_state[key] = None if "state" in key else {}



====================================================================================================
FILE: .\core\signals\state_machine.py
====================================================================================================


File Name: state_machine.py
Full Path: G:\trading_app\core\signals\state_machine.py
Size: 38.59 KB
Last Modified: 01/28/2026 10:56:16
Extension: .py

"""
Enhanced Signal State Machine with Research-Based Decision Making.
Implements advanced signal generation using:
1. OI Velocity & Gamma Exposure analysis
2. Structural Walls & Traps detection
3. Spot Divergence analysis
4. Wyckoff pattern recognition
5. Market regime awareness
"""

from datetime import datetime, timedelta
import uuid
import pandas as pd
import numpy as np
from typing import Dict, Optional, Tuple, List
from dataclasses import dataclass
from enum import Enum
import json

# Import research features
from ml.feature_contract import FEATURE_VERSION, RESEARCH_FEATURES

# ==============================
# ENUMS & DATA CLASSES
# ==============================

class SignalStrength(Enum):
    """Signal strength classification"""
    WEAK = "WEAK"        # Low confidence, minor edge
    MODERATE = "MODERATE" # Decent confidence, clear edge
    STRONG = "STRONG"    # High confidence, strong edge
    VERY_STRONG = "VERY_STRONG" # Very high confidence, major edge

class MarketRegime(Enum):
    """Market regime classification"""
    ACCUMULATION = "ACCUMULATION"      # Wyckoff accumulation
    DISTRIBUTION = "DISTRIBUTION"      # Wyckoff distribution
    UPTREND = "UPTREND"                # Strong uptrend
    DOWNTREND = "DOWNTREND"            # Strong downtrend
    RANGING = "RANGING"                # Range-bound
    BREAKOUT = "BREAKOUT"              # Breakout from range
    SQUEEZE = "SQUEEZE"                # Gamma/OI squeeze
    REVERSAL = "REVERSAL"              # Trend reversal

class TrapType(Enum):
    """Types of market traps"""
    GAMMA_TRAP = "GAMMA_TRAP"          # Gamma-induced squeeze
    OI_TRAP = "OI_TRAP"                # OI unwinding trap
    WYCKOFF_SPRING = "WYCKOFF_SPRING"  # Wyckoff spring (bear trap)
    WYCKOFF_UPTHRUST = "WYCKOFF_UPTHRUST" # Wyckoff upthrust (bull trap)
    DIVERGENCE_TRAP = "DIVERGENCE_TRAP" # Divergence-based trap

@dataclass
class SignalComponents:
    """Components of signal decision"""
    # Core components
    trend_score: float                # -1 to 1 (bearish to bullish)
    momentum_score: float             # -1 to 1 (weak to strong)
    
    # Research components
    oi_velocity_score: float          # -1 to 1 (negative to positive velocity)
    gamma_score: float               # -1 to 1 (negative to positive gamma)
    wall_interaction_score: float    # -1 to 1 (wall defense vs trap)
    divergence_score: float          # -1 to 1 (bearish to bullish divergence)
    
    # Wyckoff components
    wyckoff_phase_score: float       # -1 to 1 (distribution to accumulation)
    pattern_score: float             # -1 to 1 (bearish to bullish patterns)
    
    # Composite scores
    composite_score: float           # Overall score (-1 to 1)
    confidence: float               # Signal confidence (0-1)
    
    # Regime classification
    market_regime: MarketRegime
    regime_confidence: float

@dataclass
class TrapAnalysis:
    """Analysis of potential trap"""
    trap_type: TrapType
    strike_level: float
    direction: str  # "BULLISH" or "BEARISH"
    confidence: float
    trigger_conditions: List[str]
    expected_move_pct: float

# ==============================
# ENHANCED SIGNAL STATE MACHINE
# ==============================

class SignalStateMachine:
    """
    Advanced signal generator with research-based decision making.
    Incorporates OI velocity, gamma exposure, walls/traps, divergence.
    """
    
    def __init__(
        self,
        signal_expiry_minutes: int = 5,
        confidence_threshold: float = 0.2,
        trap_confidence_threshold: float = 0.6
    ):
        self.signal_expiry_minutes = signal_expiry_minutes
        self.confidence_threshold = confidence_threshold
        self.trap_confidence_threshold = trap_confidence_threshold
        
        # State tracking
        self.signal_history = []
        self.regime_history = []
        self.trap_detections = []
        
        # Thresholds (configurable)
        self.thresholds = {
            "oi_velocity_high": 1.5,      # œÉ
            "oi_velocity_low": -1.5,      # œÉ
            "gamma_high": 500,           # Net gamma threshold
            "gamma_low": -500,           # Net gamma threshold
            "trap_prob_high": 0.7,       # High trap probability
            "divergence_high": 0.5,      # Significant divergence
            "wall_strength_high": 0.3,   # Strong wall
            "spring_detection_high": 0.6, # Strong spring pattern
            "upthrust_detection_high": 0.6 # Strong upthrust pattern
        }
    
    # ==============================
    # RESEARCH FEATURE EXTRACTION
    # ==============================
    
    def extract_research_features(self, feature_row: pd.Series) -> Dict:
        """
        Extract and normalize research features from feature row.
        """
        feats = {}
        
        # OI Velocity features
        feats["oi_velocity"] = feature_row.get("oi_velocity", 0.0)
        feats["oi_regime_expansive"] = feature_row.get("oi_regime_expansive", 0.0)
        feats["oi_regime_constricted"] = feature_row.get("oi_regime_constricted", 0.0)
        
        # Gamma Exposure features
        feats["net_gamma"] = feature_row.get("net_gamma", 0.0)
        feats["gamma_regime_positive"] = feature_row.get("gamma_regime_positive", 0.0)
        feats["gamma_regime_negative"] = feature_row.get("gamma_regime_negative", 0.0)
        feats["gamma_flip_distance"] = feature_row.get("gamma_flip_distance", 0.0)
        
        # Structural features
        feats["wall_strength"] = feature_row.get("wall_strength", 0.0)
        feats["wall_defense_score"] = feature_row.get("wall_defense_score", 0.0)
        feats["trap_probability"] = feature_row.get("trap_probability", 0.0)
        
        # Divergence features
        feats["price_oi_divergence"] = feature_row.get("price_oi_divergence", 0.0)
        feats["price_gamma_divergence"] = feature_row.get("price_gamma_divergence", 0.0)
        feats["divergence_score"] = feature_row.get("divergence_score", 0.0)
        feats["has_divergence"] = feature_row.get("has_divergence", 0.0)
        
        # Wyckoff features
        feats["spring_detection"] = feature_row.get("spring_detection", 0.0)
        feats["upthrust_detection"] = feature_row.get("upthrust_detection", 0.0)
        feats["accumulation_score"] = feature_row.get("accumulation_score", 0.0)
        
        # Composite features
        feats["gamma_wall_interaction"] = feature_row.get("gamma_wall_interaction", 0.0)
        feats["velocity_divergence_composite"] = feature_row.get("velocity_divergence_composite", 0.0)
        feats["trap_gamma_composite"] = feature_row.get("trap_gamma_composite", 0.0)
        
        # Base features (still important)
        feats["put_call_ratio"] = feature_row.get("put_call_ratio", 1.0)
        feats["price_momentum"] = feature_row.get("price_momentum", 0.0)
        feats["ccc_slope"] = feature_row.get("ccc_slope", 0.0)
        feats["vwap_distance"] = feature_row.get("vwap_distance", 0.0)
        
        return feats
    
    # ==============================
    # COMPONENT SCORING
    # ==============================
    
    def score_oi_velocity(self, feats: Dict) -> Tuple[float, str]:
        """
        Score OI velocity component.
        
        Returns:
            score (-1 to 1), analysis
        """
        velocity = feats.get("oi_velocity", 0.0)
        expansive = feats.get("oi_regime_expansive", 0.0)
        constricted = feats.get("oi_regime_constricted", 0.0)
        
        # Velocity scoring
        if velocity > self.thresholds["oi_velocity_high"]:
            score = 1.0  # Strong buildup
            analysis = "Strong OI buildup - initiative capital entering"
        elif velocity > 0.5:
            score = 0.5  # Moderate buildup
            analysis = "Moderate OI buildup"
        elif velocity < self.thresholds["oi_velocity_low"]:
            score = -1.0  # Strong unwinding
            analysis = "Strong OI unwinding - capital exiting"
        elif velocity < -0.5:
            score = -0.5  # Moderate unwinding
            analysis = "Moderate OI unwinding"
        else:
            score = 0.0  # Neutral
            analysis = "OI velocity neutral"
        
        # Regime adjustment
        if expansive > 0.5:
            score = max(score, 0.3)  # Bias bullish
            analysis += " (EXPANSIVE regime)"
        elif constricted > 0.5:
            score = min(score, -0.3)  # Bias bearish
            analysis += " (CONSTRICTED regime)"
        
        return score, analysis
    
    def score_gamma_exposure(self, feats: Dict) -> Tuple[float, str]:
        """
        Score Gamma Exposure component.
        
        Returns:
            score (-1 to 1), analysis
        """
        net_gamma = feats.get("net_gamma", 0.0)
        gamma_pos = feats.get("gamma_regime_positive", 0.0)
        gamma_neg = feats.get("gamma_regime_negative", 0.0)
        flip_distance = feats.get("gamma_flip_distance", 1.0)
        
        # Gamma scoring
        if gamma_neg > 0.5 and abs(net_gamma) > self.thresholds["gamma_high"]:
            score = -1.0  # Strong negative gamma (accelerating)
            analysis = "Strong negative gamma - volatility acceleration likely"
        elif gamma_neg > 0.5:
            score = -0.7  # Moderate negative gamma
            analysis = "Negative gamma regime - trending moves possible"
        elif gamma_pos > 0.5 and abs(net_gamma) > self.thresholds["gamma_high"]:
            score = 0.3  # Strong positive gamma (stabilizing)
            analysis = "Strong positive gamma - range-bound/pinning likely"
        elif gamma_pos > 0.5:
            score = 0.1  # Moderate positive gamma
            analysis = "Positive gamma regime - mean reversion favored"
        else:
            score = 0.0  # Neutral
            analysis = "Gamma exposure neutral"
        
        # Flip distance adjustment (closer to flip = more uncertainty)
        if flip_distance < 0.01:  # Very close to flip
            score *= 0.5  # Reduce confidence near flip
            analysis += " (Near gamma flip)"
        
        return score, analysis
    
    def score_structure(self, feats: Dict) -> Tuple[float, str, Optional[TrapAnalysis]]:
        """
        Score structural components (walls, traps, Wyckoff).
        
        Returns:
            score (-1 to 1), analysis, trap_analysis
        """
        wall_strength = feats.get("wall_strength", 0.0)
        wall_defense = feats.get("wall_defense_score", 0.0)
        trap_prob = feats.get("trap_probability", 0.0)
        spring = feats.get("spring_detection", 0.0)
        upthrust = feats.get("upthrust_detection", 0.0)
        accumulation = feats.get("accumulation_score", 0.0)
        
        score = 0.0
        analysis = []
        trap_analysis = None
        
        # Wall analysis
        if wall_strength > self.thresholds["wall_strength_high"]:
            if wall_defense > 0.7:
                score += 0.3  # Strong defense = continuation
                analysis.append(f"Strong wall defense ({wall_strength:.2f})")
            else:
                score -= 0.2  # Weak defense = potential break
                analysis.append(f"Weak wall defense ({wall_strength:.2f})")
        
        # Trap analysis
        if trap_prob > self.thresholds["trap_prob_high"]:
            # High trap probability
            trap_type = self._classify_trap_type(feats)
            if trap_type in [TrapType.GAMMA_TRAP, TrapType.WYCKOFF_SPRING]:
                # Bullish traps
                trap_analysis = TrapAnalysis(
                    trap_type=trap_type,
                    strike_level=0,  # Would be actual strike in production
                    direction="BULLISH",
                    confidence=trap_prob,
                    trigger_conditions=["Price breach", "OI unwinding"],
                    expected_move_pct=2.0  # Estimated move
                )
                score += 0.5  # Bullish bias
                analysis.append(f"{trap_type.value} detected (conf: {trap_prob:.2f})")
            elif trap_type in [TrapType.OI_TRAP, TrapType.WYCKOFF_UPTHRUST]:
                # Bearish traps
                trap_analysis = TrapAnalysis(
                    trap_type=trap_type,
                    strike_level=0,
                    direction="BEARISH",
                    confidence=trap_prob,
                    trigger_conditions=["Price rejection", "OI buildup"],
                    expected_move_pct=-2.0
                )
                score -= 0.5  # Bearish bias
                analysis.append(f"{trap_type.value} detected (conf: {trap_prob:.2f})")
        
        # Wyckoff pattern analysis
        if spring > self.thresholds["spring_detection_high"]:
            score += 0.4  # Spring = bullish
            analysis.append(f"Wyckoff Spring detected (strength: {spring:.2f})")
        
        if upthrust > self.thresholds["upthrust_detection_high"]:
            score -= 0.4  # Upthrust = bearish
            analysis.append(f"Wyckoff Upthrust detected (strength: {upthrust:.2f})")
        
        # Accumulation/distribution
        if accumulation > 0.7:
            score += 0.3  # Accumulation = bullish
            analysis.append(f"Accumulation phase (score: {accumulation:.2f})")
        elif accumulation < 0.3:
            score -= 0.3  # Distribution = bearish
            analysis.append(f"Distribution phase (score: {accumulation:.2f})")
        
        # Clamp score
        score = max(-1.0, min(1.0, score))
        
        return score, " | ".join(analysis) if analysis else "Structure neutral", trap_analysis
    
    def score_divergence(self, feats: Dict) -> Tuple[float, str]:
        """
        Score divergence components.
        
        Returns:
            score (-1 to 1), analysis
        """
        divergence_score = feats.get("divergence_score", 0.0)
        has_divergence = feats.get("has_divergence", 0.0)
        price_oi_div = feats.get("price_oi_divergence", 0.0)
        price_gamma_div = feats.get("price_gamma_divergence", 0.0)
        
        if has_divergence < 0.5:
            return 0.0, "No significant divergence"
        
        score = 0.0
        analysis = []
        
        # Price-OI divergence (research concept)
        if abs(price_oi_div) > 1.0:  # Significant divergence
            if price_oi_div > 0:  # Price up, OI down = bearish divergence
                score -= 0.4
                analysis.append("Bearish Price-OI divergence")
            else:  # Price down, OI up = bullish divergence
                score += 0.4
                analysis.append("Bullish Price-OI divergence")
        
        # Price-Gamma divergence
        if abs(price_gamma_div) > 0.5:
            if price_gamma_div > 0:  # Price moving against gamma regime
                score -= 0.3
                analysis.append("Price-Gamma regime divergence")
            else:
                score += 0.3
                analysis.append("Price-Gamma regime convergence")
        
        # Overall divergence score
        if divergence_score > self.thresholds["divergence_high"]:
            # High divergence confidence
            if divergence_score > 0:  # Bullish divergence
                score += 0.3
                analysis.append(f"Strong bullish divergence (conf: {divergence_score:.2f})")
            else:  # Bearish divergence
                score -= 0.3
                analysis.append(f"Strong bearish divergence (conf: {abs(divergence_score):.2f})")
        
        # Clamp score
        score = max(-1.0, min(1.0, score))
        
        return score, " | ".join(analysis) if analysis else "Divergence neutral"
    
    def score_trend_momentum(self, feats: Dict) -> Tuple[float, float, str]:
        """
        Score traditional trend and momentum components.
        
        Returns:
            trend_score, momentum_score, analysis
        """
        price_momentum = feats.get("price_momentum", 0.0)
        ccc_slope = feats.get("ccc_slope", 0.0)
        vwap_distance = feats.get("vwap_distance", 0.0)
        put_call_ratio = feats.get("put_call_ratio", 1.0)
        
        # Trend score (directional bias)
        trend_score = 0.0
        
        # Price momentum
        if price_momentum > 0.01:
            trend_score += 0.3
        elif price_momentum < -0.01:
            trend_score -= 0.3
        
        # Breadth momentum (CCC slope)
        if ccc_slope > 0.001:
            trend_score += 0.2
        elif ccc_slope < -0.001:
            trend_score -= 0.2
        
        # VWAP position
        if vwap_distance > 0.005:
            trend_score += 0.2  # Above VWAP = bullish
        elif vwap_distance < -0.005:
            trend_score -= 0.2  # Below VWAP = bearish
        
        # Put-Call ratio sentiment
        if put_call_ratio < 0.8:
            trend_score += 0.1  # Low PCR = bullish
        elif put_call_ratio > 1.2:
            trend_score -= 0.1  # High PCR = bearish
        
        # Momentum score (strength of move)
        momentum_score = abs(price_momentum) * 10  # Scale to 0-1 range
        momentum_score = min(momentum_score, 1.0)
        
        # Analysis
        analysis_parts = []
        if price_momentum > 0:
            analysis_parts.append(f"Price momentum: +{price_momentum*100:.1f}%")
        else:
            analysis_parts.append(f"Price momentum: {price_momentum*100:.1f}%")
        
        if ccc_slope > 0:
            analysis_parts.append(f"Breadth improving (CCC: +{ccc_slope:.3f})")
        elif ccc_slope < 0:
            analysis_parts.append(f"Breadth weakening (CCC: {ccc_slope:.3f})")
        
        analysis = " | ".join(analysis_parts) if analysis_parts else "Momentum neutral"
        
        return trend_score, momentum_score, analysis
    
    # ==============================
    # TRAP CLASSIFICATION
    # ==============================
    
    def _classify_trap_type(self, feats: Dict) -> TrapType:
        """
        Classify trap type based on feature combination.
        """
        trap_prob = feats.get("trap_probability", 0.0)
        gamma_neg = feats.get("gamma_regime_negative", 0.0)
        spring = feats.get("spring_detection", 0.0)
        upthrust = feats.get("upthrust_detection", 0.0)
        divergence = feats.get("has_divergence", 0.0)
        
        # Check for specific trap types
        if gamma_neg > 0.5 and trap_prob > 0.6:
            return TrapType.GAMMA_TRAP
        
        if trap_prob > 0.6 and gamma_neg < 0.5:
            return TrapType.OI_TRAP
        
        if spring > self.thresholds["spring_detection_high"]:
            return TrapType.WYCKOFF_SPRING
        
        if upthrust > self.thresholds["upthrust_detection_high"]:
            return TrapType.WYCKOFF_UPTHRUST
        
        if divergence > 0.5 and trap_prob > 0.5:
            return TrapType.DIVERGENCE_TRAP
        
        # Default
        return TrapType.OI_TRAP
    
    # ==============================
    # MARKET REGIME DETECTION
    # ==============================
    
    def detect_market_regime(self, feats: Dict, components: SignalComponents) -> Tuple[MarketRegime, float]:
        """
        Detect current market regime based on features.
        
        Returns:
            regime, confidence (0-1)
        """
        regime_scores = {
            MarketRegime.ACCUMULATION: 0.0,
            MarketRegime.DISTRIBUTION: 0.0,
            MarketRegime.UPTREND: 0.0,
            MarketRegime.DOWNTREND: 0.0,
            MarketRegime.RANGING: 0.0,
            MarketRegime.BREAKOUT: 0.0,
            MarketRegime.SQUEEZE: 0.0,
            MarketRegime.REVERSAL: 0.0
        }
        
        # Extract key features
        accumulation = feats.get("accumulation_score", 0.0)
        trap_prob = feats.get("trap_probability", 0.0)
        gamma_neg = feats.get("gamma_regime_negative", 0.0)
        divergence = feats.get("has_divergence", 0.0)
        price_momentum = feats.get("price_momentum", 0.0)
        
        # Score regimes
        # Accumulation/Distribution
        if accumulation > 0.7:
            regime_scores[MarketRegime.ACCUMULATION] = accumulation
        elif accumulation < 0.3:
            regime_scores[MarketRegime.DISTRIBUTION] = 1 - accumulation
        
        # Trend detection
        if abs(price_momentum) > 0.02:  # Strong trend
            if price_momentum > 0:
                regime_scores[MarketRegime.UPTREND] = abs(price_momentum) * 10
            else:
                regime_scores[MarketRegime.DOWNTREND] = abs(price_momentum) * 10
        else:
            regime_scores[MarketRegime.RANGING] = 0.7
        
        # Breakout/Squeeze
        if trap_prob > 0.6:
            if gamma_neg > 0.5:
                regime_scores[MarketRegime.SQUEEZE] = trap_prob
            else:
                regime_scores[MarketRegime.BREAKOUT] = trap_prob
        
        # Reversal
        if divergence > 0.5 and abs(price_momentum) > 0.01:
            regime_scores[MarketRegime.REVERSAL] = divergence
        
        # Find highest scoring regime
        best_regime_item = max(regime_scores.items(), key=lambda x: x[1])
        best_regime = best_regime_item[0]
        regime_confidence = best_regime_item[1]
        
        # If no clear regime, default to RANGING
        if regime_confidence < 0.3:
            return MarketRegime.RANGING, 0.3
        
        return best_regime, min(regime_confidence, 1.0)
    
    # ==============================
    # COMPOSITE SIGNAL GENERATION
    # ==============================
    
    def compute_composite_signal(self, components: SignalComponents) -> Tuple[str, float, SignalStrength]:
        """
        Compute final signal from components.
        
        Returns:
            signal_type, confidence, strength
        """
        composite = components.composite_score
        confidence = components.confidence
        
        # Determine signal type
        if composite > 0.3 and confidence > self.confidence_threshold:
            signal_type = "BUY"
            if composite > 0.6 and confidence > 0.7:
                strength = SignalStrength.VERY_STRONG
            elif composite > 0.4:
                strength = SignalStrength.STRONG
            elif composite > 0.3:
                strength = SignalStrength.MODERATE
            else:
                strength = SignalStrength.WEAK
                
        elif composite < -0.3 and confidence > self.confidence_threshold:
            signal_type = "SELL"
            if composite < -0.6 and confidence > 0.7:
                strength = SignalStrength.VERY_STRONG
            elif composite < -0.4:
                strength = SignalStrength.STRONG
            elif composite < -0.3:
                strength = SignalStrength.MODERATE
            else:
                strength = SignalStrength.WEAK
                
        else:
            signal_type = "NEUTRAL"
            strength = SignalStrength.WEAK
        
        return signal_type, confidence, strength
    
    # ==============================
    # MAIN DECISION ENGINE
    # ==============================
    
    def decide(self, feature_row: pd.Series) -> SignalComponents:
        """
        Main decision engine with research-based scoring.
        """
        # Extract research features
        feats = self.extract_research_features(feature_row)
        
        # Score all components
        trend_score, momentum_score, trend_analysis = self.score_trend_momentum(feats)
        oi_score, oi_analysis = self.score_oi_velocity(feats)
        gamma_score, gamma_analysis = self.score_gamma_exposure(feats)
        structure_score, structure_analysis, trap_analysis = self.score_structure(feats)
        divergence_score, divergence_analysis = self.score_divergence(feats)
        
        # Calculate Wyckoff phase score
        wyckoff_score = 0.0
        pattern_score = 0.0
        
        accumulation = feats.get("accumulation_score", 0.0)
        spring = feats.get("spring_detection", 0.0)
        upthrust = feats.get("upthrust_detection", 0.0)
        
        if accumulation > 0.5:
            wyckoff_score = accumulation - 0.5  # 0 to 0.5 range
        
        pattern_score = spring - upthrust  # Positive for springs, negative for upthrusts
        
        # Calculate composite score with weights
        weights = {
            "trend": 0.15,
            "momentum": 0.10,
            "oi_velocity": 0.20,     # High weight for OI velocity (research important)
            "gamma": 0.20,           # High weight for gamma (research important)
            "structure": 0.15,
            "divergence": 0.10,
            "wyckoff": 0.05,
            "pattern": 0.05
        }
        
        weighted_sum = (
            trend_score * weights["trend"] +
            momentum_score * np.sign(trend_score) * weights["momentum"] +  # Momentum amplifies trend
            oi_score * weights["oi_velocity"] +
            gamma_score * weights["gamma"] +
            structure_score * weights["structure"] +
            divergence_score * weights["divergence"] +
            wyckoff_score * weights["wyckoff"] +
            pattern_score * weights["pattern"]
        )
        
        # Calculate confidence
        component_scores = [
            abs(trend_score), abs(oi_score), abs(gamma_score),
            abs(structure_score), abs(divergence_score)
        ]
        confidence = np.mean([s for s in component_scores if s > 0.1]) if any(s > 0.1 for s in component_scores) else 0.0
        
        # Detect market regime
        market_regime, regime_confidence = self.detect_market_regime(feats, SignalComponents(
            trend_score=trend_score,
            momentum_score=momentum_score,
            oi_velocity_score=oi_score,
            gamma_score=gamma_score,
            wall_interaction_score=structure_score,
            divergence_score=divergence_score,
            wyckoff_phase_score=wyckoff_score,
            pattern_score=pattern_score,
            composite_score=weighted_sum,
            confidence=confidence,
            market_regime=MarketRegime.RANGING,  # Temporary placeholder
            regime_confidence=0.5  # Temporary placeholder
        ))
        
        # Store trap analysis if found
        if trap_analysis:
            self.trap_detections.append({
                "timestamp": feature_row.get("timestamp", ""),
                "trap_analysis": trap_analysis,
                "features": feats
            })
        
        return SignalComponents(
            trend_score=trend_score,
            momentum_score=momentum_score,
            oi_velocity_score=oi_score,
            gamma_score=gamma_score,
            wall_interaction_score=structure_score,
            divergence_score=divergence_score,
            wyckoff_phase_score=wyckoff_score,
            pattern_score=pattern_score,
            composite_score=weighted_sum,
            confidence=confidence,
            market_regime=market_regime,
            regime_confidence=regime_confidence
        )
    
    # ==============================
    # SIGNAL OBJECT GENERATION
    # ==============================
    
    def build_signal(
        self,
        feature_row: pd.Series,
        model_version: str = "research_v2"
    ) -> Dict:
        """
        Build complete signal record with research context.
        """
        # Run decision engine
        components = self.decide(feature_row)
        
        # Compute final signal
        signal_type, confidence, strength = self.compute_composite_signal(components)
        
        # Generate signal ID
        signal_id = str(uuid.uuid4())
        
        # Current time
        now = datetime.utcnow()
        
        # Build research context
        research_context = {
            "components": {
                "trend_score": round(components.trend_score, 3),
                "momentum_score": round(components.momentum_score, 3),
                "oi_velocity_score": round(components.oi_velocity_score, 3),
                "gamma_score": round(components.gamma_score, 3),
                "wall_interaction_score": round(components.wall_interaction_score, 3),
                "divergence_score": round(components.divergence_score, 3),
                "wyckoff_phase_score": round(components.wyckoff_phase_score, 3),
                "pattern_score": round(components.pattern_score, 3),
                "composite_score": round(components.composite_score, 3)
            },
            "market_regime": components.market_regime.value,
            "regime_confidence": round(components.regime_confidence, 3),
            "signal_strength": strength.value,
            "thresholds_used": self.thresholds
        }
        
        # Build analytics summary
        analytics_summary = {
            "oi_velocity": feature_row.get("oi_velocity", 0),
            "net_gamma": feature_row.get("net_gamma", 0),
            "trap_probability": feature_row.get("trap_probability", 0),
            "divergence_score": feature_row.get("divergence_score", 0),
            "wall_strength": feature_row.get("wall_strength", 0),
            "confidence_breakdown": {
                "component_confidence": round(components.confidence, 3),
                "regime_confidence": round(components.regime_confidence, 3),
                "composite_confidence": round(confidence, 3)
            }
        }
        
        # Build rationale
        rationale_parts = []

        # Extract values from feature_row (not from components)
        trap_probability = feature_row.get("trap_probability", 0.0)
        has_divergence = feature_row.get("has_divergence", 0.0)

        # Add component analysis
        if abs(components.oi_velocity_score) > 0.3:
            direction = "bullish" if components.oi_velocity_score > 0 else "bearish"
            rationale_parts.append(f"{direction.capitalize()} OI velocity")

        if abs(components.gamma_score) > 0.3:
            regime = "negative" if components.gamma_score < 0 else "positive"
            rationale_parts.append(f"{regime.capitalize()} gamma regime")

        if trap_probability > 0.5:
            rationale_parts.append(f"High trap probability ({trap_probability:.2f})")

        if has_divergence > 0.5:
            rationale_parts.append("Significant divergence detected")

        rationale = " | ".join(rationale_parts) if rationale_parts else "rule_based_research_v2"
        
        # Build complete signal
        signal = {
            "signal_id": signal_id,
            "timestamp": feature_row["timestamp"],
            "feature_version": feature_row.get("feature_version", FEATURE_VERSION),
            "model_version": model_version,
            
            "signal_type": signal_type,
            "confidence": round(confidence, 3),
            
            "market_state": components.market_regime.value,
            "rationale": rationale,
            
            "expiry_time": (now + timedelta(minutes=self.signal_expiry_minutes)).isoformat(),
            "status": "NEW",
            
            "created_at": now.isoformat(),
            
            # Enhanced fields
            "research_context": research_context,
            "analytics_summary": analytics_summary,
            "signal_strength": strength.value,
            
            # Additional metadata
            "spot_price": feature_row.get("spot_price", 0),
            "put_call_ratio": feature_row.get("put_call_ratio", 1.0),
            "oi_velocity": feature_row.get("oi_velocity", 0),
            "net_gamma": feature_row.get("net_gamma", 0)
        }
        
        # Store in history
        self.signal_history.append({
            "timestamp": now.isoformat(),
            "signal": signal,
            "components": components
        })
        
        # Keep history manageable
        if len(self.signal_history) > 1000:
            self.signal_history = self.signal_history[-1000:]
        
        return signal
    
    # ==============================
    # UTILITY METHODS
    # ==============================
    
    def get_signal_history(self, limit: int = 10) -> List[Dict]:
        """Get recent signal history."""
        return self.signal_history[-limit:] if self.signal_history else []
    
    def get_recent_trap_detections(self, limit: int = 5) -> List[Dict]:
        """Get recent trap detections."""
        return self.trap_detections[-limit:] if self.trap_detections else []
    
    def get_performance_metrics(self) -> Dict:
        """Calculate performance metrics for recent signals."""
        if not self.signal_history:
            return {}
        
        recent_signals = self.signal_history[-100:]  # Last 100 signals
        
        buy_signals = [s for s in recent_signals if s["signal"]["signal_type"] == "BUY"]
        sell_signals = [s for s in recent_signals if s["signal"]["signal_type"] == "SELL"]
        
        metrics = {
            "total_signals": len(recent_signals),
            "buy_signals": len(buy_signals),
            "sell_signals": len(sell_signals),
            "neutral_signals": len(recent_signals) - len(buy_signals) - len(sell_signals),
            "avg_confidence": np.mean([s["signal"]["confidence"] for s in recent_signals]) if recent_signals else 0,
            "recent_regimes": {},
            "trap_detections": len(self.trap_detections)
        }
        
        # Count recent regimes
        for signal in recent_signals[-20:]:  # Last 20 signals
            regime = signal["signal"]["market_state"]
            metrics["recent_regimes"][regime] = metrics["recent_regimes"].get(regime, 0) + 1
        
        return metrics
    
    def reset(self):
        """Reset state machine history."""
        self.signal_history = []
        self.regime_history = []
        self.trap_detections = []

# ==============================
# SIGNAL VALIDATION & FILTERING
# ==============================

class SignalValidator:
    """
    Validates signals based on research criteria.
    """
    
    @staticmethod
    def validate_signal(signal: Dict, feature_row: pd.Series) -> Tuple[bool, str]:
        """
        Validate signal against research criteria.
        
        Returns:
            is_valid, reason
        """
        # Check confidence threshold
        if signal.get("confidence", 0) < 0.2:
            return False, "Confidence below threshold"
        
        # Check for trap confirmation
        trap_prob = feature_row.get("trap_probability", 0)
        if trap_prob > 0.7 and signal.get("signal_type") != "NEUTRAL":
            # High trap probability requires careful validation
            gamma_neg = feature_row.get("gamma_regime_negative", 0)
            if gamma_neg > 0.5:
                # Negative gamma with high trap = likely valid
                return True, "Gamma trap confirmed"
            else:
                return False, "High trap probability without negative gamma"
        
        # Check divergence consistency
        has_divergence = feature_row.get("has_divergence", 0)
        if has_divergence > 0.5:
            # Signal should align with divergence direction
            divergence_score = feature_row.get("divergence_score", 0)
            signal_type = signal.get("signal_type")
            
            if divergence_score > 0 and signal_type != "BUY":
                return False, "Signal contradicts bullish divergence"
            elif divergence_score < 0 and signal_type != "SELL":
                return False, "Signal contradicts bearish divergence"
        
        # Check OI velocity consistency
        oi_velocity = feature_row.get("oi_velocity", 0)
        if abs(oi_velocity) > 1.5:  # Strong OI movement
            if oi_velocity > 0 and signal.get("signal_type") != "BUY":
                return False, "Signal contradicts strong OI buildup"
            elif oi_velocity < 0 and signal.get("signal_type") != "SELL":
                return False, "Signal contradicts strong OI unwinding"
        
        return True, "Signal validated"
    
    @staticmethod
    def filter_weak_signals(signals: List[Dict], min_strength: str = "MODERATE") -> List[Dict]:
        """
        Filter signals by strength.
        
        Args:
            signals: List of signals
            min_strength: Minimum strength required
        
        Returns:
            Filtered signals
        """
        strength_order = {
            "WEAK": 0,
            "MODERATE": 1,
            "STRONG": 2,
            "VERY_STRONG": 3
        }
        
        min_strength_value = strength_order.get(min_strength, 0)
        
        filtered = []
        for signal in signals:
            signal_strength = signal.get("signal_strength", "WEAK")
            if strength_order.get(signal_strength, 0) >= min_strength_value:
                filtered.append(signal)
        
        return filtered

# ==============================
# SIGNAL ANALYTICS
# ==============================

def analyze_signal_patterns(signals: List[Dict]) -> Dict:
    """
    Analyze patterns in signal generation.
    """
    if not signals:
        return {}
    
    # Convert to DataFrame for analysis
    df = pd.DataFrame(signals)
    
    analysis = {
        "total_signals": len(df),
        "signal_distribution": df["signal_type"].value_counts().to_dict(),
        "avg_confidence": df["confidence"].mean() if "confidence" in df.columns else 0,
        "strength_distribution": df["signal_strength"].value_counts().to_dict() if "signal_strength" in df.columns else {},
        "regime_distribution": df["market_state"].value_counts().to_dict() if "market_state" in df.columns else {}
    }
    
    # Calculate success rate if PNL data available
    if "pnl" in df.columns and not df["pnl"].isna().all():
        profitable = df[df["pnl"] > 0]
        analysis["profitable_signals"] = len(profitable)
        analysis["success_rate"] = len(profitable) / len(df) * 100 if len(df) > 0 else 0
        analysis["avg_pnl"] = df["pnl"].mean()
    
    return analysis

# ==============================
# INITIALIZATION
# ==============================

def create_signal_engine(
    signal_expiry_minutes: int = 5,
    confidence_threshold: float = 0.2
) -> SignalStateMachine:
    """
    Factory function to create signal engine.
    """
    return SignalStateMachine(
        signal_expiry_minutes=signal_expiry_minutes,
        confidence_threshold=confidence_threshold
    )



====================================================================================================
FILE: .\data\instrument_master.py
====================================================================================================


File Name: instrument_master.py
Full Path: G:\trading_app\data\instrument_master.py
Size: 18.18 KB
Last Modified: 01/28/2026 00:21:32
Extension: .py

"""
Enhanced Instrument Master with robust key generation.
Handles multiple underlying formats and expiry matching.
"""

from typing import List, Dict, Any, Optional
import gzip
import json
from datetime import datetime, timedelta
from pathlib import Path
import pandas as pd
import warnings

# ==============================
# INSTRUMENT LOADING
# ==============================

def load_instruments() -> List[Dict[str, Any]]:
    """
    Load instruments from compressed JSON file.
    
    Returns:
        List of instrument dictionaries
    """
    instrument_file = Path("data/instruments.json.gz")
    
    if not instrument_file.exists():
        warnings.warn(f"Instrument file not found: {instrument_file}")
        return []
    
    try:
        with gzip.open(instrument_file, 'rt', encoding='utf-8') as f:
            data = json.load(f)
            
        if isinstance(data, list):
            print(f"‚úì Loaded {len(data)} instruments")
            return data
        else:
            warnings.warn(f"Unexpected data format: {type(data)}")
            return []
            
    except Exception as e:
        warnings.warn(f"Error loading instruments: {e}")
        return []

def save_instruments(instruments: List[Dict[str, Any]]) -> bool:
    """Save instruments to compressed JSON file."""
    try:
        instrument_file = Path("data/instruments.json.gz")
        instrument_file.parent.mkdir(parents=True, exist_ok=True)
        
        with gzip.open(instrument_file, 'wt', encoding='utf-8') as f:
            json.dump(instruments, f)
            
        print(f"‚úì Saved {len(instruments)} instruments")
        return True
    except Exception as e:
        warnings.warn(f"Error saving instruments: {e}")
        return False

# ==============================
# OPTION KEY GENERATION
# ==============================

def get_option_keys(
    underlying: str, 
    expiry: str, 
    max_keys: int = 200,
    instrument_type: Optional[str] = None
) -> List[str]:
    """
    Get option instrument keys for given underlying and expiry.
    Fixed: Matches by DATE only, not exact timestamp.
    
    Args:
        underlying: Underlying symbol (e.g., 'NIFTY', 'BANKNIFTY')
        expiry: Expiry date in 'YYYY-MM-DD' format
        max_keys: Maximum number of keys to return
        instrument_type: Filter by 'CE', 'PE', or None for both
    
    Returns:
        List of instrument keys
    """
    instruments = load_instruments()
    
    if not instruments:
        print("‚ö†Ô∏è No instruments loaded")
        return []
    
    # Parse target expiry date
    try:
        target_date = datetime.strptime(expiry, "%Y-%m-%d").date()
    except ValueError as e:
        print(f"‚ùå Invalid expiry format: {expiry}. Expected YYYY-MM-DD")
        return []
    
    option_keys = []
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        # Check if it's an option
        inst_type = instrument.get('instrument_type')
        if inst_type not in ['CE', 'PE']:
            continue
        
        # Filter by instrument type if specified
        if instrument_type and inst_type != instrument_type:
            continue
        
        # EXACT match for name
        name = instrument.get('name', '')
        if name != underlying:
            continue
        
        # Check expiry - convert timestamp to date
        instrument_expiry_ts = instrument.get('expiry')
        if not instrument_expiry_ts:
            continue
        
        # Convert timestamp to date (ignoring time component)
        try:
            instrument_expiry_date = datetime.fromtimestamp(instrument_expiry_ts / 1000).date()
        except:
            continue
        
        # Match by date only (not exact timestamp)
        if instrument_expiry_date != target_date:
            continue
        
        instrument_key = instrument.get('instrument_key')
        if instrument_key:
            option_keys.append({
                'key': instrument_key,
                'strike_price': float(instrument.get('strike_price', 0)),
                'instrument_type': inst_type,
                'instrument': instrument
            })
    
    if not option_keys:
        print(f"‚ö†Ô∏è No {underlying} options found for expiry {expiry}")
        return []
    
    # Sort by strike price
    option_keys.sort(key=lambda x: x['strike_price'])
    
    print(f"‚úÖ Found {len(option_keys)} {underlying} options for {expiry}")
    
    # Return just the keys
    return [item['key'] for item in option_keys[:max_keys]]

def get_option_keys_around_price(
    underlying: str,
    expiry: str,
    spot_price: float,
    num_strikes: int = 10,
    max_keys: int = 100
) -> List[str]:
    """
    Get option keys around current spot price.
    Fixed: Matches by DATE only, not exact timestamp.
    
    Args:
        underlying: Underlying symbol
        expiry: Expiry date in 'YYYY-MM-DD' format
        spot_price: Current spot price
        num_strikes: Number of strikes to get on each side
        max_keys: Maximum total keys to return
    
    Returns:
        List of instrument keys sorted by proximity to spot
    """
    instruments = load_instruments()
    
    if not instruments:
        return []
    
    # Parse target expiry date
    try:
        target_date = datetime.strptime(expiry, "%Y-%m-%d").date()
    except ValueError as e:
        print(f"‚ùå Invalid expiry format: {expiry}. Expected YYYY-MM-DD")
        return []
    
    all_options = []
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        # Check if it's an option
        if instrument.get('instrument_type') not in ['CE', 'PE']:
            continue
        
        # Check underlying
        name = instrument.get('name', '')
        if name != underlying:
            continue
        
        # Check expiry - convert timestamp to date
        instrument_expiry_ts = instrument.get('expiry')
        if not instrument_expiry_ts:
            continue
        
        # Convert timestamp to date (ignoring time component)
        try:
            instrument_expiry_date = datetime.fromtimestamp(instrument_expiry_ts / 1000).date()
        except:
            continue
        
        # Match by date only (not exact timestamp)
        if instrument_expiry_date != target_date:
            continue
        
        # Calculate distance from spot
        strike_price = float(instrument.get('strike_price', 0))
        distance = abs(strike_price - spot_price)
        
        instrument_key = instrument.get('instrument_key')
        if instrument_key:
            all_options.append({
                'key': instrument_key,
                'strike_price': strike_price,
                'distance': distance,
                'instrument_type': instrument.get('instrument_type'),
                'instrument': instrument
            })
    
    if not all_options:
        print(f"‚ö†Ô∏è No {underlying} options found for expiry {expiry} around spot {spot_price}")
        return []
    
    # Sort by distance from spot
    all_options.sort(key=lambda x: x['distance'])
    
    # Get closest strikes
    closest_options = all_options[:num_strikes * 2]  # CE and PE for each strike
    
    # Sort by strike price for consistency
    closest_options.sort(key=lambda x: x['strike_price'])
    
    print(f"‚úÖ Found {len(closest_options[:max_keys])} {underlying} options around spot {spot_price} for {expiry}")
    
    return [item['key'] for item in closest_options[:max_keys]]


def get_available_expiries(underlying: str) -> List[str]:
    """
    Get all available expiry dates for an underlying.
    Fixed: Uses date-only matching.
    
    Args:
        underlying: Underlying symbol
    
    Returns:
        List of expiry dates in 'YYYY-MM-DD' format
    """
    instruments = load_instruments()
    
    if not instruments:
        return []
    
    expiry_dates = set()
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        # Check if it's the right underlying
        name = instrument.get('name', '')
        if name != underlying:
            continue
        
        # Check if it's an option
        if instrument.get('instrument_type') not in ['CE', 'PE']:
            continue
        
        expiry_ts = instrument.get('expiry')
        if expiry_ts:
            try:
                # Convert timestamp to date only
                expiry_date = datetime.fromtimestamp(expiry_ts / 1000).date()
                expiry_dates.add(expiry_date.strftime('%Y-%m-%d'))
            except:
                pass
    
    return sorted(expiry_dates)

def get_nearest_expiry(underlying: str, min_days: int = 0) -> str:
    """
    Get nearest expiry date for an underlying.
    Fixed: Uses date-only comparison.
    
    Args:
        underlying: Underlying symbol
        min_days: Minimum days until expiry
    
    Returns:
        Expiry date in 'YYYY-MM-DD' format, or empty string if none found
    """
    expiry_dates = get_available_expiries(underlying)
    
    if not expiry_dates:
        return ""
    
    # Get today's date (without time)
    today = datetime.now().date()
    
    # Add min_days
    from datetime import timedelta
    target_date = today + timedelta(days=min_days)
    target_str = target_date.strftime('%Y-%m-%d')
    
    # Find first expiry on or after target date
    for expiry in expiry_dates:
        if expiry >= target_str:
            return expiry
    
    # If no future expiry, return the last one
    return expiry_dates[-1]

def get_weekly_expiry(underlying: str) -> str:
    """
    Get nearest Thursday expiry (standard weekly expiry).
    
    Args:
        underlying: Underlying symbol
    
    Returns:
        Expiry date in 'YYYY-MM-DD' format
    """
    # Find the nearest Thursday
    today = datetime.now()
    
    # Thursday is weekday 3 (Monday=0)
    days_until_thursday = (3 - today.weekday()) % 7
    if days_until_thursday == 0:
        days_until_thursday = 7  # If today is Thursday, get next Thursday
    
    next_thursday = today + timedelta(days=days_until_thursday)
    
    # Check if this Thursday is available
    thursday_str = next_thursday.strftime('%Y-%m-%d')
    available_expiries = get_available_expiries(underlying)
    
    if thursday_str in available_expiries:
        return thursday_str
    
    # If not, get the nearest available expiry
    return get_nearest_expiry(underlying)

# ==============================
# INSTRUMENT LOOKUP
# ==============================

def get_instrument_by_key(instrument_key: str) -> Optional[Dict[str, Any]]:
    """Get instrument details by instrument key."""
    instruments = load_instruments()
    
    for instrument in instruments:
        if isinstance(instrument, dict) and instrument.get('instrument_key') == instrument_key:
            return instrument
    
    return None

def get_instruments_by_type(
    instrument_type: str,
    underlying: Optional[str] = None
) -> List[Dict[str, Any]]:
    """
    Get instruments by type.
    
    Args:
        instrument_type: 'EQ', 'CE', 'PE', 'FUT', etc.
        underlying: Optional underlying symbol filter
    
    Returns:
        List of matching instruments
    """
    instruments = load_instruments()
    
    filtered = []
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        if instrument.get('instrument_type') != instrument_type:
            continue
        
        if underlying:
            name = instrument.get('name', '')
            if name != underlying:
                continue
        
        filtered.append(instrument)
    
    return filtered

# ==============================
# BATCH OPERATIONS
# ==============================

def get_batch_option_keys(
    underlying: str,
    expiries: List[str],
    max_per_expiry: int = 50
) -> Dict[str, List[str]]:
    """
    Get option keys for multiple expiries.
    
    Args:
        underlying: Underlying symbol
        expiries: List of expiry dates
        max_per_expiry: Maximum keys per expiry
    
    Returns:
        Dictionary mapping expiry to list of keys
    """
    result = {}
    
    for expiry in expiries:
        keys = get_option_keys(underlying, expiry, max_keys=max_per_expiry)
        if keys:
            result[expiry] = keys
    
    return result

def get_instrument_keys_for_strikes(
    underlying: str,
    expiry: str,
    strikes: List[float],
    option_types: List[str] = ['CE', 'PE']
) -> List[str]:
    """
    Get instrument keys for specific strikes and option types.
    
    Args:
        underlying: Underlying symbol
        expiry: Expiry date
        strikes: List of strike prices
        option_types: List of option types
    
    Returns:
        List of instrument keys
    """
    instruments = load_instruments()
    
    keys = []
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        inst_type = instrument.get('instrument_type')
        if inst_type not in option_types:
            continue
        
        # Check underlying
        name = instrument.get('name', '')
        if name != underlying:
            continue
        
        # Check expiry
        instrument_expiry = instrument.get('expiry')
        if not instrument_expiry:
            continue
        
        # Convert expiry string to timestamp
        try:
            expiry_dt = datetime.strptime(expiry, "%Y-%m-%d")
            expiry_ts = int(expiry_dt.timestamp() * 1000)
        except:
            continue
        
        if instrument_expiry != expiry_ts:
            continue
        
        # Check strike
        strike_price = float(instrument.get('strike_price', 0))
        if strike_price not in strikes:
            continue
        
        instrument_key = instrument.get('instrument_key')
        if instrument_key:
            keys.append(instrument_key)
    
    return keys

# ==============================
# UTILITIES
# ==============================

def print_instrument_summary():
    """Print summary of loaded instruments."""
    instruments = load_instruments()
    
    if not instruments:
        print("No instruments loaded")
        return
    
    print(f"Total instruments: {len(instruments)}")
    
    # Count by type
    type_counts = {}
    underlying_counts = {}
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        inst_type = instrument.get('instrument_type', 'UNKNOWN')
        type_counts[inst_type] = type_counts.get(inst_type, 0) + 1
        
        name = instrument.get('name', 'UNKNOWN')
        underlying_counts[name] = underlying_counts.get(name, 0) + 1
    
    print("\nBy instrument type:")
    for inst_type, count in sorted(type_counts.items()):
        print(f"  {inst_type}: {count}")
    
    print("\nTop underlying symbols:")
    for name, count in sorted(underlying_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"  {name}: {count}")

# ==============================
# TEST FUNCTION
# ==============================

def test_instrument_master():
    """Test the instrument master functions."""
    print("üß™ Testing Instrument Master...")
    
    # Load instruments
    instruments = load_instruments()
    print(f"‚úì Loaded {len(instruments)} instruments")
    
    # Get available expiries for NIFTY
    nifty_expiries = get_available_expiries("NIFTY")
    print(f"‚úì NIFTY expiries: {nifty_expiries}")
    
    if nifty_expiries:
        # Get nearest expiry
        nearest = get_nearest_expiry("NIFTY")
        print(f"‚úì Nearest NIFTY expiry: {nearest}")
        
        # Get option keys for nearest expiry
        if nearest:
            keys = get_option_keys("NIFTY", nearest, max_keys=10)
            print(f"‚úì Got {len(keys)} option keys for {nearest}")
            
            if keys:
                print("  Sample keys:")
                for key in keys[:3]:
                    print(f"    - {key}")
                
                # Get instrument details
                instrument = get_instrument_by_key(keys[0])
                if instrument:
                    print(f"  First instrument: {instrument.get('trading_symbol', 'N/A')}")
    
    # Test BANKNIFTY
    banknifty_expiries = get_available_expiries("BANKNIFTY")
    print(f"‚úì BANKNIFTY expiries: {banknifty_expiries}")
    
    print("\n‚úÖ Instrument master test complete")
# ==============================
# EXPIRY MANAGEMENT
# ==============================

def get_next_available_expiry(underlying: str) -> str:
    """
    Get the next available expiry date (excluding today if market closed).
    
    Args:
        underlying: Underlying symbol (e.g., 'NIFTY', 'BANKNIFTY')
    
    Returns:
        Next available expiry date in 'YYYY-MM-DD' format
    """
    from datetime import datetime
    
    # Get all available expiries
    expiries = get_available_expiries(underlying)
    if not expiries:
        return ""
    
    # Get today's date
    today = datetime.now().strftime('%Y-%m-%d')
    
    # Check if market is open (for testing, you can hardcode this)
    # For now, let's assume if today is expiry day, we should use next expiry
    market_open = False  # You can set this based on actual market hours
    
    if market_open and today in expiries:
        # Market is open and today is an expiry - use it
        return today
    else:
        # Market closed or today not available - get next expiry
        for expiry in sorted(expiries):
            if expiry > today:
                return expiry
        
        # If no future expiry, return the last one
        return expiries[-1]
# Run test if executed directly
if __name__ == "__main__":
    test_instrument_master()



====================================================================================================
FILE: .\data\instrument_master_fixed.py
====================================================================================================


File Name: instrument_master_fixed.py
Full Path: G:\trading_app\data\instrument_master_fixed.py
Size: 5.45 KB
Last Modified: 01/28/2026 00:17:11
Extension: .py

"""
FIXED version - matches by date, not exact timestamp
"""
from typing import List, Dict, Any, Optional
import gzip
import json
from datetime import datetime, timedelta
from pathlib import Path

def load_instruments() -> List[Dict[str, Any]]:
    """Load instruments from compressed JSON file."""
    instrument_file = Path("data/instruments.json.gz")
    
    if not instrument_file.exists():
        print(f"‚ùå Instrument file not found: {instrument_file}")
        return []
    
    try:
        with gzip.open(instrument_file, 'rt', encoding='utf-8') as f:
            data = json.load(f)
            
        if isinstance(data, list):
            return data
        else:
            print(f"‚ö†Ô∏è Unexpected data format: {type(data)}")
            return []
            
    except Exception as e:
        print(f"‚ùå Error loading instruments: {e}")
        return []

def get_option_keys_fixed(
    underlying: str, 
    expiry: str, 
    max_keys: int = 200,
    debug: bool = False
) -> List[str]:
    """
    Get option instrument keys for given underlying and expiry.
    Matches by DATE only, not exact timestamp.
    
    Args:
        underlying: Underlying symbol (e.g., 'NIFTY', 'BANKNIFTY')
        expiry: Expiry date in 'YYYY-MM-DD' format
        max_keys: Maximum number of keys to return
        debug: Enable debug logging
    
    Returns:
        List of instrument keys
    """
    instruments = load_instruments()
    
    if not instruments:
        if debug: print("‚ö†Ô∏è No instruments loaded")
        return []
    
    # Parse target expiry date
    try:
        target_date = datetime.strptime(expiry, "%Y-%m-%d").date()
        if debug: print(f"Looking for options expiring on: {target_date}")
    except ValueError as e:
        print(f"‚ùå Invalid expiry format: {expiry}. Expected YYYY-MM-DD")
        return []
    
    option_keys = []
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        # Check if it's an option
        inst_type = instrument.get('instrument_type')
        if inst_type not in ['CE', 'PE']:
            continue
        
        # EXACT match for name
        name = instrument.get('name', '')
        if name != underlying:
            continue
        
        # Check expiry - convert timestamp to date
        instrument_expiry_ts = instrument.get('expiry')
        if not instrument_expiry_ts:
            continue
        
        # Convert timestamp to date (ignoring time component)
        try:
            instrument_expiry_date = datetime.fromtimestamp(instrument_expiry_ts / 1000).date()
        except:
            continue
        
        # Match by date only
        if instrument_expiry_date != target_date:
            continue
        
        instrument_key = instrument.get('instrument_key')
        if instrument_key:
            option_keys.append({
                'key': instrument_key,
                'strike_price': float(instrument.get('strike_price', 0)),
                'instrument_type': inst_type,
                'instrument': instrument
            })
    
    if not option_keys:
        if debug: print(f"‚ö†Ô∏è No {underlying} options found for expiry {expiry}")
        return []
    
    # Sort by strike price
    option_keys.sort(key=lambda x: x['strike_price'])
    
    if debug: print(f"‚úÖ Found {len(option_keys)} {underlying} options for {expiry}")
    
    # Return just the keys
    return [item['key'] for item in option_keys[:max_keys]]

def get_available_expiries_fixed(underlying: str) -> List[str]:
    """
    Get all available expiry dates for an underlying.
    Fixed version that works with date-only matching.
    """
    instruments = load_instruments()
    
    if not instruments:
        return []
    
    expiry_dates = set()
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        # Check if it's the right underlying
        name = instrument.get('name', '')
        if name != underlying:
            continue
        
        # Check if it's an option
        if instrument.get('instrument_type') not in ['CE', 'PE']:
            continue
        
        expiry_ts = instrument.get('expiry')
        if expiry_ts:
            try:
                expiry_date = datetime.fromtimestamp(expiry_ts / 1000).date()
                expiry_dates.add(expiry_date.strftime('%Y-%m-%d'))
            except:
                pass
    
    return sorted(expiry_dates)

def test_fixed_functions():
    """Test the fixed functions"""
    print("üß™ Testing FIXED functions...")
    
    # Test get_available_expiries_fixed
    print("\nüîç Available expiries (fixed):")
    expiries = get_available_expiries_fixed("NIFTY")
    print(f"NIFTY expiries: {expiries[:5]}...")
    
    # Test get_option_keys_fixed
    if expiries:
        test_expiry = expiries[0] if len(expiries) > 0 else "2026-02-03"
        print(f"\nüîç Testing option keys for {test_expiry} (fixed):")
        keys = get_option_keys_fixed("NIFTY", test_expiry, max_keys=10, debug=True)
        print(f"Found {len(keys)} keys")
        
        if keys:
            print("Sample keys:")
            for key in keys[:3]:
                print(f"  - {key}")
        else:
            print("‚ùå Still no keys found!")

if __name__ == "__main__":
    test_fixed_functions()



====================================================================================================
FILE: .\data\upstox_client.py
====================================================================================================


File Name: upstox_client.py
Full Path: G:\trading_app\data\upstox_client.py
Size: 41.99 KB
Last Modified: 01/28/2026 11:58:22
Extension: .py

"""
Enhanced Upstox Client with advanced derivatives analytics.
Implements research concepts:
1. OI Velocity - Rate of change of Open Interest
2. Gamma Exposure (GEX) - Dealer hedging pressure
3. Structural Walls vs Traps detection
4. Spot Divergence analysis
5. Market microstructure insights
"""

import requests
import pandas as pd
import numpy as np
from typing import List, Dict, Optional, Tuple
import json
from datetime import datetime, timedelta
from scipy.stats import linregress
from dataclasses import dataclass
from enum import Enum
import streamlit as st

# ==============================
# CONSTANTS & CONFIG
# ==============================

BASE_URL = "https://api.upstox.com/v2"

# Research-based constants
OI_VELOCITY_LOOKBACK = 5  # periods for velocity calculation
GAMMA_LOOKBACK = 10  # strikes for gamma calculation
WALL_THRESHOLD = 0.15  # Min OI concentration for wall detection
TRAP_CONFIRMATION_WINDOW = 3  # periods for trap confirmation

class MarketRegime(Enum):
    """Market regimes based on research"""
    NORMAL = "NORMAL"
    EXPANSIVE = "EXPANSIVE"  # High OI velocity, capital inflow
    CONSTRICTED = "CONSTRICTED"  # Negative OI velocity, capital outflow
    GAMMA_POSITIVE = "GAMMA_POSITIVE"  # Stabilizing, pinning
    GAMMA_NEGATIVE = "GAMMA_NEGATIVE"  # Accelerating, squeezes
    TRAP_FORMING = "TRAP_FORMING"  # Wall breach with unwinding
    DIVERGENCE = "DIVERGENCE"  # Spot vs derivatives divergence

@dataclass
class WallAnalysis:
    """Analysis of structural walls"""
    strike: float
    oi_concentration: float
    option_type: str  # CE or PE
    is_defended: bool
    unwinding_rate: float  # OI velocity at this strike
    gamma_contribution: float
    distance_to_spot: float  # % distance from current spot

@dataclass
class TrapAnalysis:
    """Analysis of potential traps"""
    wall_strike: float
    breach_direction: str  # "UP" or "DOWN"
    confidence: float
    trigger_time: datetime
    gamma_impact: float
    oi_unwinding: float

@dataclass
class GammaProfile:
    """Gamma Exposure profile"""
    net_gamma: float
    positive_gamma_strikes: List[float]
    negative_gamma_strikes: List[float]
    flip_levels: List[float]  # Where gamma changes sign
    max_gamma_strike: float
    regime: str

# ==============================
# ENHANCED UPSTOX CLIENT
# ==============================

class UpstoxClient:
    def __init__(self, access_token: str):
        self.access_token = access_token
        self.session = requests.Session()
        self.session.headers.update({
            "Accept": "application/json",
            "Authorization": f"Bearer {access_token}"
        })
        
        # State tracking for research concepts
        self.oi_history = {}  # symbol -> list of OI values
        self.price_history = {}  # symbol -> list of prices
        self.gamma_history = {}  # symbol -> list of gamma values
        self.velocity_history = {}  # symbol -> list of velocity values
        
        # Market structure tracking
        self.walls_cache = {}
        self.traps_cache = {}
        self.gex_cache = {}
        
        # Research-based analytics
        self.analytics = MarketAnalytics()
    
    # ==============================
    # CORE REQUEST HANDLER
    # ==============================
    
    def _make_request(self, method: str, endpoint: str, **kwargs):
        """Helper method to make API requests with error handling"""
        url = f"{BASE_URL}/{endpoint}"
        
        try:
            response = self.session.request(method, url, **kwargs)
            
            # Debug logging
            # print(f"Request: {method} {url}")
            # print(f"Params: {kwargs.get('params', {})}")
            # print(f"Status: {response.status_code}")
            
            if response.status_code != 200:
                print(f"Error Response: {response.text}")
            
            response.raise_for_status()
            return response.json()
            
        except requests.exceptions.HTTPError as e:
            print(f"HTTP Error: {e}")
            try:
                error_data = response.json()
                print(f"Error Details: {json.dumps(error_data, indent=2)}")
            except:
                pass
            raise
        except Exception as e:
            print(f"Request Error: {e}")
            raise
    
    # ==============================
    # RESEARCH IMPLEMENTATION: OI VELOCITY
    # ==============================
    
    def calculate_oi_velocity(self, symbol: str, current_oi: float) -> Tuple[float, str]:
        """
        Calculate Open Interest Velocity with regime classification.
        
        Research Concept: OI Velocity measures the kinetic energy of market trends.
        High positive velocity = Initiative capital entering (Buildup)
        High negative velocity = Capital exiting (Unwinding/Covering)
        
        Returns: (velocity_score, regime)
        """
        if symbol not in self.oi_history:
            self.oi_history[symbol] = []
        
        # Add current OI to history
        self.oi_history[symbol].append(current_oi)
        
        # Keep only last N periods
        if len(self.oi_history[symbol]) > OI_VELOCITY_LOOKBACK:
            self.oi_history[symbol] = self.oi_history[symbol][-OI_VELOCITY_LOOKBACK:]
        
        if len(self.oi_history[symbol]) < 2:
            return 0.0, "INSUFFICIENT_DATA"
        
        # Calculate velocity (rate of change)
        oi_series = pd.Series(self.oi_history[symbol])
        velocity = oi_series.pct_change().iloc[-1] * 100  # % change
        
        # Normalize velocity (research concept)
        if len(oi_series) >= 3:
            std_dev = oi_series.pct_change().std() * 100
            if std_dev > 0:
                normalized_velocity = velocity / std_dev
            else:
                normalized_velocity = velocity
        else:
            normalized_velocity = velocity
        
        # Classify regime
        if normalized_velocity > 1.5:
            regime = "EXPANSIVE"  # High capital inflow
        elif normalized_velocity < -1.5:
            regime = "CONSTRICTED"  # Capital outflow
        else:
            regime = "NORMAL"
        
        # Store for trend analysis
        if symbol not in self.velocity_history:
            self.velocity_history[symbol] = []
        self.velocity_history[symbol].append({
            "timestamp": datetime.utcnow(),
            "velocity": normalized_velocity,
            "regime": regime,
            "raw_oi": current_oi
        })
        
        return normalized_velocity, regime
    
    # ==============================
    # RESEARCH IMPLEMENTATION: GAMMA EXPOSURE
    # ==============================
    
    def calculate_gamma_exposure(self, option_chain_df: pd.DataFrame, 
                                 spot_price: float) -> GammaProfile:
        """
        Calculate Gamma Exposure (GEX) with dealer hedging analysis.
        
        Research Concept: GEX predicts volatility regimes.
        Positive GEX: Dealers long gamma ‚Üí Stabilizing, pinning expected
        Negative GEX: Dealers short gamma ‚Üí Accelerating, squeezes possible
        """
        if option_chain_df.empty:
            return GammaProfile(0.0, [], [], [], 0.0, "NEUTRAL")
        
        # Calculate gamma for each strike (simplified Black-Scholes gamma)
        gamma_values = []
        flip_levels = []
        
        for _, row in option_chain_df.iterrows():
            strike = row['strike']
            option_type = row['option_type']
            oi = row['oi']
            iv = row.get('iv', 0.3)  # Default IV if not available
            
            # Simplified gamma calculation
            # In production, use proper Black-Scholes or gather from API
            if iv > 0:
                # Approximate gamma: highest ATM, decays as move OTM
                distance = abs(strike - spot_price) / spot_price
                gamma = np.exp(-distance * 100) / (iv * np.sqrt(252/365))  # Simplified
                
                # Adjust for option type
                if option_type == "PE":
                    gamma = -gamma  # Short gamma for put writers
                
                # Weight by OI
                gamma_weighted = gamma * oi
                gamma_values.append((strike, gamma_weighted))
                
                # Track sign changes for flip levels
                if len(gamma_values) > 1:
                    prev_sign = np.sign(gamma_values[-2][1])
                    curr_sign = np.sign(gamma_weighted)
                    if prev_sign != curr_sign:
                        flip_level = (strike + gamma_values[-2][0]) / 2
                        flip_levels.append(flip_level)
        
        if not gamma_values:
            return GammaProfile(0.0, [], [], [], 0.0, "NEUTRAL")
        
        # Calculate net gamma
        strikes, gammas = zip(*gamma_values)
        net_gamma = sum(gammas)
        
        # Separate positive and negative gamma strikes
        positive_strikes = [s for s, g in gamma_values if g > 0]
        negative_strikes = [s for s, g in gamma_values if g < 0]
        
        # Find strike with maximum gamma impact
        max_gamma_idx = np.argmax(np.abs(gammas))
        max_gamma_strike = strikes[max_gamma_idx]
        
        # Determine regime
        if net_gamma > 0:
            regime = "GAMMA_POSITIVE"
        elif net_gamma < 0:
            regime = "GAMMA_NEGATIVE"
        else:
            regime = "NEUTRAL"
        
        return GammaProfile(
            net_gamma=net_gamma,
            positive_gamma_strikes=positive_strikes[:5],  # Top 5
            negative_gamma_strikes=negative_strikes[:5],
            flip_levels=flip_levels,
            max_gamma_strike=max_gamma_strike,
            regime=regime
        )
    
    # ==============================
    # RESEARCH IMPLEMENTATION: WALLS VS TRAPS
    # ==============================
    
    def analyze_structural_walls(self, option_chain_df: pd.DataFrame, 
                                 spot_price: float) -> List[WallAnalysis]:
        """
        Identify structural walls with defense analysis.
        
        Research Concept: Walls are high OI concentrations that act as barriers.
        Traps form when walls breach with OI unwinding.
        """
        if option_chain_df.empty:
            return []
        
        walls = []
        
        # Group by strike to find OI concentrations
        strike_groups = option_chain_df.groupby('strike')
        
        for strike, group in strike_groups:
            total_oi = group['oi'].sum()
            call_oi = group[group['option_type'] == 'CE']['oi'].sum()
            put_oi = group[group['option_type'] == 'PE']['oi'].sum()
            
            # Determine dominant option type at this strike
            if call_oi > put_oi:
                dominant_type = "CE"
                dominant_oi = call_oi
            else:
                dominant_type = "PE"
                dominant_oi = put_oi
            
            # Calculate OI concentration (research concept)
            total_all_oi = option_chain_df['oi'].sum()
            if total_all_oi > 0:
                oi_concentration = dominant_oi / total_all_oi
            else:
                oi_concentration = 0
            
            # Check if this is a wall (high concentration)
            if oi_concentration > WALL_THRESHOLD:
                # Calculate distance from spot
                distance_pct = abs(strike - spot_price) / spot_price * 100
                
                # Analyze defense strength
                # Defense is stronger if:
                # 1. High OI concentration
                # 2. Close to spot price
                # 3. Recent OI increase (build-up)
                
                # Calculate unwinding rate (OI velocity at this strike)
                strike_key = f"{strike}_{dominant_type}"
                if strike_key in self.oi_history:
                    strike_oi_series = pd.Series(self.oi_history[strike_key])
                    if len(strike_oi_series) > 1:
                        unwinding_rate = strike_oi_series.pct_change().iloc[-1] * 100
                    else:
                        unwinding_rate = 0
                else:
                    unwinding_rate = 0
                
                # Determine if wall is being defended
                # Negative unwinding = defending (adding positions)
                # Positive unwinding = abandoning (closing positions)
                is_defended = unwinding_rate < 0  # Adding OI = defending
                
                walls.append(WallAnalysis(
                    strike=strike,
                    oi_concentration=oi_concentration,
                    option_type=dominant_type,
                    is_defended=is_defended,
                    unwinding_rate=unwinding_rate,
                    gamma_contribution=0,  # Would calculate in production
                    distance_to_spot=distance_pct
                ))
        
        # Sort by OI concentration (highest first)
        walls.sort(key=lambda x: x.oi_concentration, reverse=True)
        return walls[:5]  # Return top 5 walls
    
    def detect_traps(self, walls: List[WallAnalysis], spot_price: float,
                     oi_velocity: float) -> List[TrapAnalysis]:
        """
        Detect potential traps forming at walls.
        
        Research Concept: Traps occur when:
        1. Price breaches a wall
        2. OI starts unwinding (velocity negative)
        3. Gamma is negative (accelerating)
        4. High confidence of squeeze
        """
        traps = []
        
        for wall in walls:
            # Check if price is near wall (¬±1%)
            price_to_wall_ratio = spot_price / wall.strike
            is_near_wall = 0.99 <= price_to_wall_ratio <= 1.01
            
            if is_near_wall:
                # Check for trap conditions
                trap_confidence = 0.0
                
                # Condition 1: OI unwinding (negative velocity)
                if wall.unwinding_rate > 1.0:  # Rapid unwinding
                    trap_confidence += 0.3
                
                # Condition 2: Overall OI velocity negative
                if oi_velocity < -1.0:
                    trap_confidence += 0.3
                
                # Condition 3: Wall not defended
                if not wall.is_defended:
                    trap_confidence += 0.2
                
                # Condition 4: Price already breached (for detection)
                if (wall.option_type == "CE" and spot_price > wall.strike) or \
                   (wall.option_type == "PE" and spot_price < wall.strike):
                    trap_confidence += 0.2
                
                if trap_confidence > 0.5:  # Minimum confidence threshold
                    breach_direction = "UP" if wall.option_type == "CE" else "DOWN"
                    
                    trap = TrapAnalysis(
                        wall_strike=wall.strike,
                        breach_direction=breach_direction,
                        confidence=trap_confidence,
                        trigger_time=datetime.utcnow(),
                        gamma_impact=0,  # Would calculate actual gamma
                        oi_unwinding=wall.unwinding_rate
                    )
                    traps.append(trap)
        
        return traps
    
    # ==============================
    # RESEARCH IMPLEMENTATION: SPOT DIVERGENCE
    # ==============================
    
    def analyze_spot_divergence(self, spot_price: float, spot_velocity: float,
                               oi_velocity: float, gamma_profile: GammaProfile) -> Dict:
        """
        Analyze divergence between spot price and derivatives metrics.
        
        Research Concept: Divergence reveals internal market weakness.
        Bullish divergence: Price down but OI/Gamma improving
        Bearish divergence: Price up but OI/Gamma deteriorating
        """
        divergence_analysis = {
            "has_divergence": False,
            "type": None,  # BULLISH/BEARISH
            "confidence": 0.0,
            "metrics": {}
        }
        
        # Price vs OI divergence
        if spot_velocity > 0 and oi_velocity < -0.5:
            # Price up but OI unwinding (bearish divergence)
            divergence_analysis["has_divergence"] = True
            divergence_analysis["type"] = "BEARISH"
            divergence_analysis["confidence"] = min(abs(oi_velocity) / 2, 1.0)
            divergence_analysis["metrics"]["price_oi_divergence"] = {
                "price_change": spot_velocity,
                "oi_change": oi_velocity,
                "interpretation": "Hollow rally - price rising on covering, not new buying"
            }
        
        elif spot_velocity < 0 and oi_velocity > 0.5:
            # Price down but OI building (bullish divergence)
            divergence_analysis["has_divergence"] = True
            divergence_analysis["type"] = "BULLISH"
            divergence_analysis["confidence"] = min(oi_velocity / 2, 1.0)
            divergence_analysis["metrics"]["price_oi_divergence"] = {
                "price_change": spot_velocity,
                "oi_change": oi_velocity,
                "interpretation": "Accumulation - smart money buying despite price drop"
            }
        
        # Price vs Gamma divergence
        if gamma_profile.regime == "GAMMA_POSITIVE" and spot_velocity > 1.0:
            # Positive gamma (stabilizing) but price moving fast
            divergence_analysis["has_divergence"] = True
            divergence_analysis["type"] = "BEARISH"
            divergence_analysis["confidence"] = max(divergence_analysis["confidence"], 0.3)
            divergence_analysis["metrics"]["price_gamma_divergence"] = {
                "gamma_regime": gamma_profile.regime,
                "price_velocity": spot_velocity,
                "interpretation": "Price moving against gamma regime - likely to revert"
            }
        
        return divergence_analysis
    
    # ==============================
    # ENHANCED OPTION CHAIN FETCH
    # ==============================
    
    def fetch_option_chain_with_analytics(self, option_keys: list[str], 
                                         spot_price: float) -> Dict:
        """
        Enhanced option chain fetch with full research analytics.
        Returns comprehensive analysis including OI velocity, GEX, walls, traps.
        """
        # Fetch raw option chain
        option_chain_df = self.fetch_option_chain(option_keys)
        
        if option_chain_df.empty:
            return {
                "raw_data": option_chain_df,
                "analytics": {},
                "warnings": ["No option data available"]
            }
        
        # Calculate total OI for velocity
        total_oi = option_chain_df['oi'].sum()
        
        # OI Velocity analysis
        oi_velocity, oi_regime = self.calculate_oi_velocity("INDEX", total_oi)
        
        # Gamma Exposure analysis
        gamma_profile = self.calculate_gamma_exposure(option_chain_df, spot_price)
        
        # Structural walls analysis
        walls = self.analyze_structural_walls(option_chain_df, spot_price)
        
        # Trap detection
        traps = self.detect_traps(walls, spot_price, oi_velocity)
        
        # Spot price velocity (approximate)
        if "INDEX" not in self.price_history:
            self.price_history["INDEX"] = []
        self.price_history["INDEX"].append(spot_price)
        if len(self.price_history["INDEX"]) > OI_VELOCITY_LOOKBACK:
            self.price_history["INDEX"] = self.price_history["INDEX"][-OI_VELOCITY_LOOKBACK:]
        
        spot_velocity = 0
        if len(self.price_history["INDEX"]) >= 2:
            price_series = pd.Series(self.price_history["INDEX"])
            spot_velocity = price_series.pct_change().iloc[-1] * 100
        
        # Spot divergence analysis
        divergence = self.analyze_spot_divergence(spot_price, spot_velocity, 
                                                 oi_velocity, gamma_profile)
        
        # Market regime synthesis
        market_regime = self._synthesize_market_regime(
            oi_regime, gamma_profile.regime, divergence
        )
        
        # Compile comprehensive analysis
        analytics = {
            "timestamp": datetime.utcnow().isoformat(),
            "spot_price": spot_price,
            
            # OI Analysis
            "oi_velocity": round(oi_velocity, 3),
            "oi_regime": oi_regime,
            "total_oi": int(total_oi),
            
            # Gamma Analysis
            "gamma_exposure": {
                "net_gamma": round(gamma_profile.net_gamma, 3),
                "regime": gamma_profile.regime,
                "flip_levels": [round(x, 2) for x in gamma_profile.flip_levels],
                "max_impact_strike": gamma_profile.max_gamma_strike
            },
            
            # Structure Analysis
            "structural_walls": [
                {
                    "strike": wall.strike,
                    "type": wall.option_type,
                    "concentration": round(wall.oi_concentration, 3),
                    "defended": wall.is_defended,
                    "distance_pct": round(wall.distance_to_spot, 2)
                }
                for wall in walls
            ],
            
            # Trap Analysis
            "potential_traps": [
                {
                    "strike": trap.wall_strike,
                    "direction": trap.breach_direction,
                    "confidence": round(trap.confidence, 3),
                    "unwinding_rate": round(trap.oi_unwinding, 2)
                }
                for trap in traps
            ],
            
            # Divergence Analysis
            "spot_divergence": divergence,
            
            # Market Regime
            "market_regime": market_regime,
            "regime_confidence": self._calculate_regime_confidence(
                oi_velocity, gamma_profile.net_gamma, divergence
            )
        }
        
        # Store in cache for trend analysis
        self.walls_cache[datetime.utcnow()] = walls
        self.traps_cache[datetime.utcnow()] = traps
        self.gex_cache[datetime.utcnow()] = gamma_profile
        
        return {
            "raw_data": option_chain_df,
            "analytics": analytics,
            "market_insights": self._generate_market_insights(analytics)
        }
    
    # ==============================
    # ORIGINAL METHODS (ENHANCED)
    # ==============================
    
    def fetch_option_chain(self, option_keys: list[str]) -> pd.DataFrame:
        """Original method - fetch and normalize option chain quotes."""
        if not option_keys:
            return pd.DataFrame(
                columns=["strike", "option_type", "oi", "oi_change", "iv", "ltp", "volume"]
            )
        
        # Take max 200 keys (API limit)
        keys_to_fetch = option_keys[:200]
        
        data = self._make_request(
            "GET",
            "market-quote/quotes",
            params={"instrument_key": ",".join(keys_to_fetch)}
        )
        
        rows = []
        for key, payload in data.get("data", {}).items():
            symbol = payload.get("trading_symbol", "")
            
            rows.append({
                "strike": payload.get("strike_price"),
                "option_type": "CE" if symbol.endswith("CE") else "PE",
                "oi": payload.get("oi", 0),
                "oi_change": payload.get("oi_day_high", 0) - payload.get("oi_day_low", 0),
                "iv": payload.get("implied_volatility", 0),
                "ltp": payload.get("last_price", 0),
                "volume": payload.get("volume", 0),
                "timestamp": payload.get("timestamp")
            })
        
        return pd.DataFrame(rows)
    
    def fetch_index_quote(self, symbol: str = "NSE_INDEX|Nifty 50") -> Optional[dict]:
        """Original method - fetch index quote."""
        try:
            data = self._make_request(
                "GET",
                "market-quote/quotes",
                params={"symbol": symbol}
            )
            
            if data.get("status") == "success":
                response_key = symbol.replace("|", ":")
                quote_data = data.get("data", {}).get(response_key)
                
                if not quote_data:
                    quote_data = data.get("data", {}).get(symbol)
                
                if not quote_data and data.get("data"):
                    first_key = list(data["data"].keys())[0]
                    quote_data = data["data"][first_key]
                
                if not quote_data:
                    return None
                
                ohlc = quote_data.get("ohlc", {})
                
                return {
                    "symbol": symbol,
                    "ltp": quote_data.get("last_price"),
                    "open": ohlc.get("open"),
                    "high": ohlc.get("high"),
                    "low": ohlc.get("low"),
                    "close": ohlc.get("close"),
                    "change": quote_data.get("change"),
                    "net_change": quote_data.get("net_change"),
                    "percent_change": quote_data.get("percent_change"),
                    "volume": quote_data.get("volume"),
                    "timestamp": quote_data.get("timestamp")
                }
            else:
                print(f"API Error: {data}")
                return None
                
        except Exception as e:
            print(f"Error in fetch_index_quote: {e}")
            return None
    
    def fetch_equity_quotes(self, symbols: list[str]) -> pd.DataFrame:
        """Fixed method - fetch equity quotes with proper error handling and symbol validation."""
        if not symbols:
            return pd.DataFrame(columns=["symbol", "ltp", "open"])
        
        # Clean symbols - fix problematic symbols
        cleaned_symbols = []
        for symbol in symbols:
            # Fix common issues
            if symbol == "SBI LIFE":
                symbol = "SBILIFE"  # Remove space
            elif "+" in symbol:
                symbol = symbol.replace("+", "")  # Remove plus signs
            elif "-" in symbol:
                symbol = symbol.replace("-", "")  # Remove hyphens
            cleaned_symbols.append(symbol)
        
        symbols = cleaned_symbols  # Use cleaned symbols
        
        # DEBUG: Print cleaned symbols
        print(f"Cleaned symbols for API call: {symbols[:5]}...")
        
        # FORMAT 1: NSE|SYMBOL (This is usually the correct format for Upstox)
        instrument_symbols = [f"NSE|{s}" for s in symbols]
        
        try:
            print(f"Trying format 1 (NSE|SYMBOL): {instrument_symbols[:3]}...")
            
            response = self._make_request(
                "GET",
                "https://api.upstox.com/v2/market-quote/quotes",
                params={"symbol": ",".join(instrument_symbols)}
            )
            
            # Check if response indicates error
            if response.get("status") == "error":
                print(f"API Error in format 1: {response.get('errors', [{}])[0].get('message', 'Unknown error')}")
                raise Exception(f"API Error: {response.get('errors', [{}])[0].get('message')}")
            
            print("‚úì Format 1 succeeded")
            data = response
            
        except Exception as e:
            print(f"Format 1 failed: {e}")
            
            # FORMAT 2: Try with instrument_key instead of symbol
            instrument_symbols = [f"NSE_EQ|{s}" for s in symbols]
            print(f"Trying format 2 (NSE_EQ|SYMBOL): {instrument_symbols[:3]}...")
            
            try:
                response = self._make_request(
                    "GET",
                    "https://api.upstox.com/v2/market-quote/quotes",
                    params={"instrument_key": ",".join(instrument_symbols)}
                )
                
                # Check if response indicates error
                if response.get("status") == "error":
                    print(f"API Error in format 2: {response.get('errors', [{}])[0].get('message', 'Unknown error')}")
                    raise Exception(f"API Error: {response.get('errors', [{}])[0].get('message')}")
                
                print("‚úì Format 2 succeeded")
                data = response
                
            except Exception as e2:
                print(f"Format 2 also failed: {e2}")
                
                # FORMAT 3: Try individual symbol-by-symbol with smaller batches
                print("Trying format 3: Individual symbol requests...")
                all_data = {"data": {}}
                success_count = 0
                
                # Try each symbol individually first to identify problematic ones
                problem_symbols = []
                good_symbols = []
                
                for symbol in symbols:
                    try:
                        # Try NSE| format first
                        response = self._make_request(
                            "GET",
                            "https://api.upstox.com/v2/market-quote/quotes",
                            params={"symbol": f"NSE|{symbol}"}
                        )
                        
                        if response.get("status") == "error":
                            # Try NSE_EQ| format
                            response = self._make_request(
                                "GET",
                                "https://api.upstox.com/v2/market-quote/quotes",
                                params={"instrument_key": f"NSE_EQ|{symbol}"}
                            )
                        
                        if response.get("status") == "success" and "data" in response:
                            all_data["data"].update(response["data"])
                            success_count += 1
                            good_symbols.append(symbol)
                        else:
                            problem_symbols.append(symbol)
                            
                    except Exception as sym_e:
                        problem_symbols.append(symbol)
                        print(f"  Failed for symbol {symbol}: {sym_e}")
                        continue
                
                if success_count > 0:
                    print(f"‚úì Format 3 succeeded for {success_count}/{len(symbols)} symbols")
                    print(f"  Problem symbols: {problem_symbols}")
                    print(f"  Good symbols: {good_symbols[:5]}...")
                    data = all_data
                else:
                    print("All formats failed, returning empty dataframe")
                    return pd.DataFrame(columns=["symbol", "ltp", "open"])
        
        # Process the response - with validation
        if "data" not in data:
            print("Warning: No 'data' key in response")
            print(f"Response keys: {list(data.keys())}")
            return pd.DataFrame(columns=["symbol", "ltp", "open"])
        
        rows = []
        for key, payload in data.get("data", {}).items():
            try:
                # Extract symbol from instrument_key
                if "|" in key:
                    symbol = key.split("|")[-1]
                else:
                    symbol = key
                
                ohlc = payload.get("ohlc", {})
                
                rows.append({
                    "symbol": symbol,
                    "ltp": payload.get("last_price"),
                    "open": ohlc.get("open"),
                    "high": ohlc.get("high"),
                    "low": ohlc.get("low"),
                    "close": ohlc.get("close"),
                    "change": payload.get("change"),
                    "percent_change": payload.get("percent_change"),
                    "volume": payload.get("volume", 0),
                    "timestamp": payload.get("timestamp")
                })
            except Exception as e:
                print(f"Error processing symbol {key}: {e}")
                continue
        
        if not rows:
            print("Warning: No data processed from API response")
            return pd.DataFrame(columns=["symbol", "ltp", "open"])
        
        print(f"‚úì Successfully processed {len(rows)} equity quotes")
        return pd.DataFrame(rows)
    
    # ==============================
    # HELPER METHODS
    # ==============================
    
    def _synthesize_market_regime(self, oi_regime: str, gamma_regime: str, 
                                 divergence: Dict) -> str:
        """Synthesize overall market regime from multiple indicators."""
        regimes = []
        
        # OI-based regime
        if oi_regime == "EXPANSIVE":
            regimes.append("CAPITAL_INFLOW")
        elif oi_regime == "CONSTRICTED":
            regimes.append("CAPITAL_OUTFLOW")
        
        # Gamma-based regime
        if "POSITIVE" in gamma_regime:
            regimes.append("STABILIZING")
        elif "NEGATIVE" in gamma_regime:
            regimes.append("ACCELERATING")
        
        # Divergence-based
        if divergence["has_divergence"]:
            regimes.append(f"DIVERGENCE_{divergence['type']}")
        
        if not regimes:
            return "NEUTRAL"
        
        # Combine regimes
        if len(regimes) == 1:
            return regimes[0]
        else:
            return f"{regimes[0]}_{regimes[1]}"
    
    def _calculate_regime_confidence(self, oi_velocity: float, 
                                    net_gamma: float, divergence: Dict) -> float:
        """Calculate confidence score for market regime."""
        confidence = 0.5  # Base confidence
        
        # OI velocity confidence
        oi_confidence = min(abs(oi_velocity) / 3, 1.0)
        confidence = 0.7 * confidence + 0.3 * oi_confidence
        
        # Gamma confidence
        gamma_confidence = min(abs(net_gamma) / 1000, 1.0)  # Scale appropriately
        confidence = 0.7 * confidence + 0.3 * gamma_confidence
        
        # Divergence confidence
        if divergence["has_divergence"]:
            div_confidence = divergence["confidence"]
            confidence = 0.8 * confidence + 0.2 * div_confidence
        
        return round(confidence, 3)
    
    def _generate_market_insights(self, analytics: Dict) -> List[str]:
        """Generate human-readable market insights."""
        insights = []
        
        # OI insights
        oi_vel = analytics["oi_velocity"]
        if oi_vel > 1.5:
            insights.append("üìà Strong OI buildup - New capital entering market")
        elif oi_vel < -1.5:
            insights.append("üìâ OI unwinding - Positions being closed, watch for reversals")
        
        # Gamma insights
        gamma_regime = analytics["gamma_exposure"]["regime"]
        if gamma_regime == "GAMMA_POSITIVE":
            insights.append("üìå Positive Gamma - Market likely to pin/stabilize")
        elif gamma_regime == "GAMMA_NEGATIVE":
            insights.append("üöÄ Negative Gamma - Accelerating moves possible, watch for squeezes")
        
        # Wall insights
        walls = analytics["structural_walls"]
        if walls:
            top_wall = walls[0]
            if top_wall["defended"]:
                insights.append(f"üõ°Ô∏è Strong wall at {top_wall['strike']} ({top_wall['type']}) - Being defended")
            else:
                insights.append(f"‚ö†Ô∏è Wall at {top_wall['strike']} weakening - Monitor for breach")
        
        # Trap insights
        traps = analytics["potential_traps"]
        if traps:
            trap = traps[0]
            insights.append(f"üéØ Potential {trap['direction']} trap at {trap['strike']} - Confidence: {trap['confidence']*100:.0f}%")
        
        # Divergence insights
        if analytics["spot_divergence"]["has_divergence"]:
            div_type = analytics["spot_divergence"]["type"]
            if div_type == "BULLISH":
                insights.append("üîç Bullish divergence detected - Price down but smart money accumulating")
            else:
                insights.append("üîç Bearish divergence detected - Price up but internal weakness")
        
        return insights
    
    def get_market_analytics_summary(self) -> Dict:
        """Get summary of all market analytics."""
        return {
            "oi_velocity_history": self.velocity_history,
            "gamma_history": self.gex_cache,
            "walls_history": self.walls_cache,
            "traps_history": self.traps_cache,
            "current_regime": getattr(self, 'current_regime', 'UNKNOWN'),
            "last_update": datetime.utcnow().isoformat()
        }


# ==============================
# MARKET ANALYTICS HELPER CLASS
# ==============================

class MarketAnalytics:
    """Helper class for advanced market analytics."""
    
    @staticmethod
    def calculate_put_call_ratio(option_chain_df: pd.DataFrame) -> Dict:
        """Calculate Put-Call Ratio with breakdown."""
        put_oi = option_chain_df[option_chain_df["option_type"] == "PE"]["oi"].sum()
        call_oi = option_chain_df[option_chain_df["option_type"] == "CE"]["oi"].sum()
        put_volume = option_chain_df[option_chain_df["option_type"] == "PE"]["volume"].sum()
        call_volume = option_chain_df[option_chain_df["option_type"] == "CE"]["volume"].sum()
        
        pcr_oi = put_oi / call_oi if call_oi > 0 else 0
        pcr_volume = put_volume / call_volume if call_volume > 0 else 0
        
        return {
            "pcr_oi": round(pcr_oi, 3),
            "pcr_volume": round(pcr_volume, 3),
            "sentiment": "BEARISH" if pcr_oi > 1.2 else "BULLISH" if pcr_oi < 0.8 else "NEUTRAL",
            "put_oi": int(put_oi),
            "call_oi": int(call_oi),
            "total_oi": int(put_oi + call_oi)
        }
    
    @staticmethod
    def detect_max_pain(option_chain_df: pd.DataFrame) -> Dict:
        """Calculate Max Pain strike."""
        if option_chain_df.empty:
            return {"max_pain_strike": 0, "total_pain": 0}
        
        strikes = sorted(option_chain_df['strike'].unique())
        pain_values = []
        
        for strike in strikes:
            total_pain = 0
            
            # Calculate pain from puts
            puts = option_chain_df[(option_chain_df['strike'] == strike) & 
                                  (option_chain_df['option_type'] == 'PE')]
            for _, put in puts.iterrows():
                if strike < put['strike']:  # ITM
                    total_pain += (put['strike'] - strike) * put['oi']
            
            # Calculate pain from calls
            calls = option_chain_df[(option_chain_df['strike'] == strike) & 
                                   (option_chain_df['option_type'] == 'CE')]
            for _, call in calls.iterrows():
                if strike > call['strike']:  # ITM
                    total_pain += (strike - call['strike']) * call['oi']
            
            pain_values.append((strike, total_pain))
        
        if pain_values:
            max_pain_strike, min_pain = min(pain_values, key=lambda x: x[1])
            return {
                "max_pain_strike": max_pain_strike,
                "total_pain": int(min_pain),
                "all_pain_levels": pain_values[:10]  # Top 10
            }
        
        return {"max_pain_strike": 0, "total_pain": 0}


# ==============================
# UTILITY FUNCTIONS
# ==============================

def display_market_analytics(analytics: Dict):
    """Display market analytics in Streamlit."""
    if not analytics:
        st.info("No analytics available yet")
        return
    
    st.markdown("### üìä Advanced Market Analytics")
    
    # OI Velocity
    col1, col2, col3 = st.columns(3)
    with col1:
        oi_vel = analytics.get("oi_velocity", 0)
        regime = analytics.get("oi_regime", "N/A")
        color = "green" if oi_vel > 0 else "red"
        st.metric("OI Velocity", f"{oi_vel:.2f}œÉ", regime, delta_color="off")
    
    # Gamma Exposure
    with col2:
        gamma = analytics.get("gamma_exposure", {}).get("net_gamma", 0)
        gamma_regime = analytics.get("gamma_exposure", {}).get("regime", "N/A")
        icon = "üìå" if "POSITIVE" in gamma_regime else "üöÄ" if "NEGATIVE" in gamma_regime else "‚öñÔ∏è"
        st.metric("Gamma Exposure", f"{icon} {gamma_regime}", f"{gamma:.2f}")
    
    # Market Regime
    with col3:
        regime = analytics.get("market_regime", "N/A")
        confidence = analytics.get("regime_confidence", 0) * 100
        st.metric("Market Regime", regime, f"{confidence:.0f}% confidence")
    
    # Insights
    insights = analytics.get("market_insights", [])
    if insights:
        st.markdown("#### üí° Market Insights")
        for insight in insights[:5]:  # Top 5 insights
            st.info(insight)
    
    # Structural Walls
    walls = analytics.get("structural_walls", [])
    if walls:
        st.markdown("#### üß± Structural Walls")
        wall_df = pd.DataFrame(walls)
        st.dataframe(wall_df, use_container_width=True)
    
    # Traps
    traps = analytics.get("potential_traps", [])
    if traps:
        st.markdown("#### üéØ Potential Traps")
        for trap in traps:
            direction = trap.get("direction", "")
            strike = trap.get("strike", 0)
            confidence = trap.get("confidence", 0) * 100
            st.warning(f"{direction} trap at {strike} ({confidence:.0f}% confidence)")



====================================================================================================
FILE: .\features\breadth.py
====================================================================================================


File Name: breadth.py
Full Path: G:\trading_app\features\breadth.py
Size: 17.69 KB
Last Modified: 01/27/2026 19:42:19
Extension: .py

"""
Enhanced Breadth Features with Research Calculations.
Includes CCC analysis, market breadth, and constituent analysis.
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
from scipy import stats

# ==============================
# BREADTH FEATURE CALCULATIONS
# ==============================

def compute_breadth_features(
    constituents_df: pd.DataFrame,
    ccc_history: pd.Series
) -> Dict[str, float]:
    """
    Compute breadth-based features including research metrics.
    """
    # Base features
    ccc_value = compute_ccc_value(constituents_df)
    ccc_slope = compute_ccc_slope(ccc_history)
    
    # Advanced breadth features
    if not constituents_df.empty:
        breadth_metrics = compute_market_breadth(constituents_df)
        weight_distribution = analyze_weight_distribution(constituents_df)
        sector_analysis = analyze_sector_concentration(constituents_df)
        momentum_breadth = compute_momentum_breadth(constituents_df)
    else:
        breadth_metrics = {
            "advance_decline_ratio": 1.0,
            "percent_above_ma": 0.5,
            "breadth_momentum": 0.0
        }
        weight_distribution = {
            "top5_concentration": 0.3,
            "herfindahl_index": 0.1,
            "weight_skew": 0.0
        }
        sector_analysis = {
            "sector_concentration": 0.5,
            "dominant_sector_strength": 0.0
        }
        momentum_breadth = {
            "positive_momentum_ratio": 0.5,
            "momentum_dispersion": 0.0
        }
    
    # Combine all features
    features = {
        # Base CCC features
        "ccc_value": ccc_value,
        "ccc_slope": ccc_slope,
        
        # Market breadth
        "advance_decline_ratio": breadth_metrics["advance_decline_ratio"],
        "percent_above_ma": breadth_metrics["percent_above_ma"],
        "breadth_momentum": breadth_metrics["breadth_momentum"],
        
        # Weight distribution
        "top5_concentration": weight_distribution["top5_concentration"],
        "herfindahl_index": weight_distribution["herfindahl_index"],
        "weight_skew": weight_distribution["weight_skew"],
        
        # Sector analysis
        "sector_concentration": sector_analysis["sector_concentration"],
        "dominant_sector_strength": sector_analysis["dominant_sector_strength"],
        
        # Momentum breadth
        "positive_momentum_ratio": momentum_breadth["positive_momentum_ratio"],
        "momentum_dispersion": momentum_breadth["momentum_dispersion"],
        
        # Derived features
        "breadth_health": compute_breadth_health_score(
            ccc_value, ccc_slope, breadth_metrics
        ),
        "market_participation": compute_market_participation(constituents_df)
    }
    
    return features

# ==============================
# CCC CALCULATIONS
# ==============================

def compute_ccc_value(
    constituents_df: pd.DataFrame,
    weight_col: str = "weight",
    price_change_col: str = "price_change"
) -> float:
    """
    Compute Cumulative Constituent Contribution (CCC).
    """
    if constituents_df.empty:
        return 0.0
    
    required_cols = {weight_col, price_change_col}
    if not required_cols.issubset(constituents_df.columns):
        raise ValueError(
            f"Missing required columns: {required_cols - set(constituents_df.columns)}"
        )
    
    ccc = (constituents_df[weight_col] * constituents_df[price_change_col]).sum()
    return float(ccc)

def build_constituents_df(
    equity_quotes_df: pd.DataFrame,
    weights: Dict[str, float]
) -> pd.DataFrame:
    """
    Build constituents DataFrame with weight and price change.
    """
    rows = []
    
    for _, row in equity_quotes_df.iterrows():
        symbol = row["symbol"]
        if symbol not in weights:
            continue
        
        open_price = row["open"]
        ltp = row["ltp"]
        
        if not open_price or open_price == 0:
            continue
        
        price_change = (ltp - open_price) / open_price
        
        rows.append({
            "symbol": symbol,
            "weight": weights[symbol],
            "price_change": price_change,
            "ltp": ltp,
            "open": open_price
        })
    
    return pd.DataFrame(rows)

def compute_ccc_slope(
    ccc_series: pd.Series,
    lookback: int = 5
) -> float:
    """
    Compute slope of CCC over last N points.
    """
    if len(ccc_series) <= lookback:
        return 0.0
    
    y = ccc_series.iloc[-lookback:]
    x = np.arange(len(y))
    
    slope = np.polyfit(x, y, 1)[0]
    return float(slope)

# ==============================
# MARKET BREADTH ANALYSIS
# ==============================

def compute_market_breadth(constituents_df: pd.DataFrame) -> Dict[str, float]:
    """
    Compute comprehensive market breadth metrics.
    """
    if constituents_df.empty:
        return {
            "advance_decline_ratio": 1.0,
            "percent_above_ma": 0.5,
            "breadth_momentum": 0.0,
            "new_highs_lows": 0.0,
            "breadth_thrust": 0.0
        }
    
    # Advance-Decline ratio
    advances = constituents_df[constituents_df["price_change"] > 0]
    declines = constituents_df[constituents_df["price_change"] < 0]
    
    advance_count = len(advances)
    decline_count = len(declines)
    
    if decline_count == 0:
        advance_decline_ratio = float(advance_count) if advance_count > 0 else 1.0
    else:
        advance_decline_ratio = advance_count / decline_count
    
    # Percent above "moving average" (simplified as above open)
    above_open = constituents_df[constituents_df["price_change"] > 0]
    percent_above = len(above_open) / len(constituents_df)
    
    # Breadth momentum (weighted average of price changes)
    if constituents_df["weight"].sum() > 0:
        breadth_momentum = (constituents_df["weight"] * constituents_df["price_change"]).sum()
    else:
        breadth_momentum = constituents_df["price_change"].mean()
    
    # New highs/lows (simplified as extreme moves)
    price_change_std = constituents_df["price_change"].std()
    if price_change_std > 0:
        extreme_positives = constituents_df[
            constituents_df["price_change"] > 2 * price_change_std
        ]
        extreme_negatives = constituents_df[
            constituents_df["price_change"] < -2 * price_change_std
        ]
        new_highs_lows = (len(extreme_positives) - len(extreme_negatives)) / len(constituents_df)
    else:
        new_highs_lows = 0.0
    
    # Breadth thrust (rapid improvement in breadth)
    # Simplified as acceleration in advance-decline ratio
    breadth_thrust = 0.0  # Would require historical data
    
    return {
        "advance_decline_ratio": float(advance_decline_ratio),
        "percent_above_ma": float(percent_above),
        "breadth_momentum": float(breadth_momentum),
        "new_highs_lows": float(new_highs_lows),
        "breadth_thrust": float(breadth_thrust)
    }

# ==============================
# WEIGHT DISTRIBUTION ANALYSIS
# ==============================

def analyze_weight_distribution(constituents_df: pd.DataFrame) -> Dict[str, float]:
    """
    Analyze weight distribution of index constituents.
    """
    if constituents_df.empty:
        return {
            "top5_concentration": 0.3,
            "herfindahl_index": 0.1,
            "weight_skew": 0.0,
            "weight_gini": 0.5
        }
    
    weights = constituents_df["weight"].values
    
    # Top 5 concentration
    weights_sorted = np.sort(weights)[::-1]
    top5_concentration = np.sum(weights_sorted[:5]) if len(weights_sorted) >= 5 else np.sum(weights_sorted)
    
    # Herfindahl-Hirschman Index (concentration measure)
    herfindahl_index = np.sum(weights ** 2)
    
    # Weight skewness
    if len(weights) >= 3:
        weight_skew = stats.skew(weights)
    else:
        weight_skew = 0.0
    
    # Gini coefficient (inequality measure)
    if len(weights) >= 2:
        sorted_weights = np.sort(weights)
        n = len(sorted_weights)
        cumulative = np.cumsum(sorted_weights)
        gini = (n + 1 - 2 * np.sum(cumulative) / cumulative[-1]) / n
    else:
        gini = 0.5
    
    return {
        "top5_concentration": float(top5_concentration),
        "herfindahl_index": float(herfindahl_index),
        "weight_skew": float(weight_skew),
        "weight_gini": float(gini)
    }

# ==============================
# SECTOR ANALYSIS
# ==============================

def analyze_sector_concentration(constituents_df: pd.DataFrame) -> Dict[str, float]:
    """
    Analyze sector concentration (simplified version).
    In production, this would use actual sector data.
    """
    if constituents_df.empty:
        return {
            "sector_concentration": 0.5,
            "dominant_sector_strength": 0.0,
            "sector_correlation": 0.0
        }
    
    # Simplified: group by weight quartiles
    if len(constituents_df) >= 4:
        quartiles = pd.qcut(constituents_df["weight"], 4, labels=False)
        sector_concentration = 1.0 - len(set(quartiles)) / 4  # Higher = more concentrated
    else:
        sector_concentration = 0.5
    
    # Dominant sector strength (weight of top quartile)
    if len(constituents_df) >= 4:
        top_quartile = constituents_df.nlargest(len(constituents_df) // 4, "weight")
        dominant_sector_strength = top_quartile["weight"].sum()
    else:
        dominant_sector_strength = constituents_df["weight"].max()
    
    # Sector correlation (simplified as correlation of top weights)
    if len(constituents_df) >= 5:
        top_5 = constituents_df.nlargest(5, "weight")
        # Use price changes as proxy for sector performance
        if "price_change" in top_5.columns:
            sector_correlation = top_5["price_change"].std() / (abs(top_5["price_change"].mean()) + 1e-10)
            sector_correlation = min(sector_correlation, 1.0)
        else:
            sector_correlation = 0.5
    else:
        sector_correlation = 0.5
    
    return {
        "sector_concentration": float(sector_concentration),
        "dominant_sector_strength": float(dominant_sector_strength),
        "sector_correlation": float(sector_correlation)
    }

# ==============================
# MOMENTUM BREADTH
# ==============================

def compute_momentum_breadth(constituents_df: pd.DataFrame) -> Dict[str, float]:
    """
    Compute momentum breadth across constituents.
    """
    if constituents_df.empty or "price_change" not in constituents_df.columns:
        return {
            "positive_momentum_ratio": 0.5,
            "momentum_dispersion": 0.0,
            "momentum_trend": 0.0
        }
    
    price_changes = constituents_df["price_change"].values
    weights = constituents_df["weight"].values
    
    # Positive momentum ratio
    positive_momentum_ratio = np.sum(price_changes > 0) / len(price_changes)
    
    # Momentum dispersion (standard deviation of momentum)
    if len(price_changes) >= 2:
        momentum_dispersion = np.std(price_changes)
    else:
        momentum_dispersion = 0.0
    
    # Weighted momentum trend
    if np.sum(weights) > 0:
        weighted_momentum = np.sum(weights * price_changes) / np.sum(weights)
    else:
        weighted_momentum = np.mean(price_changes)
    
    return {
        "positive_momentum_ratio": float(positive_momentum_ratio),
        "momentum_dispersion": float(momentum_dispersion),
        "momentum_trend": float(weighted_momentum)
    }

# ==============================
# DERIVED FEATURES
# ==============================

def compute_breadth_health_score(
    ccc_value: float,
    ccc_slope: float,
    breadth_metrics: Dict[str, float]
) -> float:
    """
    Compute composite breadth health score (0-1).
    """
    scores = []
    
    # CCC value score
    ccc_score = min(abs(ccc_value) * 10, 1.0)
    scores.append(ccc_score * 0.3)
    
    # CCC slope score (positive slope is healthy)
    if ccc_slope > 0:
        slope_score = min(ccc_slope * 100, 1.0)
    else:
        slope_score = max(ccc_slope * 50, -1.0)  # Negative slope penalized
    scores.append(max(slope_score, 0) * 0.2)
    
    # Advance-decline ratio score
    adr = breadth_metrics.get("advance_decline_ratio", 1.0)
    if adr > 1.0:
        adr_score = min((adr - 1.0) * 2, 1.0)  # Cap at 1.0
    else:
        adr_score = max(adr - 0.5, 0) * 2  # Below 0.5 is bad
    scores.append(adr_score * 0.2)
    
    # Percent above MA score
    percent_above = breadth_metrics.get("percent_above_ma", 0.5)
    percent_score = abs(percent_above - 0.5) * 2  # Distance from 50%
    scores.append(percent_score * 0.15)
    
    # Breadth momentum score
    breadth_momentum = breadth_metrics.get("breadth_momentum", 0.0)
    momentum_score = min(abs(breadth_momentum) * 100, 1.0)
    scores.append(momentum_score * 0.15)
    
    return float(np.sum(scores))

def compute_market_participation(constituents_df: pd.DataFrame) -> float:
    """
    Compute market participation score (0-1).
    Higher = broader market participation in moves.
    """
    if constituents_df.empty or "price_change" not in constituents_df.columns:
        return 0.5
    
    price_changes = constituents_df["price_change"].values
    
    if len(price_changes) < 2:
        return 0.5
    
    # Participation = 1 - (fraction of stocks with near-zero changes)
    near_zero = np.sum(np.abs(price_changes) < 0.001) / len(price_changes)
    participation = 1.0 - near_zero
    
    # Adjust for direction consistency
    positive_fraction = np.sum(price_changes > 0) / len(price_changes)
    direction_consistency = max(positive_fraction, 1 - positive_fraction)
    
    participation = participation * direction_consistency
    
    return float(participation)

# ==============================
# DIVERGENCE DETECTION
# ==============================

def detect_breadth_divergence(
    price_change: float,
    breadth_metrics: Dict[str, float],
    historical_breadth: List[Dict]
) -> Tuple[bool, str, float]:
    """
    Detect divergence between price and breadth indicators.
    
    Returns:
        has_divergence, direction, confidence
    """
    if len(historical_breadth) < 5:
        return False, "NEUTRAL", 0.0
    
    # Get recent breadth values
    recent_breadth = historical_breadth[-5:]
    
    # Calculate breadth momentum
    breadth_values = [b.get("advance_decline_ratio", 1.0) for b in recent_breadth]
    breadth_momentum = np.polyfit(range(len(breadth_values)), breadth_values, 1)[0]
    
    # Price momentum (simplified)
    price_momentum = price_change
    
    # Detect divergence
    if price_momentum > 0.01 and breadth_momentum < -0.1:
        # Price up but breadth deteriorating (bearish divergence)
        confidence = min(abs(breadth_momentum) * 10, 1.0)
        return True, "BEARISH", confidence
    
    elif price_momentum < -0.01 and breadth_momentum > 0.1:
        # Price down but breadth improving (bullish divergence)
        confidence = min(abs(breadth_momentum) * 10, 1.0)
        return True, "BULLISH", confidence
    
    return False, "NEUTRAL", 0.0

# ==============================
# UTILITY FUNCTIONS
# ==============================

def calculate_breadth_indicators(constituents_df: pd.DataFrame) -> Dict[str, float]:
    """
    Calculate traditional breadth indicators.
    """
    if constituents_df.empty:
        return {}
    
    indicators = {}
    
    # McClellan Oscillator components
    advances = len(constituents_df[constituents_df["price_change"] > 0])
    declines = len(constituents_df[constituents_df["price_change"] < 0])
    
    indicators["advance_decline_line"] = advances - declines
    indicators["advance_decline_ratio"] = advances / declines if declines > 0 else advances
    
    # Arms Index (TRIN)
    if declines > 0:
        advance_volume = 1  # Simplified
        decline_volume = 1  # Simplified
        indicators["arms_index"] = (advances / declines) / (advance_volume / decline_volume)
    else:
        indicators["arms_index"] = 0.0
    
    # Percent above moving average (simplified)
    avg_change = constituents_df["price_change"].mean()
    indicators["percent_above_average"] = len(
        constituents_df[constituents_df["price_change"] > avg_change]
    ) / len(constituents_df)
    
    return indicators

def get_breadth_alerts(breadth_features: Dict[str, float]) -> List[str]:
    """
    Generate breadth-based alerts.
    """
    alerts = []
    
    # Check CCC value
    ccc_value = breadth_features.get("ccc_value", 0.0)
    if ccc_value > 0.01:
        alerts.append("Strong positive breadth (CCC > 1%)")
    elif ccc_value < -0.01:
        alerts.append("Strong negative breadth (CCC < -1%)")
    
    # Check advance-decline ratio
    adr = breadth_features.get("advance_decline_ratio", 1.0)
    if adr > 2.0:
        alerts.append("Extreme breadth: Advances >> Declines")
    elif adr < 0.5:
        alerts.append("Extreme breadth: Declines >> Advances")
    
    # Check breadth health
    health = breadth_features.get("breadth_health", 0.5)
    if health > 0.8:
        alerts.append("Excellent breadth health")
    elif health < 0.3:
        alerts.append("Poor breadth health")
    
    # Check market participation
    participation = breadth_features.get("market_participation", 0.5)
    if participation > 0.8:
        alerts.append("Broad market participation")
    elif participation < 0.3:
        alerts.append("Narrow market participation")
    
    return alerts[:3]  # Return top 3 alerts



====================================================================================================
FILE: .\features\option_features.py
====================================================================================================


File Name: option_features.py
Full Path: G:\trading_app\features\option_features.py
Size: 18.24 KB
Last Modified: 01/28/2026 10:33:55
Extension: .py

"""
Enhanced Option Features with Research Calculations.
Includes Gamma Exposure, OI Velocity, and structural analysis.
"""

import pandas as pd
import numpy as np
from datetime import datetime
from scipy.stats import norm
from typing import List, Dict, Tuple, Optional, Any, Union

# ==============================
# BLACK-SCHOLES CALCULATIONS
# ==============================

def black_scholes_greeks(
    S: float,           # Spot price
    K: float,           # Strike price
    T: float,           # Time to expiry (years)
    r: float,           # Risk-free rate
    sigma: float,       # Implied volatility
    option_type: str    # 'CE' or 'PE'
) -> Dict[str, float]:
    """
    Calculate Black-Scholes Greeks for options.
    """
    if T <= 0 or sigma <= 0:
        return {
            "delta": 0.5 if option_type == "CE" else -0.5,
            "gamma": 0.0,
            "theta": 0.0,
            "vega": 0.0
        }
    
    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))
    d2 = d1 - sigma * np.sqrt(T)
    
    if option_type == "CE":
        delta = norm.cdf(d1)
        theta = (-S * norm.pdf(d1) * sigma / (2 * np.sqrt(T)) 
                 - r * K * np.exp(-r * T) * norm.cdf(d2))
    else:  # PE
        delta = norm.cdf(d1) - 1
        theta = (-S * norm.pdf(d1) * sigma / (2 * np.sqrt(T))
                 + r * K * np.exp(-r * T) * norm.cdf(-d2))
    
    gamma = norm.pdf(d1) / (S * sigma * np.sqrt(T))
    vega = S * norm.pdf(d1) * np.sqrt(T)
    
    return {
        "delta": float(delta),
        "gamma": float(gamma),
        "theta": float(theta),
        "vega": float(vega)
    }

# ==============================
# OPTION CHAIN FEATURES
# ==============================

def compute_option_features(
    option_chain_df: pd.DataFrame,
    spot_price: float,
    expiry_datetime: datetime
) -> Dict[str, float]:
    """
    Compute option chain features including research metrics.
    """
    if option_chain_df.empty:
        return get_default_features()
    
    # Base features
    put_call_ratio = compute_put_call_ratio(option_chain_df)
    oi_delta = compute_oi_delta(option_chain_df)
    oi_concentration = compute_oi_concentration(option_chain_df)
    atm_iv = compute_atm_iv(option_chain_df, spot_price)
    iv_skew = compute_iv_skew(option_chain_df, spot_price)
    
    # Advanced features
    gamma_profile = compute_gamma_profile(option_chain_df, spot_price, expiry_datetime)
    oi_velocity = compute_oi_velocity(option_chain_df)
    max_pain = compute_max_pain(option_chain_df)
    vix_smile = compute_vix_smile(option_chain_df, spot_price)
    
    # Combine all features
    features = {
        # Base features
        "put_call_ratio": put_call_ratio,
        "oi_delta": oi_delta,
        "oi_concentration": oi_concentration,
        "atm_iv": atm_iv,
        "iv_skew": iv_skew,
        
        # Time feature
        "time_to_expiry_minutes": compute_time_to_expiry_minutes(expiry_datetime),
        
        # Research features
        "net_gamma": gamma_profile["net_gamma"],
        "gamma_skew": gamma_profile["skew"],
        "max_gamma_strike": gamma_profile["max_strike"],
        "oi_velocity": oi_velocity,
        "max_pain_strike": max_pain,
        "vix_smile": vix_smile,
        
        # Volume and OI ratios
        "call_oi_ratio": compute_call_oi_ratio(option_chain_df),
        "put_oi_ratio": compute_put_oi_ratio(option_chain_df),
        "volume_put_call_ratio": compute_volume_pcr(option_chain_df)
    }
    
    return features

# ==============================
# BASE FEATURE CALCULATIONS
# ==============================

def compute_put_call_ratio(option_chain_df: pd.DataFrame) -> float:
    """Calculate Put-Call Ratio (OI-based)."""
    if option_chain_df.empty:
        return 1.0
    
    put_oi = option_chain_df.loc[
        option_chain_df["option_type"] == "PE", "oi"
    ].sum()
    
    call_oi = option_chain_df.loc[
        option_chain_df["option_type"] == "CE", "oi"
    ].sum()
    
    if call_oi == 0:
        return 0.0
    
    return float(put_oi / call_oi)

def compute_oi_delta(option_chain_df: pd.DataFrame) -> float:
    """Calculate OI Delta (Put OI change - Call OI change)."""
    if option_chain_df.empty:
        return 0.0
    
    put_delta = option_chain_df.loc[
        option_chain_df["option_type"] == "PE", "oi_change"
    ].sum()
    
    call_delta = option_chain_df.loc[
        option_chain_df["option_type"] == "CE", "oi_change"
    ].sum()
    
    return float(put_delta - call_delta)

def compute_oi_concentration(option_chain_df: pd.DataFrame) -> float:
    """Calculate OI concentration at maximum OI strike."""
    if option_chain_df.empty:
        return 0.0
    
    total_oi = option_chain_df["oi"].sum()
    if total_oi == 0:
        return 0.0
    
    max_strike_oi = (
        option_chain_df
        .groupby("strike")["oi"]
        .sum()
        .max()
    )
    
    return float(max_strike_oi / total_oi)

def compute_atm_iv(option_chain_df: pd.DataFrame, spot_price: float) -> float:
    """Calculate At-The-Money Implied Volatility."""
    if option_chain_df.empty:
        return 0.3  # Default IV
    
    option_chain_df = option_chain_df.copy()
    option_chain_df["dist"] = abs(option_chain_df["strike"] - spot_price)
    
    atm_row = option_chain_df.sort_values("dist").iloc[0]
    iv = atm_row["iv"]
    
    return float(iv) if not np.isnan(iv) else 0.3

def compute_iv_skew(option_chain_df: pd.DataFrame, spot_price: float) -> float:
    """Calculate IV Skew (Call IV - Put IV)."""
    if option_chain_df.empty:
        return 0.0
    
    option_chain_df = option_chain_df.copy()
    option_chain_df["dist"] = abs(option_chain_df["strike"] - spot_price)
    
    # Get nearest strikes
    nearest_strikes = option_chain_df.sort_values("dist").head(10)
    
    ce_iv = nearest_strikes.loc[
        nearest_strikes["option_type"] == "CE", "iv"
    ].mean()
    
    pe_iv = nearest_strikes.loc[
        nearest_strikes["option_type"] == "PE", "iv"
    ].mean()
    
    if np.isnan(ce_iv) or np.isnan(pe_iv):
        return 0.0
    
    return float(ce_iv - pe_iv)

def compute_time_to_expiry_minutes(expiry_datetime: datetime) -> int:
    """Calculate minutes to expiry."""
    now = datetime.utcnow()
    delta = expiry_datetime - now
    
    minutes = int(delta.total_seconds() / 60)
    return max(minutes, 0)

# ==============================
# RESEARCH FEATURE CALCULATIONS
# ==============================

def compute_gamma_profile(
    option_chain_df: pd.DataFrame,
    spot_price: float,
    expiry_datetime: datetime
) -> Dict[str, float]:
    """
    Compute Gamma Exposure profile for research.
    """
    if option_chain_df.empty:
        return {
            "net_gamma": 0.0,
            "positive_gamma": 0.0,
            "negative_gamma": 0.0,
            "skew": 0.0,
            "max_strike": 0.0
        }
    
    
    # Calculate time to expiry in years
    now = datetime.utcnow()
    T = max((expiry_datetime - now).total_seconds() / (365 * 24 * 3600), 0.001)
    r = 0.05  # Risk-free rate approximation
    
    total_gamma = 0.0
    positive_gamma = 0.0
    negative_gamma = 0.0
    gamma_strikes = []
    
    for _, row in option_chain_df.iterrows():
        strike = row['strike']
        option_type = row['option_type']
        oi = row['oi']
        iv = row.get('iv', 0.3)
        
        # Calculate gamma for this strike
        greeks = black_scholes_greeks(
            S=spot_price,
            K=strike,
            T=T,
            r=r,
            sigma=iv,
            option_type=option_type
        )
        
        gamma = greeks["gamma"]
        
        # Adjust sign based on market maker position
        # Market makers are typically short options
        if option_type == "CE":
            gamma_contribution = -gamma * oi * 100  # Negative for short calls
        else:  # PE
            gamma_contribution = gamma * oi * 100   # Positive for short puts
        
        total_gamma += gamma_contribution
        
        if gamma_contribution > 0:
            positive_gamma += gamma_contribution
        else:
            negative_gamma += gamma_contribution
        
        gamma_strikes.append((strike, gamma_contribution))
    
    # Calculate gamma skew and max_strike
    if gamma_strikes:
        strikes, gammas = zip(*gamma_strikes)
        
        # Calculate skew
        if len(gammas) >= 10:
            gamma_skew = np.mean(gammas[:5]) - np.mean(gammas[-5:])
        else:
            gamma_skew = 0.0
        
        # Find max gamma strike safely
        if len(strikes) > 0 and len(gammas) > 0:
            max_idx = np.argmax(np.abs(gammas))
            max_gamma_strike = strikes[max_idx]
            # Ensure it's not None
            if max_gamma_strike is None or pd.isnull(max_gamma_strike):
                max_gamma_strike = 0.0
        else:
            max_gamma_strike = 0.0
            gamma_skew = 0.0
    else:
        gamma_skew = 0.0
        max_gamma_strike = 0.0
    
    return {
        "net_gamma": float(total_gamma),
        "positive_gamma": float(positive_gamma),
        "negative_gamma": float(negative_gamma),
        "skew": float(gamma_skew),
        "max_strike": float(max_gamma_strike)
    }

def compute_oi_velocity(option_chain_df: pd.DataFrame) -> float:
    """
    Calculate OI Velocity (rate of change of OI).
    In production, this would use historical data.
    """
    if option_chain_df.empty:
        return 0.0
    
    # Simplified velocity calculation
    # In real implementation, compare with previous snapshot
    total_oi = option_chain_df["oi"].sum()
    oi_change = option_chain_df["oi_change"].sum()
    
    if total_oi == 0:
        return 0.0
    
    velocity = oi_change / total_oi * 100
    return float(velocity)

def compute_max_pain(option_chain_df: pd.DataFrame) -> float:
    """Calculate Max Pain strike."""
    if option_chain_df.empty:
        return 0.0
    
    # Check if we have strikes
    if 'strike' not in option_chain_df.columns:
        return 0.0
    
    strikes = sorted(option_chain_df['strike'].unique())
    if not strikes:
        return 0.0
    
    pain_values = []
    
    for strike in strikes:
        total_pain = 0
        
        # Calculate pain from puts
        puts = option_chain_df[(option_chain_df['strike'] == strike) & 
                              (option_chain_df['option_type'] == 'PE')]
        for _, put in puts.iterrows():
            # Check if put is ITM (strike > spot for put pain calculation)
            # Actually for max pain, we calculate loss for option writers
            # For puts: writers lose when spot < strike
            put_strike = put['strike']
            if isinstance(put_strike, (int, float)):
                if strike < put_strike:  # ITM puts cause pain
                    put_oi = put['oi'] if pd.notnull(put['oi']) else 0
                    total_pain += (put_strike - strike) * put_oi
        
        # Calculate pain from calls
        calls = option_chain_df[(option_chain_df['strike'] == strike) & 
                               (option_chain_df['option_type'] == 'CE')]
        for _, call in calls.iterrows():
            # For calls: writers lose when spot > strike
            call_strike = call['strike']
            if isinstance(call_strike, (int, float)):
                if strike > call_strike:  # ITM calls cause pain
                    call_oi = call['oi'] if pd.notnull(call['oi']) else 0
                    total_pain += (strike - call_strike) * call_oi
        
        pain_values.append((strike, total_pain))
    
    if pain_values:
        # Find strike with minimum total pain
        min_pain_item = min(pain_values, key=lambda x: x[1])
        max_pain_strike = min_pain_item[0]
        
        # Ensure it's a valid number
        if max_pain_strike is None or pd.isnull(max_pain_strike):
            return 0.0
        return float(max_pain_strike)
    
    return 0.0

def compute_vix_smile(option_chain_df: pd.DataFrame, spot_price: float) -> float:
    """Calculate VIX smile curvature."""
    if option_chain_df.empty or spot_price <= 0:
        return 0.0
    
    # Group by distance from spot
    option_chain_df = option_chain_df.copy()
    option_chain_df['distance_pct'] = abs(option_chain_df['strike'] - spot_price) / spot_price * 100
    
    # Get IVs at different distances
    atm_iv = option_chain_df[
        option_chain_df['distance_pct'] < 2
    ]['iv'].mean()
    
    otm_iv = option_chain_df[
        (option_chain_df['distance_pct'] >= 5) & 
        (option_chain_df['distance_pct'] < 10)
    ]['iv'].mean()
    
    if np.isnan(atm_iv) or np.isnan(otm_iv):
        return 0.0
    
    # Smile = OTM IV - ATM IV (positive = smile, negative = smirk)
    return float(otm_iv - atm_iv)

# ==============================
# SUPPORTING CALCULATIONS
# ==============================

def compute_call_oi_ratio(option_chain_df: pd.DataFrame) -> float:
    """Calculate Call OI as percentage of total OI."""
    if option_chain_df.empty:
        return 0.5
    
    total_oi = option_chain_df["oi"].sum()
    call_oi = option_chain_df[option_chain_df["option_type"] == "CE"]["oi"].sum()
    
    if total_oi == 0:
        return 0.5
    
    return float(call_oi / total_oi)

def compute_put_oi_ratio(option_chain_df: pd.DataFrame) -> float:
    """Calculate Put OI as percentage of total OI."""
    if option_chain_df.empty:
        return 0.5
    
    total_oi = option_chain_df["oi"].sum()
    put_oi = option_chain_df[option_chain_df["option_type"] == "PE"]["oi"].sum()
    
    if total_oi == 0:
        return 0.5
    
    return float(put_oi / total_oi)

def compute_volume_pcr(option_chain_df: pd.DataFrame) -> float:
    """Calculate Volume-based Put-Call Ratio."""
    if option_chain_df.empty:
        return 1.0
    
    put_volume = option_chain_df.loc[
        option_chain_df["option_type"] == "PE", "volume"
    ].sum()
    
    call_volume = option_chain_df.loc[
        option_chain_df["option_type"] == "CE", "volume"
    ].sum()
    
    if call_volume == 0:
        return 0.0
    
    return float(put_volume / call_volume)

def get_default_features() -> Dict[str, float]:
    """Return default feature values when no data is available."""
    return {
        "put_call_ratio": 1.0,
        "oi_delta": 0.0,
        "oi_concentration": 0.0,
        "atm_iv": 0.3,
        "iv_skew": 0.0,
        "time_to_expiry_minutes": 0,
        "net_gamma": 0.0,
        "gamma_skew": 0.0,
        "max_gamma_strike": 0.0,
        "oi_velocity": 0.0,
        "max_pain_strike": 0.0,
        "vix_smile": 0.0,
        "call_oi_ratio": 0.5,
        "put_oi_ratio": 0.5,
        "volume_put_call_ratio": 1.0
    }

# ==============================
# UTILITY FUNCTIONS
# ==============================

def analyze_option_skew(option_chain_df: pd.DataFrame, spot_price: float) -> Dict[str, float]:
    """
    Analyze option skew across strikes.
    """
    if option_chain_df.empty:
        return {
            "call_skew": 0.0,
            "put_skew": 0.0,
            "total_skew": 0.0
        }
    
    # Separate calls and puts
    calls = option_chain_df[option_chain_df["option_type"] == "CE"].copy()
    puts = option_chain_df[option_chain_df["option_type"] == "PE"].copy()
    
    # Calculate distance from spot
    calls["distance_pct"] = (calls["strike"] - spot_price) / spot_price * 100
    puts["distance_pct"] = (spot_price - puts["strike"]) / spot_price * 100
    
    # Calculate skew (slope of IV vs distance)
    call_skew = 0.0
    put_skew = 0.0
    
    if len(calls) >= 5:
        # Sort by distance and take top 5 OTM calls
        otm_calls = calls[calls["distance_pct"] > 0].nlargest(5, "distance_pct")
        if len(otm_calls) >= 2:
            call_skew = np.polyfit(otm_calls["distance_pct"], otm_calls["iv"], 1)[0]
    
    if len(puts) >= 5:
        # Sort by distance and take top 5 OTM puts
        otm_puts = puts[puts["distance_pct"] > 0].nlargest(5, "distance_pct")
        if len(otm_puts) >= 2:
            put_skew = np.polyfit(otm_puts["distance_pct"], otm_puts["iv"], 1)[0]
    
    return {
        "call_skew": float(call_skew),
        "put_skew": float(put_skew),
        "total_skew": float(call_skew + put_skew)
    }

def detect_gamma_flip_levels(
    option_chain_df: pd.DataFrame,
    spot_price: float,
    expiry_datetime: datetime
) -> List[float]:
    """
    Detect price levels where gamma flips sign.
    """
    if option_chain_df.empty:
        return []
    
    # Calculate gamma at different price levels
    price_levels = np.linspace(spot_price * 0.95, spot_price * 1.05, 21)
    gamma_levels = []
    
    T = max((expiry_datetime - datetime.utcnow()).total_seconds() / (365 * 24 * 3600), 0.001)
    r = 0.05
    
    for price in price_levels:
        total_gamma = 0.0
        
        for _, row in option_chain_df.iterrows():
            strike = row['strike']
            option_type = row['option_type']
            oi = row['oi']
            iv = row.get('iv', 0.3)
            
            greeks = black_scholes_greeks(
                S=price,
                K=strike,
                T=T,
                r=r,
                sigma=iv,
                option_type=option_type
            )
            
            gamma = greeks["gamma"]
            
            if option_type == "CE":
                gamma_contribution = -gamma * oi
            else:
                gamma_contribution = gamma * oi
            
            total_gamma += gamma_contribution
        
        gamma_levels.append((price, total_gamma))
    
    # Find sign changes
    flip_levels = []
    for i in range(1, len(gamma_levels)):
        prev_sign = np.sign(gamma_levels[i-1][1])
        curr_sign = np.sign(gamma_levels[i][1])
        
        if prev_sign != curr_sign and prev_sign != 0 and curr_sign != 0:
            flip_price = (gamma_levels[i-1][0] + gamma_levels[i][0]) / 2
            flip_levels.append(float(flip_price))
    
    return flip_levels



====================================================================================================
FILE: .\features\price_features.py
====================================================================================================


File Name: price_features.py
Full Path: G:\trading_app\features\price_features.py
Size: 12.15 KB
Last Modified: 01/27/2026 19:39:48
Extension: .py

"""
Enhanced Price Features with Research Calculations.
Includes VWAP distance, momentum, volume analysis, and divergence detection.
"""

import pandas as pd
import numpy as np
from typing import Dict, Tuple, List
from scipy import stats

# ==============================
# PRICE FEATURE CALCULATIONS
# ==============================

def compute_price_features(
    ltp: float,
    vwap: float,
    price_series: pd.Series,
    volume_series: pd.Series
) -> Dict[str, float]:
    """
    Compute price-based features including research metrics.
    """
    # Base features
    vwap_distance = compute_vwap_distance(ltp, vwap)
    price_momentum = compute_price_momentum(price_series)
    volume_ratio = compute_volume_ratio(volume_series)
    
    # Advanced features
    trend_strength = compute_trend_strength(price_series)
    volatility = compute_volatility(price_series)
    rsi = compute_rsi(price_series)
    volume_profile = compute_volume_profile(price_series, volume_series)
    price_efficiency = compute_price_efficiency(price_series)
    
    # Divergence detection
    volume_price_divergence = compute_volume_price_divergence(price_series, volume_series)
    
    # Combine all features
    features = {
        # Base features
        "vwap_distance": vwap_distance,
        "price_momentum": price_momentum,
        "volume_ratio": volume_ratio,
        
        # Trend and momentum
        "trend_strength": trend_strength,
        "price_volatility": volatility,
        "rsi": rsi,
        "price_efficiency": price_efficiency,
        
        # Volume analysis
        "volume_trend": volume_profile["trend"],
        "volume_volatility": volume_profile["volatility"],
        "volume_clustering": volume_profile["clustering"],
        
        # Divergence
        "volume_price_divergence": volume_price_divergence,
        "has_volume_divergence": 1.0 if abs(volume_price_divergence) > 0.5 else 0.0
    }
    
    return features

# ==============================
# BASE FEATURE CALCULATIONS
# ==============================

def compute_vwap_distance(ltp: float, vwap: float) -> float:
    """Calculate distance from VWAP."""
    if vwap == 0:
        return 0.0
    
    return float((ltp - vwap) / vwap)

def compute_price_momentum(
    price_series: pd.Series,
    lookback: int = 5
) -> float:
    """Calculate price momentum over lookback period."""
    if len(price_series) <= lookback:
        return 0.0
    
    past_price = price_series.iloc[-lookback - 1]
    current_price = price_series.iloc[-1]
    
    if past_price == 0:
        return 0.0
    
    return float((current_price - past_price) / past_price)

def compute_volume_ratio(
    volume_series: pd.Series,
    lookback: int = 20
) -> float:
    """Calculate current volume relative to average."""
    if len(volume_series) < lookback:
        return 0.0
    
    current_volume = volume_series.iloc[-1]
    avg_volume = volume_series.iloc[-lookback:].mean()
    
    if avg_volume == 0:
        return 0.0
    
    return float(current_volume / avg_volume)

# ==============================
# ADVANCED FEATURE CALCULATIONS
# ==============================

def compute_trend_strength(price_series: pd.Series, lookback: int = 20) -> float:
    """
    Calculate trend strength using linear regression.
    Returns R¬≤ value (0-1) indicating trend strength.
    """
    if len(price_series) < lookback:
        return 0.0
    
    prices = price_series.iloc[-lookback:].values
    x = np.arange(len(prices))
    
    # Linear regression
    slope, intercept, r_value, p_value, std_err = stats.linregress(x, prices)
    
    # R¬≤ value indicates trend strength
    r_squared = r_value ** 2
    
    return float(r_squared)

def compute_volatility(price_series: pd.Series, lookback: int = 20) -> float:
    """Calculate price volatility (annualized)."""
    if len(price_series) <= 1:
        return 0.0
    
    returns = price_series.pct_change().dropna()
    
    if len(returns) < lookback:
        sample_returns = returns
    else:
        sample_returns = returns.iloc[-lookback:]
    
    if len(sample_returns) <= 1:
        return 0.0
    
    # Annualized volatility (assuming daily data)
    daily_vol = sample_returns.std()
    annual_vol = daily_vol * np.sqrt(252)
    
    return float(annual_vol)

def compute_rsi(price_series: pd.Series, period: int = 14) -> float:
    """Calculate Relative Strength Index."""
    if len(price_series) <= period:
        return 50.0  # Neutral
    
    delta = price_series.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
    
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    
    return float(rsi.iloc[-1]) if not pd.isna(rsi.iloc[-1]) else 50.0

def compute_price_efficiency(price_series: pd.Series) -> float:
    """
    Calculate price efficiency (random walk index).
    Higher values indicate more efficient/trending markets.
    """
    if len(price_series) < 10:
        return 0.5
    
    # Calculate Hurst exponent approximation
    n = min(len(price_series), 100)
    lags = range(2, min(n // 2, 20))
    
    if len(lags) < 2:
        return 0.5
    
    tau = []
    for lag in lags:
        # Calculate variance of lagged differences
        price_diff = np.diff(price_series.iloc[-n:], lag)
        if len(price_diff) > 1:
            tau.append(np.std(price_diff))
        else:
            tau.append(0)
    
    tau = np.array(tau)
    lags = np.array(lags)
    
    # Remove zeros
    mask = (tau > 0) & (lags > 0)
    if np.sum(mask) < 2:
        return 0.5
    
    # Linear regression in log space
    try:
        hurst = np.polyfit(np.log(lags[mask]), np.log(tau[mask]), 1)[0]
        efficiency = hurst  # H ‚âà 0.5 random walk, >0.5 trending, <0.5 mean-reverting
    except:
        efficiency = 0.5
    
    return float(efficiency)

# ==============================
# VOLUME ANALYSIS
# ==============================

def compute_volume_profile(
    price_series: pd.Series,
    volume_series: pd.Series,
    lookback: int = 20
) -> Dict[str, float]:
    """
    Compute volume profile features.
    """
    if len(volume_series) < lookback or len(price_series) < lookback:
        return {
            "trend": 0.0,
            "volatility": 0.0,
            "clustering": 0.0
        }
    
    recent_volume = volume_series.iloc[-lookback:]
    recent_prices = price_series.iloc[-lookback:]
    
    # Volume trend
    x = np.arange(len(recent_volume))
    volume_trend = np.polyfit(x, recent_volume.values, 1)[0]
    volume_trend_normalized = volume_trend / (recent_volume.mean() + 1e-10)
    
    # Volume volatility
    volume_volatility = recent_volume.std() / (recent_volume.mean() + 1e-10)
    
    # Volume clustering (autocorrelation)
    if len(recent_volume) >= 5:
        volume_clustering = recent_volume.autocorr(lag=1)
        if pd.isna(volume_clustering):
            volume_clustering = 0.0
    else:
        volume_clustering = 0.0
    
    return {
        "trend": float(volume_trend_normalized),
        "volatility": float(volume_volatility),
        "clustering": float(volume_clustering)
    }

def compute_volume_price_divergence(
    price_series: pd.Series,
    volume_series: pd.Series,
    lookback: int = 10
) -> float:
    """
    Detect divergence between price and volume.
    Positive = price up, volume down (bearish divergence)
    Negative = price down, volume up (bullish divergence)
    """
    if len(price_series) < lookback or len(volume_series) < lookback:
        return 0.0
    
    # Calculate price and volume changes
    price_change = (price_series.iloc[-1] - price_series.iloc[-lookback]) / price_series.iloc[-lookback]
    volume_change = (volume_series.iloc[-1] - volume_series.iloc[-lookback]) / (volume_series.iloc[-lookback] + 1e-10)
    
    # Normalize
    price_norm = price_change / (np.std(price_series.iloc[-lookback:].pct_change().dropna()) + 1e-10)
    volume_norm = volume_change / (np.std(volume_series.iloc[-lookback:].pct_change().dropna()) + 1e-10)
    
    # Divergence score
    divergence = price_norm - volume_norm
    
    return float(divergence)

# ==============================
# SUPPORTING CALCULATIONS
# ==============================

def detect_price_patterns(price_series: pd.Series) -> Dict[str, float]:
    """
    Detect common price patterns.
    """
    if len(price_series) < 20:
        return {
            "double_top": 0.0,
            "double_bottom": 0.0,
            "head_shoulders": 0.0,
            "triangle": 0.0
        }
    
    # Simplified pattern detection
    prices = price_series.iloc[-20:].values
    
    # Calculate peaks and troughs
    from scipy.signal import find_peaks
    
    peaks, _ = find_peaks(prices, prominence=np.std(prices) * 0.5)
    troughs, _ = find_peaks(-prices, prominence=np.std(prices) * 0.5)
    
    pattern_scores = {
        "double_top": 0.0,
        "double_bottom": 0.0,
        "head_shoulders": 0.0,
        "triangle": 0.0
    }
    
    # Double top detection
    if len(peaks) >= 2:
        peak1 = prices[peaks[-2]]
        peak2 = prices[peaks[-1]]
        if abs(peak1 - peak2) / peak1 < 0.02:  # Within 2%
            pattern_scores["double_top"] = 0.8
    
    # Double bottom detection
    if len(troughs) >= 2:
        trough1 = prices[troughs[-2]]
        trough2 = prices[troughs[-1]]
        if abs(trough1 - trough2) / trough1 < 0.02:
            pattern_scores["double_bottom"] = 0.8
    
    return pattern_scores

def calculate_support_resistance(
    price_series: pd.Series,
    window: int = 20
) -> Dict[str, float]:
    """
    Calculate support and resistance levels.
    """
    if len(price_series) < window:
        return {
            "support": 0.0,
            "resistance": 0.0,
            "current_position": 0.5
        }
    
    recent_prices = price_series.iloc[-window:]
    
    support = recent_prices.min()
    resistance = recent_prices.max()
    current = price_series.iloc[-1]
    
    if resistance - support == 0:
        position = 0.5
    else:
        position = (current - support) / (resistance - support)
    
    return {
        "support": float(support),
        "resistance": float(resistance),
        "current_position": float(position)
    }

# ==============================
# DIVERGENCE DETECTION
# ==============================

def detect_momentum_divergence(
    price_series: pd.Series,
    momentum_series: pd.Series
) -> Tuple[bool, str, float]:
    """
    Detect divergence between price and momentum oscillator.
    
    Returns:
        has_divergence, direction, confidence
    """
    if len(price_series) < 10 or len(momentum_series) < 10:
        return False, "NEUTRAL", 0.0
    
    # Get recent peaks in price and momentum
    price_peaks = []
    momentum_peaks = []
    
    # Simplified peak detection
    for i in range(5, len(price_series) - 5):
        if price_series.iloc[i] == price_series.iloc[i-5:i+5].max():
            price_peaks.append((i, price_series.iloc[i]))
        if momentum_series.iloc[i] == momentum_series.iloc[i-5:i+5].max():
            momentum_peaks.append((i, momentum_series.iloc[i]))
    
    if len(price_peaks) < 2 or len(momentum_peaks) < 2:
        return False, "NEUTRAL", 0.0
    
    # Check for divergence
    price_trend = price_peaks[-1][1] - price_peaks[-2][1]
    momentum_trend = momentum_peaks[-1][1] - momentum_peaks[-2][1]
    
    # Bullish divergence: price makes lower low, momentum makes higher low
    # Bearish divergence: price makes higher high, momentum makes lower high
    
    if price_trend > 0 and momentum_trend < 0:
        # Bearish divergence
        confidence = min(abs(momentum_trend) / 10, 1.0)
        return True, "BEARISH", confidence
    
    elif price_trend < 0 and momentum_trend > 0:
        # Bullish divergence
        confidence = min(abs(momentum_trend) / 10, 1.0)
        return True, "BULLISH", confidence
    
    return False, "NEUTRAL", 0.0



====================================================================================================
FILE: .\intelligence\market_state.py
====================================================================================================


File Name: market_state.py
Full Path: G:\trading_app\intelligence\market_state.py
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .py




====================================================================================================
FILE: .\intelligence\signal_logic.py
====================================================================================================


File Name: signal_logic.py
Full Path: G:\trading_app\intelligence\signal_logic.py
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .py




====================================================================================================
FILE: .\ml\feature_contract.py
====================================================================================================


File Name: feature_contract.py
Full Path: G:\trading_app\ml\feature_contract.py
Size: 16.29 KB
Last Modified: 01/27/2026 19:13:41
Extension: .py

"""
ENHANCED FEATURE CONTRACT - SINGLE SOURCE OF TRUTH
Incorporates research concepts: OI Velocity, Gamma Exposure, Walls/Traps, Spot Divergence

Used by:
- Live data collection
- ML training
- ML inference
- Backtesting
- Signal generation
"""

# ==============================
# FEATURE VERSION
# ==============================

FEATURE_VERSION = "v2.0"  # Updated for research features

# ==============================
# FEATURE CATEGORIES
# ==============================

# Base features (original set)
BASE_FEATURES = [
    "timestamp",
    
    # Option structure (original)
    "put_call_ratio",
    "oi_delta",
    "oi_concentration",
    "atm_iv",
    "iv_skew",
    
    # Price & flow (original)
    "vwap_distance",
    "price_momentum",
    "volume_ratio",
    
    # Breadth (original)
    "ccc_value",
    "ccc_slope",
    
    # Time context
    "time_to_expiry_minutes"
]

# Research features (enhanced)
RESEARCH_FEATURES = [
    # OI Velocity features
    "oi_velocity",
    "oi_velocity_ma",
    "oi_velocity_std",
    "oi_regime_expansive",      # 1.0 if EXPANSIVE, else 0.0
    "oi_regime_constricted",    # 1.0 if CONSTRICTED, else 0.0
    
    # Gamma Exposure features
    "net_gamma",
    "gamma_regime_positive",    # 1.0 if POSITIVE, else 0.0
    "gamma_regime_negative",    # 1.0 if NEGATIVE, else 0.0
    "gamma_flip_distance",      # Distance to nearest gamma flip (normalized)
    "max_gamma_strike_distance",# Distance to max gamma strike (normalized)
    
    # Structural features (Walls & Traps)
    "wall_strength",            # Combined strength of top walls (0-1)
    "wall_defense_score",       # How well walls are defended (0-1)
    "trap_probability",         # Probability of trap formation (0-1)
    
    # Divergence features
    "price_oi_divergence",      # Divergence between price and OI
    "price_gamma_divergence",   # Divergence between price and gamma
    "divergence_score",         # Combined divergence score (0-1)
    "has_divergence",           # 1.0 if significant divergence, else 0.0
    
    # Market microstructure
    "max_pain_distance",        # Distance to max pain (normalized)
    "vix_smile",                # Volatility smile curvature
    "skewness",                 # Option skew (put IV - call IV)
    
    # Wyckoff-inspired features
    "spring_detection",         # Bear trap probability (0-1)
    "upthrust_detection",       # Bull trap probability (0-1)
    "accumulation_score",       # Accumulation phase score (0-1)
    
    # Derived composite features
    "gamma_wall_interaction",   # wall_strength * abs(net_gamma)
    "velocity_divergence_composite",  # oi_velocity * divergence_score
    "trap_gamma_composite"      # trap_probability * gamma_regime_negative
]

# ==============================
# COMPLETE FEATURE SET
# ==============================

# All features (base + research)
FEATURE_COLUMNS = BASE_FEATURES + RESEARCH_FEATURES

# Target variable
TARGET_COLUMN = "future_return_5m"

# Metadata columns
METADATA_COLUMNS = [
    "feature_version",
    "timestamp"
]

# Primary keys for database
PRIMARY_KEYS = ["timestamp"]

# ==============================
# FEATURE GROUPS FOR ANALYSIS
# ==============================

FEATURE_GROUPS = {
    "option_structure": [
        "put_call_ratio",
        "oi_delta", 
        "oi_concentration",
        "atm_iv",
        "iv_skew",
        "skewness",
        "vix_smile"
    ],
    
    "price_momentum": [
        "vwap_distance",
        "price_momentum",
        "volume_ratio",
        "ccc_value",
        "ccc_slope"
    ],
    
    "oi_analysis": [
        "oi_velocity",
        "oi_velocity_ma",
        "oi_velocity_std",
        "oi_regime_expansive",
        "oi_regime_constricted"
    ],
    
    "gamma_exposure": [
        "net_gamma",
        "gamma_regime_positive",
        "gamma_regime_negative",
        "gamma_flip_distance",
        "max_gamma_strike_distance"
    ],
    
    "structure_analysis": [
        "wall_strength",
        "wall_defense_score",
        "trap_probability",
        "spring_detection",
        "upthrust_detection",
        "accumulation_score"
    ],
    
    "divergence_analysis": [
        "price_oi_divergence",
        "price_gamma_divergence",
        "divergence_score",
        "has_divergence"
    ],
    
    "composite_features": [
        "gamma_wall_interaction",
        "velocity_divergence_composite",
        "trap_gamma_composite"
    ],
    
    "context_features": [
        "time_to_expiry_minutes",
        "max_pain_distance"
    ]
}

# ==============================
# FEATURE DESCRIPTIONS
# ==============================

FEATURE_DESCRIPTIONS = {
    # Base features
    "put_call_ratio": "Put OI / Call OI ratio. >1.0 = bearish sentiment",
    "oi_delta": "Net OI change (Put OI change - Call OI change)",
    "oi_concentration": "Maximum OI concentration at any single strike",
    "atm_iv": "At-the-money implied volatility",
    "iv_skew": "Call IV - Put IV (positive = call skew, negative = put skew)",
    "vwap_distance": "(LTP - VWAP) / VWAP. Positive = above VWAP (bullish)",
    "price_momentum": "Normalized price momentum over lookback period",
    "volume_ratio": "Current volume / average volume",
    "ccc_value": "Cumulative Constituent Contribution - breadth indicator",
    "ccc_slope": "Slope of CCC over lookback period",
    "time_to_expiry_minutes": "Minutes remaining until option expiry",
    
    # OI Velocity features
    "oi_velocity": "Normalized rate of change of Open Interest (œÉ)",
    "oi_velocity_ma": "Moving average of OI velocity",
    "oi_velocity_std": "Standard deviation of OI velocity",
    "oi_regime_expansive": "1.0 if OI velocity > 1.5œÉ (capital inflow), else 0.0",
    "oi_regime_constricted": "1.0 if OI velocity < -1.5œÉ (capital outflow), else 0.0",
    
    # Gamma Exposure features
    "net_gamma": "Net Gamma Exposure of market makers",
    "gamma_regime_positive": "1.0 if net_gamma > 0 (stabilizing/pinning), else 0.0",
    "gamma_regime_negative": "1.0 if net_gamma < 0 (accelerating), else 0.0",
    "gamma_flip_distance": "Distance to nearest gamma flip level (normalized)",
    "max_gamma_strike_distance": "Distance to strike with maximum gamma impact",
    
    # Structural features
    "wall_strength": "Combined strength of structural walls (0-1 scale)",
    "wall_defense_score": "How strongly walls are being defended (0-1)",
    "trap_probability": "Probability of trap/squeeze formation (0-1)",
    "spring_detection": "Wyckoff spring pattern detection (0-1)",
    "upthrust_detection": "Wyckoff upthrust pattern detection (0-1)",
    "accumulation_score": "Accumulation phase score (0-1)",
    
    # Divergence features
    "price_oi_divergence": "Divergence between price change and OI change",
    "price_gamma_divergence": "Divergence between price change and gamma change",
    "divergence_score": "Combined divergence confidence (0-1)",
    "has_divergence": "1.0 if significant divergence detected, else 0.0",
    
    # Market microstructure
    "max_pain_distance": "Distance to max pain strike (normalized)",
    "vix_smile": "Volatility smile curvature (ATM IV - OTM IV)",
    "skewness": "Option skew (Put IV - Call IV)",
    
    # Composite features
    "gamma_wall_interaction": "Interaction between gamma and wall strength",
    "velocity_divergence_composite": "OI velocity multiplied by divergence score",
    "trap_gamma_composite": "Trap probability weighted by negative gamma regime"
}

# ==============================
# FEATURE IMPORTANCE GUIDELINES
# ==============================

# Expected impact on returns (for initial model weighting)
FEATURE_IMPACT = {
    "high_impact": [
        "oi_velocity",
        "net_gamma",
        "trap_probability",
        "divergence_score",
        "wall_strength"
    ],
    
    "medium_impact": [
        "put_call_ratio",
        "price_momentum",
        "ccc_slope",
        "gamma_regime_negative",
        "has_divergence",
        "spring_detection"
    ],
    
    "low_impact": [
        "oi_concentration",
        "vwap_distance",
        "volume_ratio",
        "max_pain_distance",
        "skewness"
    ],
    
    "contextual": [
        "time_to_expiry_minutes",
        "atm_iv",
        "vix_smile",
        "accumulation_score"
    ]
}

# ==============================
# FEATURE VALIDATION RULES
# ==============================

FEATURE_VALIDATION = {
    "value_ranges": {
        "put_call_ratio": (0, 5),
        "oi_velocity": (-10, 10),
        "net_gamma": (-1e6, 1e6),
        "trap_probability": (0, 1),
        "divergence_score": (0, 1),
        "wall_strength": (0, 1)
    },
    
    "required_features": [
        "timestamp",
        "feature_version",
        "put_call_ratio",
        "vwap_distance",
        "price_momentum",
        "ccc_value"
    ],
    
    "derived_features": [
        "oi_regime_expansive",
        "oi_regime_constricted",
        "gamma_regime_positive",
        "gamma_regime_negative",
        "has_divergence"
    ]
}

# ==============================
# MODEL INPUT CONFIGURATION
# ==============================

# Features for different model types
MODEL_FEATURE_SETS = {
    "full_model": FEATURE_COLUMNS,
    
    "research_model": RESEARCH_FEATURES,
    
    "momentum_model": [
        "price_momentum",
        "volume_ratio",
        "ccc_slope",
        "oi_velocity",
        "net_gamma"
    ],
    
    "divergence_model": [
        "price_oi_divergence",
        "price_gamma_divergence",
        "divergence_score",
        "has_divergence",
        "trap_probability"
    ],
    
    "structure_model": [
        "wall_strength",
        "wall_defense_score",
        "trap_probability",
        "spring_detection",
        "upthrust_detection"
    ],
    
    "quick_model": [
        "put_call_ratio",
        "oi_velocity",
        "net_gamma",
        "trap_probability",
        "divergence_score"
    ]
}

# ==============================
# UTILITY FUNCTIONS
# ==============================

def get_feature_group(feature_name: str) -> str:
    """Get the group name for a feature."""
    for group_name, features in FEATURE_GROUPS.items():
        if feature_name in features:
            return group_name
    return "unknown"

def validate_feature_name(feature_name: str) -> bool:
    """Check if a feature name is valid."""
    return feature_name in FEATURE_COLUMNS or feature_name in METADATA_COLUMNS

def get_feature_description(feature_name: str) -> str:
    """Get description for a feature."""
    return FEATURE_DESCRIPTIONS.get(feature_name, "No description available")

def get_features_by_impact(impact_level: str) -> list:
    """Get features by impact level."""
    return FEATURE_IMPACT.get(impact_level, [])

def get_model_features(model_type: str = "full_model") -> list:
    """Get feature set for specific model type."""
    return MODEL_FEATURE_SETS.get(model_type, FEATURE_COLUMNS)

def print_feature_summary():
    """Print summary of all features."""
    print(f"=== FEATURE CONTRACT v{FEATURE_VERSION} ===")
    print(f"Total features: {len(FEATURE_COLUMNS)}")
    print(f"Base features: {len(BASE_FEATURES)}")
    print(f"Research features: {len(RESEARCH_FEATURES)}")
    print()
    
    print("Feature Groups:")
    for group_name, features in FEATURE_GROUPS.items():
        print(f"  {group_name}: {len(features)} features")
    
    print()
    print("Top Impact Features:")
    for feature in FEATURE_IMPACT["high_impact"][:5]:
        print(f"  ‚Ä¢ {feature}: {FEATURE_DESCRIPTIONS.get(feature, '')}")

# ==============================
# DATABASE SCHEMA
# ==============================

# Schema for market_features table
MARKET_FEATURES_SCHEMA = {
    "table_name": "market_features",
    "columns": {
        "timestamp": "TEXT PRIMARY KEY",
        "feature_version": "TEXT",
        "future_return_5m": "REAL",
        
        # Base features
        "put_call_ratio": "REAL",
        "oi_delta": "REAL",
        "oi_concentration": "REAL",
        "atm_iv": "REAL",
        "iv_skew": "REAL",
        "vwap_distance": "REAL",
        "price_momentum": "REAL",
        "volume_ratio": "REAL",
        "ccc_value": "REAL",
        "ccc_slope": "REAL",
        "time_to_expiry_minutes": "INTEGER",
        
        # Research features (added in v2.0)
        "oi_velocity": "REAL",
        "oi_velocity_ma": "REAL",
        "oi_velocity_std": "REAL",
        "oi_regime_expansive": "REAL",
        "oi_regime_constricted": "REAL",
        "net_gamma": "REAL",
        "gamma_regime_positive": "REAL",
        "gamma_regime_negative": "REAL",
        "gamma_flip_distance": "REAL",
        "max_gamma_strike_distance": "REAL",
        "wall_strength": "REAL",
        "wall_defense_score": "REAL",
        "trap_probability": "REAL",
        "price_oi_divergence": "REAL",
        "price_gamma_divergence": "REAL",
        "divergence_score": "REAL",
        "has_divergence": "REAL",
        "max_pain_distance": "REAL",
        "vix_smile": "REAL",
        "skewness": "REAL",
        "spring_detection": "REAL",
        "upthrust_detection": "REAL",
        "accumulation_score": "REAL",
        "gamma_wall_interaction": "REAL",
        "velocity_divergence_composite": "REAL",
        "trap_gamma_composite": "REAL"
    },
    "indexes": [
        "CREATE INDEX idx_timestamp ON market_features(timestamp)",
        "CREATE INDEX idx_feature_version ON market_features(feature_version)",
        "CREATE INDEX idx_oi_velocity ON market_features(oi_velocity)",
        "CREATE INDEX idx_net_gamma ON market_features(net_gamma)"
    ]
}

# ==============================
# MIGRATION UTILITIES
# ==============================

def get_migration_sql(from_version: str, to_version: str) -> list:
    """
    Get SQL migration statements for feature version updates.
    
    Args:
        from_version: Current feature version
        to_version: Target feature version
    
    Returns:
        List of SQL statements to migrate
    """
    migrations = []
    
    if from_version == "v1.0" and to_version == "v2.0":
        # Add research feature columns
        research_columns = [
            "oi_velocity REAL DEFAULT 0.0",
            "oi_velocity_ma REAL DEFAULT 0.0",
            "oi_velocity_std REAL DEFAULT 0.0",
            "oi_regime_expansive REAL DEFAULT 0.0",
            "oi_regime_constricted REAL DEFAULT 0.0",
            "net_gamma REAL DEFAULT 0.0",
            "gamma_regime_positive REAL DEFAULT 0.0",
            "gamma_regime_negative REAL DEFAULT 0.0",
            "gamma_flip_distance REAL DEFAULT 0.0",
            "max_gamma_strike_distance REAL DEFAULT 0.0",
            "wall_strength REAL DEFAULT 0.0",
            "wall_defense_score REAL DEFAULT 0.0",
            "trap_probability REAL DEFAULT 0.0",
            "price_oi_divergence REAL DEFAULT 0.0",
            "price_gamma_divergence REAL DEFAULT 0.0",
            "divergence_score REAL DEFAULT 0.0",
            "has_divergence REAL DEFAULT 0.0",
            "max_pain_distance REAL DEFAULT 0.0",
            "vix_smile REAL DEFAULT 0.0",
            "skewness REAL DEFAULT 0.0",
            "spring_detection REAL DEFAULT 0.0",
            "upthrust_detection REAL DEFAULT 0.0",
            "accumulation_score REAL DEFAULT 0.0",
            "gamma_wall_interaction REAL DEFAULT 0.0",
            "velocity_divergence_composite REAL DEFAULT 0.0",
            "trap_gamma_composite REAL DEFAULT 0.0"
        ]
        
        for column_def in research_columns:
            column_name = column_def.split()[0]
            migrations.append(f"ALTER TABLE market_features ADD COLUMN {column_def}")
        
        # Add indexes for new features
        migrations.append("CREATE INDEX idx_oi_velocity ON market_features(oi_velocity)")
        migrations.append("CREATE INDEX idx_net_gamma ON market_features(net_gamma)")
        migrations.append("CREATE INDEX idx_trap_probability ON market_features(trap_probability)")
    
    return migrations

# ==============================
# INITIALIZATION
# ==============================

if __name__ == "__main__":
    print_feature_summary()



====================================================================================================
FILE: .\ml\inference.py
====================================================================================================


File Name: inference.py
Full Path: G:\trading_app\ml\inference.py
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .py




====================================================================================================
FILE: .\ml\training.py
====================================================================================================


File Name: training.py
Full Path: G:\trading_app\ml\training.py
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .py




====================================================================================================
FILE: .\risk\position_sizing.py
====================================================================================================


File Name: position_sizing.py
Full Path: G:\trading_app\risk\position_sizing.py
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .py




====================================================================================================
FILE: .\risk\stoploss.py
====================================================================================================


File Name: stoploss.py
Full Path: G:\trading_app\risk\stoploss.py
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .py




====================================================================================================
FILE: .\scripts\fix_database.py
====================================================================================================


File Name: fix_database.py
Full Path: G:\trading_app\scripts\fix_database.py
Size: 4.7 KB
Last Modified: 01/27/2026 20:46:32
Extension: .py

"""
Database Fix Script - Run this to fix migration issues.
"""

import sys
from pathlib import Path
import sqlite3

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))


# Add this function to scripts/fix_database.py or create a new script

def add_details_json_column():
    """Add missing details_json column to system_health table."""
    try:
        with get_connection() as conn:
            # Check if column exists
            cur = conn.execute("PRAGMA table_info(system_health)")
            columns = [row[1] for row in cur.fetchall()]
            
            if "details_json" not in columns:
                print("Adding details_json column to system_health table...")
                conn.execute("""
                    ALTER TABLE system_health 
                    ADD COLUMN details_json TEXT DEFAULT '{}'
                """)
                print("‚úì Added details_json column")
            else:
                print("‚úì details_json column already exists")
    except Exception as e:
        print(f"‚ùå Error adding details_json column: {e}")
        
def fix_database():
    """Fix database migration issues."""
    print("=" * 60)
    print("DATABASE FIX SCRIPT")
    print("=" * 60)
    
    db_path = Path("G:/trading_app/storage/trading.db")
    
    if not db_path.exists():
        print("Database file not found. Creating new database...")
        from storage.repository import initialize_storage
        initialize_storage()
        return
    
    print(f"Fixing database at: {db_path}")
    
    # Connect to database
    conn = sqlite3.connect(str(db_path))
    
    try:
        # Check if market_features table exists
        cur = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='market_features'")
        if not cur.fetchone():
            print("market_features table doesn't exist. Creating...")
            conn.close()
            from storage.repository import initialize_storage
            initialize_storage()
            return
        
        # Get current columns
        cur = conn.execute("PRAGMA table_info(market_features)")
        columns = [row[1] for row in cur.fetchall()]
        print(f"Current columns in market_features: {columns}")
        
        # Research columns that should exist
        research_columns = [
            "oi_velocity", "oi_velocity_ma", "oi_velocity_std",
            "oi_regime_expansive", "oi_regime_constricted",
            "net_gamma", "gamma_regime_positive", "gamma_regime_negative",
            "gamma_flip_distance", "max_gamma_strike_distance",
            "wall_strength", "wall_defense_score", "trap_probability",
            "price_oi_divergence", "price_gamma_divergence", "divergence_score",
            "has_divergence", "max_pain_distance", "vix_smile", "skewness",
            "spring_detection", "upthrust_detection", "accumulation_score",
            "gamma_wall_interaction", "velocity_divergence_composite", "trap_gamma_composite"
        ]
        
        # Add missing columns
        for column in research_columns:
            if column not in columns:
                print(f"Adding column: {column}")
                try:
                    conn.execute(f"ALTER TABLE market_features ADD COLUMN {column} REAL DEFAULT 0.0")
                    print(f"  ‚úì Added {column}")
                except sqlite3.OperationalError as e:
                    if "duplicate column" in str(e):
                        print(f"  ‚ö†Ô∏è Column {column} already exists")
                    else:
                        print(f"  ‚ùå Error adding {column}: {e}")
        
        # Check database_info table
        cur = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='database_info'")
        if not cur.fetchone():
            print("Creating database_info table...")
            conn.execute("""
                CREATE TABLE database_info (
                    key TEXT PRIMARY KEY,
                    value TEXT,
                    updated_at TEXT
                )
            """)
        
        # Set feature version to v2.0
        from datetime import datetime
        conn.execute(
            "INSERT OR REPLACE INTO database_info (key, value, updated_at) VALUES (?, ?, ?)",
            ("feature_version", "v2.0", datetime.utcnow().isoformat())
        )
        
        conn.commit()
        print("‚úì Database fixed successfully!")
        
    except Exception as e:
        print(f"‚ùå Error fixing database: {e}")
        import traceback
        traceback.print_exc()
    finally:
        conn.close()

if __name__ == "__main__":
    fix_database()



====================================================================================================
FILE: .\scripts\initialize_database.py
====================================================================================================


File Name: initialize_database.py
Full Path: G:\trading_app\scripts\initialize_database.py
Size: 1.33 KB
Last Modified: 01/27/2026 19:46:53
Extension: .py

"""
Database Initialization Script.
Run this once to set up the database with research features.
"""

import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from storage.repository import initialize_database, migrate_database
from ml.feature_contract import FEATURE_VERSION

def main():
    """Initialize and migrate database."""
    print("=" * 60)
    print("DATABASE INITIALIZATION SCRIPT")
    print("=" * 60)
    
    try:
        # Initialize database
        print("\n1. Initializing database...")
        initialize_database()
        
        # Migrate to current feature version
        print(f"\n2. Migrating to feature version {FEATURE_VERSION}...")
        success = migrate_database(FEATURE_VERSION)
        
        if success:
            print(f"\n‚úÖ Database initialized successfully!")
            print(f"   Feature Version: {FEATURE_VERSION}")
            print(f"   Database Path: storage/trading.db")
        else:
            print("\n‚ùå Database migration failed!")
            sys.exit(1)
            
    except Exception as e:
        print(f"\n‚ùå Error during initialization: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()



====================================================================================================
FILE: .\storage\repository - Copy.py
====================================================================================================


File Name: repository - Copy.py
Full Path: G:\trading_app\storage\repository - Copy.py
Size: 37.96 KB
Last Modified: 01/27/2026 19:19:21
Extension: .py

"""
Enhanced Storage Repository with Research Features Support.
Handles database operations for market features, signals, and analytics.
Includes migration system for feature version updates.
"""

import sqlite3
import pandas as pd
from typing import Dict, Optional, List, Any, Tuple
from pathlib import Path
from datetime import datetime, timedelta
import json
import numpy as np
from contextlib import contextmanager

# Import feature contract for schema
from ml.feature_contract import (
    FEATURE_VERSION, 
    FEATURE_COLUMNS, 
    METADATA_COLUMNS,
    MARKET_FEATURES_SCHEMA,
    get_migration_sql
)

# =========================
# DATABASE CONFIG
# =========================

DB_PATH = Path("G:/trading_app/storage/trading.db")

# Ensure directory exists
DB_PATH.parent.mkdir(parents=True, exist_ok=True)

# =========================
# CONNECTION MANAGEMENT
# =========================

@contextmanager
def get_connection() -> sqlite3.Connection:
    """
    Context manager for database connections with automatic cleanup.
    """
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    
    # Enable WAL mode for better concurrency
    conn.execute("PRAGMA journal_mode=WAL")
    conn.execute("PRAGMA synchronous=NORMAL")
    conn.execute("PRAGMA foreign_keys=ON")
    
    try:
        yield conn
        conn.commit()
    except Exception as e:
        conn.rollback()
        raise e
    finally:
        conn.close()

def initialize_database():
    """
    Initialize database with all required tables and indexes.
    """
    with get_connection() as conn:
        # Create market_features table
        columns = MARKET_FEATURES_SCHEMA["columns"]
        column_defs = [f"{name} {dtype}" for name, dtype in columns.items()]
        
        create_table_sql = f"""
            CREATE TABLE IF NOT EXISTS market_features (
                {', '.join(column_defs)}
            )
        """
        
        conn.execute(create_table_sql)
        
        # Create indexes
        for index_sql in MARKET_FEATURES_SCHEMA.get("indexes", []):
            try:
                conn.execute(index_sql)
            except sqlite3.OperationalError as e:
                if "already exists" not in str(e):
                    raise
        
        # Create signals table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS signals (
                signal_id TEXT PRIMARY KEY,
                timestamp TEXT NOT NULL,
                feature_version TEXT NOT NULL,
                model_version TEXT NOT NULL,
                signal_type TEXT NOT NULL,
                confidence REAL,
                market_state TEXT,
                rationale TEXT,
                expiry_time TEXT,
                status TEXT,
                created_at TEXT,
                research_context TEXT,
                analytics_summary TEXT,
                pnl REAL DEFAULT NULL,
                exit_time TEXT,
                exit_price REAL,
                stop_loss_hit INTEGER DEFAULT 0,
                take_profit_hit INTEGER DEFAULT 0,
                trade_duration_seconds INTEGER
            )
        """)
        
        # Create signals indexes
        conn.execute("CREATE INDEX IF NOT EXISTS idx_signals_timestamp ON signals(timestamp)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_signals_status ON signals(status)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_signals_type ON signals(signal_type)")
        
        # Create model_registry table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS model_registry (
                model_version TEXT PRIMARY KEY,
                feature_version TEXT NOT NULL,
                algorithm TEXT NOT NULL,
                trained_on_start TEXT,
                trained_on_end TEXT,
                metrics_json TEXT,
                feature_importance_json TEXT,
                created_at TEXT,
                is_active INTEGER DEFAULT 0
            )
        """)
        
        # Create system_health table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS system_health (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                component TEXT NOT NULL,
                status TEXT NOT NULL,
                message TEXT,
                details_json TEXT
            )
        """)
        
        # Create research_analytics table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS research_analytics (
                timestamp TEXT PRIMARY KEY,
                oi_velocity REAL,
                oi_regime TEXT,
                net_gamma REAL,
                gamma_regime TEXT,
                wall_strength REAL,
                trap_probability REAL,
                divergence_score REAL,
                market_regime TEXT,
                confidence REAL,
                insights_json TEXT,
                walls_json TEXT,
                traps_json TEXT,
                created_at TEXT
            )
        """)
        
        # Create analytics indexes
        conn.execute("CREATE INDEX IF NOT EXISTS idx_analytics_timestamp ON research_analytics(timestamp)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_analytics_regime ON research_analytics(market_regime)")
        
        # Create feature_history table for tracking feature changes
        conn.execute("""
            CREATE TABLE IF NOT EXISTS feature_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                feature_name TEXT NOT NULL,
                old_value REAL,
                new_value REAL,
                change_pct REAL,
                created_at TEXT
            )
        """)
        
        # Create database_info table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS database_info (
                key TEXT PRIMARY KEY,
                value TEXT,
                updated_at TEXT
            )
        """)
        
        # Initialize database info
        conn.execute(
            "INSERT OR REPLACE INTO database_info (key, value, updated_at) VALUES (?, ?, ?)",
            ("feature_version", FEATURE_VERSION, datetime.utcnow().isoformat())
        )
        
        # Log initialization
        log_system_health(
            "database",
            "INITIALIZED",
            f"Database initialized with feature version {FEATURE_VERSION}"
        )
        
        print(f"‚úì Database initialized at {DB_PATH}")
        print(f"‚úì Feature version: {FEATURE_VERSION}")

# =========================
# DATABASE MIGRATION
# =========================

def migrate_database(target_version: str = FEATURE_VERSION) -> bool:
    """
    Migrate database to target feature version.
    
    Args:
        target_version: Target feature version
    
    Returns:
        True if migration successful
    """
    try:
        with get_connection() as conn:
            # Get current version
            cur = conn.execute("SELECT value FROM database_info WHERE key = 'feature_version'")
            row = cur.fetchone()
            current_version = row["value"] if row else "v1.0"
            
            if current_version == target_version:
                print(f"Database already at version {target_version}")
                return True
            
            print(f"Migrating database from {current_version} to {target_version}")
            
            # Get migration SQL
            migrations = get_migration_sql(current_version, target_version)
            
            if not migrations:
                print("No migration required")
                return True
            
            # Execute migrations
            for migration_sql in migrations:
                try:
                    conn.execute(migration_sql)
                    print(f"‚úì Executed: {migration_sql[:50]}...")
                except sqlite3.OperationalError as e:
                    if "already exists" in str(e) or "duplicate column" in str(e):
                        print(f"‚ö†Ô∏è Skipping (already exists): {migration_sql[:50]}...")
                    else:
                        raise
            
            # Update version
            conn.execute(
                "UPDATE database_info SET value = ?, updated_at = ? WHERE key = 'feature_version'",
                (target_version, datetime.utcnow().isoformat())
            )
            
            # Log migration
            log_system_health(
                "database",
                "MIGRATED",
                f"Migrated from {current_version} to {target_version}",
                json.dumps({"migrations_applied": len(migrations)})
            )
            
            print(f"‚úì Database migrated to version {target_version}")
            return True
            
    except Exception as e:
        print(f"‚ùå Migration failed: {e}")
        log_system_health(
            "database",
            "MIGRATION_FAILED",
            f"Failed to migrate: {str(e)}"
        )
        return False

# =========================
# MARKET FEATURES OPERATIONS
# =========================

def insert_market_features(df: pd.DataFrame) -> None:
    """
    Insert feature rows into market_features table.
    
    Args:
        df: DataFrame with feature columns matching MARKET_FEATURES_SCHEMA
    """
    if df.empty:
        return
    
    # Ensure feature_version is set
    if "feature_version" not in df.columns:
        df["feature_version"] = FEATURE_VERSION
    
    # Ensure timestamp is string
    df["timestamp"] = df["timestamp"].astype(str)
    
    # Ensure all required columns exist
    required_columns = list(MARKET_FEATURES_SCHEMA["columns"].keys())
    for col in required_columns:
        if col not in df.columns:
            df[col] = None  # Add missing columns with NULL
    
    # Select only columns that exist in schema
    df_to_insert = df[required_columns].copy()
    
    # Convert NaN to None for SQL
    df_to_insert = df_to_insert.replace({np.nan: None})
    
    try:
        with get_connection() as conn:
            # Insert data
            df_to_insert.to_sql(
                name="market_features",
                con=conn,
                if_exists="append",
                index=False
            )
            
            # Log successful insert
            log_system_health(
                "features",
                "INSERTED",
                f"Inserted {len(df)} feature rows",
                json.dumps({
                    "timestamp": df["timestamp"].iloc[0],
                    "feature_version": df["feature_version"].iloc[0],
                    "row_count": len(df)
                })
            )
            
            # Track feature changes for important features
            track_feature_changes(df_to_insert)
            
    except Exception as e:
        print(f"‚ùå Error inserting features: {e}")
        log_system_health(
            "features",
            "INSERT_FAILED",
            f"Failed to insert features: {str(e)}",
            json.dumps({"error": str(e)})
        )
        raise

def fetch_latest_features(feature_version: str = FEATURE_VERSION) -> Optional[pd.DataFrame]:
    """
    Fetch the most recent feature row for inference.
    
    Args:
        feature_version: Feature version to fetch
    
    Returns:
        DataFrame with latest features or None
    """
    query = """
        SELECT *
        FROM market_features
        WHERE feature_version = ?
        ORDER BY timestamp DESC
        LIMIT 1
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(feature_version,))
            
            if not df.empty:
                log_system_health(
                    "features",
                    "FETCHED",
                    f"Fetched latest features for version {feature_version}"
                )
            
            return df if not df.empty else None
            
    except Exception as e:
        print(f"‚ùå Error fetching features: {e}")
        return None

def fetch_features_by_time_range(
    start_time: str,
    end_time: str,
    feature_version: str = FEATURE_VERSION
) -> pd.DataFrame:
    """
    Fetch features within a time range.
    
    Args:
        start_time: Start timestamp (inclusive)
        end_time: End timestamp (inclusive)
        feature_version: Feature version filter
    
    Returns:
        DataFrame with features in range
    """
    query = """
        SELECT *
        FROM market_features
        WHERE feature_version = ?
          AND timestamp BETWEEN ? AND ?
        ORDER BY timestamp ASC
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(
                query,
                conn,
                params=(feature_version, start_time, end_time)
            )
            
            log_system_health(
                "features",
                "FETCHED_RANGE",
                f"Fetched {len(df)} features from {start_time} to {end_time}"
            )
            
            return df
            
    except Exception as e:
        print(f"‚ùå Error fetching features by range: {e}")
        return pd.DataFrame()

def fetch_training_data(
    feature_version: str = FEATURE_VERSION,
    start_ts: Optional[str] = None,
    end_ts: Optional[str] = None,
    min_confidence: float = 0.0
) -> pd.DataFrame:
    """
    Fetch labeled data for ML training with research features.
    
    Args:
        feature_version: Feature version
        start_ts: Start timestamp
        end_ts: End timestamp
        min_confidence: Minimum signal confidence for training
    
    Returns:
        DataFrame with features and labels
    """
    # Build WHERE clause
    conditions = ["feature_version = ?", "future_return_5m IS NOT NULL"]
    params = [feature_version]
    
    if start_ts:
        conditions.append("timestamp >= ?")
        params.append(start_ts)
    
    if end_ts:
        conditions.append("timestamp <= ?")
        params.append(end_ts)
    
    where_clause = " AND ".join(conditions)
    
    query = f"""
        SELECT mf.*, s.confidence as signal_confidence
        FROM market_features mf
        LEFT JOIN signals s ON mf.timestamp = s.timestamp AND s.status = 'VALIDATED'
        WHERE {where_clause}
        ORDER BY mf.timestamp ASC
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=params)
            
            # Filter by confidence if needed
            if min_confidence > 0 and 'signal_confidence' in df.columns:
                df = df[df['signal_confidence'] >= min_confidence]
            
            log_system_health(
                "training",
                "FETCHED",
                f"Fetched {len(df)} training samples"
            )
            
            return df
            
    except Exception as e:
        print(f"‚ùå Error fetching training data: {e}")
        return pd.DataFrame()

# =========================
# RESEARCH ANALYTICS OPERATIONS
# =========================

def insert_research_analytics(analytics: Dict) -> None:
    """
    Insert research analytics for tracking and visualization.
    
    Args:
        analytics: Dictionary with research analytics data
    """
    if not analytics:
        return
    
    # Prepare data
    timestamp = analytics.get("timestamp", datetime.utcnow().isoformat())
    
    data = {
        "timestamp": timestamp,
        "oi_velocity": analytics.get("oi_velocity"),
        "oi_regime": analytics.get("oi_regime"),
        "net_gamma": analytics.get("gamma_exposure", {}).get("net_gamma"),
        "gamma_regime": analytics.get("gamma_exposure", {}).get("regime"),
        "wall_strength": analytics.get("structural_walls", [{}])[0].get("concentration", 0) if analytics.get("structural_walls") else 0,
        "trap_probability": analytics.get("potential_traps", [{}])[0].get("confidence", 0) if analytics.get("potential_traps") else 0,
        "divergence_score": analytics.get("spot_divergence", {}).get("confidence", 0),
        "market_regime": analytics.get("market_regime"),
        "confidence": analytics.get("regime_confidence"),
        "insights_json": json.dumps(analytics.get("market_insights", [])),
        "walls_json": json.dumps(analytics.get("structural_walls", [])),
        "traps_json": json.dumps(analytics.get("potential_traps", [])),
        "created_at": datetime.utcnow().isoformat()
    }
    
    # Clean NaN values
    for key, value in data.items():
        if isinstance(value, float) and np.isnan(value):
            data[key] = None
    
    try:
        with get_connection() as conn:
            columns = ", ".join(data.keys())
            placeholders = ", ".join(["?"] * len(data))
            values = list(data.values())
            
            conn.execute(
                f"INSERT OR REPLACE INTO research_analytics ({columns}) VALUES ({placeholders})",
                values
            )
            
    except Exception as e:
        print(f"‚ùå Error inserting research analytics: {e}")

def fetch_recent_analytics(limit: int = 100) -> pd.DataFrame:
    """
    Fetch recent research analytics for visualization.
    
    Args:
        limit: Number of rows to fetch
    
    Returns:
        DataFrame with analytics
    """
    query = """
        SELECT *
        FROM research_analytics
        ORDER BY timestamp DESC
        LIMIT ?
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(limit,))
            return df
    except Exception as e:
        print(f"‚ùå Error fetching analytics: {e}")
        return pd.DataFrame()

# =========================
# SIGNAL STORE OPERATIONS (ENHANCED)
# =========================

def insert_signal(signal: Dict) -> None:
    """
    Insert a trading signal with research context.
    
    Args:
        signal: Signal dictionary with research context
    """
    if not signal:
        return
    
    # Add research context if available
    research_context = signal.get("research_context", {})
    if research_context and not isinstance(research_context, str):
        signal["research_context"] = json.dumps(research_context)
    
    # Add analytics summary
    analytics_summary = signal.get("analytics_summary", {})
    if analytics_summary and not isinstance(analytics_summary, str):
        signal["analytics_summary"] = json.dumps(analytics_summary)
    
    # Ensure all required fields
    required_fields = [
        "signal_id", "timestamp", "feature_version", "model_version",
        "signal_type", "confidence", "status", "created_at"
    ]
    
    for field in required_fields:
        if field not in signal:
            print(f"‚ö†Ô∏è Missing required field in signal: {field}")
            signal[field] = ""
    
    try:
        with get_connection() as conn:
            columns = ", ".join(signal.keys())
            placeholders = ", ".join(["?"] * len(signal))
            values = list(signal.values())
            
            conn.execute(
                f"INSERT INTO signals ({columns}) VALUES ({placeholders})",
                values
            )
            
            log_system_health(
                "signals",
                "INSERTED",
                f"Signal {signal['signal_id'][:8]} inserted: {signal['signal_type']} (conf: {signal['confidence']})",
                json.dumps({
                    "signal_id": signal["signal_id"],
                    "type": signal["signal_type"],
                    "confidence": signal["confidence"]
                })
            )
            
    except Exception as e:
        print(f"‚ùå Error inserting signal: {e}")
        log_system_health(
            "signals",
            "INSERT_FAILED",
            f"Failed to insert signal: {str(e)}"
        )
        raise

def signal_exists(feature_timestamp: str) -> bool:
    """
    Check if a signal already exists for given timestamp.
    
    Args:
        feature_timestamp: Feature timestamp
    
    Returns:
        True if signal exists
    """
    query = """
        SELECT 1 FROM signals WHERE timestamp = ? LIMIT 1
    """
    
    try:
        with get_connection() as conn:
            cur = conn.execute(query, (feature_timestamp,))
            return cur.fetchone() is not None
    except Exception as e:
        print(f"‚ùå Error checking signal existence: {e}")
        return False

def validate_new_signals(confidence_threshold: float = 0.2):
    """
    Promote NEW signals to VALIDATED if confidence is sufficient.
    
    Args:
        confidence_threshold: Minimum confidence for validation
    """
    query = """
        UPDATE signals
        SET status = 'VALIDATED'
        WHERE status = 'NEW'
          AND confidence >= ?
    """
    
    try:
        with get_connection() as conn:
            result = conn.execute(query, (confidence_threshold,))
            updated_count = result.rowcount
            
            if updated_count > 0:
                log_system_health(
                    "signals",
                    "VALIDATED",
                    f"Validated {updated_count} signals (conf >= {confidence_threshold})"
                )
                
    except Exception as e:
        print(f"‚ùå Error validating signals: {e}")
        log_system_health(
            "signals",
            "VALIDATION_FAILED",
            f"Signal validation failed: {str(e)}"
        )

def expire_old_signals():
    """
    Expire VALIDATED signals whose expiry_time has passed.
    """
    query = """
        UPDATE signals
        SET status = 'EXPIRED'
        WHERE status = 'VALIDATED'
          AND expiry_time < ?
    """
    
    now = datetime.utcnow().isoformat()
    
    try:
        with get_connection() as conn:
            result = conn.execute(query, (now,))
            expired_count = result.rowcount
            
            if expired_count > 0:
                log_system_health(
                    "signals",
                    "EXPIRED",
                    f"Expired {expired_count} signals"
                )
                
    except Exception as e:
        print(f"‚ùå Error expiring signals: {e}")

def update_signal_status(signal_id: str, new_status: str, 
                        pnl: Optional[float] = None,
                        exit_time: Optional[str] = None,
                        exit_price: Optional[float] = None) -> None:
    """
    Update signal lifecycle status with trade results.
    
    Args:
        signal_id: Signal ID
        new_status: New status
        pnl: Profit/Loss if applicable
        exit_time: Exit timestamp
        exit_price: Exit price
    """
    # Build update query
    updates = ["status = ?"]
    params = [new_status]
    
    if pnl is not None:
        updates.append("pnl = ?")
        params.append(pnl)
    
    if exit_time:
        updates.append("exit_time = ?")
        params.append(exit_time)
    
    if exit_price is not None:
        updates.append("exit_price = ?")
        params.append(exit_price)
    
    # Calculate trade duration if exit_time is provided
    if exit_time:
        updates.append("""
            trade_duration_seconds = (
                CAST((julianday(?) - julianday(created_at)) * 86400 AS INTEGER)
            )
        """)
        params.append(exit_time)
    
    query = f"""
        UPDATE signals
        SET {', '.join(updates)}
        WHERE signal_id = ?
    """
    
    params.append(signal_id)
    
    try:
        with get_connection() as conn:
            conn.execute(query, params)
            
            log_system_health(
                "signals",
                "UPDATED",
                f"Signal {signal_id[:8]} updated to {new_status}"
            )
            
    except Exception as e:
        print(f"‚ùå Error updating signal: {e}")
        log_system_health(
            "signals",
            "UPDATE_FAILED",
            f"Failed to update signal {signal_id}: {str(e)}"
        )

def fetch_active_signals(limit: int = 10) -> pd.DataFrame:
    """
    Fetch active signals for display.
    
    Args:
        limit: Maximum number of signals to fetch
    
    Returns:
        DataFrame with active signals
    """
    query = """
        SELECT *
        FROM signals
        WHERE status IN ('NEW','VALIDATED')
        ORDER BY created_at DESC
        LIMIT ?
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(limit,))
            
            # Parse JSON fields
            for col in ["research_context", "analytics_summary"]:
                if col in df.columns:
                    df[col] = df[col].apply(
                        lambda x: json.loads(x) if x and isinstance(x, str) else {}
                    )
            
            return df
            
    except Exception as e:
        print(f"‚ùå Error fetching active signals: {e}")
        return pd.DataFrame()

def fetch_signal_performance(days: int = 30) -> Dict:
    """
    Fetch signal performance metrics.
    
    Args:
        days: Number of days to analyze
    
    Returns:
        Dictionary with performance metrics
    """
    cutoff_date = (datetime.utcnow() - timedelta(days=days)).isoformat()
    
    query = """
        SELECT 
            signal_type,
            status,
            COUNT(*) as count,
            AVG(confidence) as avg_confidence,
            AVG(pnl) as avg_pnl,
            SUM(CASE WHEN pnl > 0 THEN 1 ELSE 0 END) as winners,
            SUM(CASE WHEN pnl <= 0 THEN 1 ELSE 0 END) as losers
        FROM signals
        WHERE created_at >= ?
        GROUP BY signal_type, status
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(cutoff_date,))
            
            if df.empty:
                return {}
            
            # Calculate overall metrics
            total_signals = df['count'].sum()
            profitable_signals = df[df['signal_type'] == 'BUY']['winners'].sum() + \
                                df[df['signal_type'] == 'SELL']['winners'].sum()
            
            performance = {
                "total_signals": int(total_signals),
                "profitable_signals": int(profitable_signals),
                "win_rate": round(profitable_signals / total_signals * 100, 1) if total_signals > 0 else 0,
                "breakdown": df.to_dict('records'),
                "period_days": days
            }
            
            return performance
            
    except Exception as e:
        print(f"‚ùå Error fetching performance: {e}")
        return {}

# =========================
# MODEL REGISTRY OPERATIONS
# =========================

def register_model(model_info: Dict) -> None:
    """
    Register trained ML model metadata.
    
    Args:
        model_info: Model information dictionary
    """
    required_keys = [
        "model_version", "feature_version", "algorithm",
        "trained_on_start", "trained_on_end", "metrics_json"
    ]
    
    for key in required_keys:
        if key not in model_info:
            raise ValueError(f"Missing required key: {key}")
    
    # Set created_at if not provided
    if "created_at" not in model_info:
        model_info["created_at"] = datetime.utcnow().isoformat()
    
    try:
        with get_connection() as conn:
            # Deactivate other models for this feature version
            conn.execute("""
                UPDATE model_registry
                SET is_active = 0
                WHERE feature_version = ?
            """, (model_info["feature_version"],))
            
            # Insert new model as active
            columns = ", ".join(model_info.keys())
            placeholders = ", ".join(["?"] * len(model_info))
            values = list(model_info.values())
            
            conn.execute(
                f"INSERT INTO model_registry ({columns}) VALUES ({placeholders})",
                values
            )
            
            log_system_health(
                "models",
                "REGISTERED",
                f"Model {model_info['model_version']} registered"
            )
            
    except Exception as e:
        print(f"‚ùå Error registering model: {e}")
        raise

def fetch_active_model(feature_version: str = FEATURE_VERSION) -> Optional[Dict]:
    """
    Fetch the active model for a given feature version.
    
    Args:
        feature_version: Feature version
    
    Returns:
        Model info dictionary or None
    """
    query = """
        SELECT *
        FROM model_registry
        WHERE feature_version = ?
          AND is_active = 1
        ORDER BY created_at DESC
        LIMIT 1
    """
    
    try:
        with get_connection() as conn:
            row = conn.execute(query, (feature_version,)).fetchone()
            return dict(row) if row else None
    except Exception as e:
        print(f"‚ùå Error fetching active model: {e}")
        return None

# =========================
# SYSTEM HEALTH LOGGING
# =========================

def log_system_health(
    component: str,
    status: str,
    message: Optional[str] = None,
    details_json: Optional[str] = None
) -> None:
    """
    Log system health status.
    
    Args:
        component: System component
        status: Status (OK, ERROR, WARNING, etc.)
        message: Optional message
        details_json: Optional JSON details
    """
    query = """
        INSERT INTO system_health (timestamp, component, status, message, details_json)
        VALUES (?, ?, ?, ?, ?)
    """
    
    params = (
        datetime.utcnow().isoformat(),
        component,
        status,
        message,
        details_json
    )
    
    try:
        with get_connection() as conn:
            conn.execute(query, params)
    except Exception as e:
        print(f"‚ùå Error logging system health: {e}")

def fetch_recent_health_logs(limit: int = 50) -> pd.DataFrame:
    """
    Fetch recent system health logs.
    
    Args:
        limit: Number of logs to fetch
    
    Returns:
        DataFrame with health logs
    """
    query = """
        SELECT *
        FROM system_health
        ORDER BY timestamp DESC
        LIMIT ?
    """
    
    try:
        with get_connection() as conn:
            return pd.read_sql_query(query, conn, params=(limit,))
    except Exception as e:
        print(f"‚ùå Error fetching health logs: {e}")
        return pd.DataFrame()

# =========================
# FEATURE HISTORY TRACKING
# =========================

def track_feature_changes(feature_df: pd.DataFrame) -> None:
    """
    Track significant feature changes for monitoring.
    
    Args:
        feature_df: DataFrame with features
    """
    if feature_df.empty or len(feature_df) < 2:
        return
    
    # Only track important features
    important_features = [
        "oi_velocity", "net_gamma", "trap_probability",
        "divergence_score", "wall_strength", "put_call_ratio"
    ]
    
    current = feature_df.iloc[-1]
    previous = feature_df.iloc[-2] if len(feature_df) > 1 else current
    
    changes = []
    for feature in important_features:
        if feature in current and feature in previous:
            old_val = previous[feature]
            new_val = current[feature]
            
            if old_val is not None and new_val is not None and old_val != 0:
                change_pct = ((new_val - old_val) / abs(old_val)) * 100
                
                # Only track significant changes (>10%)
                if abs(change_pct) > 10:
                    changes.append({
                        "timestamp": current["timestamp"],
                        "feature_name": feature,
                        "old_value": old_val,
                        "new_value": new_val,
                        "change_pct": change_pct,
                        "created_at": datetime.utcnow().isoformat()
                    })
    
    if changes:
        try:
            with get_connection() as conn:
                for change in changes:
                    conn.execute("""
                        INSERT INTO feature_history 
                        (timestamp, feature_name, old_value, new_value, change_pct, created_at)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (
                        change["timestamp"], change["feature_name"],
                        change["old_value"], change["new_value"],
                        change["change_pct"], change["created_at"]
                    ))
        except Exception as e:
            print(f"‚ùå Error tracking feature changes: {e}")

# =========================
# DATABASE MAINTENANCE
# =========================

def cleanup_old_data(days_to_keep: int = 30) -> Dict:
    """
    Clean up old data from database.
    
    Args:
        days_to_keep: Number of days of data to keep
    
    Returns:
        Dictionary with cleanup statistics
    """
    cutoff_date = (datetime.utcnow() - timedelta(days=days_to_keep)).isoformat()
    
    cleanup_stats = {}
    
    try:
        with get_connection() as conn:
            # Clean old market features
            result = conn.execute(
                "DELETE FROM market_features WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["market_features"] = result.rowcount
            
            # Clean old signals
            result = conn.execute(
                "DELETE FROM signals WHERE created_at < ? AND status = 'EXPIRED'",
                (cutoff_date,)
            )
            cleanup_stats["signals"] = result.rowcount
            
            # Clean old system health logs
            result = conn.execute(
                "DELETE FROM system_health WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["system_health"] = result.rowcount
            
            # Clean old research analytics
            result = conn.execute(
                "DELETE FROM research_analytics WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["research_analytics"] = result.rowcount
            
            # Clean old feature history
            result = conn.execute(
                "DELETE FROM feature_history WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["feature_history"] = result.rowcount
            
            # Vacuum database
            conn.execute("VACUUM")
            
            log_system_health(
                "database",
                "CLEANED",
                f"Cleaned up {sum(cleanup_stats.values())} old records",
                json.dumps(cleanup_stats)
            )
            
            return cleanup_stats
            
    except Exception as e:
        print(f"‚ùå Error during cleanup: {e}")
        log_system_health(
            "database",
            "CLEANUP_FAILED",
            f"Cleanup failed: {str(e)}"
        )
        return {}

# =========================
# DATABASE INFO & STATS
# =========================

def get_database_stats() -> Dict:
    """
    Get database statistics.
    
    Returns:
        Dictionary with database statistics
    """
    stats = {}
    
    try:
        with get_connection() as conn:
            # Table counts
            tables = [
                "market_features", "signals", "model_registry",
                "system_health", "research_analytics", "feature_history"
            ]
            
            for table in tables:
                cur = conn.execute(f"SELECT COUNT(*) as count FROM {table}")
                row = cur.fetchone()
                stats[f"{table}_count"] = row["count"] if row else 0
            
            # Latest feature timestamp
            cur = conn.execute("SELECT MAX(timestamp) as latest FROM market_features")
            row = cur.fetchone()
            stats["latest_feature"] = row["latest"] if row else None
            
            # Feature version
            cur = conn.execute("SELECT value FROM database_info WHERE key = 'feature_version'")
            row = cur.fetchone()
            stats["feature_version"] = row["value"] if row else "unknown"
            
            # Active signals
            cur = conn.execute("""
                SELECT COUNT(*) as count 
                FROM signals 
                WHERE status IN ('NEW','VALIDATED')
            """)
            row = cur.fetchone()
            stats["active_signals"] = row["count"] if row else 0
            
            # Database size
            db_size = DB_PATH.stat().st_size if DB_PATH.exists() else 0
            stats["database_size_mb"] = round(db_size / (1024 * 1024), 2)
            
            return stats
            
    except Exception as e:
        print(f"‚ùå Error getting database stats: {e}")
        return {}

# =========================
# INITIALIZATION
# =========================

def initialize_storage():
    """
    Initialize storage system.
    """
    print("Initializing storage system...")
    initialize_database()
    
    # Check and run migrations if needed
    current_version = get_database_stats().get("feature_version", "v1.0")
    if current_version != FEATURE_VERSION:
        print(f"Migrating from {current_version} to {FEATURE_VERSION}")
        migrate_database(FEATURE_VERSION)
    
    print("‚úì Storage system initialized")

# Run initialization when module is imported
initialize_storage()



====================================================================================================
FILE: .\storage\repository.py
====================================================================================================


File Name: repository.py
Full Path: G:\trading_app\storage\repository.py
Size: 40.25 KB
Last Modified: 01/27/2026 20:06:38
Extension: .py

"""
Enhanced Storage Repository with Research Features Support - FIXED VERSION
Handles database operations for market features, signals, and analytics.
Includes migration system for feature version updates.
"""

import sqlite3
import pandas as pd
from typing import Dict, Optional, List, Any, Tuple
from pathlib import Path
from datetime import datetime, timedelta
import json
import numpy as np
from contextlib import contextmanager

# Import feature contract for schema
from ml.feature_contract import (
    FEATURE_VERSION, 
    FEATURE_COLUMNS, 
    METADATA_COLUMNS,
    MARKET_FEATURES_SCHEMA,
    get_migration_sql
)

# =========================
# DATABASE CONFIG
# =========================

DB_PATH = Path("G:/trading_app/storage/trading.db")

# Ensure directory exists
DB_PATH.parent.mkdir(parents=True, exist_ok=True)

# =========================
# CONNECTION MANAGEMENT
# =========================

@contextmanager
def get_connection() -> sqlite3.Connection:
    """
    Context manager for database connections with automatic cleanup.
    """
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    
    # Enable WAL mode for better concurrency
    conn.execute("PRAGMA journal_mode=WAL")
    conn.execute("PRAGMA synchronous=NORMAL")
    conn.execute("PRAGMA foreign_keys=ON")
    
    try:
        yield conn
        conn.commit()
    except Exception as e:
        conn.rollback()
        raise e
    finally:
        conn.close()

def initialize_database():
    """
    Initialize database with all required tables and indexes.
    """
    with get_connection() as conn:
        # Check if market_features table exists and get its columns
        cur = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='market_features'")
        table_exists = cur.fetchone() is not None
        
        if table_exists:
            # Get existing columns
            cur = conn.execute("PRAGMA table_info(market_features)")
            existing_columns = {row[1] for row in cur.fetchall()}
            print(f"Existing columns in market_features: {existing_columns}")
        else:
            existing_columns = set()
        
        # Get expected columns from schema
        expected_columns = set(MARKET_FEATURES_SCHEMA["columns"].keys())
        print(f"Expected columns in market_features: {expected_columns}")
        
        # Create table if it doesn't exist
        if not table_exists:
            print("Creating market_features table from scratch...")
            columns = MARKET_FEATURES_SCHEMA["columns"]
            column_defs = [f"{name} {dtype}" for name, dtype in columns.items()]
            
            create_table_sql = f"""
                CREATE TABLE IF NOT EXISTS market_features (
                    {', '.join(column_defs)}
                )
            """
            
            conn.execute(create_table_sql)
            print("market_features table created successfully")
        
        # Check for missing columns and add them
        missing_columns = expected_columns - existing_columns
        if missing_columns:
            print(f"Adding missing columns: {missing_columns}")
            
            for column in missing_columns:
                dtype = MARKET_FEATURES_SCHEMA["columns"][column]
                try:
                    conn.execute(f"ALTER TABLE market_features ADD COLUMN {column} {dtype}")
                    print(f"  ‚úì Added column: {column}")
                except sqlite3.OperationalError as e:
                    if "duplicate column" in str(e):
                        print(f"  ‚ö†Ô∏è Column {column} already exists")
                    else:
                        raise
        
        # Create other tables
        print("Creating other tables...")
        
        # Create signals table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS signals (
                signal_id TEXT PRIMARY KEY,
                timestamp TEXT NOT NULL,
                feature_version TEXT NOT NULL,
                model_version TEXT NOT NULL,
                signal_type TEXT NOT NULL,
                confidence REAL,
                market_state TEXT,
                rationale TEXT,
                expiry_time TEXT,
                status TEXT,
                created_at TEXT,
                research_context TEXT,
                analytics_summary TEXT,
                pnl REAL DEFAULT NULL,
                exit_time TEXT,
                exit_price REAL,
                stop_loss_hit INTEGER DEFAULT 0,
                take_profit_hit INTEGER DEFAULT 0,
                trade_duration_seconds INTEGER
            )
        """)
        
        # Create model_registry table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS model_registry (
                model_version TEXT PRIMARY KEY,
                feature_version TEXT NOT NULL,
                algorithm TEXT NOT NULL,
                trained_on_start TEXT,
                trained_on_end TEXT,
                metrics_json TEXT,
                feature_importance_json TEXT,
                created_at TEXT,
                is_active INTEGER DEFAULT 0
            )
        """)
        
        # Create system_health table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS system_health (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                component TEXT NOT NULL,
                status TEXT NOT NULL,
                message TEXT,
                details_json TEXT
            )
        """)
        
        # Create research_analytics table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS research_analytics (
                timestamp TEXT PRIMARY KEY,
                oi_velocity REAL,
                oi_regime TEXT,
                net_gamma REAL,
                gamma_regime TEXT,
                wall_strength REAL,
                trap_probability REAL,
                divergence_score REAL,
                market_regime TEXT,
                confidence REAL,
                insights_json TEXT,
                walls_json TEXT,
                traps_json TEXT,
                created_at TEXT
            )
        """)
        
        # Create feature_history table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS feature_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                feature_name TEXT NOT NULL,
                old_value REAL,
                new_value REAL,
                change_pct REAL,
                created_at TEXT
            )
        """)
        
        # Create database_info table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS database_info (
                key TEXT PRIMARY KEY,
                value TEXT,
                updated_at TEXT
            )
        """)
        
        # Create indexes (skip if they already exist)
        print("Creating indexes...")
        index_sqls = [
            # market_features indexes
            "CREATE INDEX IF NOT EXISTS idx_timestamp ON market_features(timestamp)",
            "CREATE INDEX IF NOT EXISTS idx_feature_version ON market_features(feature_version)",
            "CREATE INDEX IF NOT EXISTS idx_oi_velocity ON market_features(oi_velocity)",
            "CREATE INDEX IF NOT EXISTS idx_net_gamma ON market_features(net_gamma)",
            
            # signals indexes
            "CREATE INDEX IF NOT EXISTS idx_signals_timestamp ON signals(timestamp)",
            "CREATE INDEX IF NOT EXISTS idx_signals_status ON signals(status)",
            "CREATE INDEX IF NOT EXISTS idx_signals_type ON signals(signal_type)",
            
            # research_analytics indexes
            "CREATE INDEX IF NOT EXISTS idx_analytics_timestamp ON research_analytics(timestamp)",
            "CREATE INDEX IF NOT EXISTS idx_analytics_regime ON research_analytics(market_regime)"
        ]
        
        for index_sql in index_sqls:
            try:
                conn.execute(index_sql)
            except sqlite3.OperationalError as e:
                if "already exists" not in str(e):
                    print(f"  ‚ö†Ô∏è Could not create index: {e}")
        
        # Initialize database info
        current_version = get_current_feature_version(conn)
        if not current_version:
            conn.execute(
                "INSERT INTO database_info (key, value, updated_at) VALUES (?, ?, ?)",
                ("feature_version", FEATURE_VERSION, datetime.utcnow().isoformat())
            )
            print(f"‚úì Set feature version to {FEATURE_VERSION}")
        else:
            print(f"‚úì Current feature version: {current_version}")
        
        # Log initialization
        _log_system_health_conn(
            conn,
            "database",
            "INITIALIZED",
            f"Database initialized with feature version {FEATURE_VERSION}"
        )
        
        print(f"‚úì Database initialized at {DB_PATH}")

def get_current_feature_version(conn: sqlite3.Connection) -> Optional[str]:
    """Get current feature version from database."""
    try:
        cur = conn.execute("SELECT value FROM database_info WHERE key = 'feature_version'")
        row = cur.fetchone()
        return row["value"] if row else None
    except:
        return None

# =========================
# DATABASE MIGRATION
# =========================

def migrate_database(target_version: str = FEATURE_VERSION) -> bool:
    """
    Migrate database to target feature version.
    """
    try:
        with get_connection() as conn:
            # Get current version
            current_version = get_current_feature_version(conn)
            if not current_version:
                current_version = "v1.0"
            
            if current_version == target_version:
                print(f"Database already at version {target_version}")
                return True
            
            print(f"Migrating database from {current_version} to {target_version}")
            
            # Get migration SQL
            migrations = get_migration_sql(current_version, target_version)
            
            if not migrations:
                print("No migration required")
                return True
            
            # Execute migrations
            for migration_sql in migrations:
                try:
                    print(f"Executing: {migration_sql[:80]}...")
                    conn.execute(migration_sql)
                except sqlite3.OperationalError as e:
                    if "already exists" in str(e) or "duplicate column" in str(e):
                        print(f"  ‚ö†Ô∏è Skipping (already exists): {migration_sql[:50]}...")
                    else:
                        print(f"  ‚ùå Error: {e}")
                        raise
            
            # Update version
            conn.execute(
                "INSERT OR REPLACE INTO database_info (key, value, updated_at) VALUES (?, ?, ?)",
                (target_version, target_version, datetime.utcnow().isoformat())
            )
            
            # Log migration
            _log_system_health_conn(
                conn,
                "database",
                "MIGRATED",
                f"Migrated from {current_version} to {target_version}",
                json.dumps({"migrations_applied": len(migrations)})
            )
            
            print(f"‚úì Database migrated to version {target_version}")
            return True
            
    except Exception as e:
        print(f"‚ùå Migration failed: {e}")
        import traceback
        traceback.print_exc()
        return False

# =========================
# SYSTEM HEALTH LOGGING (PRIVATE HELPER)
# =========================

def _log_system_health_conn(
    conn: sqlite3.Connection,
    component: str,
    status: str,
    message: Optional[str] = None,
    details_json: Optional[str] = None
) -> None:
    """
    PRIVATE: Log system health status (requires connection).
    """
    query = """
        INSERT INTO system_health (timestamp, component, status, message, details_json)
        VALUES (?, ?, ?, ?, ?)
    """
    
    params = (
        datetime.utcnow().isoformat(),
        component,
        status,
        message,
        details_json
    )
    
    try:
        conn.execute(query, params)
    except Exception as e:
        print(f"‚ùå Error logging system health: {e}")

# =========================
# PUBLIC SYSTEM HEALTH LOGGING
# =========================

def log_system_health(
    component: str,
    status: str,
    message: Optional[str] = None,
    details_json: Optional[str] = None
) -> None:
    """
    PUBLIC: Log system health status.
    """
    with get_connection() as conn:
        _log_system_health_conn(conn, component, status, message, details_json)

# =========================
# INITIALIZATION (MODIFIED)
# =========================

def initialize_storage():
    """
    Initialize storage system.
    """
    print("Initializing storage system...")
    
    try:
        # Initialize database structure
        initialize_database()
        
        # Get current version
        with get_connection() as conn:
            current_version = get_current_feature_version(conn)
        
        # Check and run migrations if needed
        if current_version != FEATURE_VERSION:
            print(f"Current version: {current_version}, Target: {FEATURE_VERSION}")
            print(f"Migrating from {current_version} to {FEATURE_VERSION}")
            success = migrate_database(FEATURE_VERSION)
            
            if not success:
                print("‚ö†Ô∏è Migration may have issues, but continuing...")
        
        print("‚úì Storage system initialized")
        
    except Exception as e:
        print(f"‚ùå Error initializing storage: {e}")
        import traceback
        traceback.print_exc()
        raise

# =========================
# MARKET FEATURES OPERATIONS
# =========================

def insert_market_features(df: pd.DataFrame) -> None:
    """
    Insert feature rows into market_features table.
    """
    if df.empty:
        return
    
    # Ensure feature_version is set
    if "feature_version" not in df.columns:
        df["feature_version"] = FEATURE_VERSION
    
    # Ensure timestamp is string
    df["timestamp"] = df["timestamp"].astype(str)
    
    # Get expected columns from schema
    expected_columns = set(MARKET_FEATURES_SCHEMA["columns"].keys())
    
    # Add any missing columns with NULL
    for col in expected_columns:
        if col not in df.columns:
            df[col] = None
    
    # Select only columns that exist in schema
    df_to_insert = df[list(expected_columns)].copy()
    
    # Convert NaN to None for SQL
    df_to_insert = df_to_insert.replace({np.nan: None})
    
    try:
        with get_connection() as conn:
            # Insert data
            df_to_insert.to_sql(
                name="market_features",
                con=conn,
                if_exists="append",
                index=False
            )
            
            print(f"‚úì Inserted {len(df)} feature rows")
            
    except Exception as e:
        print(f"‚ùå Error inserting features: {e}")
        raise

def fetch_latest_features(feature_version: str = FEATURE_VERSION) -> Optional[pd.DataFrame]:
    """
    Fetch the most recent feature row for inference.
    """
    query = """
        SELECT *
        FROM market_features
        WHERE feature_version = ?
        ORDER BY timestamp DESC
        LIMIT 1
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(feature_version,))
            return df if not df.empty else None
            
    except Exception as e:
        print(f"‚ùå Error fetching features: {e}")
        return None

def fetch_features_by_time_range(
    start_time: str,
    end_time: str,
    feature_version: str = FEATURE_VERSION
) -> pd.DataFrame:
    """
    Fetch features within a time range.
    
    Args:
        start_time: Start timestamp (inclusive)
        end_time: End timestamp (inclusive)
        feature_version: Feature version filter
    
    Returns:
        DataFrame with features in range
    """
    query = """
        SELECT *
        FROM market_features
        WHERE feature_version = ?
          AND timestamp BETWEEN ? AND ?
        ORDER BY timestamp ASC
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(
                query,
                conn,
                params=(feature_version, start_time, end_time)
            )
            
            log_system_health(
                "features",
                "FETCHED_RANGE",
                f"Fetched {len(df)} features from {start_time} to {end_time}"
            )
            
            return df
            
    except Exception as e:
        print(f"‚ùå Error fetching features by range: {e}")
        return pd.DataFrame()

def fetch_training_data(
    feature_version: str = FEATURE_VERSION,
    start_ts: Optional[str] = None,
    end_ts: Optional[str] = None,
    min_confidence: float = 0.0
) -> pd.DataFrame:
    """
    Fetch labeled data for ML training with research features.
    
    Args:
        feature_version: Feature version
        start_ts: Start timestamp
        end_ts: End timestamp
        min_confidence: Minimum signal confidence for training
    
    Returns:
        DataFrame with features and labels
    """
    # Build WHERE clause
    conditions = ["feature_version = ?", "future_return_5m IS NOT NULL"]
    params = [feature_version]
    
    if start_ts:
        conditions.append("timestamp >= ?")
        params.append(start_ts)
    
    if end_ts:
        conditions.append("timestamp <= ?")
        params.append(end_ts)
    
    where_clause = " AND ".join(conditions)
    
    query = f"""
        SELECT mf.*, s.confidence as signal_confidence
        FROM market_features mf
        LEFT JOIN signals s ON mf.timestamp = s.timestamp AND s.status = 'VALIDATED'
        WHERE {where_clause}
        ORDER BY mf.timestamp ASC
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=params)
            
            # Filter by confidence if needed
            if min_confidence > 0 and 'signal_confidence' in df.columns:
                df = df[df['signal_confidence'] >= min_confidence]
            
            log_system_health(
                "training",
                "FETCHED",
                f"Fetched {len(df)} training samples"
            )
            
            return df
            
    except Exception as e:
        print(f"‚ùå Error fetching training data: {e}")
        return pd.DataFrame()

# =========================
# RESEARCH ANALYTICS OPERATIONS
# =========================

def insert_research_analytics(analytics: Dict) -> None:
    """
    Insert research analytics for tracking and visualization.
    
    Args:
        analytics: Dictionary with research analytics data
    """
    if not analytics:
        return
    
    # Prepare data
    timestamp = analytics.get("timestamp", datetime.utcnow().isoformat())
    
    data = {
        "timestamp": timestamp,
        "oi_velocity": analytics.get("oi_velocity"),
        "oi_regime": analytics.get("oi_regime"),
        "net_gamma": analytics.get("gamma_exposure", {}).get("net_gamma"),
        "gamma_regime": analytics.get("gamma_exposure", {}).get("regime"),
        "wall_strength": analytics.get("structural_walls", [{}])[0].get("concentration", 0) if analytics.get("structural_walls") else 0,
        "trap_probability": analytics.get("potential_traps", [{}])[0].get("confidence", 0) if analytics.get("potential_traps") else 0,
        "divergence_score": analytics.get("spot_divergence", {}).get("confidence", 0),
        "market_regime": analytics.get("market_regime"),
        "confidence": analytics.get("regime_confidence"),
        "insights_json": json.dumps(analytics.get("market_insights", [])),
        "walls_json": json.dumps(analytics.get("structural_walls", [])),
        "traps_json": json.dumps(analytics.get("potential_traps", [])),
        "created_at": datetime.utcnow().isoformat()
    }
    
    # Clean NaN values
    for key, value in data.items():
        if isinstance(value, float) and np.isnan(value):
            data[key] = None
    
    try:
        with get_connection() as conn:
            columns = ", ".join(data.keys())
            placeholders = ", ".join(["?"] * len(data))
            values = list(data.values())
            
            conn.execute(
                f"INSERT OR REPLACE INTO research_analytics ({columns}) VALUES ({placeholders})",
                values
            )
            
    except Exception as e:
        print(f"‚ùå Error inserting research analytics: {e}")

def fetch_recent_analytics(limit: int = 100) -> pd.DataFrame:
    """
    Fetch recent research analytics for visualization.
    
    Args:
        limit: Number of rows to fetch
    
    Returns:
        DataFrame with analytics
    """
    query = """
        SELECT *
        FROM research_analytics
        ORDER BY timestamp DESC
        LIMIT ?
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(limit,))
            return df
    except Exception as e:
        print(f"‚ùå Error fetching analytics: {e}")
        return pd.DataFrame()

# =========================
# SIGNAL STORE OPERATIONS (ENHANCED)
# =========================

def insert_signal(signal: Dict) -> None:
    """
    Insert a trading signal with research context.
    
    Args:
        signal: Signal dictionary with research context
    """
    if not signal:
        return
    
    # Add research context if available
    research_context = signal.get("research_context", {})
    if research_context and not isinstance(research_context, str):
        signal["research_context"] = json.dumps(research_context)
    
    # Add analytics summary
    analytics_summary = signal.get("analytics_summary", {})
    if analytics_summary and not isinstance(analytics_summary, str):
        signal["analytics_summary"] = json.dumps(analytics_summary)
    
    # Ensure all required fields
    required_fields = [
        "signal_id", "timestamp", "feature_version", "model_version",
        "signal_type", "confidence", "status", "created_at"
    ]
    
    for field in required_fields:
        if field not in signal:
            print(f"‚ö†Ô∏è Missing required field in signal: {field}")
            signal[field] = ""
    
    try:
        with get_connection() as conn:
            columns = ", ".join(signal.keys())
            placeholders = ", ".join(["?"] * len(signal))
            values = list(signal.values())
            
            conn.execute(
                f"INSERT INTO signals ({columns}) VALUES ({placeholders})",
                values
            )
            
            log_system_health(
                "signals",
                "INSERTED",
                f"Signal {signal['signal_id'][:8]} inserted: {signal['signal_type']} (conf: {signal['confidence']})",
                json.dumps({
                    "signal_id": signal["signal_id"],
                    "type": signal["signal_type"],
                    "confidence": signal["confidence"]
                })
            )
            
    except Exception as e:
        print(f"‚ùå Error inserting signal: {e}")
        log_system_health(
            "signals",
            "INSERT_FAILED",
            f"Failed to insert signal: {str(e)}"
        )
        raise

def signal_exists(feature_timestamp: str) -> bool:
    """
    Check if a signal already exists for given timestamp.
    
    Args:
        feature_timestamp: Feature timestamp
    
    Returns:
        True if signal exists
    """
    query = """
        SELECT 1 FROM signals WHERE timestamp = ? LIMIT 1
    """
    
    try:
        with get_connection() as conn:
            cur = conn.execute(query, (feature_timestamp,))
            return cur.fetchone() is not None
    except Exception as e:
        print(f"‚ùå Error checking signal existence: {e}")
        return False

def validate_new_signals(confidence_threshold: float = 0.2):
    """
    Promote NEW signals to VALIDATED if confidence is sufficient.
    
    Args:
        confidence_threshold: Minimum confidence for validation
    """
    query = """
        UPDATE signals
        SET status = 'VALIDATED'
        WHERE status = 'NEW'
          AND confidence >= ?
    """
    
    try:
        with get_connection() as conn:
            result = conn.execute(query, (confidence_threshold,))
            updated_count = result.rowcount
            
            if updated_count > 0:
                log_system_health(
                    "signals",
                    "VALIDATED",
                    f"Validated {updated_count} signals (conf >= {confidence_threshold})"
                )
                
    except Exception as e:
        print(f"‚ùå Error validating signals: {e}")
        log_system_health(
            "signals",
            "VALIDATION_FAILED",
            f"Signal validation failed: {str(e)}"
        )

def expire_old_signals():
    """
    Expire VALIDATED signals whose expiry_time has passed.
    """
    query = """
        UPDATE signals
        SET status = 'EXPIRED'
        WHERE status = 'VALIDATED'
          AND expiry_time < ?
    """
    
    now = datetime.utcnow().isoformat()
    
    try:
        with get_connection() as conn:
            result = conn.execute(query, (now,))
            expired_count = result.rowcount
            
            if expired_count > 0:
                log_system_health(
                    "signals",
                    "EXPIRED",
                    f"Expired {expired_count} signals"
                )
                
    except Exception as e:
        print(f"‚ùå Error expiring signals: {e}")

def update_signal_status(signal_id: str, new_status: str, 
                        pnl: Optional[float] = None,
                        exit_time: Optional[str] = None,
                        exit_price: Optional[float] = None) -> None:
    """
    Update signal lifecycle status with trade results.
    
    Args:
        signal_id: Signal ID
        new_status: New status
        pnl: Profit/Loss if applicable
        exit_time: Exit timestamp
        exit_price: Exit price
    """
    # Build update query
    updates = ["status = ?"]
    params = [new_status]
    
    if pnl is not None:
        updates.append("pnl = ?")
        params.append(pnl)
    
    if exit_time:
        updates.append("exit_time = ?")
        params.append(exit_time)
    
    if exit_price is not None:
        updates.append("exit_price = ?")
        params.append(exit_price)
    
    # Calculate trade duration if exit_time is provided
    if exit_time:
        updates.append("""
            trade_duration_seconds = (
                CAST((julianday(?) - julianday(created_at)) * 86400 AS INTEGER)
            )
        """)
        params.append(exit_time)
    
    query = f"""
        UPDATE signals
        SET {', '.join(updates)}
        WHERE signal_id = ?
    """
    
    params.append(signal_id)
    
    try:
        with get_connection() as conn:
            conn.execute(query, params)
            
            log_system_health(
                "signals",
                "UPDATED",
                f"Signal {signal_id[:8]} updated to {new_status}"
            )
            
    except Exception as e:
        print(f"‚ùå Error updating signal: {e}")
        log_system_health(
            "signals",
            "UPDATE_FAILED",
            f"Failed to update signal {signal_id}: {str(e)}"
        )

def fetch_active_signals(limit: int = 10) -> pd.DataFrame:
    """Fetch active signals for display."""
    query = """
        SELECT *
        FROM signals
        WHERE status IN ('NEW','VALIDATED')
        ORDER BY created_at DESC
        LIMIT ?
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(limit,))
            return df
    except Exception as e:
        print(f"‚ùå Error fetching active signals: {e}")
        return pd.DataFrame()

def fetch_signal_performance(days: int = 30) -> Dict:
    """
    Fetch signal performance metrics.
    
    Args:
        days: Number of days to analyze
    
    Returns:
        Dictionary with performance metrics
    """
    cutoff_date = (datetime.utcnow() - timedelta(days=days)).isoformat()
    
    query = """
        SELECT 
            signal_type,
            status,
            COUNT(*) as count,
            AVG(confidence) as avg_confidence,
            AVG(pnl) as avg_pnl,
            SUM(CASE WHEN pnl > 0 THEN 1 ELSE 0 END) as winners,
            SUM(CASE WHEN pnl <= 0 THEN 1 ELSE 0 END) as losers
        FROM signals
        WHERE created_at >= ?
        GROUP BY signal_type, status
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(cutoff_date,))
            
            if df.empty:
                return {}
            
            # Calculate overall metrics
            total_signals = df['count'].sum()
            profitable_signals = df[df['signal_type'] == 'BUY']['winners'].sum() + \
                                df[df['signal_type'] == 'SELL']['winners'].sum()
            
            performance = {
                "total_signals": int(total_signals),
                "profitable_signals": int(profitable_signals),
                "win_rate": round(profitable_signals / total_signals * 100, 1) if total_signals > 0 else 0,
                "breakdown": df.to_dict('records'),
                "period_days": days
            }
            
            return performance
            
    except Exception as e:
        print(f"‚ùå Error fetching performance: {e}")
        return {}

# =========================
# MODEL REGISTRY OPERATIONS
# =========================

def register_model(model_info: Dict) -> None:
    """
    Register trained ML model metadata.
    
    Args:
        model_info: Model information dictionary
    """
    required_keys = [
        "model_version", "feature_version", "algorithm",
        "trained_on_start", "trained_on_end", "metrics_json"
    ]
    
    for key in required_keys:
        if key not in model_info:
            raise ValueError(f"Missing required key: {key}")
    
    # Set created_at if not provided
    if "created_at" not in model_info:
        model_info["created_at"] = datetime.utcnow().isoformat()
    
    try:
        with get_connection() as conn:
            # Deactivate other models for this feature version
            conn.execute("""
                UPDATE model_registry
                SET is_active = 0
                WHERE feature_version = ?
            """, (model_info["feature_version"],))
            
            # Insert new model as active
            columns = ", ".join(model_info.keys())
            placeholders = ", ".join(["?"] * len(model_info))
            values = list(model_info.values())
            
            conn.execute(
                f"INSERT INTO model_registry ({columns}) VALUES ({placeholders})",
                values
            )
            
            log_system_health(
                "models",
                "REGISTERED",
                f"Model {model_info['model_version']} registered"
            )
            
    except Exception as e:
        print(f"‚ùå Error registering model: {e}")
        raise

def fetch_active_model(feature_version: str = FEATURE_VERSION) -> Optional[Dict]:
    """
    Fetch the active model for a given feature version.
    
    Args:
        feature_version: Feature version
    
    Returns:
        Model info dictionary or None
    """
    query = """
        SELECT *
        FROM model_registry
        WHERE feature_version = ?
          AND is_active = 1
        ORDER BY created_at DESC
        LIMIT 1
    """
    
    try:
        with get_connection() as conn:
            row = conn.execute(query, (feature_version,)).fetchone()
            return dict(row) if row else None
    except Exception as e:
        print(f"‚ùå Error fetching active model: {e}")
        return None

# =========================
# SYSTEM HEALTH LOGGING (PUBLIC)
# =========================

def fetch_recent_health_logs(limit: int = 50) -> pd.DataFrame:
    """
    Fetch recent system health logs.
    
    Args:
        limit: Number of logs to fetch
    
    Returns:
        DataFrame with health logs
    """
    query = """
        SELECT *
        FROM system_health
        ORDER BY timestamp DESC
        LIMIT ?
    """
    
    try:
        with get_connection() as conn:
            return pd.read_sql_query(query, conn, params=(limit,))
    except Exception as e:
        print(f"‚ùå Error fetching health logs: {e}")
        return pd.DataFrame()

# =========================
# FEATURE HISTORY TRACKING
# =========================

def track_feature_changes(feature_df: pd.DataFrame) -> None:
    """
    Track significant feature changes for monitoring.
    
    Args:
        feature_df: DataFrame with features
    """
    if feature_df.empty or len(feature_df) < 2:
        return
    
    # Only track important features
    important_features = [
        "oi_velocity", "net_gamma", "trap_probability",
        "divergence_score", "wall_strength", "put_call_ratio"
    ]
    
    current = feature_df.iloc[-1]
    previous = feature_df.iloc[-2] if len(feature_df) > 1 else current
    
    changes = []
    for feature in important_features:
        if feature in current and feature in previous:
            old_val = previous[feature]
            new_val = current[feature]
            
            if old_val is not None and new_val is not None and old_val != 0:
                change_pct = ((new_val - old_val) / abs(old_val)) * 100
                
                # Only track significant changes (>10%)
                if abs(change_pct) > 10:
                    changes.append({
                        "timestamp": current["timestamp"],
                        "feature_name": feature,
                        "old_value": old_val,
                        "new_value": new_val,
                        "change_pct": change_pct,
                        "created_at": datetime.utcnow().isoformat()
                    })
    
    if changes:
        try:
            with get_connection() as conn:
                for change in changes:
                    conn.execute("""
                        INSERT INTO feature_history 
                        (timestamp, feature_name, old_value, new_value, change_pct, created_at)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (
                        change["timestamp"], change["feature_name"],
                        change["old_value"], change["new_value"],
                        change["change_pct"], change["created_at"]
                    ))
        except Exception as e:
            print(f"‚ùå Error tracking feature changes: {e}")

# =========================
# DATABASE MAINTENANCE
# =========================

def cleanup_old_data(days_to_keep: int = 30) -> Dict:
    """
    Clean up old data from database.
    
    Args:
        days_to_keep: Number of days of data to keep
    
    Returns:
        Dictionary with cleanup statistics
    """
    cutoff_date = (datetime.utcnow() - timedelta(days=days_to_keep)).isoformat()
    
    cleanup_stats = {}
    
    try:
        with get_connection() as conn:
            # Clean old market features
            result = conn.execute(
                "DELETE FROM market_features WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["market_features"] = result.rowcount
            
            # Clean old signals
            result = conn.execute(
                "DELETE FROM signals WHERE created_at < ? AND status = 'EXPIRED'",
                (cutoff_date,)
            )
            cleanup_stats["signals"] = result.rowcount
            
            # Clean old system health logs
            result = conn.execute(
                "DELETE FROM system_health WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["system_health"] = result.rowcount
            
            # Clean old research analytics
            result = conn.execute(
                "DELETE FROM research_analytics WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["research_analytics"] = result.rowcount
            
            # Clean old feature history
            result = conn.execute(
                "DELETE FROM feature_history WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["feature_history"] = result.rowcount
            
            # Vacuum database
            conn.execute("VACUUM")
            
            log_system_health(
                "database",
                "CLEANED",
                f"Cleaned up {sum(cleanup_stats.values())} old records",
                json.dumps(cleanup_stats)
            )
            
            return cleanup_stats
            
    except Exception as e:
        print(f"‚ùå Error during cleanup: {e}")
        log_system_health(
            "database",
            "CLEANUP_FAILED",
            f"Cleanup failed: {str(e)}"
        )
        return {}
# =========================
# DATABASE INFO & STATS
# =========================

def get_database_stats() -> Dict:
    """Get database statistics."""
    stats = {}
    
    try:
        with get_connection() as conn:
            # Table counts
            tables = [
                "market_features", "signals", "model_registry",
                "system_health", "research_analytics", "feature_history"
            ]
            
            for table in tables:
                try:
                    cur = conn.execute(f"SELECT COUNT(*) as count FROM {table}")
                    row = cur.fetchone()
                    stats[f"{table}_count"] = row["count"] if row else 0
                except:
                    stats[f"{table}_count"] = 0
            
            # Latest feature timestamp
            try:
                cur = conn.execute("SELECT MAX(timestamp) as latest FROM market_features")
                row = cur.fetchone()
                stats["latest_feature"] = row["latest"] if row else None
            except:
                stats["latest_feature"] = None
            
            # Feature version
            stats["feature_version"] = get_current_feature_version(conn) or "unknown"
            
            # Database size
            if DB_PATH.exists():
                db_size = DB_PATH.stat().st_size
                stats["database_size_mb"] = round(db_size / (1024 * 1024), 2)
            else:
                stats["database_size_mb"] = 0
            
            return stats
            
    except Exception as e:
        print(f"‚ùå Error getting database stats: {e}")
        return {}


# =========================
# INITIALIZATION
# =========================

def initialize_storage():
    """
    Initialize storage system.
    """
    print("Initializing storage system...")
    initialize_database()
    
    # Check and run migrations if needed
    current_version = get_database_stats().get("feature_version", "v1.0")
    if current_version != FEATURE_VERSION:
        print(f"Migrating from {current_version} to {FEATURE_VERSION}")
        migrate_database(FEATURE_VERSION)
    
    print("‚úì Storage system initialized")

# Run initialization when module is imported
initialize_storage()



====================================================================================================
FILE: .\ui\charts.py
====================================================================================================


File Name: charts.py
Full Path: G:\trading_app\ui\charts.py
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .py




====================================================================================================
FILE: .\ui\heatmap.py
====================================================================================================


File Name: heatmap.py
Full Path: G:\trading_app\ui\heatmap.py
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .py




====================================================================================================
FILE: .\ui\option_chain.py
====================================================================================================


File Name: option_chain.py
Full Path: G:\trading_app\ui\option_chain.py
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .py




====================================================================================================
FILE: .\ui\signals.py
====================================================================================================


File Name: signals.py
Full Path: G:\trading_app\ui\signals.py
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .py




====================================================================================================
FILE: .\utils\expiry_utils.py
====================================================================================================


File Name: expiry_utils.py
Full Path: G:\trading_app\utils\expiry_utils.py
Size: 2.46 KB
Last Modified: 01/27/2026 23:57:39
Extension: .py

"""
Utility functions for expiry date selection
"""
from datetime import datetime, time
from data.instrument_master import get_available_expiries

def is_market_open() -> bool:
    """
    Check if market is currently open.
    NSE market hours: 9:15 AM to 3:30 PM IST, Monday to Friday
    """
    now = datetime.now()
    
    # Check if it's a weekday
    if now.weekday() >= 5:  # Saturday (5) or Sunday (6)
        return False
    
    # Check time
    market_open = time(9, 15)  # 9:15 AM IST
    market_close = time(15, 30)  # 3:30 PM IST
    
    current_time = now.time()
    return market_open <= current_time <= market_close

def get_trading_expiry(underlying: str) -> str:
    """
    Get appropriate expiry date for trading.
    - If market is open and today is expiry: use today
    - If market is closed and today is expiry: use next expiry
    - Otherwise: use nearest expiry
    """
    expiries = get_available_expiries(underlying)
    if not expiries:
        return ""
    
    today = datetime.now().strftime('%Y-%m-%d')
    
    # Check if today is an expiry day
    if today in expiries:
        if is_market_open():
            # Market is open on expiry day - use today
            return today
        else:
            # Market closed on expiry day - use next expiry
            for expiry in sorted(expiries):
                if expiry > today:
                    return expiry
            return expiries[-1]  # Fallback to last expiry
    else:
        # Today is not expiry day - get nearest future expiry
        for expiry in sorted(expiries):
            if expiry >= today:
                return expiry
        return expiries[-1]  # Fallback to last expiry

def get_expiry_for_backtesting(underlying: str, reference_date: str = None) -> str:
    """
    Get expiry for backtesting (can use historical expiries).
    
    Args:
        underlying: Underlying symbol
        reference_date: Reference date in 'YYYY-MM-DD' format (default: today)
    
    Returns:
        Expiry date for backtesting
    """
    expiries = get_available_expiries(underlying)
    if not expiries:
        return ""
    
    if reference_date:
        ref_date = reference_date
    else:
        ref_date = datetime.now().strftime('%Y-%m-%d')
    
    # Find the nearest expiry on or after reference date
    for expiry in sorted(expiries):
        if expiry >= ref_date:
            return expiry
    
    return expiries[-1]



****************************************************************************************************
CATEGORY: Documentation (1 files)
****************************************************************************************************


====================================================================================================
FILE: .\codebase_export.txt
====================================================================================================


File Name: codebase_export.txt
Full Path: G:\trading_app\codebase_export.txt
Size: 1026.53 KB
Last Modified: 01/28/2026 15:51:25
Extension: .txt


PROJECT OVERVIEW
Generated: 2026-01-28 15:51:25
Base Path: G:\trading_app


FILE STATISTICS
Total Files: 42
.py: 34 files
.json: 2 files
.txt: 1 files
.bat: 1 files
.key: 1 files
.enc: 1 files
.sql: 1 files
.db: 1 files

----------------------------------------------------------------------------------------------------


****************************************************************************************************
CATEGORY: Source Code Files (34 files)
****************************************************************************************************


====================================================================================================
FILE: .\app - Copy.py
====================================================================================================


File Name: app - Copy.py
Full Path: G:\trading_app\app - Copy.py
Size: 74.67 KB
Last Modified: 01/28/2026 14:12:50
Extension: .py

"""
ENHANCED TRADING TOOL - Research-Based Live Feature Engine
Incorporates OI Velocity, Gamma Exposure, Walls/Traps, and Divergence analysis.
"""

import os
import sys

# Fix Conda environment issue
if 'conda' in sys.version.lower():
    try:
        # Try to fix conda path issues
        os.environ['PATH'] = os.path.join(os.environ.get('CONDA_PREFIX', ''), 'bin') + ':' + os.environ['PATH']
    except:
        pass
# ==============================
# STREAMLIT CONFIG FIX - ADD THIS
# ==============================
import os
os.environ['STREAMLIT_SERVER_FILE_WATCHER_TYPE'] = 'none'
import time 
import threading  # <-- ADD THIS LINE
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from datetime import datetime, timezone
import json
from pathlib import Path
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Enhanced core components
from core.session import UpstoxSession, initialize_session, display_session_status
from data.upstox_client import UpstoxClient, MarketAnalytics, display_market_analytics
from data.instrument_master import get_option_keys

try:
    from core.feature_pipeline import build_and_store_features, create_feature_summary
except ImportError as e:
    st.error(f"Feature pipeline import error: {e}")
    # Define dummy functions to prevent crash
    def build_and_store_features(*args, **kwargs):
        return {"error": "Feature pipeline not loaded"}
    def create_feature_summary(*args, **kwargs):
        return {"error": "Feature pipeline not loaded"}
from core.scheduler import MarketScheduler
from core.signals.state_machine import SignalStateMachine, SignalValidator, create_signal_engine
from features.breadth import build_constituents_df
from ml.feature_contract import FEATURE_VERSION

# Enhanced storage
from storage.repository import (
    fetch_latest_features,
    insert_signal,
    signal_exists,
    validate_new_signals,
    expire_old_signals,
    fetch_active_signals,
    fetch_recent_analytics,
    get_database_stats,
    fetch_signal_performance,
    insert_research_analytics
)

# ==============================
# PAGE CONFIGURATION
# ==============================

st.set_page_config(
    page_title="Algorithmic Trading Research Platform",
    page_icon="üìà",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E3A8A;
        font-weight: 700;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #374151;
        font-weight: 600;
        margin-top: 1.5rem;
        margin-bottom: 1rem;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
    }
    .research-insight {
        background: #F0F9FF;
        border-left: 4px solid #3B82F6;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
    .trap-alert {
        background: #FEF3C7;
        border-left: 4px solid #F59E0B;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
    .divergence-alert {
        background: #FEE2E2;
        border-left: 4px solid #EF4444;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
</style>
""", unsafe_allow_html=True)

# ==============================
# APP HEADER
# ==============================

st.markdown('<div class="main-header">üìä Algorithmic Trading Research Platform</div>', unsafe_allow_html=True)
st.markdown("""
<div style='color: #6B7280; margin-bottom: 2rem;'>
    Advanced derivatives analytics with OI Velocity, Gamma Exposure, Structural Walls/Traps, and Spot Divergence detection.
    Version: Research v2.0
</div>
""", unsafe_allow_html=True)

# ==============================
# INITIALIZATION
# ==============================

def initialize_app_state():
    """Initialize application state with research components."""
    
    # Initialize session
    initialize_session()
    
    # Initialize state variables
    if "initialized" not in st.session_state:
        st.session_state.initialized = True
        
        # Market data series
        for key in ["price_series", "volume_series", "ccc_history", "oi_velocity_history", "gamma_history"]:
            if key not in st.session_state:
                st.session_state[key] = pd.Series(dtype=float)
        
        # Research analytics
        if "research_analytics" not in st.session_state:
            st.session_state.research_analytics = []
        
        # Signal engine
        if "signal_engine" not in st.session_state:
            st.session_state.signal_engine = create_signal_engine(
                signal_expiry_minutes=5,
                confidence_threshold=0.2
            )
        
        # Scheduler
        if "scheduler" not in st.session_state:
            st.session_state.scheduler = MarketScheduler(interval_seconds=30)
            # Initialize scheduler properties
            st.session_state.scheduler._last_run = None
            st.session_state.scheduler._total_executions = 0
        
        # Load Nifty weights
        if "nifty_weights" not in st.session_state:
            try:
                with open("config/nifty_weights.json", "r") as f:
                    st.session_state.nifty_weights = json.load(f)
            except:
                st.session_state.nifty_weights = {}
        
        # Instrument master path
        if "instrument_master" not in st.session_state:
            st.session_state.instrument_master = Path("G:/trading_app/data/instruments.json.gz")
        
        # Configuration - SMART EXPIRY SELECTION
        if "config" not in st.session_state:
            from utils.expiry_utils import get_trading_expiry
            
            # Get smart expiry
            underlying = "NIFTY"
            expiry_date_str = get_trading_expiry(underlying)
            
            if not expiry_date_str:
                # Fallback to next available date from instrument master
                from data.instrument_master import get_next_available_expiry
                expiry_date_str = get_next_available_expiry(underlying)
                
                if not expiry_date_str:
                    # Final fallback to a known date
                    expiry_date_str = "2026-02-03"
            
            # Convert string to datetime object
            expiry_date = datetime.strptime(expiry_date_str, "%Y-%m-%d")
            
            st.session_state.config = {
                "index_symbol": "NSE_INDEX|Nifty 50",
                "underlying": underlying,
                "expiry_date": expiry_date,
                "max_option_keys": 200
            }
            
            # Show expiry info
            from utils.expiry_utils import is_market_open
            market_status = "open" if is_market_open() else "closed"
            
            # Get today's date
            today_str = datetime.now().strftime('%Y-%m-%d')
            
            # Show appropriate message
            if expiry_date_str == today_str:
                st.info(f"üìÖ Using TODAY's expiry: {expiry_date_str} (Market: {market_status})")
            else:
                st.info(f"üìÖ Using NEXT expiry: {expiry_date_str} (Today: {today_str}, Market: {market_status})")
        
        # Option keys cache
        if "option_keys" not in st.session_state:
            st.session_state.option_keys = []
        
        # Market regime tracking
        if "market_regime" not in st.session_state:
            st.session_state.market_regime = "UNKNOWN"
        
        # Trap detection history
        if "trap_detections" not in st.session_state:
            st.session_state.trap_detections = []
        
        st.success("‚úÖ Application state initialized with research components")

# Run initialization
initialize_app_state()

# ==============================
# AUTHENTICATION
# ==============================

st.markdown('<div class="sub-header">üîê Authentication</div>', unsafe_allow_html=True)

# Authenticate - this will show login button if not authenticated
access_token = UpstoxSession.authenticate()

# CRITICAL: Check if authenticate() actually returned a token
# If it returns None, we need to stop execution here
if not access_token:
    # authenticate() should have shown login button and stopped with st.stop()
    # But if we reach here, something went wrong
    st.error("‚ùå Authentication required. Please log in to continue.")
    
    # Show login button manually as fallback
    login_url = UpstoxSession.get_login_url()
    st.markdown(f"""
    <div style="text-align: center; margin: 20px 0;">
        <a href="{login_url}" target="_blank">
            <button style="
                background-color: #00d09c;
                color: white;
                padding: 15px 30px;
                font-size: 18px;
                font-weight: bold;
                border: none;
                border-radius: 10px;
                cursor: pointer;
                width: 60%;
                margin: 20px 0;
                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            ">
            üìà Click to Login with Upstox
            </button>
        </a>
    </div>
    """, unsafe_allow_html=True)
    
    # STOP the app here
    st.stop()

# If we reach here, we have a token
# Initialize client
try:
    client = UpstoxClient(access_token)
    st.success("‚úÖ Upstox client initialized")
        
except Exception as e:
    st.error(f"‚ùå Error initializing Upstox client: {e}")
    st.stop()

# ==============================
# SIDEBAR - SESSION STATUS
# ==============================

with st.sidebar:
    st.markdown("## üß≠ Navigation")
    
    # Display session status
    display_session_status()
    
    # Database stats
    st.markdown("---")
    st.markdown("### üìä Database")
    
    db_stats = get_database_stats()
    if db_stats:
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Features", f"{db_stats.get('market_features_count', 0):,}")
        with col2:
            st.metric("Signals", f"{db_stats.get('signals_count', 0):,}")
    
    # Market hours info
    st.markdown("---")
    st.markdown("### üïí Market Hours")
    
    scheduler: MarketScheduler = st.session_state.scheduler
    now = datetime.now()
    
    if scheduler._is_market_open(now):
        st.success("‚úÖ Market Open")
        next_run = scheduler._last_run + timedelta(seconds=scheduler.interval_seconds) if scheduler._last_run else now
        time_to_next = (next_run - now).total_seconds()
        if time_to_next > 0:
            st.info(f"Next cycle in: {int(time_to_next)}s")
    else:
        st.warning("‚è∏Ô∏è Market Closed")
    
    # Quick actions
    st.markdown("---")
    st.markdown("### ‚ö° Quick Actions")
    
    if st.button("üîÑ Clear Cache", width='stretch'):
        for key in list(st.session_state.keys()):
            if key not in ["initialized", "instrument_master", "config", "nifty_weights"]:
                del st.session_state[key]
        st.rerun()
    
    if st.button("üìä View Performance", width='stretch'):
        st.session_state.show_performance = True
    
    if st.button("üõ†Ô∏è System Logs", width='stretch'):
        st.session_state.show_logs = True

def retry_on_failure(max_retries=3, delay=5):
    """Decorator for retrying failed executions"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    print(f"‚ö†Ô∏è Attempt {attempt + 1}/{max_retries} failed: {e}")
                    if attempt < max_retries - 1:
                        print(f"‚è∏Ô∏è Retrying in {delay}s...")
                        time.sleep(delay)
                    else:
                        print(f"‚ùå All {max_retries} attempts failed")
                        raise
            return None
        return wrapper
    return decorator

def log_execution_time(func):
    """Decorator for logging execution time"""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        print(f"‚è±Ô∏è Starting {func.__name__}...")
        
        result = func(*args, **kwargs)
        
        execution_time = time.time() - start_time
        print(f"‚è±Ô∏è {func.__name__} completed in {execution_time:.2f}s")
        
        return result
    return wrapper
# ==============================
# RESEARCH EXECUTION CYCLE
# ==============================

@log_execution_time
@retry_on_failure(max_retries=2, delay=3)
def execute_research_cycle(config=None, nifty_weights=None, option_keys=None):
    """
    Enhanced execution cycle with research analytics.
    Returns comprehensive research data.
    """
    # Use provided parameters or fall back to session state
    if config is None:
        try:
            config = st.session_state.config
        except (KeyError, AttributeError):
            print("‚ö†Ô∏è Config not found in session state")
            return None
    
    if nifty_weights is None:
        nifty_weights = st.session_state.get("nifty_weights", {})
    
    if option_keys is None:
        option_keys = st.session_state.get("option_keys", [])
    
    research_data = {}
    
    try:
        # ------------------------------
        # 1. INDEX QUOTE
        # ------------------------------
        print(f"üìä Fetching index quote for {config.get('index_symbol', 'NSE_INDEX|Nifty 50')}")
        index_data = client.fetch_index_quote(config["index_symbol"])
        
        if not index_data or index_data.get("ltp") is None:
            print("‚ùå No index data received")
            return None
        
        ltp = float(index_data["ltp"])
        open_price = float(index_data["open"])
        volume = 0.0  # index has no volume
        
        research_data["index"] = {
            "ltp": ltp,
            "open": open_price,
            "change": index_data.get("percent_change", 0),
            "timestamp": index_data.get("timestamp", datetime.now(timezone.utc).isoformat())
        }
        
        print(f"‚úÖ Index data: {ltp} (Change: {index_data.get('percent_change', 0)}%)")
        
        # ------------------------------
        # 2. UPDATE PRICE SERIES
        # ------------------------------
        # Create new series safely
        try:
            if "price_series" not in st.session_state:
                st.session_state.price_series = pd.Series(dtype=float)
            
            if "volume_series" not in st.session_state:
                st.session_state.volume_series = pd.Series(dtype=float)
            
            # Update price series
            new_price_series = pd.concat([
                st.session_state.price_series, 
                pd.Series([ltp], index=[datetime.now()])
            ]).tail(200)
            
            # Update volume series
            new_volume_series = pd.concat([
                st.session_state.volume_series, 
                pd.Series([volume], index=[datetime.now()])
            ]).tail(200)
            
            # Store back to session state
            st.session_state.price_series = new_price_series
            st.session_state.volume_series = new_volume_series
            
            print(f"üìà Price series updated: {len(st.session_state.price_series)} data points")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error updating price series: {e}")
            # Initialize fresh series
            st.session_state.price_series = pd.Series([ltp], index=[datetime.now()])
            st.session_state.volume_series = pd.Series([volume], index=[datetime.now()])
        
        # ------------------------------
        # 3. BREADTH ANALYSIS (CCC)
        # ------------------------------
        print("üìä Starting breadth analysis...")
        try:
            if not nifty_weights:
                print("‚ö†Ô∏è No Nifty weights found, skipping breadth analysis")
                ccc_value = 0.0
                constituents_df = pd.DataFrame()
            else:
                equity_quotes_df = client.fetch_equity_quotes(
                    symbols=list(nifty_weights.keys())
                )
                
                constituents_df = build_constituents_df(
                    equity_quotes_df=equity_quotes_df,
                    weights=nifty_weights
                )
                
                if not constituents_df.empty:
                    ccc_value = (
                        constituents_df["weight"] * constituents_df["price_change"]
                    ).sum()
                    print(f"‚úÖ CCC Value: {ccc_value:.4f}")
                else:
                    ccc_value = 0.0
                    print("‚ö†Ô∏è Empty constituents dataframe")
        except Exception as e:
            print(f"‚ö†Ô∏è Breadth analysis error: {e}")
            ccc_value = 0.0
            constituents_df = pd.DataFrame()
        
        # Update CCC history
        try:
            if "ccc_history" not in st.session_state:
                st.session_state.ccc_history = pd.Series(dtype=float)
            
            st.session_state.ccc_history = pd.concat([
                st.session_state.ccc_history, 
                pd.Series([ccc_value], index=[datetime.now()])
            ]).tail(200)
        except:
            st.session_state.ccc_history = pd.Series([ccc_value], index=[datetime.now()])
        
        research_data["breadth"] = {
            "ccc_value": ccc_value,
            "constituents_count": len(constituents_df),
            "positive_constituents": len(constituents_df[constituents_df["price_change"] > 0]) if not constituents_df.empty else 0
        }
        
        # ------------------------------
        # 4. OPTION INSTRUMENT DISCOVERY
        # ------------------------------
        print("üîç Fetching option instruments...")
        try:
            if not option_keys:
                expiry_str = config["expiry_date"].strftime("%Y-%m-%d")
                option_keys = get_option_keys(
                    underlying=config["underlying"],
                    expiry=expiry_str,
                    max_keys=config.get("max_option_keys", 200)
                )
                st.session_state.option_keys = option_keys
                print(f"‚úÖ Found {len(option_keys)} option keys")
        except Exception as e:
            print(f"‚ö†Ô∏è Option discovery error: {e}")
            option_keys = []
            st.session_state.option_keys = []
        
        if not option_keys:
            print("‚ö†Ô∏è No option keys found")
            return research_data  # Return what we have so far
        
        # ------------------------------
        # 5. ENHANCED OPTION CHAIN ANALYSIS
        # ------------------------------
        print("üìä Analyzing option chain...")
        option_analytics = client.fetch_option_chain_with_analytics(
            option_keys,
            ltp
        )
        
        if not option_analytics or "raw_data" not in option_analytics:
            print("‚ùå Failed to fetch option analytics")
            return research_data  # Return what we have so far
        
        option_chain_df = option_analytics["raw_data"]
        analytics = option_analytics.get("analytics", {})
        insights = option_analytics.get("market_insights", [])
        
        research_data["options"] = {
            "analytics": analytics,
            "insights": insights,
            "chain_size": len(option_chain_df),
            "put_call_ratio": analytics.get("put_call_ratio", 1.0) if isinstance(analytics.get("put_call_ratio"), (int, float)) else 1.0
        }
        
        print(f"‚úÖ Option chain analyzed: {len(option_chain_df)} strikes, PCR: {analytics.get('put_call_ratio', 'N/A')}")
        
        # Store analytics for visualization
        if analytics:
            try:
                if "research_analytics" not in st.session_state:
                    st.session_state.research_analytics = []
                
                st.session_state.research_analytics.append({
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    **analytics
                })
                
                # Keep only recent analytics
                if len(st.session_state.research_analytics) > 100:
                    st.session_state.research_analytics = st.session_state.research_analytics[-100:]
                
                # Update market regime
                st.session_state.market_regime = analytics.get("market_regime", "UNKNOWN")
                
                # Store in database
                insert_research_analytics(analytics)
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error storing analytics: {e}")
        
        # ------------------------------
        # 6. FEATURE PIPELINE
        # ------------------------------
        print("‚öôÔ∏è Running feature pipeline...")
        try:
            snapshot_ts = pd.Timestamp.utcnow().floor("ms")
            
            feature_result = build_and_store_features(
                timestamp=snapshot_ts,
                option_chain_df=option_chain_df,
                spot_price=ltp,
                expiry_datetime=config["expiry_date"],
                
                ltp=ltp,
                vwap=open_price,
                price_series=st.session_state.price_series,
                volume_series=st.session_state.volume_series,
                
                constituents_df=constituents_df,
                ccc_history=st.session_state.ccc_history,
                
                # Enhanced parameters
                client=client,
                option_keys=option_keys
            )
            
            if feature_result:
                research_data["features"] = feature_result.get("features", {})
                research_data["feature_summary"] = create_feature_summary(feature_result["features"])
                print("‚úÖ Feature pipeline completed")
            else:
                print("‚ö†Ô∏è Feature pipeline returned empty result")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Feature pipeline error: {e}")
            import traceback
            traceback.print_exc()
        
        # ------------------------------
        # 7. SIGNAL GENERATION
        # ------------------------------
        print("üéØ Checking for signals...")
        try:
            feature_row = fetch_latest_features(FEATURE_VERSION)
            if feature_row is not None and not feature_row.empty:
                # Check if signal already exists
                timestamp_str = feature_row.iloc[0]["timestamp"]
                
                if not signal_exists(timestamp_str):
                    # Generate signal
                    signal_engine: SignalStateMachine = st.session_state.signal_engine
                    signal = signal_engine.build_signal(feature_row.iloc[0])
                    
                    # Validate signal
                    is_valid, reason = SignalValidator.validate_signal(signal, feature_row.iloc[0])
                    
                    if is_valid and signal["signal_type"] != "NEUTRAL":
                        # Add research context
                        signal["research_validation"] = {
                            "is_valid": is_valid,
                            "reason": reason,
                            "timestamp": datetime.now(timezone.utc).isoformat()
                        }
                        
                        # Insert signal
                        insert_signal(signal)
                        
                        research_data["signal"] = {
                            "generated": True,
                            "type": signal["signal_type"],
                            "confidence": signal["confidence"],
                            "strength": signal.get("signal_strength", "UNKNOWN")
                        }
                        print(f"‚úÖ Signal generated: {signal['signal_type']} ({signal['confidence']:.0%})")
                    else:
                        research_data["signal"] = {
                            "generated": False,
                            "reason": reason if not is_valid else "NEUTRAL signal"
                        }
                        print(f"‚ÑπÔ∏è No valid signal: {reason if not is_valid else 'NEUTRAL'}")
            else:
                print("‚ÑπÔ∏è No features available for signal generation")
        except Exception as e:
            print(f"‚ö†Ô∏è Signal generation error: {e}")
        
        # ------------------------------
        # 8. SIGNAL LIFECYCLE MANAGEMENT
        # ------------------------------
        try:
            validate_new_signals(confidence_threshold=0.2)
            expire_old_signals()
        except Exception as e:
            print(f"‚ö†Ô∏è Signal lifecycle error: {e}")
        
        # ------------------------------
        # 9. TRAP DETECTION
        # ------------------------------
        if analytics and "potential_traps" in analytics:
            traps = analytics["potential_traps"]
            if traps:
                try:
                    if "trap_detections" not in st.session_state:
                        st.session_state.trap_detections = []
                    
                    for trap in traps:
                        if trap.get("confidence", 0) > 0.6:
                            st.session_state.trap_detections.append({
                                "timestamp": datetime.now(timezone.utc).isoformat(),
                                "trap": trap,
                                "market_regime": analytics.get("market_regime")
                            })
                    
                    # Keep only recent traps
                    if len(st.session_state.trap_detections) > 20:
                        st.session_state.trap_detections = st.session_state.trap_detections[-20:]
                    
                    research_data["traps"] = traps
                    
                    if traps:
                        print(f"üéØ Detected {len(traps)} potential traps")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Trap detection error: {e}")
        
        # ------------------------------
        # 10. SUCCESSFUL COMPLETION
        # ------------------------------
        print("‚úÖ Research cycle completed successfully")
        return research_data
        
    except Exception as e:
        print(f"‚ùå Critical error in research cycle: {e}")
        import traceback
        traceback.print_exc()
        return None

# ==============================
# ENHANCED AUTO-REFRESH SYSTEM
# ==============================

class AutoRefreshManager:
    """Robust auto-refresh manager with error handling and state persistence"""
    
    def __init__(self, scheduler, config, nifty_weights, option_keys):
        self.scheduler = scheduler
        self.config = config
        self.nifty_weights = nifty_weights
        self.option_keys = option_keys
        self.last_execution_time = None
        self.execution_count = 0
        self.errors = []
        self.max_retries = 3
        self.retry_delay = 5
        
    def should_execute(self):
        """Determine if execution should run based on time and market conditions"""
        now = datetime.now()
        
        # Check market hours
        if not self.scheduler._is_market_open(now):
            return False, "Market closed"
        
        # Check if first run
        if self.last_execution_time is None:
            return True, "First execution"
        
        # Check time since last execution
        time_since_last = (now - self.last_execution_time).total_seconds()
        
        # Debug log to see what's happening
        print(f"üïí Auto-refresh timing: {int(time_since_last)}s since last, interval: {self.scheduler.interval_seconds}s")
        
        if time_since_last >= self.scheduler.interval_seconds:
            return True, f"Interval reached ({int(time_since_last)}s > {self.scheduler.interval_seconds}s)"
        
        return False, f"Not due yet ({int(self.scheduler.interval_seconds - time_since_last)}s remaining)"
    
    def execute_with_retry(self):
        """Execute research cycle with retry logic"""
        for attempt in range(self.max_retries):
            try:
                print(f"üîÑ Attempt {attempt + 1}/{self.max_retries} to execute research cycle")
                
                result = execute_research_cycle(
                    config=self.config,
                    nifty_weights=self.nifty_weights,
                    option_keys=self.option_keys
                )
                
                if result:
                    self.last_execution_time = datetime.now()
                    self.execution_count += 1
                    return True, result, f"Execution successful (attempt {attempt + 1})"
                else:
                    return False, None, "Research cycle returned no data"
                    
            except Exception as e:
                error_msg = f"Attempt {attempt + 1} failed: {str(e)}"
                self.errors.append({
                    "timestamp": datetime.now(),
                    "attempt": attempt + 1,
                    "error": str(e)
                })
                
                if attempt < self.max_retries - 1:
                    print(f"‚è∏Ô∏è Retrying in {self.retry_delay}s...")
                    time.sleep(self.retry_delay)
                else:
                    return False, None, f"All retries failed. Last error: {str(e)}"
        
        return False, None, "Max retries exceeded"
    
    def get_status(self):
        """Get current status of auto-refresh manager"""
        status = {
            "last_execution": self.last_execution_time.strftime("%H:%M:%S") if self.last_execution_time else "Never",
            "execution_count": self.execution_count,
            "is_market_open": self.scheduler._is_market_open(datetime.now()),
            "interval_seconds": self.scheduler.interval_seconds,
            "error_count": len(self.errors)
        }
        
        if self.last_execution_time:
            time_since = (datetime.now() - self.last_execution_time).total_seconds()
            status["time_since_last"] = f"{int(time_since)}s"
            status["time_until_next"] = f"{max(0, int(self.scheduler.interval_seconds - time_since))}s"
        
        return status

# Initialize auto-refresh manager
if "auto_refresh_manager" not in st.session_state:
    st.session_state.auto_refresh_manager = AutoRefreshManager(
        scheduler=st.session_state.scheduler,
        config=st.session_state.config,
        nifty_weights=st.session_state.nifty_weights,
        option_keys=st.session_state.option_keys
    )

# Execute auto-refresh
def execute_auto_refresh():
    """Main auto-refresh execution function"""
    manager = st.session_state.auto_refresh_manager
    
    try:
        # Check if we should execute
        should_execute, reason = manager.should_execute()
        
        if should_execute:
            print(f"üèÉ‚Äç‚ôÇÔ∏è Auto-execution triggered: {reason}")
            
            # Execute with retry logic
            success, result, message = manager.execute_with_retry()
            
            if success:
                print(f"‚úÖ {message}")
                
                # Update UI state if needed
                if result and "options" in result and "analytics" in result["options"]:
                    analytics = result["options"]["analytics"]
                    if analytics:
                        # Update market regime
                        st.session_state.market_regime = analytics.get("market_regime", "UNKNOWN")
                        
                        # Store analytics
                        if "research_analytics" not in st.session_state:
                            st.session_state.research_analytics = []
                        st.session_state.research_analytics.append({
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                            **analytics
                        })
                        # Keep only recent analytics
                        if len(st.session_state.research_analytics) > 100:
                            st.session_state.research_analytics = st.session_state.research_analytics[-100:]
                
                return True, message
            else:
                print(f"‚ùå Auto-execution failed: {message}")
                return False, message
        else:
            # Log status periodically
            if manager.last_execution_time:
                time_since = (datetime.now() - manager.last_execution_time).total_seconds()
                if int(time_since) % 10 == 0:  # Log every 10 seconds
                    print(f"‚è±Ô∏è Auto-refresh status: {reason}")
            return None, reason
            
    except Exception as e:
        error_msg = f"Auto-refresh system error: {str(e)}"
        print(f"üî• {error_msg}")
        manager.errors.append({
            "timestamp": datetime.now(),
            "error": error_msg
        })
        return False, error_msg

# Run auto-refresh
execute_auto_refresh()

# ==============================
# ENHANCED BACKGROUND SCHEDULER
# ==============================

class BackgroundScheduler:
    """Robust background scheduler with error handling and monitoring"""
    
    def __init__(self, check_interval=5):
        self.check_interval = check_interval
        self.running = False
        self.thread = None
        self.errors = []
        self.last_check = None
        self.health_checks = []
        
    def health_check(self):
        """Perform system health check"""
        checks = []
        
        # Check scheduler
        try:
            scheduler = st.session_state.scheduler
            checks.append({
                "component": "Scheduler",
                "status": "HEALTHY" if scheduler else "UNKNOWN",
                "details": f"Interval: {scheduler.interval_seconds}s" if scheduler else "Not initialized"
            })
        except Exception as e:
            checks.append({
                "component": "Scheduler",
                "status": "ERROR",
                "details": str(e)
            })
        
        # Check auto-refresh manager
        try:
            manager = st.session_state.auto_refresh_manager
            checks.append({
                "component": "AutoRefresh",
                "status": "HEALTHY",
                "details": f"Executions: {manager.execution_count}"
            })
        except Exception as e:
            checks.append({
                "component": "AutoRefresh",
                "status": "ERROR",
                "details": str(e)
            })
        
        self.health_checks = checks
        return checks
    
    def start(self):
        """Start background scheduler thread"""
        if self.running:
            print("‚ö†Ô∏è Background scheduler already running")
            return
        
        def background_task():
            print("üöÄ Starting enhanced background scheduler...")
            self.running = True
            
            while self.running:
                try:
                    self.last_check = datetime.now()
                    
                    # Perform health check every minute
                    if not self.health_checks or (datetime.now() - self.last_check).seconds > 60:
                        self.health_check()
                    
                    # Log status every 30 seconds
                    current_time = datetime.now()
                    if current_time.second % 30 == 0:
                        manager = st.session_state.auto_refresh_manager
                        if manager.last_execution_time:
                            time_since = (current_time - manager.last_execution_time).total_seconds()
                            print(f"üìä Background monitor: Last execution {int(time_since)}s ago, {manager.execution_count} total runs")
                    
                    time.sleep(self.check_interval)
                    
                except Exception as e:
                    error_msg = f"Background scheduler error: {str(e)}"
                    print(f"üî• {error_msg}")
                    self.errors.append({
                        "timestamp": datetime.now(),
                        "error": error_msg
                    })
                    time.sleep(30)  # Wait longer on error
        
        self.thread = threading.Thread(target=background_task, daemon=True)
        self.thread.start()
        print("‚úÖ Enhanced background scheduler started")
    
    def stop(self):
        """Stop background scheduler"""
        self.running = False
        if self.thread:
            self.thread.join(timeout=5)
        print("üõë Background scheduler stopped")
    
    def get_status(self):
        """Get scheduler status"""
        return {
            "running": self.running,
            "check_interval": self.check_interval,
            "last_check": self.last_check.strftime("%H:%M:%S") if self.last_check else "Never",
            "error_count": len(self.errors),
            "health_checks": self.health_checks
        }

# Initialize and start background scheduler
if "background_scheduler" not in st.session_state:
    st.session_state.background_scheduler = BackgroundScheduler(check_interval=5)
    st.session_state.background_scheduler.start()


def get_scheduler_status():
    """Get comprehensive scheduler status"""
    scheduler = st.session_state.scheduler
    manager = st.session_state.auto_refresh_manager
    
    status = {
        "scheduler_active": True,
        "interval_seconds": scheduler.interval_seconds,
        "total_executions": scheduler._total_executions,
        "last_run": scheduler._last_run.strftime("%H:%M:%S") if scheduler._last_run else "Never",
        "is_market_open": scheduler._is_market_open(datetime.now()),
        "auto_refresh_status": manager.get_status()
    }
    
    # Calculate next run time
    if scheduler._last_run:
        next_run = scheduler._last_run + timedelta(seconds=scheduler.interval_seconds)
        time_until_next = (next_run - datetime.now()).total_seconds()
        status["next_run"] = next_run.strftime("%H:%M:%S") if time_until_next > 0 else "Now"
        status["time_until_next"] = f"{max(0, int(time_until_next))}s"
    else:
        status["next_run"] = "Pending"
        status["time_until_next"] = "N/A"
    
    return status

# ==============================
# CREATE TABS
# ==============================

# Create tabs for different views
tab1, tab2, tab3, tab4 = st.tabs([
    "üìà Live Analytics", 
    "üéØ Signals", 
    "üîç Research", 
    "‚öôÔ∏è Configuration"
])

# ==============================
# AUTO-REFRESH WORKING SOLUTION
# ==============================

# Create a placeholder for auto-refresh
refresh_placeholder = st.empty()

# Check if we need to auto-refresh
if "next_refresh_time" not in st.session_state:
    st.session_state.next_refresh_time = time.time() + 30  # First refresh in 30 seconds

current_time = time.time()

if current_time >= st.session_state.next_refresh_time:
    # Time to refresh!
    print(f"üîÑ AUTO-REFRESH EXECUTING at {datetime.now().strftime('%H:%M:%S')}")
    
    # Execute research cycle
    with st.spinner("Auto-refreshing data..."):
        research_data = execute_research_cycle()
    
    if research_data:
        print(f"‚úÖ Auto-refresh completed successfully")
        
        # Update next refresh time
        st.session_state.next_refresh_time = current_time + 30
        
        # Update state
        st.session_state.scheduler._last_run = datetime.now()
        st.session_state.scheduler._total_executions += 1
        st.session_state.auto_refresh_manager.last_execution_time = datetime.now()
        st.session_state.auto_refresh_manager.execution_count += 1
        
        # Show success message
        refresh_placeholder.success(f"‚úÖ Auto-refresh completed at {datetime.now().strftime('%H:%M:%S')}")
        time.sleep(1)
        refresh_placeholder.empty()
        
        # Force a rerun to update UI
        st.rerun()
    else:
        print("‚ùå Auto-refresh failed")
        refresh_placeholder.error("Auto-refresh failed")
        time.sleep(2)
        refresh_placeholder.empty()
else:
    # Show countdown
    time_until_next = st.session_state.next_refresh_time - current_time
    if time_until_next > 0:
        print(f"‚è±Ô∏è Next auto-refresh in {int(time_until_next)}s")
        
        # Update countdown every 10 seconds
        if int(time_until_next) % 10 == 0:
            refresh_placeholder.info(f"Next auto-refresh in {int(time_until_next)} seconds...")


# ==============================
# TAB 1: LIVE ANALYTICS
# ==============================

with tab1:
    # Add JavaScript auto-refresh
    auto_refresh_js = """
    <script>
    // Auto-refresh every 30 seconds
    setTimeout(function() {
        window.location.reload();
    }, 30000);
    </script>
    """
    st.components.v1.html(auto_refresh_js, height=0)
    
    st.markdown('<div class="sub-header">üìä Live Market Analytics</div>', unsafe_allow_html=True)
   
    
    # Execute cycle - but this is just for UI display now
    scheduler: MarketScheduler = st.session_state.scheduler
    
    # Track last execution time
    if "last_execution_time" not in st.session_state:
        st.session_state.last_execution_time = None
    
    # Display background scheduler status
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if scheduler._last_run:
            time_since_last = (datetime.now() - scheduler._last_run).total_seconds()
            status_text = "‚úÖ Running" if time_since_last < 60 else "‚ö†Ô∏è Stale"
            st.metric("Scheduler", status_text, delta=f"{int(time_since_last)}s ago")
        else:
            st.metric("Scheduler", "‚è≥ Starting", delta="Initializing")
    
    with col2:
        if st.session_state.price_series.size > 0:
            current_price = st.session_state.price_series.iloc[-1]
            prev_price = st.session_state.price_series.iloc[-2] if st.session_state.price_series.size > 1 else current_price
            change_pct = ((current_price - prev_price) / prev_price * 100) if prev_price > 0 else 0
            st.metric("Nifty 50", f"{current_price:,.0f}", f"{change_pct:+.2f}%")
    
    with col3:
        if st.session_state.research_analytics:
            latest = st.session_state.research_analytics[-1]
            regime = latest.get("market_regime", "UNKNOWN")
            st.metric("Market Regime", regime)
    
    with col4:
        if st.session_state.research_analytics:
            latest = st.session_state.research_analytics[-1]
            oi_vel = latest.get("oi_velocity", 0)
            st.metric("OI Velocity", f"{oi_vel:+.2f}œÉ")
    
    # Manual execution button - still works
    if st.button("üîÑ Execute Research Cycle Now", type="primary", width='stretch'):
        with st.spinner("Executing research cycle..."):
            research_data = execute_research_cycle()
            if research_data:
                st.success("‚úÖ Research cycle executed successfully")
                
                # Display insights if available
                if "options" in research_data and "insights" in research_data["options"]:
                    for insight in research_data["options"]["insights"][:3]:
                        st.info(insight)
            else:
                st.error("‚ùå Research cycle failed")
    
    st.markdown("---")
    
    # Display enhanced status
    st.markdown("---")
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        status = get_scheduler_status()
        if status["scheduler_active"]:
            if status["last_run"] != "Never":
                time_since = (datetime.now() - st.session_state.scheduler._last_run).total_seconds() if st.session_state.scheduler._last_run else 999
                if time_since < 60:
                    st.metric("Scheduler", "‚úÖ Active", delta=f"{int(time_since)}s ago")
                else:
                    st.metric("Scheduler", "‚ö†Ô∏è Stale", delta=f"{int(time_since)}s ago")
            else:
                st.metric("Scheduler", "‚è≥ Initializing", delta="First run pending")
        else:
            st.metric("Scheduler", "‚ùå Inactive", delta="Check configuration")

    with col2:
        manager_status = st.session_state.auto_refresh_manager.get_status()
        st.metric("Auto Refresh", f"Run #{manager_status['execution_count']}", 
                delta=manager_status['last_execution'])

    with col3:
        if manager_status.get('time_until_next'):
            st.metric("Next Run In", manager_status['time_until_next'])

    with col4:
        error_count = manager_status.get('error_count', 0)
        if error_count == 0:
            st.metric("Errors", "‚úÖ 0", delta="Clean")
        else:
            st.metric("Errors", f"‚ö†Ô∏è {error_count}", delta="Check logs")

    # Control buttons
    col1, col2, col3 = st.columns(3)

    with col1:
        if st.button("üîÑ Force Run Now", type="primary", width='stretch'):
            # Force immediate execution
            st.session_state.auto_refresh_manager.last_execution_time = None
            st.rerun()

    with col2:
        if st.button("‚è∏Ô∏è Pause Auto-Refresh", width='stretch'):
            # Temporarily disable auto-refresh
            original_interval = st.session_state.scheduler.interval_seconds
            st.session_state.scheduler.interval_seconds = 3600  # 1 hour
            st.success(f"Auto-refresh paused. Original interval: {original_interval}s")
            st.rerun()

    with col3:
        if st.button("‚ñ∂Ô∏è Resume Auto-Refresh", width='stretch'):
            # Restore auto-refresh
            st.session_state.scheduler.interval_seconds = 30
            st.success("Auto-refresh resumed (30s interval)")
            st.rerun()
    # Display background thread info
    with st.expander("üîÑ Background Scheduler Info", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            if "scheduler_thread" in st.session_state:
                thread = st.session_state.scheduler_thread
                st.success("‚úÖ Background thread active")
                st.info(f"Thread ID: {thread.ident}")
                st.info(f"Thread alive: {thread.is_alive()}")
            else:
                st.error("‚ùå Background thread not running")
        
        with col2:
            if scheduler._last_run:
                st.metric("Last Execution", scheduler._last_run.strftime("%H:%M:%S"))
                time_since = (datetime.now() - scheduler._last_run).total_seconds()
                st.metric("Time Since", f"{int(time_since)}s")
            else:
                st.metric("Last Execution", "Never")
            
            st.metric("Total Executions", scheduler._total_executions)
    
    
    # Display latest analytics if available
    if st.session_state.research_analytics:
        latest_analytics = st.session_state.research_analytics[-1]
        
        # Create columns for analytics display
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### üìà OI & Gamma Analysis")
            
            # OI Velocity gauge
            oi_velocity = latest_analytics.get("oi_velocity", 0)
            fig_oi = go.Figure(go.Indicator(
                mode="gauge+number+delta",
                value=oi_velocity,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "OI Velocity (œÉ)"},
                delta={'reference': 0},
                gauge={
                    'axis': {'range': [-3, 3]},
                    'steps': [
                        {'range': [-3, -1.5], 'color': "lightgray"},
                        {'range': [-1.5, 1.5], 'color': "gray"},
                        {'range': [1.5, 3], 'color': "lightgray"}
                    ],
                    'threshold': {
                        'line': {'color': "red", 'width': 4},
                        'thickness': 0.75,
                        'value': oi_velocity
                    }
                }
            ))
            fig_oi.update_layout(height=250)
            st.plotly_chart(fig_oi, width='stretch')
        
        with col2:
            st.markdown("#### üß± Structure Analysis")
            
            # Structural metrics
            wall_strength = latest_analytics.get("wall_strength", 0)
            trap_prob = latest_analytics.get("trap_probability", 0)
            
            col_a, col_b = st.columns(2)
            with col_a:
                st.metric("Wall Strength", f"{wall_strength:.2f}")
            with col_b:
                st.metric("Trap Probability", f"{trap_prob:.2%}")
            
            # Display traps if any
            traps = latest_analytics.get("potential_traps", [])
            if traps:
                st.markdown("##### üéØ Detected Traps")
                for trap in traps[:2]:  # Show max 2
                    direction = trap.get("direction", "").upper()
                    confidence = trap.get("confidence", 0)
                    strike = trap.get("strike", 0)
                    
                    if confidence > 0.6:
                        st.markdown(f"""
                        <div class="trap-alert">
                            <strong>{direction} Trap</strong> at {strike:,.0f}<br>
                            Confidence: {confidence:.0%}<br>
                            Unwinding: {trap.get('unwinding_rate', 0):+.1f}%
                        </div>
                        """, unsafe_allow_html=True)
        
        # Price chart
        st.markdown("---")
        st.markdown("#### üìâ Price & Indicators")
        
        if len(st.session_state.price_series) > 1:
            # Create subplot for price and indicators
            fig = make_subplots(
                rows=3, cols=1,
                shared_xaxes=True,
                vertical_spacing=0.05,
                subplot_titles=("Nifty 50 Price", "OI Velocity", "Gamma Exposure"),
                row_heights=[0.5, 0.25, 0.25]
            )
            
            # Price
            price_data = st.session_state.price_series.reset_index(drop=True)
            fig.add_trace(
                go.Scatter(
                    y=price_data.values,
                    mode='lines',
                    name='Price',
                    line=dict(color='#3B82F6', width=2)
                ),
                row=1, col=1
            )
            
            # OI Velocity (if available in analytics history)
            if len(st.session_state.research_analytics) > 1:
                oi_velocities = [a.get("oi_velocity", 0) for a in st.session_state.research_analytics[-50:]]
                fig.add_trace(
                    go.Scatter(
                        y=oi_velocities,
                        mode='lines',
                        name='OI Velocity',
                        line=dict(color='#10B981', width=1)
                    ),
                    row=2, col=1
                )
                
                # Add zero line
                fig.add_hline(y=0, line_dash="dash", line_color="gray", row=2, col=1)
            
            # Gamma (if available)
            if len(st.session_state.research_analytics) > 1:
                gamma_values = [a.get("gamma_exposure", {}).get("net_gamma", 0) for a in st.session_state.research_analytics[-50:]]
                fig.add_trace(
                    go.Scatter(
                        y=gamma_values,
                        mode='lines',
                        name='Net Gamma',
                        line=dict(color='#8B5CF6', width=1)
                    ),
                    row=3, col=1
                )
                
                # Add zero line
                fig.add_hline(y=0, line_dash="dash", line_color="gray", row=3, col=1)
            
            fig.update_layout(height=600, showlegend=True)
            st.plotly_chart(fig, width='stretch')

# ==============================
# TAB 2: SIGNALS
# ==============================

with tab2:
    st.markdown('<div class="sub-header">üéØ Trading Signals</div>', unsafe_allow_html=True)
    
    # Fetch active signals
    active_signals = fetch_active_signals(limit=10)
    
    if not active_signals.empty:
        # Display signals in columns
        for _, signal in active_signals.iterrows():
            col1, col2, col3, col4 = st.columns([2, 1, 1, 1])
            
            with col1:
                signal_type = signal["signal_type"]
                color = "green" if signal_type == "BUY" else "red" if signal_type == "SELL" else "gray"
                st.markdown(f"### <span style='color:{color};'>{signal_type}</span>", unsafe_allow_html=True)
                
                # Rationale
                if pd.notna(signal["rationale"]):
                    st.caption(signal["rationale"])
            
            with col2:
                confidence = signal["confidence"]
                st.metric("Confidence", f"{confidence:.0%}")
            
            with col3:
                strength = signal.get("signal_strength", "UNKNOWN")
                st.metric("Strength", strength)
            
            with col4:
                status = signal["status"]
                badge_color = "blue" if status == "NEW" else "green" if status == "VALIDATED" else "orange"
                st.markdown(f"<div style='padding: 5px 10px; background-color:{badge_color}; color:white; border-radius:5px; text-align:center;'>{status}</div>", 
                           unsafe_allow_html=True)
            
            # Research context (expandable)
            with st.expander("Research Details"):
                if pd.notna(signal.get("research_context")):
                    try:
                        context = json.loads(signal["research_context"]) if isinstance(signal["research_context"], str) else signal["research_context"]
                        st.json(context)
                    except:
                        st.write("No research context available")
            
            st.markdown("---")
        
        # Performance metrics
        st.markdown("#### üìä Signal Performance")
        performance = fetch_signal_performance(days=7)
        
        if performance:
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Total Signals", performance.get("total_signals", 0))
            
            with col2:
                st.metric("Profitable", performance.get("profitable_signals", 0))
            
            with col3:
                win_rate = performance.get("win_rate", 0)
                st.metric("Win Rate", f"{win_rate:.1f}%")
            
            with col4:
                period = performance.get("period_days", 7)
                st.metric("Period", f"{period} days")
        
        # Signal distribution chart
        if not active_signals.empty:
            st.markdown("#### üìà Signal Distribution")
            
            fig = go.Figure(data=[
                go.Pie(
                    labels=active_signals["signal_type"].value_counts().index,
                    values=active_signals["signal_type"].value_counts().values,
                    hole=.3
                )
            ])
            
            fig.update_layout(
                height=300,
                showlegend=True,
                margin=dict(t=0, b=0, l=0, r=0)
            )
            
            st.plotly_chart(fig, width='stretch')
    
    else:
        st.info("üì≠ No active signals. The system will generate signals during market hours.")

# ==============================
# TAB 3: RESEARCH
# ==============================

with tab3:
    st.markdown('<div class="sub-header">üîç Research & Analytics</div>', unsafe_allow_html=True)
    
    # Research metrics
    if st.session_state.research_analytics:
        latest = st.session_state.research_analytics[-1]
        
        # Key research metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            net_gamma = latest.get("gamma_exposure", {}).get("net_gamma", 0)
            st.metric("Net Gamma", f"{net_gamma:,.0f}")
        
        with col2:
            pcr = latest.get("put_call_ratio", 1.0)
            st.metric("Put/Call Ratio", f"{pcr:.2f}")
        
        with col3:
            divergence = latest.get("divergence_score", 0)
            st.metric("Divergence Score", f"{divergence:.2f}")
        
        with col4:
            regime = latest.get("market_regime", "UNKNOWN")
            st.metric("Market Regime", regime)
        
        # Display market insights
        st.markdown("#### üí° Market Insights")
        
        insights = latest.get("market_insights", [])
        if insights:
            for insight in insights[:5]:
                if "trap" in insight.lower():
                    st.markdown(f'<div class="trap-alert">{insight}</div>', unsafe_allow_html=True)
                elif "divergence" in insight.lower():
                    st.markdown(f'<div class="divergence-alert">{insight}</div>', unsafe_allow_html=True)
                else:
                    st.markdown(f'<div class="research-insight">{insight}</div>', unsafe_allow_html=True)
        else:
            st.info("No insights available yet")
        
        # Structural walls
        st.markdown("#### üß± Structural Walls")
        
        walls = latest.get("structural_walls", [])
        if walls:
            walls_df = pd.DataFrame(walls)
            st.dataframe(walls_df, width='stretch')
        
        # Trap detection history
        st.markdown("#### üéØ Trap Detection History")
        
        if st.session_state.trap_detections:
            traps_df = pd.DataFrame([
                {
                    "Time": pd.to_datetime(t["timestamp"]).strftime("%H:%M:%S"),
                    "Type": t["trap"].get("direction", ""),
                    "Strike": t["trap"].get("strike", 0),
                    "Confidence": t["trap"].get("confidence", 0),
                    "Regime": t.get("market_regime", "")
                }
                for t in st.session_state.trap_detections
            ])
            
            if not traps_df.empty:
                st.dataframe(traps_df, width='stretch')
        
        # Research analytics over time
        st.markdown("#### üìä Analytics Timeline")
        
        if len(st.session_state.research_analytics) > 5:
            # Convert to DataFrame for plotting
            analytics_df = pd.DataFrame(st.session_state.research_analytics[-50:])
            
            if "timestamp" in analytics_df.columns:
                analytics_df["time"] = pd.to_datetime(analytics_df["timestamp"]).dt.strftime("%H:%M")
                
                # Plot OI Velocity and Gamma over time
                fig = make_subplots(
                    rows=2, cols=1,
                    shared_xaxes=True,
                    subplot_titles=("OI Velocity", "Net Gamma"),
                    vertical_spacing=0.1
                )
                
                if "oi_velocity" in analytics_df.columns:
                    fig.add_trace(
                        go.Scatter(
                            x=analytics_df["time"],
                            y=analytics_df["oi_velocity"],
                            mode='lines+markers',
                            name='OI Velocity',
                            line=dict(color='#10B981')
                        ),
                        row=1, col=1
                    )
                
                if "gamma_exposure" in analytics_df.columns:
                    # Extract net_gamma from gamma_exposure dict
                    try:
                        gamma_values = []
                        for g in analytics_df["gamma_exposure"]:
                            if isinstance(g, dict) and "net_gamma" in g:
                                gamma_values.append(g["net_gamma"])
                            else:
                                gamma_values.append(0)
                        
                        fig.add_trace(
                            go.Scatter(
                                x=analytics_df["time"],
                                y=gamma_values,
                                mode='lines+markers',
                                name='Net Gamma',
                                line=dict(color='#8B5CF6')
                            ),
                            row=2, col=1
                        )
                    except:
                        pass
                
                fig.update_layout(height=400, showlegend=True)
                st.plotly_chart(fig, width='stretch')

# ==============================
# TAB 4: CONFIGURATION
# ==============================

with tab4:
    st.markdown('<div class="sub-header">‚öôÔ∏è System Configuration</div>', unsafe_allow_html=True)
    
    # Available indices and underlyings
    available_indices = [
        "NSE_INDEX|Nifty 50",
        "NSE_INDEX|Nifty Bank", 
        "NSE_INDEX|Nifty Fin Service",
        "NSE_INDEX|Nifty Midcap 100",
        "NSE_INDEX|Nifty Next 50",
        "BSE_INDEX|S&P BSE SENSEX"  
    ]
    
    available_underlyings = [
        "NIFTY",
        "BANKNIFTY", 
        "FINNIFTY",
        "MIDCPNIFTY",
        "SENSEX"  
    ]
    
    # Get available expiries for the selected underlying
    current_underlying = st.session_state.config["underlying"]
    from data.instrument_master import get_available_expiries
    
    try:
        available_expiries = get_available_expiries(current_underlying)
        if not available_expiries:
            available_expiries = ["2026-01-27", "2026-02-03", "2026-02-10"]
    except:
        available_expiries = ["2026-01-27", "2026-02-03", "2026-02-10"]
    
    # Configuration form
    with st.form("configuration_form"):
        col1, col2 = st.columns(2)
        
        with col1:
            # Index Symbol Dropdown
            index_symbol = st.selectbox(
                "Index Symbol", 
                options=available_indices,
                index=available_indices.index(st.session_state.config["index_symbol"]) 
                if st.session_state.config["index_symbol"] in available_indices else 0
            )
            
            # Underlying Symbol Dropdown
            underlying = st.selectbox(
                "Underlying Symbol", 
                options=available_underlyings,
                index=available_underlyings.index(st.session_state.config["underlying"])
                if st.session_state.config["underlying"] in available_underlyings else 0
            )
            
            # Expiry Date Dropdown
            # Find today's date in available_expiries
            today_str = datetime.now().strftime("%Y-%m-%d")
            default_index = 0

            if today_str in available_expiries:
                default_index = available_expiries.index(today_str)
            elif today_str == "2026-01-28":  # Today's date
                # Add today to available expiries if it's not there
                available_expiries.insert(0, today_str)
                default_index = 0

            expiry_date = st.selectbox(
                "Expiry Date",
                options=available_expiries,
                index=default_index  # ‚úÖ Now selects today if available
            )
            
            # Convert selected expiry to datetime
            if expiry_date:
                try:
                    expiry_datetime = datetime.strptime(expiry_date, "%Y-%m-%d")
                    days_to_expiry = (expiry_datetime.date() - datetime.now().date()).days
                    st.info(f"Days to expiry: {days_to_expiry}")
                except:
                    st.warning("Invalid expiry date format")
        
        with col2:
            max_option_keys = st.number_input(
                "Max Option Keys", 
                min_value=50, 
                max_value=500, 
                value=st.session_state.config["max_option_keys"],
                help="Maximum number of option strikes to fetch"
            )
            
            scheduler_interval = st.slider(
                "Scheduler Interval (seconds)", 
                min_value=10, 
                max_value=300, 
                value=30,
                step=5,
                help="How often to refresh data (10-300 seconds)"
            )
            
            confidence_threshold = st.slider(
                "Signal Confidence Threshold", 
                min_value=0.0, 
                max_value=1.0, 
                value=0.2, 
                step=0.05,
                format="%.2f",
                help="Minimum confidence level for signals (0.0-1.0)"
            )
            
            # Advanced options expander
            with st.expander("Advanced Options"):
                enable_live_trading = st.checkbox(
                    "Enable Live Trading", 
                    value=False,
                    help="WARNING: Enable actual order placement"
                )
                
                risk_per_trade = st.number_input(
                    "Risk per Trade (%)",
                    min_value=0.1,
                    max_value=5.0,
                    value=1.0,
                    step=0.1,
                    format="%.1f",
                    help="Maximum risk per trade as percentage of capital"
                )
        
        # Submit button
        submit_col1, submit_col2, submit_col3 = st.columns([1, 2, 1])
        with submit_col2:
            if st.form_submit_button("üíæ Save Configuration", width='stretch'):
                # Update configuration
                st.session_state.config.update({
                    "index_symbol": index_symbol,
                    "underlying": underlying,
                    "expiry_date": datetime.strptime(expiry_date, "%Y-%m-%d") if expiry_date else datetime.now(timezone.utc) + timedelta(days=3),
                    "max_option_keys": max_option_keys,
                    "enable_live_trading": enable_live_trading,
                    "risk_per_trade": risk_per_trade
                })
                
                # Update scheduler
                st.session_state.scheduler = MarketScheduler(interval_seconds=scheduler_interval)
                
                # Update signal engine
                st.session_state.signal_engine = create_signal_engine(
                    signal_expiry_minutes=5,
                    confidence_threshold=confidence_threshold
                )
                
                # Clear option keys cache
                st.session_state.option_keys = []
                
                st.success("‚úÖ Configuration saved!")
                st.rerun()
    
    st.markdown("---")
    
    # Quick Configuration Presets
    st.markdown("#### üöÄ Quick Presets")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üìä NIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty 50",
                "underlying": "NIFTY"
            })
            st.success("Set to NIFTY Weekly")
            st.rerun()
    
    with col2:
        if st.button("üè¶ BANKNIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty Bank",
                "underlying": "BANKNIFTY"
            })
            st.success("Set to BANKNIFTY Weekly")
            st.rerun()
    
    with col3:
        if st.button("üí≥ FINNIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty Fin Service",
                "underlying": "FINNIFTY"
            })
            st.success("Set to FINNIFTY Weekly")
            st.rerun()
    
    # Current Configuration Display
    st.markdown("---")
    st.markdown("#### üñ•Ô∏è Current Configuration")
    
    config_col1, config_col2 = st.columns(2)
    
    with config_col1:
        st.markdown("**Trading Settings:**")
        st.text(f"Index: {st.session_state.config['index_symbol']}")
        st.text(f"Underlying: {st.session_state.config['underlying']}")
        st.text(f"Expiry: {st.session_state.config['expiry_date'].strftime('%Y-%m-%d')}")
        st.text(f"Max Options: {st.session_state.config['max_option_keys']}")
    
    with config_col2:
        st.markdown("**System Settings:**")
        st.text(f"Scheduler: {st.session_state.scheduler.interval_seconds}s")
        st.text(f"Signal Confidence: {confidence_threshold:.2f}")
        st.text(f"Live Trading: {'Enabled' if st.session_state.config.get('enable_live_trading', False) else 'Disabled'}")
        if st.session_state.config.get('enable_live_trading', False):
            st.text(f"Risk per Trade: {st.session_state.config.get('risk_per_trade', 1.0)}%")
    
    # Diagnostic tools (keep your existing code here)
    st.markdown("---")
    st.markdown("#### üîß Diagnostic Tools")
    # ... keep your existing diagnostic tools code ...
    
    # Diagnostic tools
    st.markdown("---")
    st.markdown("#### üîß Diagnostic Tools")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üß™ Test Upstox Connection", width='stretch'):
            # Test connection by trying to fetch profile
            try:
                profile = client.fetch_profile()
                if profile:
                    st.success(f"‚úÖ Upstox connection working")
                else:
                    st.error("‚ùå Failed to fetch profile")
            except Exception as e:
                st.error(f"‚ùå Connection test failed: {e}")
    
    with col2:
        if st.button("üìä Test Feature Pipeline", width='stretch'):
            try:
                # Get latest features
                features = fetch_latest_features(FEATURE_VERSION)
                if features is not None and not features.empty():
                    st.success(f"‚úÖ Feature pipeline working ({len(features)} features)")
                else:
                    st.warning("‚ö†Ô∏è No features found")
            except Exception as e:
                st.error(f"‚ùå Feature pipeline error: {e}")
    
    with col3:
        if st.button("üîÑ Clear Option Keys Cache", width='stretch'):
            st.session_state.option_keys = []
            st.success("‚úÖ Option keys cache cleared")

# ==============================
# PERFORMANCE MODAL
# ==============================

if st.session_state.get("show_performance", False):
    with st.expander("üìà Performance Analytics", expanded=True):
        st.markdown("### üìà Trading Performance")
        
        performance = fetch_signal_performance(days=30)
        
        if performance:
            # Overall metrics
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Total Trades", performance["total_signals"])
            
            with col2:
                st.metric("Profitable Trades", performance["profitable_signals"])
            
            with col3:
                win_rate = performance["win_rate"]
                color = "normal" if win_rate > 50 else "inverse"
                st.metric("Win Rate", f"{win_rate:.1f}%", delta_color=color)
            
            with col4:
                st.metric("Analysis Period", f"{performance['period_days']} days")
            
            # Breakdown by signal type
            st.markdown("#### Breakdown by Signal Type")
            
            if "breakdown" in performance:
                breakdown_df = pd.DataFrame(performance["breakdown"])
                st.dataframe(breakdown_df, width='stretch')
        
        st.session_state.show_performance = False

# ==============================
# LOGS MODAL
# ==============================

if st.session_state.get("show_logs", False):
    with st.expander("üìã System Logs", expanded=True):
        st.markdown("### üìã Recent System Activity")
        
        # Could fetch from system_health table here
        st.info("Logs would be displayed here from the database.")
        
        st.session_state.show_logs = False

# ==============================
# FOOTER
# ==============================

st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #6B7280; font-size: 0.9rem;'>
    Algorithmic Trading Research Platform v2.0 | 
    Research Features: OI Velocity, Gamma Exposure, Walls/Traps, Divergence Analysis
</div>
""", unsafe_allow_html=True)

# Simple auto-refresh timer
import time

if "last_auto_run" not in st.session_state:
    st.session_state.last_auto_run = time.time()

current_time = time.time()
time_since_last = current_time - st.session_state.last_auto_run

if time_since_last > 30:  # 30 seconds
    print(f"üîÑ Simple timer triggered after {int(time_since_last)}s")
    
    # Execute research cycle
    research_data = execute_research_cycle()
    
    if research_data:
        st.session_state.last_auto_run = current_time
        print(f"‚úÖ Auto-refresh executed successfully")
        
        # Update scheduler's last run time
        st.session_state.scheduler._last_run = datetime.now()
        st.session_state.scheduler._total_executions += 1
        
        # Update auto-refresh manager
        st.session_state.auto_refresh_manager.last_execution_time = datetime.now()
        st.session_state.auto_refresh_manager.execution_count += 1
        
        # Force UI update
        st.rerun()
    else:
        print("‚ùå Auto-refresh failed")
else:
    print(f"‚è±Ô∏è Next auto-refresh in {int(30 - time_since_last)}s")



====================================================================================================
FILE: .\app.py
====================================================================================================


File Name: app.py
Full Path: G:\trading_app\app.py
Size: 74.67 KB
Last Modified: 01/28/2026 15:40:44
Extension: .py

"""
ENHANCED TRADING TOOL - Research-Based Live Feature Engine
Incorporates OI Velocity, Gamma Exposure, Walls/Traps, and Divergence analysis.
"""

import os
import sys

# Fix Conda environment issue
if 'conda' in sys.version.lower():
    try:
        # Try to fix conda path issues
        os.environ['PATH'] = os.path.join(os.environ.get('CONDA_PREFIX', ''), 'bin') + ':' + os.environ['PATH']
    except:
        pass
# ==============================
# STREAMLIT CONFIG FIX - ADD THIS
# ==============================
import os
os.environ['STREAMLIT_SERVER_FILE_WATCHER_TYPE'] = 'none'
import time 
import threading  # <-- ADD THIS LINE
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from datetime import datetime, timezone
import json
from pathlib import Path
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Enhanced core components
from core.session import UpstoxSession, initialize_session, display_session_status
from data.upstox_client import UpstoxClient, MarketAnalytics, display_market_analytics
from data.instrument_master import get_option_keys

try:
    from core.feature_pipeline import build_and_store_features, create_feature_summary
except ImportError as e:
    st.error(f"Feature pipeline import error: {e}")
    # Define dummy functions to prevent crash
    def build_and_store_features(*args, **kwargs):
        return {"error": "Feature pipeline not loaded"}
    def create_feature_summary(*args, **kwargs):
        return {"error": "Feature pipeline not loaded"}
from core.scheduler import MarketScheduler
from core.signals.state_machine import SignalStateMachine, SignalValidator, create_signal_engine
from features.breadth import build_constituents_df
from ml.feature_contract import FEATURE_VERSION

# Enhanced storage
from storage.repository import (
    fetch_latest_features,
    insert_signal,
    signal_exists,
    validate_new_signals,
    expire_old_signals,
    fetch_active_signals,
    fetch_recent_analytics,
    get_database_stats,
    fetch_signal_performance,
    insert_research_analytics
)

# ==============================
# PAGE CONFIGURATION
# ==============================

st.set_page_config(
    page_title="Algorithmic Trading Research Platform",
    page_icon="üìà",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E3A8A;
        font-weight: 700;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #374151;
        font-weight: 600;
        margin-top: 1.5rem;
        margin-bottom: 1rem;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
    }
    .research-insight {
        background: #F0F9FF;
        border-left: 4px solid #3B82F6;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
    .trap-alert {
        background: #FEF3C7;
        border-left: 4px solid #F59E0B;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
    .divergence-alert {
        background: #FEE2E2;
        border-left: 4px solid #EF4444;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
</style>
""", unsafe_allow_html=True)

# ==============================
# APP HEADER
# ==============================

st.markdown('<div class="main-header">üìä Algorithmic Trading Research Platform</div>', unsafe_allow_html=True)
st.markdown("""
<div style='color: #6B7280; margin-bottom: 2rem;'>
    Advanced derivatives analytics with OI Velocity, Gamma Exposure, Structural Walls/Traps, and Spot Divergence detection.
    Version: Research v2.0
</div>
""", unsafe_allow_html=True)

# ==============================
# INITIALIZATION
# ==============================

def initialize_app_state():
    """Initialize application state with research components."""
    
    # Initialize session
    initialize_session()
    
    # Initialize state variables
    if "initialized" not in st.session_state:
        st.session_state.initialized = True
        
        # Market data series
        for key in ["price_series", "volume_series", "ccc_history", "oi_velocity_history", "gamma_history"]:
            if key not in st.session_state:
                st.session_state[key] = pd.Series(dtype=float)
        
        # Research analytics
        if "research_analytics" not in st.session_state:
            st.session_state.research_analytics = []
        
        # Signal engine
        if "signal_engine" not in st.session_state:
            st.session_state.signal_engine = create_signal_engine(
                signal_expiry_minutes=5,
                confidence_threshold=0.2
            )
        
        # Scheduler
        if "scheduler" not in st.session_state:
            st.session_state.scheduler = MarketScheduler(interval_seconds=30)
            # Initialize scheduler properties
            st.session_state.scheduler._last_run = None
            st.session_state.scheduler._total_executions = 0
        
        # Load Nifty weights
        if "nifty_weights" not in st.session_state:
            try:
                with open("config/nifty_weights.json", "r") as f:
                    st.session_state.nifty_weights = json.load(f)
            except:
                st.session_state.nifty_weights = {}
        
        # Instrument master path
        if "instrument_master" not in st.session_state:
            st.session_state.instrument_master = Path("G:/trading_app/data/instruments.json.gz")
        
        # Configuration - SMART EXPIRY SELECTION
        if "config" not in st.session_state:
            from utils.expiry_utils import get_trading_expiry
            
            # Get smart expiry
            underlying = "NIFTY"
            expiry_date_str = get_trading_expiry(underlying)
            
            if not expiry_date_str:
                # Fallback to next available date from instrument master
                from data.instrument_master import get_next_available_expiry
                expiry_date_str = get_next_available_expiry(underlying)
                
                if not expiry_date_str:
                    # Final fallback to a known date
                    expiry_date_str = "2026-02-03"
            
            # Convert string to datetime object
            expiry_date = datetime.strptime(expiry_date_str, "%Y-%m-%d")
            
            st.session_state.config = {
                "index_symbol": "NSE_INDEX|Nifty 50",
                "underlying": underlying,
                "expiry_date": expiry_date,
                "max_option_keys": 200
            }
            
            # Show expiry info
            from utils.expiry_utils import is_market_open
            market_status = "open" if is_market_open() else "closed"
            
            # Get today's date
            today_str = datetime.now().strftime('%Y-%m-%d')
            
            # Show appropriate message
            if expiry_date_str == today_str:
                st.info(f"üìÖ Using TODAY's expiry: {expiry_date_str} (Market: {market_status})")
            else:
                st.info(f"üìÖ Using NEXT expiry: {expiry_date_str} (Today: {today_str}, Market: {market_status})")
        
        # Option keys cache
        if "option_keys" not in st.session_state:
            st.session_state.option_keys = []
        
        # Market regime tracking
        if "market_regime" not in st.session_state:
            st.session_state.market_regime = "UNKNOWN"
        
        # Trap detection history
        if "trap_detections" not in st.session_state:
            st.session_state.trap_detections = []
        
        st.success("‚úÖ Application state initialized with research components")

# Run initialization
initialize_app_state()

# ==============================
# AUTHENTICATION
# ==============================

st.markdown('<div class="sub-header">üîê Authentication</div>', unsafe_allow_html=True)

# Authenticate - this will show login button if not authenticated
access_token = UpstoxSession.authenticate()

# CRITICAL: Check if authenticate() actually returned a token
# If it returns None, we need to stop execution here
if not access_token:
    # authenticate() should have shown login button and stopped with st.stop()
    # But if we reach here, something went wrong
    st.error("‚ùå Authentication required. Please log in to continue.")
    
    # Show login button manually as fallback
    login_url = UpstoxSession.get_login_url()
    st.markdown(f"""
    <div style="text-align: center; margin: 20px 0;">
        <a href="{login_url}" target="_blank">
            <button style="
                background-color: #00d09c;
                color: white;
                padding: 15px 30px;
                font-size: 18px;
                font-weight: bold;
                border: none;
                border-radius: 10px;
                cursor: pointer;
                width: 60%;
                margin: 20px 0;
                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            ">
            üìà Click to Login with Upstox
            </button>
        </a>
    </div>
    """, unsafe_allow_html=True)
    
    # STOP the app here
    st.stop()

# If we reach here, we have a token
# Initialize client
try:
    client = UpstoxClient(access_token)
    st.success("‚úÖ Upstox client initialized")
        
except Exception as e:
    st.error(f"‚ùå Error initializing Upstox client: {e}")
    st.stop()

# ==============================
# SIDEBAR - SESSION STATUS
# ==============================

with st.sidebar:
    st.markdown("## üß≠ Navigation")
    
    # Display session status
    display_session_status()
    
    # Database stats
    st.markdown("---")
    st.markdown("### üìä Database")
    
    db_stats = get_database_stats()
    if db_stats:
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Features", f"{db_stats.get('market_features_count', 0):,}")
        with col2:
            st.metric("Signals", f"{db_stats.get('signals_count', 0):,}")
    
    # Market hours info
    st.markdown("---")
    st.markdown("### üïí Market Hours")
    
    scheduler: MarketScheduler = st.session_state.scheduler
    now = datetime.now()
    
    if scheduler._is_market_open(now):
        st.success("‚úÖ Market Open")
        next_run = scheduler._last_run + timedelta(seconds=scheduler.interval_seconds) if scheduler._last_run else now
        time_to_next = (next_run - now).total_seconds()
        if time_to_next > 0:
            st.info(f"Next cycle in: {int(time_to_next)}s")
    else:
        st.warning("‚è∏Ô∏è Market Closed")
    
    # Quick actions
    st.markdown("---")
    st.markdown("### ‚ö° Quick Actions")
    
    if st.button("üîÑ Clear Cache", width='stretch'):
        for key in list(st.session_state.keys()):
            if key not in ["initialized", "instrument_master", "config", "nifty_weights"]:
                del st.session_state[key]
        st.rerun()
    
    if st.button("üìä View Performance", width='stretch'):
        st.session_state.show_performance = True
    
    if st.button("üõ†Ô∏è System Logs", width='stretch'):
        st.session_state.show_logs = True

def retry_on_failure(max_retries=3, delay=5):
    """Decorator for retrying failed executions"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    print(f"‚ö†Ô∏è Attempt {attempt + 1}/{max_retries} failed: {e}")
                    if attempt < max_retries - 1:
                        print(f"‚è∏Ô∏è Retrying in {delay}s...")
                        time.sleep(delay)
                    else:
                        print(f"‚ùå All {max_retries} attempts failed")
                        raise
            return None
        return wrapper
    return decorator

def log_execution_time(func):
    """Decorator for logging execution time"""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        print(f"‚è±Ô∏è Starting {func.__name__}...")
        
        result = func(*args, **kwargs)
        
        execution_time = time.time() - start_time
        print(f"‚è±Ô∏è {func.__name__} completed in {execution_time:.2f}s")
        
        return result
    return wrapper
# ==============================
# RESEARCH EXECUTION CYCLE
# ==============================

@log_execution_time
@retry_on_failure(max_retries=2, delay=3)
def execute_research_cycle(config=None, nifty_weights=None, option_keys=None):
    """
    Enhanced execution cycle with research analytics.
    Returns comprehensive research data.
    """
    # Use provided parameters or fall back to session state
    if config is None:
        try:
            config = st.session_state.config
        except (KeyError, AttributeError):
            print("‚ö†Ô∏è Config not found in session state")
            return None
    
    if nifty_weights is None:
        nifty_weights = st.session_state.get("nifty_weights", {})
    
    if option_keys is None:
        option_keys = st.session_state.get("option_keys", [])
    
    research_data = {}
    
    try:
        # ------------------------------
        # 1. INDEX QUOTE
        # ------------------------------
        print(f"üìä Fetching index quote for {config.get('index_symbol', 'NSE_INDEX|Nifty 50')}")
        index_data = client.fetch_index_quote(config["index_symbol"])
        
        if not index_data or index_data.get("ltp") is None:
            print("‚ùå No index data received")
            return None
        
        ltp = float(index_data["ltp"])
        open_price = float(index_data["open"])
        volume = 0.0  # index has no volume
        
        research_data["index"] = {
            "ltp": ltp,
            "open": open_price,
            "change": index_data.get("percent_change", 0),
            "timestamp": index_data.get("timestamp", datetime.now(timezone.utc).isoformat())
        }
        
        print(f"‚úÖ Index data: {ltp} (Change: {index_data.get('percent_change', 0)}%)")
        
        # ------------------------------
        # 2. UPDATE PRICE SERIES
        # ------------------------------
        # Create new series safely
        try:
            if "price_series" not in st.session_state:
                st.session_state.price_series = pd.Series(dtype=float)
            
            if "volume_series" not in st.session_state:
                st.session_state.volume_series = pd.Series(dtype=float)
            
            # Update price series
            new_price_series = pd.concat([
                st.session_state.price_series, 
                pd.Series([ltp], index=[datetime.now()])
            ]).tail(200)
            
            # Update volume series
            new_volume_series = pd.concat([
                st.session_state.volume_series, 
                pd.Series([volume], index=[datetime.now()])
            ]).tail(200)
            
            # Store back to session state
            st.session_state.price_series = new_price_series
            st.session_state.volume_series = new_volume_series
            
            print(f"üìà Price series updated: {len(st.session_state.price_series)} data points")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error updating price series: {e}")
            # Initialize fresh series
            st.session_state.price_series = pd.Series([ltp], index=[datetime.now()])
            st.session_state.volume_series = pd.Series([volume], index=[datetime.now()])
        
        # ------------------------------
        # 3. BREADTH ANALYSIS (CCC)
        # ------------------------------
        print("üìä Starting breadth analysis...")
        try:
            if not nifty_weights:
                print("‚ö†Ô∏è No Nifty weights found, skipping breadth analysis")
                ccc_value = 0.0
                constituents_df = pd.DataFrame()
            else:
                equity_quotes_df = client.fetch_equity_quotes(
                    symbols=list(nifty_weights.keys())
                )
                
                constituents_df = build_constituents_df(
                    equity_quotes_df=equity_quotes_df,
                    weights=nifty_weights
                )
                
                if not constituents_df.empty:
                    ccc_value = (
                        constituents_df["weight"] * constituents_df["price_change"]
                    ).sum()
                    print(f"‚úÖ CCC Value: {ccc_value:.4f}")
                else:
                    ccc_value = 0.0
                    print("‚ö†Ô∏è Empty constituents dataframe")
        except Exception as e:
            print(f"‚ö†Ô∏è Breadth analysis error: {e}")
            ccc_value = 0.0
            constituents_df = pd.DataFrame()
        
        # Update CCC history
        try:
            if "ccc_history" not in st.session_state:
                st.session_state.ccc_history = pd.Series(dtype=float)
            
            st.session_state.ccc_history = pd.concat([
                st.session_state.ccc_history, 
                pd.Series([ccc_value], index=[datetime.now()])
            ]).tail(200)
        except:
            st.session_state.ccc_history = pd.Series([ccc_value], index=[datetime.now()])
        
        research_data["breadth"] = {
            "ccc_value": ccc_value,
            "constituents_count": len(constituents_df),
            "positive_constituents": len(constituents_df[constituents_df["price_change"] > 0]) if not constituents_df.empty else 0
        }
        
        # ------------------------------
        # 4. OPTION INSTRUMENT DISCOVERY
        # ------------------------------
        print("üîç Fetching option instruments...")
        try:
            if not option_keys:
                expiry_str = config["expiry_date"].strftime("%Y-%m-%d")
                option_keys = get_option_keys(
                    underlying=config["underlying"],
                    expiry=expiry_str,
                    max_keys=config.get("max_option_keys", 200)
                )
                st.session_state.option_keys = option_keys
                print(f"‚úÖ Found {len(option_keys)} option keys")
        except Exception as e:
            print(f"‚ö†Ô∏è Option discovery error: {e}")
            option_keys = []
            st.session_state.option_keys = []
        
        if not option_keys:
            print("‚ö†Ô∏è No option keys found")
            return research_data  # Return what we have so far
        
        # ------------------------------
        # 5. ENHANCED OPTION CHAIN ANALYSIS
        # ------------------------------
        print("üìä Analyzing option chain...")
        option_analytics = client.fetch_option_chain_with_analytics(
            option_keys,
            ltp
        )
        
        if not option_analytics or "raw_data" not in option_analytics:
            print("‚ùå Failed to fetch option analytics")
            return research_data  # Return what we have so far
        
        option_chain_df = option_analytics["raw_data"]
        analytics = option_analytics.get("analytics", {})
        insights = option_analytics.get("market_insights", [])
        
        research_data["options"] = {
            "analytics": analytics,
            "insights": insights,
            "chain_size": len(option_chain_df),
            "put_call_ratio": analytics.get("put_call_ratio", 1.0) if isinstance(analytics.get("put_call_ratio"), (int, float)) else 1.0
        }
        
        print(f"‚úÖ Option chain analyzed: {len(option_chain_df)} strikes, PCR: {analytics.get('put_call_ratio', 'N/A')}")
        
        # Store analytics for visualization
        if analytics:
            try:
                if "research_analytics" not in st.session_state:
                    st.session_state.research_analytics = []
                
                st.session_state.research_analytics.append({
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    **analytics
                })
                
                # Keep only recent analytics
                if len(st.session_state.research_analytics) > 100:
                    st.session_state.research_analytics = st.session_state.research_analytics[-100:]
                
                # Update market regime
                st.session_state.market_regime = analytics.get("market_regime", "UNKNOWN")
                
                # Store in database
                insert_research_analytics(analytics)
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error storing analytics: {e}")
        
        # ------------------------------
        # 6. FEATURE PIPELINE
        # ------------------------------
        print("‚öôÔ∏è Running feature pipeline...")
        try:
            snapshot_ts = pd.Timestamp.utcnow().floor("ms")
            
            feature_result = build_and_store_features(
                timestamp=snapshot_ts,
                option_chain_df=option_chain_df,
                spot_price=ltp,
                expiry_datetime=config["expiry_date"],
                
                ltp=ltp,
                vwap=open_price,
                price_series=st.session_state.price_series,
                volume_series=st.session_state.volume_series,
                
                constituents_df=constituents_df,
                ccc_history=st.session_state.ccc_history,
                
                # Enhanced parameters
                client=client,
                option_keys=option_keys
            )
            
            if feature_result:
                research_data["features"] = feature_result.get("features", {})
                research_data["feature_summary"] = create_feature_summary(feature_result["features"])
                print("‚úÖ Feature pipeline completed")
            else:
                print("‚ö†Ô∏è Feature pipeline returned empty result")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Feature pipeline error: {e}")
            import traceback
            traceback.print_exc()
        
        # ------------------------------
        # 7. SIGNAL GENERATION
        # ------------------------------
        print("üéØ Checking for signals...")
        try:
            feature_row = fetch_latest_features(FEATURE_VERSION)
            if feature_row is not None and not feature_row.empty:
                # Check if signal already exists
                timestamp_str = feature_row.iloc[0]["timestamp"]
                
                if not signal_exists(timestamp_str):
                    # Generate signal
                    signal_engine: SignalStateMachine = st.session_state.signal_engine
                    signal = signal_engine.build_signal(feature_row.iloc[0])
                    
                    # Validate signal
                    is_valid, reason = SignalValidator.validate_signal(signal, feature_row.iloc[0])
                    
                    if is_valid and signal["signal_type"] != "NEUTRAL":
                        # Add research context
                        signal["research_validation"] = {
                            "is_valid": is_valid,
                            "reason": reason,
                            "timestamp": datetime.now(timezone.utc).isoformat()
                        }
                        
                        # Insert signal
                        insert_signal(signal)
                        
                        research_data["signal"] = {
                            "generated": True,
                            "type": signal["signal_type"],
                            "confidence": signal["confidence"],
                            "strength": signal.get("signal_strength", "UNKNOWN")
                        }
                        print(f"‚úÖ Signal generated: {signal['signal_type']} ({signal['confidence']:.0%})")
                    else:
                        research_data["signal"] = {
                            "generated": False,
                            "reason": reason if not is_valid else "NEUTRAL signal"
                        }
                        print(f"‚ÑπÔ∏è No valid signal: {reason if not is_valid else 'NEUTRAL'}")
            else:
                print("‚ÑπÔ∏è No features available for signal generation")
        except Exception as e:
            print(f"‚ö†Ô∏è Signal generation error: {e}")
        
        # ------------------------------
        # 8. SIGNAL LIFECYCLE MANAGEMENT
        # ------------------------------
        try:
            validate_new_signals(confidence_threshold=0.2)
            expire_old_signals()
        except Exception as e:
            print(f"‚ö†Ô∏è Signal lifecycle error: {e}")
        
        # ------------------------------
        # 9. TRAP DETECTION
        # ------------------------------
        if analytics and "potential_traps" in analytics:
            traps = analytics["potential_traps"]
            if traps:
                try:
                    if "trap_detections" not in st.session_state:
                        st.session_state.trap_detections = []
                    
                    for trap in traps:
                        if trap.get("confidence", 0) > 0.6:
                            st.session_state.trap_detections.append({
                                "timestamp": datetime.now(timezone.utc).isoformat(),
                                "trap": trap,
                                "market_regime": analytics.get("market_regime")
                            })
                    
                    # Keep only recent traps
                    if len(st.session_state.trap_detections) > 20:
                        st.session_state.trap_detections = st.session_state.trap_detections[-20:]
                    
                    research_data["traps"] = traps
                    
                    if traps:
                        print(f"üéØ Detected {len(traps)} potential traps")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Trap detection error: {e}")
        
        # ------------------------------
        # 10. SUCCESSFUL COMPLETION
        # ------------------------------
        print("‚úÖ Research cycle completed successfully")
        return research_data
        
    except Exception as e:
        print(f"‚ùå Critical error in research cycle: {e}")
        import traceback
        traceback.print_exc()
        return None

# ==============================
# ENHANCED AUTO-REFRESH SYSTEM
# ==============================

class AutoRefreshManager:
    """Robust auto-refresh manager with error handling and state persistence"""
    
    def __init__(self, scheduler, config, nifty_weights, option_keys):
        self.scheduler = scheduler
        self.config = config
        self.nifty_weights = nifty_weights
        self.option_keys = option_keys
        self.last_execution_time = None
        self.execution_count = 0
        self.errors = []
        self.max_retries = 3
        self.retry_delay = 5
        
    def should_execute(self):
        """Determine if execution should run based on time and market conditions"""
        now = datetime.now()
        
        # Check market hours
        if not self.scheduler._is_market_open(now):
            return False, "Market closed"
        
        # Check if first run
        if self.last_execution_time is None:
            return True, "First execution"
        
        # Check time since last execution
        time_since_last = (now - self.last_execution_time).total_seconds()
        
        # Debug log to see what's happening
        print(f"üïí Auto-refresh timing: {int(time_since_last)}s since last, interval: {self.scheduler.interval_seconds}s")
        
        if time_since_last >= self.scheduler.interval_seconds:
            return True, f"Interval reached ({int(time_since_last)}s > {self.scheduler.interval_seconds}s)"
        
        return False, f"Not due yet ({int(self.scheduler.interval_seconds - time_since_last)}s remaining)"
    
    def execute_with_retry(self):
        """Execute research cycle with retry logic"""
        for attempt in range(self.max_retries):
            try:
                print(f"üîÑ Attempt {attempt + 1}/{self.max_retries} to execute research cycle")
                
                result = execute_research_cycle(
                    config=self.config,
                    nifty_weights=self.nifty_weights,
                    option_keys=self.option_keys
                )
                
                if result:
                    self.last_execution_time = datetime.now()
                    self.execution_count += 1
                    return True, result, f"Execution successful (attempt {attempt + 1})"
                else:
                    return False, None, "Research cycle returned no data"
                    
            except Exception as e:
                error_msg = f"Attempt {attempt + 1} failed: {str(e)}"
                self.errors.append({
                    "timestamp": datetime.now(),
                    "attempt": attempt + 1,
                    "error": str(e)
                })
                
                if attempt < self.max_retries - 1:
                    print(f"‚è∏Ô∏è Retrying in {self.retry_delay}s...")
                    time.sleep(self.retry_delay)
                else:
                    return False, None, f"All retries failed. Last error: {str(e)}"
        
        return False, None, "Max retries exceeded"
    
    def get_status(self):
        """Get current status of auto-refresh manager"""
        status = {
            "last_execution": self.last_execution_time.strftime("%H:%M:%S") if self.last_execution_time else "Never",
            "execution_count": self.execution_count,
            "is_market_open": self.scheduler._is_market_open(datetime.now()),
            "interval_seconds": self.scheduler.interval_seconds,
            "error_count": len(self.errors)
        }
        
        if self.last_execution_time:
            time_since = (datetime.now() - self.last_execution_time).total_seconds()
            status["time_since_last"] = f"{int(time_since)}s"
            status["time_until_next"] = f"{max(0, int(self.scheduler.interval_seconds - time_since))}s"
        
        return status

# Initialize auto-refresh manager
if "auto_refresh_manager" not in st.session_state:
    st.session_state.auto_refresh_manager = AutoRefreshManager(
        scheduler=st.session_state.scheduler,
        config=st.session_state.config,
        nifty_weights=st.session_state.nifty_weights,
        option_keys=st.session_state.option_keys
    )

# Execute auto-refresh
def execute_auto_refresh():
    """Main auto-refresh execution function"""
    manager = st.session_state.auto_refresh_manager
    
    try:
        # Check if we should execute
        should_execute, reason = manager.should_execute()
        
        if should_execute:
            print(f"üèÉ‚Äç‚ôÇÔ∏è Auto-execution triggered: {reason}")
            
            # Execute with retry logic
            success, result, message = manager.execute_with_retry()
            
            if success:
                print(f"‚úÖ {message}")
                
                # Update UI state if needed
                if result and "options" in result and "analytics" in result["options"]:
                    analytics = result["options"]["analytics"]
                    if analytics:
                        # Update market regime
                        st.session_state.market_regime = analytics.get("market_regime", "UNKNOWN")
                        
                        # Store analytics
                        if "research_analytics" not in st.session_state:
                            st.session_state.research_analytics = []
                        st.session_state.research_analytics.append({
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                            **analytics
                        })
                        # Keep only recent analytics
                        if len(st.session_state.research_analytics) > 100:
                            st.session_state.research_analytics = st.session_state.research_analytics[-100:]
                
                return True, message
            else:
                print(f"‚ùå Auto-execution failed: {message}")
                return False, message
        else:
            # Log status periodically
            if manager.last_execution_time:
                time_since = (datetime.now() - manager.last_execution_time).total_seconds()
                if int(time_since) % 10 == 0:  # Log every 10 seconds
                    print(f"‚è±Ô∏è Auto-refresh status: {reason}")
            return None, reason
            
    except Exception as e:
        error_msg = f"Auto-refresh system error: {str(e)}"
        print(f"üî• {error_msg}")
        manager.errors.append({
            "timestamp": datetime.now(),
            "error": error_msg
        })
        return False, error_msg

# Run auto-refresh
execute_auto_refresh()

# ==============================
# ENHANCED BACKGROUND SCHEDULER
# ==============================

class BackgroundScheduler:
    """Robust background scheduler with error handling and monitoring"""
    
    def __init__(self, check_interval=5):
        self.check_interval = check_interval
        self.running = False
        self.thread = None
        self.errors = []
        self.last_check = None
        self.health_checks = []
        
    def health_check(self):
        """Perform system health check"""
        checks = []
        
        # Check scheduler
        try:
            scheduler = st.session_state.scheduler
            checks.append({
                "component": "Scheduler",
                "status": "HEALTHY" if scheduler else "UNKNOWN",
                "details": f"Interval: {scheduler.interval_seconds}s" if scheduler else "Not initialized"
            })
        except Exception as e:
            checks.append({
                "component": "Scheduler",
                "status": "ERROR",
                "details": str(e)
            })
        
        # Check auto-refresh manager
        try:
            manager = st.session_state.auto_refresh_manager
            checks.append({
                "component": "AutoRefresh",
                "status": "HEALTHY",
                "details": f"Executions: {manager.execution_count}"
            })
        except Exception as e:
            checks.append({
                "component": "AutoRefresh",
                "status": "ERROR",
                "details": str(e)
            })
        
        self.health_checks = checks
        return checks
    
    def start(self):
        """Start background scheduler thread"""
        if self.running:
            print("‚ö†Ô∏è Background scheduler already running")
            return
        
        def background_task():
            print("üöÄ Starting enhanced background scheduler...")
            self.running = True
            
            while self.running:
                try:
                    self.last_check = datetime.now()
                    
                    # Perform health check every minute
                    if not self.health_checks or (datetime.now() - self.last_check).seconds > 60:
                        self.health_check()
                    
                    # Log status every 30 seconds
                    current_time = datetime.now()
                    if current_time.second % 30 == 0:
                        manager = st.session_state.auto_refresh_manager
                        if manager.last_execution_time:
                            time_since = (current_time - manager.last_execution_time).total_seconds()
                            print(f"üìä Background monitor: Last execution {int(time_since)}s ago, {manager.execution_count} total runs")
                    
                    time.sleep(self.check_interval)
                    
                except Exception as e:
                    error_msg = f"Background scheduler error: {str(e)}"
                    print(f"üî• {error_msg}")
                    self.errors.append({
                        "timestamp": datetime.now(),
                        "error": error_msg
                    })
                    time.sleep(30)  # Wait longer on error
        
        self.thread = threading.Thread(target=background_task, daemon=True)
        self.thread.start()
        print("‚úÖ Enhanced background scheduler started")
    
    def stop(self):
        """Stop background scheduler"""
        self.running = False
        if self.thread:
            self.thread.join(timeout=5)
        print("üõë Background scheduler stopped")
    
    def get_status(self):
        """Get scheduler status"""
        return {
            "running": self.running,
            "check_interval": self.check_interval,
            "last_check": self.last_check.strftime("%H:%M:%S") if self.last_check else "Never",
            "error_count": len(self.errors),
            "health_checks": self.health_checks
        }

# Initialize and start background scheduler
if "background_scheduler" not in st.session_state:
    st.session_state.background_scheduler = BackgroundScheduler(check_interval=5)
    st.session_state.background_scheduler.start()


def get_scheduler_status():
    """Get comprehensive scheduler status"""
    scheduler = st.session_state.scheduler
    manager = st.session_state.auto_refresh_manager
    
    status = {
        "scheduler_active": True,
        "interval_seconds": scheduler.interval_seconds,
        "total_executions": scheduler._total_executions,
        "last_run": scheduler._last_run.strftime("%H:%M:%S") if scheduler._last_run else "Never",
        "is_market_open": scheduler._is_market_open(datetime.now()),
        "auto_refresh_status": manager.get_status()
    }
    
    # Calculate next run time
    if scheduler._last_run:
        next_run = scheduler._last_run + timedelta(seconds=scheduler.interval_seconds)
        time_until_next = (next_run - datetime.now()).total_seconds()
        status["next_run"] = next_run.strftime("%H:%M:%S") if time_until_next > 0 else "Now"
        status["time_until_next"] = f"{max(0, int(time_until_next))}s"
    else:
        status["next_run"] = "Pending"
        status["time_until_next"] = "N/A"
    
    return status

# ==============================
# CREATE TABS
# ==============================

# Create tabs for different views
tab1, tab2, tab3, tab4 = st.tabs([
    "üìà Live Analytics", 
    "üéØ Signals", 
    "üîç Research", 
    "‚öôÔ∏è Configuration"
])

# ==============================
# AUTO-REFRESH WORKING SOLUTION
# ==============================

# Create a placeholder for auto-refresh
refresh_placeholder = st.empty()

# Check if we need to auto-refresh
if "next_refresh_time" not in st.session_state:
    st.session_state.next_refresh_time = time.time() + 30  # First refresh in 30 seconds

current_time = time.time()

if current_time >= st.session_state.next_refresh_time:
    # Time to refresh!
    print(f"üîÑ AUTO-REFRESH EXECUTING at {datetime.now().strftime('%H:%M:%S')}")
    
    # Execute research cycle
    with st.spinner("Auto-refreshing data..."):
        research_data = execute_research_cycle()
    
    if research_data:
        print(f"‚úÖ Auto-refresh completed successfully")
        
        # Update next refresh time
        st.session_state.next_refresh_time = current_time + 30
        
        # Update state
        st.session_state.scheduler._last_run = datetime.now()
        st.session_state.scheduler._total_executions += 1
        st.session_state.auto_refresh_manager.last_execution_time = datetime.now()
        st.session_state.auto_refresh_manager.execution_count += 1
        
        # Show success message
        refresh_placeholder.success(f"‚úÖ Auto-refresh completed at {datetime.now().strftime('%H:%M:%S')}")
        time.sleep(1)
        refresh_placeholder.empty()
        
        # Force a rerun to update UI
        st.rerun()
    else:
        print("‚ùå Auto-refresh failed")
        refresh_placeholder.error("Auto-refresh failed")
        time.sleep(2)
        refresh_placeholder.empty()
else:
    # Show countdown
    time_until_next = st.session_state.next_refresh_time - current_time
    if time_until_next > 0:
        print(f"‚è±Ô∏è Next auto-refresh in {int(time_until_next)}s")
        
        # Update countdown every 10 seconds
        if int(time_until_next) % 10 == 0:
            refresh_placeholder.info(f"Next auto-refresh in {int(time_until_next)} seconds...")


# ==============================
# TAB 1: LIVE ANALYTICS
# ==============================

with tab1:
    # Add JavaScript auto-refresh
    auto_refresh_js = """
    <script>
    // Auto-refresh every 30 seconds
    setTimeout(function() {
        window.location.reload();
    }, 30000);
    </script>
    """
    st.components.v1.html(auto_refresh_js, height=0)
    
    st.markdown('<div class="sub-header">üìä Live Market Analytics</div>', unsafe_allow_html=True)
   
    
    # Execute cycle - but this is just for UI display now
    scheduler: MarketScheduler = st.session_state.scheduler
    
    # Track last execution time
    if "last_execution_time" not in st.session_state:
        st.session_state.last_execution_time = None
    
    # Display background scheduler status
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if scheduler._last_run:
            time_since_last = (datetime.now() - scheduler._last_run).total_seconds()
            status_text = "‚úÖ Running" if time_since_last < 60 else "‚ö†Ô∏è Stale"
            st.metric("Scheduler", status_text, delta=f"{int(time_since_last)}s ago")
        else:
            st.metric("Scheduler", "‚è≥ Starting", delta="Initializing")
    
    with col2:
        if st.session_state.price_series.size > 0:
            current_price = st.session_state.price_series.iloc[-1]
            prev_price = st.session_state.price_series.iloc[-2] if st.session_state.price_series.size > 1 else current_price
            change_pct = ((current_price - prev_price) / prev_price * 100) if prev_price > 0 else 0
            st.metric("Nifty 50", f"{current_price:,.0f}", f"{change_pct:+.2f}%")
    
    with col3:
        if st.session_state.research_analytics:
            latest = st.session_state.research_analytics[-1]
            regime = latest.get("market_regime", "UNKNOWN")
            st.metric("Market Regime", regime)
    
    with col4:
        if st.session_state.research_analytics:
            latest = st.session_state.research_analytics[-1]
            oi_vel = latest.get("oi_velocity", 0)
            st.metric("OI Velocity", f"{oi_vel:+.2f}œÉ")
    
    # Manual execution button - still works
    if st.button("üîÑ Execute Research Cycle Now", type="primary", width='stretch'):
        with st.spinner("Executing research cycle..."):
            research_data = execute_research_cycle()
            if research_data:
                st.success("‚úÖ Research cycle executed successfully")
                
                # Display insights if available
                if "options" in research_data and "insights" in research_data["options"]:
                    for insight in research_data["options"]["insights"][:3]:
                        st.info(insight)
            else:
                st.error("‚ùå Research cycle failed")
    
    st.markdown("---")
    
    # Display enhanced status
    st.markdown("---")
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        status = get_scheduler_status()
        if status["scheduler_active"]:
            if status["last_run"] != "Never":
                time_since = (datetime.now() - st.session_state.scheduler._last_run).total_seconds() if st.session_state.scheduler._last_run else 999
                if time_since < 60:
                    st.metric("Scheduler", "‚úÖ Active", delta=f"{int(time_since)}s ago")
                else:
                    st.metric("Scheduler", "‚ö†Ô∏è Stale", delta=f"{int(time_since)}s ago")
            else:
                st.metric("Scheduler", "‚è≥ Initializing", delta="First run pending")
        else:
            st.metric("Scheduler", "‚ùå Inactive", delta="Check configuration")

    with col2:
        manager_status = st.session_state.auto_refresh_manager.get_status()
        st.metric("Auto Refresh", f"Run #{manager_status['execution_count']}", 
                delta=manager_status['last_execution'])

    with col3:
        if manager_status.get('time_until_next'):
            st.metric("Next Run In", manager_status['time_until_next'])

    with col4:
        error_count = manager_status.get('error_count', 0)
        if error_count == 0:
            st.metric("Errors", "‚úÖ 0", delta="Clean")
        else:
            st.metric("Errors", f"‚ö†Ô∏è {error_count}", delta="Check logs")

    # Control buttons
    col1, col2, col3 = st.columns(3)

    with col1:
        if st.button("üîÑ Force Run Now", type="primary", width='stretch'):
            # Force immediate execution
            st.session_state.auto_refresh_manager.last_execution_time = None
            st.rerun()

    with col2:
        if st.button("‚è∏Ô∏è Pause Auto-Refresh", width='stretch'):
            # Temporarily disable auto-refresh
            original_interval = st.session_state.scheduler.interval_seconds
            st.session_state.scheduler.interval_seconds = 3600  # 1 hour
            st.success(f"Auto-refresh paused. Original interval: {original_interval}s")
            st.rerun()

    with col3:
        if st.button("‚ñ∂Ô∏è Resume Auto-Refresh", width='stretch'):
            # Restore auto-refresh
            st.session_state.scheduler.interval_seconds = 30
            st.success("Auto-refresh resumed (30s interval)")
            st.rerun()
    # Display background thread info
    with st.expander("üîÑ Background Scheduler Info", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            if "scheduler_thread" in st.session_state:
                thread = st.session_state.scheduler_thread
                st.success("‚úÖ Background thread active")
                st.info(f"Thread ID: {thread.ident}")
                st.info(f"Thread alive: {thread.is_alive()}")
            else:
                st.error("‚ùå Background thread not running")
        
        with col2:
            if scheduler._last_run:
                st.metric("Last Execution", scheduler._last_run.strftime("%H:%M:%S"))
                time_since = (datetime.now() - scheduler._last_run).total_seconds()
                st.metric("Time Since", f"{int(time_since)}s")
            else:
                st.metric("Last Execution", "Never")
            
            st.metric("Total Executions", scheduler._total_executions)
    
    
    # Display latest analytics if available
    if st.session_state.research_analytics:
        latest_analytics = st.session_state.research_analytics[-1]
        
        # Create columns for analytics display
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### üìà OI & Gamma Analysis")
            
            # OI Velocity gauge
            oi_velocity = latest_analytics.get("oi_velocity", 0)
            fig_oi = go.Figure(go.Indicator(
                mode="gauge+number+delta",
                value=oi_velocity,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "OI Velocity (œÉ)"},
                delta={'reference': 0},
                gauge={
                    'axis': {'range': [-3, 3]},
                    'steps': [
                        {'range': [-3, -1.5], 'color': "lightgray"},
                        {'range': [-1.5, 1.5], 'color': "gray"},
                        {'range': [1.5, 3], 'color': "lightgray"}
                    ],
                    'threshold': {
                        'line': {'color': "red", 'width': 4},
                        'thickness': 0.75,
                        'value': oi_velocity
                    }
                }
            ))
            fig_oi.update_layout(height=250)
            st.plotly_chart(fig_oi, width='stretch')
        
        with col2:
            st.markdown("#### üß± Structure Analysis")
            
            # Structural metrics
            wall_strength = latest_analytics.get("wall_strength", 0)
            trap_prob = latest_analytics.get("trap_probability", 0)
            
            col_a, col_b = st.columns(2)
            with col_a:
                st.metric("Wall Strength", f"{wall_strength:.2f}")
            with col_b:
                st.metric("Trap Probability", f"{trap_prob:.2%}")
            
            # Display traps if any
            traps = latest_analytics.get("potential_traps", [])
            if traps:
                st.markdown("##### üéØ Detected Traps")
                for trap in traps[:2]:  # Show max 2
                    direction = trap.get("direction", "").upper()
                    confidence = trap.get("confidence", 0)
                    strike = trap.get("strike", 0)
                    
                    if confidence > 0.6:
                        st.markdown(f"""
                        <div class="trap-alert">
                            <strong>{direction} Trap</strong> at {strike:,.0f}<br>
                            Confidence: {confidence:.0%}<br>
                            Unwinding: {trap.get('unwinding_rate', 0):+.1f}%
                        </div>
                        """, unsafe_allow_html=True)
        
        # Price chart
        st.markdown("---")
        st.markdown("#### üìâ Price & Indicators")
        
        if len(st.session_state.price_series) > 1:
            # Create subplot for price and indicators
            fig = make_subplots(
                rows=3, cols=1,
                shared_xaxes=True,
                vertical_spacing=0.05,
                subplot_titles=("Nifty 50 Price", "OI Velocity", "Gamma Exposure"),
                row_heights=[0.5, 0.25, 0.25]
            )
            
            # Price
            price_data = st.session_state.price_series.reset_index(drop=True)
            fig.add_trace(
                go.Scatter(
                    y=price_data.values,
                    mode='lines',
                    name='Price',
                    line=dict(color='#3B82F6', width=2)
                ),
                row=1, col=1
            )
            
            # OI Velocity (if available in analytics history)
            if len(st.session_state.research_analytics) > 1:
                oi_velocities = [a.get("oi_velocity", 0) for a in st.session_state.research_analytics[-50:]]
                fig.add_trace(
                    go.Scatter(
                        y=oi_velocities,
                        mode='lines',
                        name='OI Velocity',
                        line=dict(color='#10B981', width=1)
                    ),
                    row=2, col=1
                )
                
                # Add zero line
                fig.add_hline(y=0, line_dash="dash", line_color="gray", row=2, col=1)
            
            # Gamma (if available)
            if len(st.session_state.research_analytics) > 1:
                gamma_values = [a.get("gamma_exposure", {}).get("net_gamma", 0) for a in st.session_state.research_analytics[-50:]]
                fig.add_trace(
                    go.Scatter(
                        y=gamma_values,
                        mode='lines',
                        name='Net Gamma',
                        line=dict(color='#8B5CF6', width=1)
                    ),
                    row=3, col=1
                )
                
                # Add zero line
                fig.add_hline(y=0, line_dash="dash", line_color="gray", row=3, col=1)
            
            fig.update_layout(height=600, showlegend=True)
            st.plotly_chart(fig, width='stretch')

# ==============================
# TAB 2: SIGNALS
# ==============================

with tab2:
    st.markdown('<div class="sub-header">üéØ Trading Signals</div>', unsafe_allow_html=True)
    
    # Fetch active signals
    active_signals = fetch_active_signals(limit=10)
    
    if not active_signals.empty:
        # Display signals in columns
        for _, signal in active_signals.iterrows():
            col1, col2, col3, col4 = st.columns([2, 1, 1, 1])
            
            with col1:
                signal_type = signal["signal_type"]
                color = "green" if signal_type == "BUY" else "red" if signal_type == "SELL" else "gray"
                st.markdown(f"### <span style='color:{color};'>{signal_type}</span>", unsafe_allow_html=True)
                
                # Rationale
                if pd.notna(signal["rationale"]):
                    st.caption(signal["rationale"])
            
            with col2:
                confidence = signal["confidence"]
                st.metric("Confidence", f"{confidence:.0%}")
            
            with col3:
                strength = signal.get("signal_strength", "UNKNOWN")
                st.metric("Strength", strength)
            
            with col4:
                status = signal["status"]
                badge_color = "blue" if status == "NEW" else "green" if status == "VALIDATED" else "orange"
                st.markdown(f"<div style='padding: 5px 10px; background-color:{badge_color}; color:white; border-radius:5px; text-align:center;'>{status}</div>", 
                           unsafe_allow_html=True)
            
            # Research context (expandable)
            with st.expander("Research Details"):
                if pd.notna(signal.get("research_context")):
                    try:
                        context = json.loads(signal["research_context"]) if isinstance(signal["research_context"], str) else signal["research_context"]
                        st.json(context)
                    except:
                        st.write("No research context available")
            
            st.markdown("---")
        
        # Performance metrics
        st.markdown("#### üìä Signal Performance")
        performance = fetch_signal_performance(days=7)
        
        if performance:
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Total Signals", performance.get("total_signals", 0))
            
            with col2:
                st.metric("Profitable", performance.get("profitable_signals", 0))
            
            with col3:
                win_rate = performance.get("win_rate", 0)
                st.metric("Win Rate", f"{win_rate:.1f}%")
            
            with col4:
                period = performance.get("period_days", 7)
                st.metric("Period", f"{period} days")
        
        # Signal distribution chart
        if not active_signals.empty:
            st.markdown("#### üìà Signal Distribution")
            
            fig = go.Figure(data=[
                go.Pie(
                    labels=active_signals["signal_type"].value_counts().index,
                    values=active_signals["signal_type"].value_counts().values,
                    hole=.3
                )
            ])
            
            fig.update_layout(
                height=300,
                showlegend=True,
                margin=dict(t=0, b=0, l=0, r=0)
            )
            
            st.plotly_chart(fig, width='stretch')
    
    else:
        st.info("üì≠ No active signals. The system will generate signals during market hours.")

# ==============================
# TAB 3: RESEARCH
# ==============================

with tab3:
    st.markdown('<div class="sub-header">üîç Research & Analytics</div>', unsafe_allow_html=True)
    
    # Research metrics
    if st.session_state.research_analytics:
        latest = st.session_state.research_analytics[-1]
        
        # Key research metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            net_gamma = latest.get("gamma_exposure", {}).get("net_gamma", 0)
            st.metric("Net Gamma", f"{net_gamma:,.0f}")
        
        with col2:
            pcr = latest.get("put_call_ratio", 1.0)
            st.metric("Put/Call Ratio", f"{pcr:.2f}")
        
        with col3:
            divergence = latest.get("divergence_score", 0)
            st.metric("Divergence Score", f"{divergence:.2f}")
        
        with col4:
            regime = latest.get("market_regime", "UNKNOWN")
            st.metric("Market Regime", regime)
        
        # Display market insights
        st.markdown("#### üí° Market Insights")
        
        insights = latest.get("market_insights", [])
        if insights:
            for insight in insights[:5]:
                if "trap" in insight.lower():
                    st.markdown(f'<div class="trap-alert">{insight}</div>', unsafe_allow_html=True)
                elif "divergence" in insight.lower():
                    st.markdown(f'<div class="divergence-alert">{insight}</div>', unsafe_allow_html=True)
                else:
                    st.markdown(f'<div class="research-insight">{insight}</div>', unsafe_allow_html=True)
        else:
            st.info("No insights available yet")
        
        # Structural walls
        st.markdown("#### üß± Structural Walls")
        
        walls = latest.get("structural_walls", [])
        if walls:
            walls_df = pd.DataFrame(walls)
            st.dataframe(walls_df, width='stretch')
        
        # Trap detection history
        st.markdown("#### üéØ Trap Detection History")
        
        if st.session_state.trap_detections:
            traps_df = pd.DataFrame([
                {
                    "Time": pd.to_datetime(t["timestamp"]).strftime("%H:%M:%S"),
                    "Type": t["trap"].get("direction", ""),
                    "Strike": t["trap"].get("strike", 0),
                    "Confidence": t["trap"].get("confidence", 0),
                    "Regime": t.get("market_regime", "")
                }
                for t in st.session_state.trap_detections
            ])
            
            if not traps_df.empty:
                st.dataframe(traps_df, width='stretch')
        
        # Research analytics over time
        st.markdown("#### üìä Analytics Timeline")
        
        if len(st.session_state.research_analytics) > 5:
            # Convert to DataFrame for plotting
            analytics_df = pd.DataFrame(st.session_state.research_analytics[-50:])
            
            if "timestamp" in analytics_df.columns:
                analytics_df["time"] = pd.to_datetime(analytics_df["timestamp"]).dt.strftime("%H:%M")
                
                # Plot OI Velocity and Gamma over time
                fig = make_subplots(
                    rows=2, cols=1,
                    shared_xaxes=True,
                    subplot_titles=("OI Velocity", "Net Gamma"),
                    vertical_spacing=0.1
                )
                
                if "oi_velocity" in analytics_df.columns:
                    fig.add_trace(
                        go.Scatter(
                            x=analytics_df["time"],
                            y=analytics_df["oi_velocity"],
                            mode='lines+markers',
                            name='OI Velocity',
                            line=dict(color='#10B981')
                        ),
                        row=1, col=1
                    )
                
                if "gamma_exposure" in analytics_df.columns:
                    # Extract net_gamma from gamma_exposure dict
                    try:
                        gamma_values = []
                        for g in analytics_df["gamma_exposure"]:
                            if isinstance(g, dict) and "net_gamma" in g:
                                gamma_values.append(g["net_gamma"])
                            else:
                                gamma_values.append(0)
                        
                        fig.add_trace(
                            go.Scatter(
                                x=analytics_df["time"],
                                y=gamma_values,
                                mode='lines+markers',
                                name='Net Gamma',
                                line=dict(color='#8B5CF6')
                            ),
                            row=2, col=1
                        )
                    except:
                        pass
                
                fig.update_layout(height=400, showlegend=True)
                st.plotly_chart(fig, width='stretch')

# ==============================
# TAB 4: CONFIGURATION
# ==============================

with tab4:
    st.markdown('<div class="sub-header">‚öôÔ∏è System Configuration</div>', unsafe_allow_html=True)
    
    # Available indices and underlyings
    available_indices = [
        "NSE_INDEX|Nifty 50",
        "NSE_INDEX|Nifty Bank", 
        "NSE_INDEX|Nifty Fin Service",
        "NSE_INDEX|Nifty Midcap 100",
        "NSE_INDEX|Nifty Next 50",
        "BSE_INDEX|S&P BSE SENSEX"  
    ]
    
    available_underlyings = [
        "NIFTY",
        "BANKNIFTY", 
        "FINNIFTY",
        "MIDCPNIFTY",
        "SENSEX"  
    ]
    
    # Get available expiries for the selected underlying
    current_underlying = st.session_state.config["underlying"]
    from data.instrument_master import get_available_expiries
    
    try:
        available_expiries = get_available_expiries(current_underlying)
        if not available_expiries:
            available_expiries = ["2026-01-27", "2026-02-03", "2026-02-10"]
    except:
        available_expiries = ["2026-01-27", "2026-02-03", "2026-02-10"]
    
    # Configuration form
    with st.form("configuration_form"):
        col1, col2 = st.columns(2)
        
        with col1:
            # Index Symbol Dropdown
            index_symbol = st.selectbox(
                "Index Symbol", 
                options=available_indices,
                index=available_indices.index(st.session_state.config["index_symbol"]) 
                if st.session_state.config["index_symbol"] in available_indices else 0
            )
            
            # Underlying Symbol Dropdown
            underlying = st.selectbox(
                "Underlying Symbol", 
                options=available_underlyings,
                index=available_underlyings.index(st.session_state.config["underlying"])
                if st.session_state.config["underlying"] in available_underlyings else 0
            )
            
            # Expiry Date Dropdown
            # Find today's date in available_expiries
            today_str = datetime.now().strftime("%Y-%m-%d")
            default_index = 0

            if today_str in available_expiries:
                default_index = available_expiries.index(today_str)
            elif today_str == "2026-01-28":  # Today's date
                # Add today to available expiries if it's not there
                available_expiries.insert(0, today_str)
                default_index = 0

            expiry_date = st.selectbox(
                "Expiry Date",
                options=available_expiries,
                index=default_index  # ‚úÖ Now selects today if available
            )
            
            # Convert selected expiry to datetime
            if expiry_date:
                try:
                    expiry_datetime = datetime.strptime(expiry_date, "%Y-%m-%d")
                    days_to_expiry = (expiry_datetime.date() - datetime.now().date()).days
                    st.info(f"Days to expiry: {days_to_expiry}")
                except:
                    st.warning("Invalid expiry date format")
        
        with col2:
            max_option_keys = st.number_input(
                "Max Option Keys", 
                min_value=50, 
                max_value=500, 
                value=st.session_state.config["max_option_keys"],
                help="Maximum number of option strikes to fetch"
            )
            
            scheduler_interval = st.slider(
                "Scheduler Interval (seconds)", 
                min_value=10, 
                max_value=300, 
                value=30,
                step=5,
                help="How often to refresh data (10-300 seconds)"
            )
            
            confidence_threshold = st.slider(
                "Signal Confidence Threshold", 
                min_value=0.0, 
                max_value=1.0, 
                value=0.2, 
                step=0.05,
                format="%.2f",
                help="Minimum confidence level for signals (0.0-1.0)"
            )
            
            # Advanced options expander
            with st.expander("Advanced Options"):
                enable_live_trading = st.checkbox(
                    "Enable Live Trading", 
                    value=False,
                    help="WARNING: Enable actual order placement"
                )
                
                risk_per_trade = st.number_input(
                    "Risk per Trade (%)",
                    min_value=0.1,
                    max_value=5.0,
                    value=1.0,
                    step=0.1,
                    format="%.1f",
                    help="Maximum risk per trade as percentage of capital"
                )
        
        # Submit button
        submit_col1, submit_col2, submit_col3 = st.columns([1, 2, 1])
        with submit_col2:
            if st.form_submit_button("üíæ Save Configuration", width='stretch'):
                # Update configuration
                st.session_state.config.update({
                    "index_symbol": index_symbol,
                    "underlying": underlying,
                    "expiry_date": datetime.strptime(expiry_date, "%Y-%m-%d") if expiry_date else datetime.now(timezone.utc) + timedelta(days=3),
                    "max_option_keys": max_option_keys,
                    "enable_live_trading": enable_live_trading,
                    "risk_per_trade": risk_per_trade
                })
                
                # Update scheduler
                st.session_state.scheduler = MarketScheduler(interval_seconds=scheduler_interval)
                
                # Update signal engine
                st.session_state.signal_engine = create_signal_engine(
                    signal_expiry_minutes=5,
                    confidence_threshold=confidence_threshold
                )
                
                # Clear option keys cache
                st.session_state.option_keys = []
                
                st.success("‚úÖ Configuration saved!")
                st.rerun()
    
    st.markdown("---")
    
    # Quick Configuration Presets
    st.markdown("#### üöÄ Quick Presets")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üìä NIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty 50",
                "underlying": "NIFTY"
            })
            st.success("Set to NIFTY Weekly")
            st.rerun()
    
    with col2:
        if st.button("üè¶ BANKNIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty Bank",
                "underlying": "BANKNIFTY"
            })
            st.success("Set to BANKNIFTY Weekly")
            st.rerun()
    
    with col3:
        if st.button("üí≥ FINNIFTY Weekly", width='stretch'):
            st.session_state.config.update({
                "index_symbol": "NSE_INDEX|Nifty Fin Service",
                "underlying": "FINNIFTY"
            })
            st.success("Set to FINNIFTY Weekly")
            st.rerun()
    
    # Current Configuration Display
    st.markdown("---")
    st.markdown("#### üñ•Ô∏è Current Configuration")
    
    config_col1, config_col2 = st.columns(2)
    
    with config_col1:
        st.markdown("**Trading Settings:**")
        st.text(f"Index: {st.session_state.config['index_symbol']}")
        st.text(f"Underlying: {st.session_state.config['underlying']}")
        st.text(f"Expiry: {st.session_state.config['expiry_date'].strftime('%Y-%m-%d')}")
        st.text(f"Max Options: {st.session_state.config['max_option_keys']}")
    
    with config_col2:
        st.markdown("**System Settings:**")
        st.text(f"Scheduler: {st.session_state.scheduler.interval_seconds}s")
        st.text(f"Signal Confidence: {confidence_threshold:.2f}")
        st.text(f"Live Trading: {'Enabled' if st.session_state.config.get('enable_live_trading', False) else 'Disabled'}")
        if st.session_state.config.get('enable_live_trading', False):
            st.text(f"Risk per Trade: {st.session_state.config.get('risk_per_trade', 1.0)}%")
    
    # Diagnostic tools (keep your existing code here)
    st.markdown("---")
    st.markdown("#### üîß Diagnostic Tools")
    # ... keep your existing diagnostic tools code ...
    
    # Diagnostic tools
    st.markdown("---")
    st.markdown("#### üîß Diagnostic Tools")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üß™ Test Upstox Connection", width='stretch'):
            # Test connection by trying to fetch profile
            try:
                profile = client.fetch_profile()
                if profile:
                    st.success(f"‚úÖ Upstox connection working")
                else:
                    st.error("‚ùå Failed to fetch profile")
            except Exception as e:
                st.error(f"‚ùå Connection test failed: {e}")
    
    with col2:
        if st.button("üìä Test Feature Pipeline", width='stretch'):
            try:
                # Get latest features
                features = fetch_latest_features(FEATURE_VERSION)
                if features is not None and not features.empty():
                    st.success(f"‚úÖ Feature pipeline working ({len(features)} features)")
                else:
                    st.warning("‚ö†Ô∏è No features found")
            except Exception as e:
                st.error(f"‚ùå Feature pipeline error: {e}")
    
    with col3:
        if st.button("üîÑ Clear Option Keys Cache", width='stretch'):
            st.session_state.option_keys = []
            st.success("‚úÖ Option keys cache cleared")

# ==============================
# PERFORMANCE MODAL
# ==============================

if st.session_state.get("show_performance", False):
    with st.expander("üìà Performance Analytics", expanded=True):
        st.markdown("### üìà Trading Performance")
        
        performance = fetch_signal_performance(days=30)
        
        if performance:
            # Overall metrics
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Total Trades", performance["total_signals"])
            
            with col2:
                st.metric("Profitable Trades", performance["profitable_signals"])
            
            with col3:
                win_rate = performance["win_rate"]
                color = "normal" if win_rate > 50 else "inverse"
                st.metric("Win Rate", f"{win_rate:.1f}%", delta_color=color)
            
            with col4:
                st.metric("Analysis Period", f"{performance['period_days']} days")
            
            # Breakdown by signal type
            st.markdown("#### Breakdown by Signal Type")
            
            if "breakdown" in performance:
                breakdown_df = pd.DataFrame(performance["breakdown"])
                st.dataframe(breakdown_df, width='stretch')
        
        st.session_state.show_performance = False

# ==============================
# LOGS MODAL
# ==============================

if st.session_state.get("show_logs", False):
    with st.expander("üìã System Logs", expanded=True):
        st.markdown("### üìã Recent System Activity")
        
        # Could fetch from system_health table here
        st.info("Logs would be displayed here from the database.")
        
        st.session_state.show_logs = False

# ==============================
# FOOTER
# ==============================

st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #6B7280; font-size: 0.9rem;'>
    Algorithmic Trading Research Platform v2.0 | 
    Research Features: OI Velocity, Gamma Exposure, Walls/Traps, Divergence Analysis
</div>
""", unsafe_allow_html=True)

# Simple auto-refresh timer
import time

if "last_auto_run" not in st.session_state:
    st.session_state.last_auto_run = time.time()

current_time = time.time()
time_since_last = current_time - st.session_state.last_auto_run

if time_since_last > 30:  # 30 seconds
    print(f"üîÑ Simple timer triggered after {int(time_since_last)}s")
    
    # Execute research cycle
    research_data = execute_research_cycle()
    
    if research_data:
        st.session_state.last_auto_run = current_time
        print(f"‚úÖ Auto-refresh executed successfully")
        
        # Update scheduler's last run time
        st.session_state.scheduler._last_run = datetime.now()
        st.session_state.scheduler._total_executions += 1
        
        # Update auto-refresh manager
        st.session_state.auto_refresh_manager.last_execution_time = datetime.now()
        st.session_state.auto_refresh_manager.execution_count += 1
        
        # Force UI update
        st.rerun()
    else:
        print("‚ùå Auto-refresh failed")
else:
    print(f"‚è±Ô∏è Next auto-refresh in {int(30 - time_since_last)}s")



====================================================================================================
FILE: .\backtest\__init__.py
====================================================================================================


File Name: __init__.py
Full Path: G:\trading_app\backtest\__init__.py
Size: 0 KB
Last Modified: 01/25/2026 18:09:44
Extension: .py




====================================================================================================
FILE: .\backtest\engine.py
====================================================================================================


File Name: engine.py
Full Path: G:\trading_app\backtest\engine.py
Size: 3.3 KB
Last Modified: 01/25/2026 18:07:12
Extension: .py

import sqlite3
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path

from core.signals.state_machine import SignalStateMachine


DB_PATH = Path("G:/trading_app/storage/trading.db")


class BacktestEngine:
    """
    Replays historical features to simulate signals and trades.
    """

    def __init__(
        self,
        feature_version: str,
        holding_minutes: int = 5,
        quantity: int = 1
    ):
        self.feature_version = feature_version
        self.holding_minutes = holding_minutes
        self.quantity = quantity
        self.signal_engine = SignalStateMachine(signal_expiry_minutes=holding_minutes)

    # ==============================
    # LOAD HISTORICAL FEATURES
    # ==============================

    def load_features(self) -> pd.DataFrame:
        with sqlite3.connect(DB_PATH) as conn:
            df = pd.read_sql(
                """
                SELECT *
                FROM market_features
                WHERE feature_version = ?
                ORDER BY timestamp
                """,
                conn,
                params=(self.feature_version,)
            )

        df["timestamp"] = pd.to_datetime(df["timestamp"])
        return df

    # ==============================
    # RUN BACKTEST
    # ==============================

    def run(self) -> pd.DataFrame:
        df = self.load_features()
        if df.empty:
            raise RuntimeError("No features found for backtest")

        trades = []
        open_trade = None

        for i in range(len(df) - 1):
            row = df.iloc[i]
            next_row = df.iloc[i + 1]

            # If no open trade ‚Üí check for signal
            if open_trade is None:
                signal = self.signal_engine.build_signal(row)

                if signal["signal_type"] in ("BUY", "SELL"):
                    open_trade = {
                        "signal_id": signal["signal_id"],
                        "direction": signal["signal_type"],
                        "entry_time": next_row["timestamp"],
                        "entry_price": next_row["spot_price"],
                        "expiry_time": next_row["timestamp"]
                                       + timedelta(minutes=self.holding_minutes)
                    }

            # If trade open ‚Üí check exit
            if open_trade is not None:
                if next_row["timestamp"] >= open_trade["expiry_time"]:
                    exit_price = next_row["spot_price"]

                    pnl = (
                        (exit_price - open_trade["entry_price"])
                        if open_trade["direction"] == "BUY"
                        else (open_trade["entry_price"] - exit_price)
                    ) * self.quantity

                    trades.append({
                        "signal_id": open_trade["signal_id"],
                        "entry_time": open_trade["entry_time"],
                        "exit_time": next_row["timestamp"],
                        "entry_price": open_trade["entry_price"],
                        "exit_price": exit_price,
                        "quantity": self.quantity,
                        "pnl": pnl
                    })

                    open_trade = None

        return pd.DataFrame(trades)




====================================================================================================
FILE: .\backtest\metrics.py
====================================================================================================


File Name: metrics.py
Full Path: G:\trading_app\backtest\metrics.py
Size: 1.51 KB
Last Modified: 01/25/2026 18:15:15
Extension: .py

import pandas as pd
import numpy as np


def compute_backtest_metrics(trades_df: pd.DataFrame) -> dict:
    """
    Compute performance metrics from backtest trades.
    Safe for empty DataFrames.
    """

    if trades_df is None or trades_df.empty:
        return {
            "total_trades": 0,
            "win_rate": 0.0,
            "total_pnl": 0.0,
            "avg_pnl": 0.0,
            "max_drawdown": 0.0,
            "sharpe": 0.0,
            "expectancy": 0.0
        }

    pnl = trades_df["pnl"]

    total_trades = len(pnl)
    wins = pnl[pnl > 0]
    losses = pnl[pnl <= 0]

    win_rate = len(wins) / total_trades if total_trades else 0.0
    total_pnl = pnl.sum()
    avg_pnl = pnl.mean()

    # Equity curve
    equity = pnl.cumsum()
    running_max = equity.cummax()
    drawdown = equity - running_max
    max_drawdown = drawdown.min()

    # Sharpe (PnL-based)
    sharpe = 0.0
    if pnl.std() != 0:
        sharpe = (pnl.mean() / pnl.std()) * np.sqrt(len(pnl))

    # Expectancy
    avg_win = wins.mean() if not wins.empty else 0.0
    avg_loss = losses.mean() if not losses.empty else 0.0
    expectancy = (win_rate * avg_win) + ((1 - win_rate) * avg_loss)

    return {
        "total_trades": total_trades,
        "win_rate": round(win_rate, 3),
        "total_pnl": round(total_pnl, 2),
        "avg_pnl": round(avg_pnl, 2),
        "max_drawdown": round(max_drawdown, 2),
        "sharpe": round(sharpe, 3),
        "expectancy": round(expectancy, 2)
    }




====================================================================================================
FILE: .\backtest\run_backtest.py
====================================================================================================


File Name: run_backtest.py
Full Path: G:\trading_app\backtest\run_backtest.py
Size: 0.45 KB
Last Modified: 01/25/2026 18:15:27
Extension: .py

from backtest.engine import BacktestEngine
from backtest.metrics import compute_backtest_metrics
from ml.feature_contract import FEATURE_VERSION

engine = BacktestEngine(
    feature_version=FEATURE_VERSION,
    holding_minutes=5,
    quantity=1
)

results = engine.run()
metrics = compute_backtest_metrics(results)

print("Backtest Results:")
print(results)

print("\nBacktest Metrics:")
for k, v in metrics.items():
    print(f"{k}: {v}")




====================================================================================================
FILE: .\core\debug_session.py
====================================================================================================


File Name: debug_session.py
Full Path: G:\trading_app\core\debug_session.py
Size: 2.08 KB
Last Modified: 01/27/2026 22:44:52
Extension: .py

import streamlit as st
from pathlib import Path

st.set_page_config(layout="wide", page_title="Debug Auth")

st.title("üîç Authentication Debug")

# Check token file
token_path = Path("data/tokens/upstox_tokens.enc")
st.write(f"Token path: {token_path}")
st.write(f"Token exists: {token_path.exists()}")

# Simulate what authenticate() should do
if not token_path.exists():
    st.success("‚úÖ Perfect! Token file doesn't exist - should show login button")
    
    # Show what the login button should look like
    st.markdown("---")
    st.subheader("This is what should appear:")
    
    login_url = "https://api.upstox.com/v2/login/authorization/dialog?response_type=code&client_id=e9bc6b14-447a-4d2f-aa32-2a4aecbafe56&redirect_uri=http://127.0.0.1:8501/callback&scope=order placement portfolio"
    
    st.markdown(f"""
    <div style="text-align: center;">
        <a href="{login_url}" target="_blank">
            <button style="
                background-color: #00d09c;
                color: white;
                padding: 15px 30px;
                font-size: 18px;
                font-weight: bold;
                border: none;
                border-radius: 10px;
                cursor: pointer;
                width: 80%;
                margin: 20px 0;
                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            ">
            üìà Login with Upstox
            </button>
        </a>
    </div>
    """, unsafe_allow_html=True)
    
    st.warning("If you don't see this in the main app, the issue is in how authenticate() is being called.")
else:
    st.warning("Token file exists - try deleting it with: `rm data/tokens/upstox_tokens.enc`")

# Check how authenticate is called in app.py
st.markdown("---")
st.subheader("How authenticate() is called in app.py:")
st.code("""
# This should be in your app.py:
from core.session import UpstoxSession

# Early in the app initialization:
token = UpstoxSession.authenticate()

# If token is None, authenticate() should have shown login button
# and stopped execution with st.stop()
""")



====================================================================================================
FILE: .\core\feature_pipeline.py
====================================================================================================


File Name: feature_pipeline.py
Full Path: G:\trading_app\core\feature_pipeline.py
Size: 27.89 KB
Last Modified: 01/28/2026 11:09:52
Extension: .py

"""
Enhanced Feature Pipeline with Research-Based Feature Engineering.
Implements all research concepts:
1. OI Velocity & Gamma Exposure features
2. Structural Walls & Traps features
3. Spot Divergence features
4. Market microstructure features
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, Optional, Tuple
from dataclasses import dataclass
from scipy import stats
from datetime import timezone

from ml.feature_contract import FEATURE_VERSION, FEATURE_COLUMNS, TARGET_COLUMN
from storage.repository import insert_market_features
from data.upstox_client import UpstoxClient, MarketAnalytics

# Import feature computation modules
from features.option_features import compute_option_features
from features.price_features import compute_price_features
from features.breadth import compute_breadth_features

# ==============================
# RESEARCH-BASED FEATURE ENGINEERING
# ==============================

@dataclass
class ResearchFeatures:
    """Container for research-based features"""
    # OI Velocity features
    oi_velocity: float
    oi_velocity_ma: float  # Moving average
    oi_velocity_std: float  # Standard deviation
    oi_regime: str  # EXPANSIVE/NORMAL/CONSTRICTED
    
    # Gamma Exposure features
    net_gamma: float
    gamma_regime: str  # POSITIVE/NEGATIVE/NEUTRAL
    gamma_flip_distance: float  # Distance to nearest gamma flip
    max_gamma_strike: float
    
    # Structural features
    wall_strength: float  # Combined strength of top walls
    wall_defense_score: float  # Defense score (0-1)
    trap_probability: float  # Probability of trap formation
    
    # Divergence features
    price_oi_divergence: float  # Divergence between price and OI
    price_gamma_divergence: float  # Divergence between price and gamma
    divergence_score: float  # Combined divergence score
    
    # Market microstructure
    put_call_ratio: float
    max_pain_distance: float  # Distance to max pain
    vix_smile: float  # Volatility smile curvature
    skewness: float  # Option skew
    
    # Wyckoff-inspired features
    spring_detection: float  # Bear trap probability
    upthrust_detection: float  # Bull trap probability
    accumulation_score: float  # Accumulation phase score

class EnhancedFeatureEngine:
    """
    Enhanced feature engineering engine incorporating research concepts.
    Transforms raw market data into research-based predictive features.
    """
    
    def __init__(self, lookback_periods: int = 20):
        self.lookback = lookback_periods
        self.feature_history = []
        self.oi_velocity_history = []
        self.gamma_history = []
        
    def extract_research_features(self, 
                                option_chain_analytics: Dict,
                                spot_price: float,
                                price_series: pd.Series,
                                volume_series: pd.Series,
                                constituents_df: pd.DataFrame) -> ResearchFeatures:
        """
        Extract research-based features from market analytics.
        """
        analytics = option_chain_analytics.get("analytics", {})
        
        # Add null checks
        if not analytics:
            # Return default/empty features
            return ResearchFeatures(
                oi_velocity=0.0,
                oi_velocity_ma=0.0,
                oi_velocity_std=0.0,
                oi_regime="NORMAL",
                net_gamma=0.0,
                gamma_regime="NEUTRAL",
                gamma_flip_distance=0.0,
                max_gamma_strike=0.0,
                wall_strength=0.0,
                wall_defense_score=0.0,
                trap_probability=0.0,
                price_oi_divergence=0.0,
                price_gamma_divergence=0.0,
                divergence_score=0.0,
                put_call_ratio=1.0,
                max_pain_distance=0.0,
                vix_smile=0.0,
                skewness=0.0,
                spring_detection=0.0,
                upthrust_detection=0.0,
                accumulation_score=0.0
            )
        
        # OI Velocity features
        oi_velocity = analytics.get("oi_velocity", 0.0)
        oi_regime = analytics.get("oi_regime", "NORMAL")
        
        # Update history and calculate statistics
        self.oi_velocity_history.append(oi_velocity)
        if len(self.oi_velocity_history) > self.lookback:
            self.oi_velocity_history = self.oi_velocity_history[-self.lookback:]
        
        oi_velocity_ma = np.mean(self.oi_velocity_history) if self.oi_velocity_history else 0.0
        oi_velocity_std = np.std(self.oi_velocity_history) if len(self.oi_velocity_history) > 1 else 1.0
        
        # Gamma Exposure features
        gamma_data = analytics.get("gamma_exposure", {})
        net_gamma = gamma_data.get("net_gamma", 0.0)
        gamma_regime = gamma_data.get("regime", "NEUTRAL")
        flip_levels = gamma_data.get("flip_levels", [])
        max_gamma_strike = gamma_data.get("max_impact_strike", 0.0)
        
        # Calculate distance to nearest gamma flip
        gamma_flip_distance = 0.0
        if flip_levels:
            distances = [abs(level - spot_price) / spot_price for level in flip_levels]
            gamma_flip_distance = min(distances) if distances else 0.0
        
        # Update gamma history
        self.gamma_history.append(net_gamma)
        if len(self.gamma_history) > self.lookback:
            self.gamma_history = self.gamma_history[-self.lookback:]
        
        # Structural features
        walls = analytics.get("structural_walls", [])
        traps = analytics.get("potential_traps", [])
        
        # Wall strength (weighted by concentration and defense)
        wall_strength = 0.0
        wall_defense_score = 0.0
        if walls:
            for wall in walls:
                concentration = wall.get("concentration", 0.0)
                defended = 1.0 if wall.get("defended", False) else 0.0
                distance = wall.get("distance_pct", 100) / 100  # Normalize
                
                # Walls closer to spot have more impact
                proximity_factor = 1.0 / (1.0 + distance)
                wall_strength += concentration * proximity_factor
                wall_defense_score += defended * concentration
        
        # Normalize wall features
        wall_strength = min(wall_strength, 1.0)
        wall_defense_score = min(wall_defense_score, 1.0)
        
        # Trap probability
        trap_probability = 0.0
        if traps:
            trap_confidences = [trap.get("confidence", 0.0) for trap in traps]
            trap_probability = max(trap_confidences) if trap_confidences else 0.0
        
        # Divergence features
        divergence_data = analytics.get("spot_divergence", {})
        has_divergence = divergence_data.get("has_divergence", False)
        divergence_type = divergence_data.get("type", "")
        divergence_confidence = divergence_data.get("confidence", 0.0)
        
        # Calculate price-OI divergence
        price_change = 0.0
        if len(price_series) >= 2:
            price_change = (price_series.iloc[-1] - price_series.iloc[-2]) / price_series.iloc[-2] * 100
        
        price_oi_divergence = abs(price_change - oi_velocity) if oi_velocity != 0 else 0.0
        
        # Calculate price-gamma divergence
        price_gamma_divergence = 0.0
        if self.gamma_history and len(self.gamma_history) >= 2:
            gamma_change = self.gamma_history[-1] - self.gamma_history[-2]
            price_gamma_divergence = abs(price_change - gamma_change * 100)
        
        divergence_score = divergence_confidence if has_divergence else 0.0
        
        # Market microstructure features
        pcr_data = MarketAnalytics.calculate_put_call_ratio(option_chain_analytics.get("raw_data", pd.DataFrame()))
        put_call_ratio = pcr_data.get("pcr_oi", 0.0)
        
        max_pain_data = MarketAnalytics.detect_max_pain(option_chain_analytics.get("raw_data", pd.DataFrame()))
        max_pain_strike = max_pain_data.get("max_pain_strike", 0.0)
        max_pain_distance = abs(spot_price - max_pain_strike) / spot_price if spot_price > 0 else 0.0
        
        # Calculate VIX smile (simplified)
        vix_smile = self._calculate_vix_smile(option_chain_analytics.get("raw_data", pd.DataFrame()), spot_price)
        
        # Calculate skewness
        skewness = self._calculate_option_skew(option_chain_analytics.get("raw_data", pd.DataFrame()))
        
        # Wyckoff-inspired features
        spring_detection = self._detect_spring_pattern(price_series, volume_series, constituents_df)
        upthrust_detection = self._detect_upthrust_pattern(price_series, volume_series, constituents_df)
        accumulation_score = self._calculate_accumulation_score(price_series, volume_series, oi_velocity)
        
        return ResearchFeatures(
            oi_velocity=oi_velocity,
            oi_velocity_ma=oi_velocity_ma,
            oi_velocity_std=oi_velocity_std,
            oi_regime=oi_regime,
            
            net_gamma=net_gamma,
            gamma_regime=gamma_regime,
            gamma_flip_distance=gamma_flip_distance,
            max_gamma_strike=max_gamma_strike,
            
            wall_strength=wall_strength,
            wall_defense_score=wall_defense_score,
            trap_probability=trap_probability,
            
            price_oi_divergence=price_oi_divergence,
            price_gamma_divergence=price_gamma_divergence,
            divergence_score=divergence_score,
            
            put_call_ratio=put_call_ratio,
            max_pain_distance=max_pain_distance,
            vix_smile=vix_smile,
            skewness=skewness,
            
            spring_detection=spring_detection,
            upthrust_detection=upthrust_detection,
            accumulation_score=accumulation_score
        )
    
    def _calculate_vix_smile(self, option_chain_df: pd.DataFrame, spot_price: float) -> float:
        """Calculate volatility smile curvature."""
        if option_chain_df.empty or spot_price <= 0:
            return 0.0
        
        # Group by distance from spot
        option_chain_df = option_chain_df.copy()
        option_chain_df['distance_pct'] = abs(option_chain_df['strike'] - spot_price) / spot_price * 100
        
        # Bin by distance
        bins = [0, 2, 5, 10, 20]
        smiles = []
        
        for i in range(len(bins) - 1):
            lower = bins[i]
            upper = bins[i + 1]
            
            mask = (option_chain_df['distance_pct'] >= lower) & (option_chain_df['distance_pct'] < upper)
            bin_iv = option_chain_df.loc[mask, 'iv'].mean()
            
            if not np.isnan(bin_iv):
                smiles.append(bin_iv)
        
        # Calculate smile curvature (higher = steeper smile)
        if len(smiles) >= 3:
            return smiles[0] - smiles[-1]  # ATM vs far OTM
        return 0.0
    
    def _calculate_option_skew(self, option_chain_df: pd.DataFrame) -> float:
        """Calculate option skew (put IV - call IV)."""
        if option_chain_df.empty:
            return 0.0
        
        put_iv = option_chain_df[option_chain_df['option_type'] == 'PE']['iv'].mean()
        call_iv = option_chain_df[option_chain_df['option_type'] == 'CE']['iv'].mean()
        
        if np.isnan(put_iv) or np.isnan(call_iv):
            return 0.0
        
        return put_iv - call_iv  # Positive = put skew (bearish), Negative = call skew (bullish)
    
    def _detect_spring_pattern(self, price_series: pd.Series, 
                              volume_series: pd.Series,
                              constituents_df: pd.DataFrame) -> float:
        """Detect Wyckoff spring pattern (bear trap)."""
        if len(price_series) < 10 or len(volume_series) < 10:
            return 0.0
        
        # Simplified spring detection
        recent_prices = price_series.iloc[-5:].values
        recent_volumes = volume_series.iloc[-5:].values
        
        # Spring: price makes lower low but closes above previous low
        if len(recent_prices) >= 5:
            low1 = np.min(recent_prices[:3])  # First low
            low2 = np.min(recent_prices[2:])  # Second low (potential spring)
            close = recent_prices[-1]
            
            # Volume spike on second low
            volume_spike = recent_volumes[-2] > np.mean(recent_volumes[:-1]) * 1.5
            
            if low2 < low1 * 0.995 and close > low1 and volume_spike:
                # Calculate spring probability
                price_recovery = (close - low2) / low2
                return min(price_recovery * 10, 1.0)
        
        return 0.0
    
    def _detect_upthrust_pattern(self, price_series: pd.Series,
                                volume_series: pd.Series,
                                constituents_df: pd.DataFrame) -> float:
        """Detect Wyckoff upthrust pattern (bull trap)."""
        if len(price_series) < 10 or len(volume_series) < 10:
            return 0.0
        
        recent_prices = price_series.iloc[-5:].values
        recent_volumes = volume_series.iloc[-5:].values
        
        # Upthrust: price makes higher high but closes below previous high
        if len(recent_prices) >= 5:
            high1 = np.max(recent_prices[:3])  # First high
            high2 = np.max(recent_prices[2:])  # Second high (potential upthrust)
            close = recent_prices[-1]
            
            # Volume spike on second high
            volume_spike = recent_volumes[-2] > np.mean(recent_volumes[:-1]) * 1.5
            
            if high2 > high1 * 1.005 and close < high1 and volume_spike:
                # Calculate upthrust probability
                price_rejection = (high2 - close) / high2
                return min(price_rejection * 10, 1.0)
        
        return 0.0
    
    def _calculate_accumulation_score(self, price_series: pd.Series,
                                     volume_series: pd.Series,
                                     oi_velocity: float) -> float:
        """Calculate accumulation/distribution score."""
        if len(price_series) < 20 or len(volume_series) < 20:
            return 0.0
        
        # Price in trading range
        price_range = price_series.iloc[-20:]
        range_high = price_range.max()
        range_low = price_range.min()
        range_width = (range_high - range_low) / range_low
        
        # Low volatility in range (accumulation)
        volatility = price_range.pct_change().std()
        
        # Volume analysis
        volume_trend = np.polyfit(range(len(volume_series.iloc[-20:])), 
                                 volume_series.iloc[-20:].values, 1)[0]
        
        # OI building during range (accumulation)
        oi_building = oi_velocity > 0.5
        
        # Calculate accumulation score
        score = 0.0
        
        # Narrow range with low volatility
        if range_width < 0.02 and volatility < 0.005:
            score += 0.3
        
        # Volume declining or stable (not distribution)
        if volume_trend <= 0:
            score += 0.2
        
        # OI building
        if oi_building:
            score += 0.3
        
        # Price near range lows (better accumulation)
        current_price = price_series.iloc[-1]
        if current_price < range_low * 1.02:
            score += 0.2
        
        return min(score, 1.0)

# ==============================
# ENHANCED FEATURE PIPELINE
# ==============================

# Global feature engine
_feature_engine = EnhancedFeatureEngine()

def build_and_store_features(
    *,
    timestamp: pd.Timestamp,
    option_chain_df: pd.DataFrame,
    spot_price: float,
    expiry_datetime,
    ltp: float,
    vwap: float,
    price_series: pd.Series,
    volume_series: pd.Series,
    constituents_df: pd.DataFrame,
    ccc_history: pd.Series,
    client: Optional[UpstoxClient] = None,
    option_keys: Optional[list] = None
) -> Optional[Dict]:
    """
    Enhanced feature pipeline with research-based feature engineering.
    
    Args:
        client: UpstoxClient instance (required for advanced analytics)
        option_keys: Option keys for fetching enhanced analytics
    
    Returns:
        Dictionary with features and research analytics
    """
    
    # ------------------------------
    # TIMESTAMP VALIDATION
    # ------------------------------
    if not isinstance(timestamp, pd.Timestamp):
        raise TypeError("timestamp must be pandas.Timestamp")
    
    timestamp_str = timestamp.strftime("%Y-%m-%dT%H:%M:%S.%f")
    
    # ------------------------------
    # FETCH ENHANCED ANALYTICS (if client provided)
    # ------------------------------
    research_analytics = None
    if client and option_keys and len(option_keys) > 0:
        try:
            # Get comprehensive analytics
            research_analytics = client.fetch_option_chain_with_analytics(
                option_keys, spot_price
            )
        except Exception as e:
            print(f"Warning: Could not fetch enhanced analytics: {e}")
            research_analytics = None
    
    # ------------------------------
    # COMPUTE BASE FEATURES
    # ------------------------------
    option_feats = compute_option_features(option_chain_df, spot_price, expiry_datetime)
    price_feats = compute_price_features(ltp, vwap, price_series, volume_series)
    breadth_feats = compute_breadth_features(constituents_df, ccc_history)
    
    # ------------------------------
    # COMPUTE RESEARCH FEATURES
    # ------------------------------
    research_feats_dict = {}
    if research_analytics:
        try:
            research_feats = _feature_engine.extract_research_features(
                research_analytics,
                spot_price,
                price_series,
                volume_series,
                constituents_df
            )
            
            # Convert research features to dictionary
            research_feats_dict = {
                # OI Velocity features
                "oi_velocity": research_feats.oi_velocity,
                "oi_velocity_ma": research_feats.oi_velocity_ma,
                "oi_velocity_std": research_feats.oi_velocity_std,
                "oi_regime_expansive": 1.0 if research_feats.oi_regime == "EXPANSIVE" else 0.0,
                "oi_regime_constricted": 1.0 if research_feats.oi_regime == "CONSTRICTED" else 0.0,
                
                # Gamma Exposure features
                "net_gamma": research_feats.net_gamma,
                "gamma_regime_positive": 1.0 if "POSITIVE" in research_feats.gamma_regime else 0.0,
                "gamma_regime_negative": 1.0 if "NEGATIVE" in research_feats.gamma_regime else 0.0,
                "gamma_flip_distance": research_feats.gamma_flip_distance,
                "max_gamma_strike_distance": abs(spot_price - research_feats.max_gamma_strike) / spot_price,
                
                # Structural features
                "wall_strength": research_feats.wall_strength,
                "wall_defense_score": research_feats.wall_defense_score,
                "trap_probability": research_feats.trap_probability,
                
                # Divergence features
                "price_oi_divergence": research_feats.price_oi_divergence,
                "price_gamma_divergence": research_feats.price_gamma_divergence,
                "divergence_score": research_feats.divergence_score,
                "has_divergence": 1.0 if research_feats.divergence_score > 0.3 else 0.0,
                
                # Market microstructure
                "put_call_ratio": research_feats.put_call_ratio,
                "max_pain_distance": research_feats.max_pain_distance,
                "vix_smile": research_feats.vix_smile,
                "skewness": research_feats.skewness,
                
                # Wyckoff features
                "spring_detection": research_feats.spring_detection,
                "upthrust_detection": research_feats.upthrust_detection,
                "accumulation_score": research_feats.accumulation_score,
                
                # Derived features
                "gamma_wall_interaction": research_feats.wall_strength * abs(research_feats.net_gamma),
                "velocity_divergence_composite": research_feats.oi_velocity * research_feats.divergence_score,
                "trap_gamma_composite": research_feats.trap_probability * (1.0 if research_feats.gamma_regime == "NEGATIVE" else 0.0)
            }
        except Exception as e:
            print(f"Warning: Research feature extraction failed: {e}")
            research_feats_dict = {}
    
    # ------------------------------
    # BUILD FEATURE ROW
    # ------------------------------
    feature_row = {
        "timestamp": timestamp_str,
        "feature_version": FEATURE_VERSION,
        "future_return_5m": None
    }
    
    # Fill ALL required feature columns explicitly
    for col in FEATURE_COLUMNS:
        if col == "timestamp":
            continue
        feature_row[col] = (
            option_feats.get(col) or
            price_feats.get(col) or
            breadth_feats.get(col) or
            0.0
        )
    
    # Add time to expiry with proper timezone handling
    def normalize_datetime(dt):
        """Normalize datetime to UTC timezone-aware."""
        if dt is None:
            return None
        
        # If it's already timezone-aware, convert to UTC
        if hasattr(dt, 'tzinfo') and dt.tzinfo is not None:
            return dt.astimezone(timezone.utc)
        
        # If it's timezone-naive, assume UTC and make it aware
        if isinstance(dt, pd.Timestamp):
            if dt.tz is None:
                return dt.tz_localize('UTC')
            else:
                return dt.tz_convert('UTC')
        elif isinstance(dt, datetime):
            return dt.replace(tzinfo=timezone.utc)
        
        return dt
    
    # Normalize both timestamps to UTC
    if expiry_datetime and timestamp:
        expiry_utc = normalize_datetime(expiry_datetime)
        timestamp_utc = normalize_datetime(timestamp)
        
        if expiry_utc and timestamp_utc:
            time_diff = (expiry_utc - timestamp_utc).total_seconds() / 60
            feature_row["time_to_expiry_minutes"] = max(int(time_diff), 0)
        else:
            feature_row["time_to_expiry_minutes"] = 0
    else:
        feature_row["time_to_expiry_minutes"] = 0
    
    # Merge research features
    feature_row.update(research_feats_dict)
    
    # ------------------------------
    # VALIDATE AND PERSIST
    # ------------------------------
    # Create DataFrame with all columns (base + research)
    all_columns = list(feature_row.keys())
    df = pd.DataFrame([feature_row], columns=all_columns)
    
    # Debug info
    print(f"FEATURE PIPELINE: Generated {len(feature_row)} features")
    print(f"Timestamp: {timestamp_str}")
    print(f"Spot Price: {spot_price}")
    
    if research_analytics:
        print(f"Research Analytics: {len(research_feats_dict)} research features added")
        # Log key metrics
        analytics = research_analytics.get("analytics", {})
        print(f"OI Velocity: {analytics.get('oi_velocity', 'N/A')}")
        print(f"Gamma Regime: {analytics.get('gamma_exposure', {}).get('regime', 'N/A')}")
        print(f"Market Regime: {analytics.get('market_regime', 'N/A')}")
    
    # Check for NULLs
    null_cols = df.columns[df.isnull().any()].tolist()
    if null_cols:
        print(f"WARNING: NULL values in columns: {null_cols}")
        # Fill NULLs with 0 for numeric columns
        for col in null_cols:
            if col in df.select_dtypes(include=[np.number]).columns:
                df[col] = df[col].fillna(0.0)
    
    assert not df.empty, "Feature DataFrame is empty"
    
    # Persist to database
    insert_market_features(df)
    
    print(f"‚úì Features stored successfully at {timestamp_str}")
    
    # Return comprehensive result
    return {
        "features": feature_row,
        "research_analytics": research_analytics.get("analytics", {}) if research_analytics else {},
        "market_insights": research_analytics.get("market_insights", []) if research_analytics else [],
        "timestamp": timestamp_str,
        "spot_price": spot_price
    }

# ==============================
# UTILITY FUNCTIONS
# ==============================

def create_feature_summary(feature_row: Dict) -> Dict:
    """Create a summary of key features for display."""
    summary = {
        "timestamp": feature_row.get("timestamp", "N/A"),
        "base_features": {},
        "research_features": {},
        "signals": {}
    }
    
    # Base features
    base_keys = ["put_call_ratio", "vwap_distance", "price_momentum", 
                 "ccc_value", "ccc_slope", "time_to_expiry_minutes"]
    for key in base_keys:
        if key in feature_row:
            summary["base_features"][key] = feature_row[key]
    
    # Research features (top level)
    research_keys = ["oi_velocity", "net_gamma", "trap_probability", 
                    "divergence_score", "wall_strength"]
    for key in research_keys:
        if key in feature_row:
            summary["research_features"][key] = feature_row[key]
    
    # Generate signals
    signals = []
    
    # OI Velocity signal
    oi_vel = feature_row.get("oi_velocity", 0)
    if oi_vel > 1.5:
        signals.append("üìà Strong OI Buildup")
    elif oi_vel < -1.5:
        signals.append("üìâ OI Unwinding")
    
    # Gamma signal
    gamma_regime_pos = feature_row.get("gamma_regime_positive", 0)
    gamma_regime_neg = feature_row.get("gamma_regime_negative", 0)
    if gamma_regime_pos > 0.5:
        signals.append("üìå Positive Gamma (Stabilizing)")
    elif gamma_regime_neg > 0.5:
        signals.append("üöÄ Negative Gamma (Accelerating)")
    
    # Trap signal
    trap_prob = feature_row.get("trap_probability", 0)
    if trap_prob > 0.7:
        signals.append("üéØ High Trap Probability")
    elif trap_prob > 0.5:
        signals.append("‚ö†Ô∏è Moderate Trap Risk")
    
    # Divergence signal
    has_div = feature_row.get("has_divergence", 0)
    if has_div > 0.5:
        signals.append("üîç Divergence Detected")
    
    summary["signals"] = signals
    
    return summary

def validate_feature_contract(feature_row: Dict) -> bool:
    """Validate feature row against contract."""
    # Check required columns
    required_base = ["timestamp", "feature_version"]
    for col in required_base:
        if col not in feature_row:
            print(f"Missing required column: {col}")
            return False
    
    # Check data types
    for col, value in feature_row.items():
        if col == "timestamp":
            continue
        if col == "feature_version":
            if not isinstance(value, str):
                print(f"Invalid type for {col}: expected str, got {type(value)}")
                return False
        elif col in ["oi_regime_expansive", "oi_regime_constricted", 
                    "gamma_regime_positive", "gamma_regime_negative", "has_divergence"]:
            # Binary features
            if not isinstance(value, (int, float)):
                print(f"Invalid type for {col}: expected numeric, got {type(value)}")
                return False
        elif isinstance(value, (int, float)):
            # Numeric features - check for extreme values
            if abs(value) > 1e6:  # Unreasonable large value
                print(f"Extreme value for {col}: {value}")
                return False
            if pd.isna(value):
                print(f"NaN value for {col}")
                return False
    
    return True



====================================================================================================
FILE: .\core\scheduler.py
====================================================================================================


File Name: scheduler.py
Full Path: G:\trading_app\core\scheduler.py
Size: 14.82 KB
Last Modified: 01/28/2026 12:17:22
Extension: .py

from datetime import datetime, time, timedelta, timezone
from typing import Callable, Optional, Dict
import threading
import time as time_module  # Rename to avoid conflict with datetime.time
import streamlit as st
import numpy as np  # Add numpy import for metrics

class MarketScheduler:
    """
    Enhanced scheduler with research-aware execution control.
    
    Features:
    - Market hours detection
    - Intelligent interval adjustment
    - Execution throttling
    - Performance monitoring
    - Error recovery
    """
    
    def __init__(
        self,
        interval_seconds: int = 30,
        market_open: time = time(9, 15),  # CORRECT: use time objects
        market_close: time = time(15, 30),  # CORRECT: use time objects
        pre_market_minutes: int = 15,
        post_market_minutes: int = 15
    ):
        self.interval_seconds = interval_seconds
        self.market_open = market_open
        self.market_close = market_close
        self.pre_market_minutes = pre_market_minutes
        self.post_market_minutes = post_market_minutes
        
        # Execution tracking
        self._last_run: Optional[datetime] = None
        self._last_success: Optional[datetime] = None
        self._consecutive_failures: int = 0
        self._total_executions: int = 0
        self._lock = threading.Lock()
        
        # Performance metrics
        self.execution_times = []
        self.error_log = []
        
        # Adaptive interval (can adjust based on market conditions)
        self.min_interval = 10  # seconds
        self.max_interval = 300  # seconds
        self._current_interval = interval_seconds
        
    # ==============================
    # TIME MANAGEMENT
    # ==============================
    
    def _is_market_open(self, now: datetime) -> bool:
        """
        Check if market is open, including pre/post market.
        """
        t = now.time()
        
        # Pre-market period
        pre_market_start = time(
            self.market_open.hour,
            max(0, self.market_open.minute - self.pre_market_minutes)  # Ensure minutes don't go negative
        )
        
        # Post-market period
        post_market_end_minute = self.market_close.minute + self.post_market_minutes
        post_market_end_hour = self.market_close.hour + (post_market_end_minute // 60)
        post_market_end_minute = post_market_end_minute % 60
        post_market_end = time(
            post_market_end_hour,
            post_market_end_minute
        )
        
        return pre_market_start <= t <= post_market_end
    
    def _is_core_market_hours(self, now: datetime) -> bool:
        """Check if it's core trading hours."""
        t = now.time()
        return self.market_open <= t <= self.market_close
    
    def _get_time_to_market_open(self, now: datetime) -> Optional[float]:
        """Get seconds until market opens."""
        market_open_today = datetime.combine(now.date(), self.market_open)
        
        if now < market_open_today:
            return (market_open_today - now).total_seconds()
        
        # Market already open today, check tomorrow
        tomorrow = now.date() + timedelta(days=1)
        market_open_tomorrow = datetime.combine(tomorrow, self.market_open)
        return (market_open_tomorrow - now).total_seconds()
    
    def _get_market_status(self, now: datetime) -> Dict[str, any]:
        """Get detailed market status."""
        status = {
            "is_market_open": self._is_market_open(now),
            "is_core_hours": self._is_core_market_hours(now),
            "current_time": now,
            "market_open": self.market_open,
            "market_close": self.market_close
        }
        
        if not status["is_market_open"]:
            time_to_open = self._get_time_to_market_open(now)
            if time_to_open:
                status["time_to_open_hours"] = time_to_open / 3600
                status["next_open"] = now + timedelta(seconds=time_to_open)
        
        return status
    
    # ==============================
    # EXECUTION MANAGEMENT
    # ==============================
    
    def _is_due(self, now: datetime) -> bool:
        """Check if execution is due."""
        if self._last_run is None:
            return True
        
        delta = (now - self._last_run).total_seconds()
        return delta >= self._current_interval
    
    def _adjust_interval(self, execution_time: float, success: bool):
        """
        Adaptively adjust execution interval based on performance.
        """
        if not success:
            self._consecutive_failures += 1
            
            # Increase interval on consecutive failures
            if self._consecutive_failures >= 3:
                self._current_interval = min(
                    self._current_interval * 1.5,
                    self.max_interval
                )
                self._consecutive_failures = 0
        else:
            self._consecutive_failures = 0
            self._last_success = datetime.now()
            
            # Adjust interval based on execution time
            if execution_time > self._current_interval * 0.8:
                # Execution taking too long, increase interval
                self._current_interval = min(
                    self._current_interval * 1.2,
                    self.max_interval
                )
            elif execution_time < self._current_interval * 0.3:
                # Execution fast, can decrease interval (but not below min)
                self._current_interval = max(
                    self._current_interval * 0.9,
                    self.min_interval
                )
    
    def _should_execute(self, now: datetime, market_regime: Optional[str] = None) -> bool:
        """
        Determine if execution should proceed based on multiple factors.
        """
        # Basic checks
        if not self._is_market_open(now):
            return False
        
        if not self._is_due(now):
            return False
        
        # Check for too many recent failures
        if self._consecutive_failures >= 5:
            if self._last_success:
                time_since_success = (now - self._last_success).total_seconds()
                if time_since_success < 300:  # 5 minutes
                    return False  # Wait after multiple failures
        
        # Market regime-based adjustments
        if market_regime:
            # Execute more frequently during volatile regimes
            if market_regime in ["SQUEEZE", "BREAKOUT", "ACCELERATING"]:
                self._current_interval = max(self.min_interval, self.interval_seconds * 0.7)
            # Execute less frequently during stable regimes
            elif market_regime in ["RANGING", "STABILIZING"]:
                self._current_interval = min(self.max_interval, self.interval_seconds * 1.3)
        
        return True
    
    # ==============================
    # EXECUTION ENTRYPOINT
    # ==============================
    
    def run_if_due(self, run_cycle: Callable[[], any], market_regime: Optional[str] = None) -> Dict[str, any]:
        """
        Execute run_cycle() if due, with enhanced monitoring.
        
        Returns:
            Execution result dictionary
        """
        now = datetime.now()
        
        # Check if should execute
        if not self._should_execute(now, market_regime):
            return {
                "executed": False,
                "reason": "Not due or market closed",
                "next_execution_in": self._get_next_execution_time(now),
                "market_status": self._get_market_status(now)
            }
        
        # Thread-safe execution
        with self._lock:
            # Double-check inside lock
            if not self._is_due(now):
                return {
                    "executed": False,
                    "reason": "Already executed by another thread",
                    "next_execution_in": self._get_next_execution_time(now)
                }
            
            execution_start = datetime.now()
            result = {
                "executed": True,
                "start_time": execution_start.isoformat(),
                "success": False,
                "error": None,
                "execution_time": 0.0
            }
            
            try:
                # Execute the cycle
                cycle_result = run_cycle()
                execution_end = datetime.now()
                execution_time = (execution_end - execution_start).total_seconds()
                
                # Update tracking
                self._last_run = execution_start
                self._total_executions += 1
                self.execution_times.append(execution_time)
                
                # Keep only recent execution times
                if len(self.execution_times) > 100:
                    self.execution_times = self.execution_times[-100:]
                
                # Update result
                result.update({
                    "success": True,
                    "execution_time": execution_time,
                    "end_time": execution_end.isoformat(),
                    "cycle_result": cycle_result,
                    "current_interval": self._current_interval
                })
                
                # Adjust interval based on performance
                self._adjust_interval(execution_time, success=True)
                
            except Exception as e:
                execution_end = datetime.now()
                execution_time = (execution_end - execution_start).total_seconds()
                
                # Log error
                self.error_log.append({
                    "timestamp": execution_start.isoformat(),
                    "error": str(e),
                    "execution_time": execution_time
                })
                
                # Keep error log manageable
                if len(self.error_log) > 50:
                    self.error_log = self.error_log[-50:]
                
                # Update result
                result.update({
                    "success": False,
                    "error": str(e),
                    "execution_time": execution_time,
                    "end_time": execution_end.isoformat()
                })
                
                # Adjust interval on failure
                self._adjust_interval(execution_time, success=False)
                
                # Update last run time even on failure (to prevent rapid retry)
                self._last_run = execution_start
                self._total_executions += 1
            
            return result
    
    # ==============================
    # MONITORING & METRICS
    # ==============================
    
    def get_metrics(self) -> Dict[str, any]:
        """Get scheduler performance metrics."""
        metrics = {
            "total_executions": self._total_executions,
            "current_interval": self._current_interval,
            "last_run": self._last_run.isoformat() if self._last_run else None,
            "last_success": self._last_success.isoformat() if self._last_success else None,
            "consecutive_failures": self._consecutive_failures,
            "recent_error_count": len(self.error_log[-10:]),
            "is_market_open_now": self._is_market_open(datetime.now())
        }
        
        # Add execution time statistics
        if self.execution_times:
            metrics.update({
                "avg_execution_time": np.mean(self.execution_times),
                "max_execution_time": np.max(self.execution_times),
                "min_execution_time": np.min(self.execution_times),
                "recent_avg_execution_time": np.mean(self.execution_times[-10:]) if len(self.execution_times) >= 10 else None
            })
        
        return metrics
    
    def _get_next_execution_time(self, now: datetime) -> float:
        """Get seconds until next execution."""
        if self._last_run is None:
            return 0.0
        
        next_run = self._last_run + timedelta(seconds=self._current_interval)
        return max((next_run - now).total_seconds(), 0.0)
    
    def reset(self):
        """Reset scheduler state."""
        with self._lock:
            self._last_run = None
            self._last_success = None
            self._consecutive_failures = 0
            self._total_executions = 0
            self.execution_times = []
            self.error_log = []
            self._current_interval = self.interval_seconds
    
    def set_interval(self, interval_seconds: int):
        """Dynamically set execution interval."""
        with self._lock:
            self.interval_seconds = interval_seconds
            self._current_interval = max(
                min(interval_seconds, self.max_interval),
                self.min_interval
            )

# ==============================
# UTILITY FUNCTIONS
# ==============================

def create_market_scheduler(
    interval_seconds: int = 30,
    market_open: str = "09:15",
    market_close: str = "15:30"
) -> MarketScheduler:
    """
    Factory function to create market scheduler.
    
    Args:
        interval_seconds: Execution interval in seconds
        market_open: Market open time (HH:MM)
        market_close: Market close time (HH:MM)
    """
    # Parse time strings
    open_hour, open_minute = map(int, market_open.split(":"))
    close_hour, close_minute = map(int, market_close.split(":"))
    
    return MarketScheduler(
        interval_seconds=interval_seconds,
        market_open=time(open_hour, open_minute),  # CORRECT
        market_close=time(close_hour, close_minute)  # CORRECT
    )

def display_scheduler_status(scheduler: MarketScheduler):
    """Display scheduler status in Streamlit."""
    if not scheduler:
        return
    
    metrics = scheduler.get_metrics()
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if metrics["is_market_open_now"]:
            st.success("üü¢ Market Open")
        else:
            st.warning("üî¥ Market Closed")
        
        next_exec = scheduler._get_next_execution_time(datetime.now())
        if next_exec > 0:
            st.metric("Next Execution", f"{int(next_exec)}s")
    
    with col2:
        st.metric("Interval", f"{metrics['current_interval']}s")
        
        if metrics.get("avg_execution_time"):
            st.metric("Avg Exec Time", f"{metrics['avg_execution_time']:.1f}s")
    
    with col3:
        st.metric("Total Executions", metrics["total_executions"])
        
        if metrics["recent_error_count"] > 0:
            st.error(f"Recent Errors: {metrics['recent_error_count']}")



====================================================================================================
FILE: .\core\session - Copy.py
====================================================================================================


File Name: session - Copy.py
Full Path: G:\trading_app\core\session - Copy.py
Size: 22.76 KB
Last Modified: 01/27/2026 21:39:47
Extension: .py

"""
Enhanced Upstox Session Manager with persistent token storage,
refresh mechanism, and market state tracking.
Incorporates research insights: funding liquidity monitoring via OI velocity
and market microstructure analysis.
"""

import os
import json
import requests
import streamlit as st
from urllib.parse import urlencode
from typing import Optional, Dict, Tuple
from datetime import datetime, timedelta
import pickle
from pathlib import Path
from cryptography.fernet import Fernet
import hashlib

# ==============================
# CONFIG (EDIT THESE)
# ==============================

UPSTOX_AUTH_URL = "https://api.upstox.com/v2/login/authorization/dialog"
UPSTOX_TOKEN_URL = "https://api.upstox.com/v2/login/authorization/token"
UPSTOX_PROFILE_URL = "https://api.upstox.com/v2/user/profile"

# Get from Streamlit secrets or environment
try:
    CLIENT_ID = st.secrets["UPSTOX_CLIENT_ID"]
    CLIENT_SECRET = st.secrets["UPSTOX_CLIENT_SECRET"]
    REDIRECT_URI = st.secrets["UPSTOX_REDIRECT_URI"]
except:
    CLIENT_ID = os.getenv("UPSTOX_CLIENT_ID")
    CLIENT_SECRET = os.getenv("UPSTOX_CLIENT_SECRET")
    REDIRECT_URI = os.getenv("UPSTOX_REDIRECT_URI")

# ==============================
# SECURE TOKEN STORAGE
# ==============================

class TokenStorage:
    """
    Secure token storage with encryption and persistence.
    Stores tokens locally with automatic refresh capability.
    """
    
    def __init__(self, storage_path: str = "data/tokens"):
        self.storage_dir = Path(storage_path)
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate or load encryption key
        self.key_file = self.storage_dir / ".key"
        if self.key_file.exists():
            with open(self.key_file, 'rb') as f:
                self.encryption_key = f.read()
        else:
            self.encryption_key = Fernet.generate_key()
            with open(self.key_file, 'wb') as f:
                f.write(self.encryption_key)
        
        self.cipher = Fernet(self.encryption_key)
        self.token_file = self.storage_dir / "upstox_tokens.enc"
        
        # Track funding liquidity state (research concept)
        self.funding_state = {
            "last_oi_velocity": 0.0,
            "liquidity_regime": "NORMAL",  # NORMAL, CONSTRICTED, EXPANSIVE
            "last_update": datetime.utcnow()
        }
    
    def _generate_user_hash(self) -> str:
        """Generate unique user identifier for multi-user support"""
        # In production, use actual user ID. Here we use client ID as proxy
        return hashlib.sha256(f"{CLIENT_ID}_{REDIRECT_URI}".encode()).hexdigest()[:16]
    
    def save_tokens(self, access_token: str, refresh_token: str, expires_in: int):
        """Securely save tokens with expiration"""
        token_data = {
            "access_token": access_token,
            "refresh_token": refresh_token,
            "expires_at": (datetime.utcnow() + timedelta(seconds=expires_in)).isoformat(),
            "client_id": CLIENT_ID,
            "user_hash": self._generate_user_hash(),
            "created_at": datetime.utcnow().isoformat()
        }
        
        # Encrypt and save
        encrypted = self.cipher.encrypt(pickle.dumps(token_data))
        with open(self.token_file, 'wb') as f:
            f.write(encrypted)
        
        st.success("‚úì Authentication tokens saved securely")
    
    def load_tokens(self) -> Optional[Dict]:
        """Load and decrypt tokens, check expiration"""
        if not self.token_file.exists():
            return None
        
        try:
            with open(self.token_file, 'rb') as f:
                encrypted = f.read()
            
            token_data = pickle.loads(self.cipher.decrypt(encrypted))
            
            # Check expiration
            expires_at = datetime.fromisoformat(token_data["expires_at"])
            if datetime.utcnow() > expires_at - timedelta(minutes=5):  # 5 min buffer
                st.warning("‚ö†Ô∏è Access token expired or near expiry")
                return None
            
            # Verify client matches
            if token_data.get("client_id") != CLIENT_ID:
                st.error("Client ID mismatch - reauthentication required")
                return None
            
            return token_data
        except Exception as e:
            st.error(f"Token load error: {e}")
            return None
    
    def update_funding_state(self, oi_velocity: float):
        """
        Update funding liquidity state based on OI velocity.
        Research concept: Monitor capital flow intensity.
        """
        self.funding_state["last_oi_velocity"] = oi_velocity
        
        # Determine liquidity regime (research concept)
        if oi_velocity > 1.5:
            regime = "EXPANSIVE"  # High capital inflow
        elif oi_velocity < -1.5:
            regime = "CONSTRICTED"  # Capital outflow
        else:
            regime = "NORMAL"
        
        if self.funding_state["liquidity_regime"] != regime:
            st.info(f"üí∞ Liquidity regime changed: {regime} (OI Velocity: {oi_velocity:.2f})")
        
        self.funding_state["liquidity_regime"] = regime
        self.funding_state["last_update"] = datetime.utcnow()
    
    def get_funding_state(self) -> Dict:
        """Get current funding liquidity state"""
        return self.funding_state.copy()

# ==============================
# ENHANCED SESSION MANAGER
# ==============================

class UpstoxSession:
    """
    Enhanced session manager with:
    1. Persistent token storage
    2. Automatic token refresh
    3. Funding liquidity tracking
    4. Market state awareness
    """
    
    SESSION_KEY = "upstox_access_token"
    SESSION_REFRESH_KEY = "upstox_refresh_token"
    SESSION_PROFILE_KEY = "upstox_profile"
    
    _token_storage = TokenStorage()
    _market_state = {
        "current_regime": None,
        "last_velocity": 0.0,
        "gamma_regime": None,  # POSITIVE/NEGATIVE from research
        "wall_levels": {"call": None, "put": None},
        "trap_zones": []
    }
    
    @staticmethod
    def is_authenticated() -> bool:
        """Check if user has valid authentication"""
        if UpstoxSession.SESSION_KEY in st.session_state:
            return True
        
        # Check persistent storage
        tokens = UpstoxSession._token_storage.load_tokens()
        if tokens:
            # Load into session state
            st.session_state[UpstoxSession.SESSION_KEY] = tokens["access_token"]
            st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = tokens["refresh_token"]
            return True
        
        return False
    
    @staticmethod
    def get_access_token() -> Optional[str]:
        """Get current access token, refresh if needed"""
        if not UpstoxSession.is_authenticated():
            return None
        
        # Check if token is in session state
        if UpstoxSession.SESSION_KEY in st.session_state:
            return st.session_state[UpstoxSession.SESSION_KEY]
        
        return None
    
    @staticmethod
    def logout() -> None:
        """Secure logout - clear all session and storage"""
        for key in [UpstoxSession.SESSION_KEY, 
                   UpstoxSession.SESSION_REFRESH_KEY,
                   UpstoxSession.SESSION_PROFILE_KEY]:
            if key in st.session_state:
                del st.session_state[key]
        
        # Clear persistent storage
        storage_file = Path("data/tokens/upstox_tokens.enc")
        if storage_file.exists():
            storage_file.unlink()
        
        # Clear query params
        st.query_params.clear()
        st.success("‚úì Logged out successfully")
        st.rerun()
    
    # ==========================
    # TOKEN MANAGEMENT
    # ==========================
    
    @staticmethod
    def refresh_access_token(refresh_token: str) -> Optional[Tuple[str, str]]:
        """
        Refresh expired access token using refresh token.
        Returns (new_access_token, new_refresh_token) or None
        """
        headers = {
            "Content-Type": "application/x-www-form-urlencoded",
            "Accept": "application/json"
        }
        
        data = {
            "grant_type": "refresh_token",
            "refresh_token": refresh_token,
            "client_id": CLIENT_ID,
            "client_secret": CLIENT_SECRET
        }
        
        try:
            response = requests.post(
                UPSTOX_TOKEN_URL,
                headers=headers,
                data=data,
                timeout=10
            )
            
            if response.status_code == 200:
                token_data = response.json()
                return token_data.get("access_token"), token_data.get("refresh_token")
            else:
                st.error(f"Token refresh failed: {response.text}")
                return None
                
        except Exception as e:
            st.error(f"Token refresh error: {e}")
            return None
    
    @staticmethod
    def exchange_code_for_token(code: str) -> Optional[Dict]:
        """Exchange authorization code for tokens with full token data"""
        headers = {
            "Content-Type": "application/x-www-form-urlencoded",
            "Accept": "application/json"
        }
        
        data = {
            "code": code,
            "client_id": CLIENT_ID,
            "client_secret": CLIENT_SECRET,
            "redirect_uri": REDIRECT_URI,
            "grant_type": "authorization_code"
        }
        
        try:
            response = requests.post(
                UPSTOX_TOKEN_URL,
                headers=headers,
                data=data,
                timeout=10
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                st.error(f"Token exchange failed: {response.text}")
                return None
                
        except Exception as e:
            st.error(f"Token exchange error: {e}")
            return None
    
    # ==========================
    # USER PROFILE
    # ==========================
    
    @staticmethod
    def get_user_profile(access_token: str) -> Optional[Dict]:
        """Fetch user profile for session context"""
        headers = {
            "Accept": "application/json",
            "Authorization": f"Bearer {access_token}"
        }
        
        try:
            response = requests.get(UPSTOX_PROFILE_URL, headers=headers, timeout=10)
            if response.status_code == 200:
                return response.json().get("data", {})
            return None
        except Exception as e:
            st.warning(f"Profile fetch warning: {e}")
            return None
    
    # ==========================
    # MARKET STATE TRACKING (Research Integration)
    # ==========================
    
    @staticmethod
    def update_market_state(oi_velocity: float, gamma_exposure: float, 
                           wall_levels: Dict, spot_divergence: float):
        """
        Update comprehensive market state based on research concepts.
        
        Args:
            oi_velocity: Rate of change of Open Interest
            gamma_exposure: Net Gamma Exposure (positive/negative)
            wall_levels: Dict with 'call' and 'put' wall strike levels
            spot_divergence: Divergence between spot and derivative metrics
        """
        # Update funding liquidity state
        UpstoxSession._token_storage.update_funding_state(oi_velocity)
        
        # Determine gamma regime (research concept)
        if gamma_exposure > 0:
            gamma_regime = "POSITIVE"  # Stabilizing, pinning expected
        elif gamma_exposure < 0:
            gamma_regime = "NEGATIVE"  # Accelerating, squeezes possible
        else:
            gamma_regime = "NEUTRAL"
        
        # Detect potential traps (research concept)
        trap_zones = []
        current_price = 0  # Would be passed in production
        
        # Check if price is near walls with high OI unwinding
        for side, level in wall_levels.items():
            if level and abs(current_price - level) / level < 0.005:  # Within 0.5%
                # High probability of trap if OI velocity is negative (unwinding)
                if oi_velocity < -1.0:
                    trap_zones.append({
                        "side": side,
                        "level": level,
                        "type": "GAMMA_TRAP" if abs(gamma_exposure) > 0.5 else "OI_TRAP",
                        "confidence": min(abs(oi_velocity) / 3, 1.0)
                    })
        
        # Update market state
        UpstoxSession._market_state.update({
            "last_velocity": oi_velocity,
            "gamma_regime": gamma_regime,
            "wall_levels": wall_levels,
            "trap_zones": trap_zones,
            "spot_divergence": spot_divergence,
            "funding_regime": UpstoxSession._token_storage.get_funding_state()["liquidity_regime"],
            "timestamp": datetime.utcnow().isoformat()
        })
    
    @staticmethod
    def get_market_state() -> Dict:
        """Get current market state analysis"""
        return UpstoxSession._market_state.copy()
    
    # ==========================
    # STREAMLIT ENTRYPOINT (ENHANCED)
    # ==========================
    
    @staticmethod
    def authenticate() -> str:
        """
        Enhanced authentication flow with:
        1. Persistent token storage
        2. Automatic refresh
        3. User profile loading
        4. Market state initialization
        """
        
        # Already authenticated in session
        if UpstoxSession.is_authenticated():
            token = UpstoxSession.get_access_token()
            
            # Load profile if not loaded
            if UpstoxSession.SESSION_PROFILE_KEY not in st.session_state:
                profile = UpstoxSession.get_user_profile(token)
                if profile:
                    st.session_state[UpstoxSession.SESSION_PROFILE_KEY] = profile
            
            return token
        
        # Check persistent storage
        stored_tokens = UpstoxSession._token_storage.load_tokens()
        if stored_tokens:
            # Check if token needs refresh
            expires_at = datetime.fromisoformat(stored_tokens["expires_at"])
            if datetime.utcnow() > expires_at - timedelta(minutes=10):
                # Attempt refresh
                new_tokens = UpstoxSession.refresh_access_token(
                    stored_tokens["refresh_token"]
                )
                if new_tokens:
                    access_token, refresh_token = new_tokens
                    UpstoxSession._token_storage.save_tokens(
                        access_token, refresh_token, 86400
                    )
                    st.session_state[UpstoxSession.SESSION_KEY] = access_token
                    st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = refresh_token
                    st.success("‚úì Token refreshed automatically")
                else:
                    # Refresh failed, need re-auth
                    st.warning("Session expired, please login again")
            else:
                # Use stored token
                st.session_state[UpstoxSession.SESSION_KEY] = stored_tokens["access_token"]
                st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = stored_tokens["refresh_token"]
            
            # Load profile
            token = UpstoxSession.get_access_token()
            profile = UpstoxSession.get_user_profile(token)
            if profile:
                st.session_state[UpstoxSession.SESSION_PROFILE_KEY] = profile
            
            return token
        
        # Check for OAuth callback
        query_params = st.query_params
        if "code" in query_params:
            try:
                # Exchange code for tokens
                token_data = UpstoxSession.exchange_code_for_token(query_params["code"])
                if token_data:
                    # Save tokens
                    UpstoxSession._token_storage.save_tokens(
                        token_data["access_token"],
                        token_data.get("refresh_token", ""),
                        token_data.get("expires_in", 86400)
                    )
                    
                    # Store in session
                    st.session_state[UpstoxSession.SESSION_KEY] = token_data["access_token"]
                    if "refresh_token" in token_data:
                        st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = token_data["refresh_token"]
                    
                    # Load profile
                    profile = UpstoxSession.get_user_profile(token_data["access_token"])
                    if profile:
                        st.session_state[UpstoxSession.SESSION_PROFILE_KEY] = profile
                    
                    # Clean URL
                    st.query_params.clear()
                    st.rerun()
                    
            except Exception as e:
                st.error(f"Authentication failed: {str(e)}")
                st.stop()
        
        # Show login interface with market context
        st.markdown("""
        <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                    padding: 30px; border-radius: 15px; color: white;'>
            <h2 style='color: white;'>üîê Algorithmic Trading Platform Login</h2>
            <p style='font-size: 16px;'>
                Access real-time derivatives analytics including:<br>
                ‚Ä¢ OI Velocity & Gamma Exposure (GEX)<br>
                ‚Ä¢ Structural Walls & Trap Detection<br>
                ‚Ä¢ Spot Divergence Analysis<br>
                ‚Ä¢ Market Microstructure Insights
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("---")
        
        # Login button using Streamlit's native button
        login_url = UpstoxSession.get_login_url()
        
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            if st.button("üìà Login with Upstox", type="primary", use_container_width=True):
                # Redirect to Upstox login
                js = f'window.open("{login_url}", "_blank")'
                st.components.v1.html(f"""
                    <script>
                        {js}
                    </script>
                """, height=0)
            
            st.markdown("""
            <div style="margin-top: 20px; padding: 15px; background: #f8f9fa; border-radius: 10px;">
                <small>üîê <strong>Secure:</strong> Tokens are encrypted and stored locally</small><br>
                <small>üîÑ <strong>Persistent:</strong> Session persists across restarts</small><br>
                <small>üìä <strong>Enhanced:</strong> Market state tracking enabled</small>
            </div>
            """, unsafe_allow_html=True)
            
            st.markdown("""
            <div style="margin-top: 20px; padding: 15px; background: #f8f9fa; border-radius: 10px;">
                <small>üîê <strong>Secure:</strong> Tokens are encrypted and stored locally</small><br>
                <small>üîÑ <strong>Persistent:</strong> Session persists across restarts</small><br>
                <small>üìä <strong>Enhanced:</strong> Market state tracking enabled</small>
            </div>
            """, unsafe_allow_html=True)
        
        st.stop()
    
    @staticmethod
    def get_login_url() -> str:
        """Generate OAuth login URL"""
        params = {
            "response_type": "code",
            "client_id": CLIENT_ID,
            "redirect_uri": REDIRECT_URI,
            "scope": "order placement portfolio"  # Add required scopes
        }
        return f"{UPSTOX_AUTH_URL}?{urlencode(params)}"
    
    @staticmethod
    def get_session_info() -> Dict:
        """Get comprehensive session information"""
        return {
            "authenticated": UpstoxSession.is_authenticated(),
            "has_profile": UpstoxSession.SESSION_PROFILE_KEY in st.session_state,
            "profile": st.session_state.get(UpstoxSession.SESSION_PROFILE_KEY, {}),
            "funding_state": UpstoxSession._token_storage.get_funding_state(),
            "market_state": UpstoxSession.get_market_state(),
            "storage_path": str(UpstoxSession._token_storage.storage_dir)
        }

# ==============================
# SESSION UTILITIES
# ==============================

def display_session_status():
    """Display session status in Streamlit sidebar"""
    if UpstoxSession.is_authenticated():
        with st.sidebar:
            st.markdown("---")
            st.markdown("### üß† Session Status")
            
            # User info
            profile = st.session_state.get(UpstoxSession.SESSION_PROFILE_KEY, {})
            if profile:
                st.markdown(f"**User:** {profile.get('user_name', 'N/A')}")
                st.markdown(f"**Email:** {profile.get('email', 'N/A')}")
            
            # Market state
            market_state = UpstoxSession.get_market_state()
            if market_state.get("funding_regime"):
                regime = market_state["funding_regime"]
                color = {
                    "EXPANSIVE": "üü¢",
                    "NORMAL": "üü°", 
                    "CONSTRICTED": "üî¥"
                }.get(regime, "‚ö™")
                st.markdown(f"**Funding:** {color} {regime}")
            
            if market_state.get("gamma_regime"):
                gamma = market_state["gamma_regime"]
                icon = "üìå" if gamma == "POSITIVE" else "üöÄ" if gamma == "NEGATIVE" else "‚öñÔ∏è"
                st.markdown(f"**Gamma:** {icon} {gamma}")
            
            # Logout button
            if st.button("üö™ Logout", type="secondary", use_container_width=True):
                UpstoxSession.logout()

# ==============================
# INITIALIZATION
# ==============================

def initialize_session():
    """
    Initialize session with enhanced capabilities.
    Call this at the start of your app.
    """
    # Ensure token storage directory exists
    Path("data/tokens").mkdir(parents=True, exist_ok=True)
    
    # Initialize session state keys
    session_keys = [
        UpstoxSession.SESSION_KEY,
        UpstoxSession.SESSION_REFRESH_KEY,
        UpstoxSession.SESSION_PROFILE_KEY,
        "market_state",
        "funding_analysis"
    ]
    
    for key in session_keys:
        if key not in st.session_state:
            st.session_state[key] = None if "state" in key else {}



====================================================================================================
FILE: .\core\session.py
====================================================================================================


File Name: session.py
Full Path: G:\trading_app\core\session.py
Size: 22.02 KB
Last Modified: 01/27/2026 23:00:51
Extension: .py

"""
Enhanced Upstox Session Manager with persistent token storage,
refresh mechanism, and market state tracking.
Incorporates research insights: funding liquidity monitoring via OI velocity
and market microstructure analysis.
"""

import os
import json
import requests
import streamlit as st
from urllib.parse import urlencode
from typing import Optional, Dict, Tuple
from datetime import datetime, timedelta
import pickle
from pathlib import Path
from cryptography.fernet import Fernet
import hashlib

# ==============================
# CONFIG (EDIT THESE)
# ==============================

# Upstox API v3 endpoints
UPSTOX_AUTH_URL = "https://api.upstox.com/v2/login/authorization/dialog"
UPSTOX_TOKEN_URL = "https://api.upstox.com/v2/login/authorization/token"
UPSTOX_PROFILE_URL = "https://api.upstox.com/v2/user/profile"

# Get from Streamlit secrets or environment
def get_config():
    """Get configuration from secrets.toml or environment variables."""
    # Try Streamlit secrets first
    try:
        if hasattr(st, 'secrets'):
            return {
                'client_id': st.secrets.get("UPSTOX_CLIENT_ID"),
                'client_secret': st.secrets.get("UPSTOX_CLIENT_SECRET"),
                'redirect_uri': st.secrets.get("UPSTOX_REDIRECT_URI", "http://127.0.0.1:8501/callback")
            }
    except:
        pass
    
    # Fallback to environment variables
    return {
        'client_id': os.getenv("UPSTOX_CLIENT_ID"),
        'client_secret': os.getenv("UPSTOX_CLIENT_SECRET"),
        'redirect_uri': os.getenv("UPSTOX_REDIRECT_URI", "http://127.0.0.1:8501/callback")
    }

config = get_config()
CLIENT_ID = config['client_id']
CLIENT_SECRET = config['client_secret']
REDIRECT_URI = config['redirect_uri']

# ==============================
# SECURE TOKEN STORAGE
# ==============================

class TokenStorage:
    """
    Secure token storage with encryption and persistence.
    Stores tokens locally with automatic refresh capability.
    """
    
    def __init__(self, storage_path: str = "data/tokens"):
        self.storage_dir = Path(storage_path)
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate or load encryption key
        self.key_file = self.storage_dir / ".key"
        if self.key_file.exists():
            with open(self.key_file, 'rb') as f:
                self.encryption_key = f.read()
        else:
            self.encryption_key = Fernet.generate_key()
            with open(self.key_file, 'wb') as f:
                f.write(self.encryption_key)
        
        self.cipher = Fernet(self.encryption_key)
        self.token_file = self.storage_dir / "upstox_tokens.enc"
        
        # Track funding liquidity state (research concept)
        self.funding_state = {
            "last_oi_velocity": 0.0,
            "liquidity_regime": "NORMAL",  # NORMAL, CONSTRICTED, EXPANSIVE
            "last_update": datetime.utcnow()
        }
    
    def _generate_user_hash(self) -> str:
        """Generate unique user identifier for multi-user support"""
        # In production, use actual user ID. Here we use client ID as proxy
        return hashlib.sha256(f"{CLIENT_ID}_{REDIRECT_URI}".encode()).hexdigest()[:16]
    
    def save_tokens(self, access_token: str, refresh_token: str, expires_in: int):
        """Securely save tokens with expiration"""
        token_data = {
            "access_token": access_token,
            "refresh_token": refresh_token,
            "expires_at": (datetime.utcnow() + timedelta(seconds=expires_in)).isoformat(),
            "client_id": CLIENT_ID,
            "user_hash": self._generate_user_hash(),
            "created_at": datetime.utcnow().isoformat()
        }
        
        # Encrypt and save
        encrypted = self.cipher.encrypt(pickle.dumps(token_data))
        with open(self.token_file, 'wb') as f:
            f.write(encrypted)
        
        st.success("‚úì Authentication tokens saved securely")
    
    def load_tokens(self) -> Optional[Dict]:
        """Load and decrypt tokens, check expiration"""
        if not self.token_file.exists():
            return None
        
        try:
            with open(self.token_file, 'rb') as f:
                encrypted = f.read()
            
            token_data = pickle.loads(self.cipher.decrypt(encrypted))
            
            # Check expiration
            expires_at = datetime.fromisoformat(token_data["expires_at"])
            if datetime.utcnow() > expires_at - timedelta(minutes=5):  # 5 min buffer
                st.warning("‚ö†Ô∏è Access token expired or near expiry")
                return None
            
            # Verify client matches
            if token_data.get("client_id") != CLIENT_ID:
                st.error("Client ID mismatch - reauthentication required")
                return None
            
            return token_data
        except Exception as e:
            st.error(f"Token load error: {e}")
            return None
    
    def update_funding_state(self, oi_velocity: float):
        """
        Update funding liquidity state based on OI velocity.
        Research concept: Monitor capital flow intensity.
        """
        self.funding_state["last_oi_velocity"] = oi_velocity
        
        # Determine liquidity regime (research concept)
        if oi_velocity > 1.5:
            regime = "EXPANSIVE"  # High capital inflow
        elif oi_velocity < -1.5:
            regime = "CONSTRICTED"  # Capital outflow
        else:
            regime = "NORMAL"
        
        if self.funding_state["liquidity_regime"] != regime:
            st.info(f"üí∞ Liquidity regime changed: {regime} (OI Velocity: {oi_velocity:.2f})")
        
        self.funding_state["liquidity_regime"] = regime
        self.funding_state["last_update"] = datetime.utcnow()
    
    def get_funding_state(self) -> Dict:
        """Get current funding liquidity state"""
        return self.funding_state.copy()

# ==============================
# ENHANCED SESSION MANAGER
# ==============================

class UpstoxSession:
    """
    Enhanced session manager with:
    1. Persistent token storage
    2. Automatic token refresh
    3. Funding liquidity tracking
    4. Market state awareness
    """
    
    SESSION_KEY = "upstox_access_token"
    SESSION_REFRESH_KEY = "upstox_refresh_token"
    SESSION_PROFILE_KEY = "upstox_profile"
    
    _token_storage = TokenStorage()
    _market_state = {
        "current_regime": None,
        "last_velocity": 0.0,
        "gamma_regime": None,  # POSITIVE/NEGATIVE from research
        "wall_levels": {"call": None, "put": None},
        "trap_zones": []
    }
    
    @staticmethod
    def is_authenticated() -> bool:
        """Check if user has valid authentication"""
        if UpstoxSession.SESSION_KEY in st.session_state:
            return True
        
        # Check persistent storage
        tokens = UpstoxSession._token_storage.load_tokens()
        if tokens:
            # Load into session state
            st.session_state[UpstoxSession.SESSION_KEY] = tokens["access_token"]
            st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = tokens["refresh_token"]
            return True
        
        return False
    
    @staticmethod
    def get_access_token() -> Optional[str]:
        """Get current access token, refresh if needed"""
        if not UpstoxSession.is_authenticated():
            return None
        
        # Check if token is in session state
        if UpstoxSession.SESSION_KEY in st.session_state:
            return st.session_state[UpstoxSession.SESSION_KEY]
        
        return None
    
    @staticmethod
    def logout() -> None:
        """Secure logout - clear all session and storage"""
        for key in [UpstoxSession.SESSION_KEY, 
                   UpstoxSession.SESSION_REFRESH_KEY,
                   UpstoxSession.SESSION_PROFILE_KEY]:
            if key in st.session_state:
                del st.session_state[key]
        
        # Clear persistent storage
        storage_file = Path("data/tokens/upstox_tokens.enc")
        if storage_file.exists():
            storage_file.unlink()
        
        # Clear query params
        st.query_params.clear()
        st.success("‚úì Logged out successfully")
        st.rerun()
    
    # ==========================
    # TOKEN MANAGEMENT
    # ==========================
    
    @staticmethod
    def refresh_access_token(refresh_token: str) -> Optional[Tuple[str, str]]:
        """
        Refresh expired access token using refresh token.
        Returns (new_access_token, new_refresh_token) or None
        """
        headers = {
            "Content-Type": "application/x-www-form-urlencoded",
            "Accept": "application/json"
        }
        
        data = {
            "grant_type": "refresh_token",
            "refresh_token": refresh_token,
            "client_id": CLIENT_ID,
            "client_secret": CLIENT_SECRET
        }
        
        try:
            response = requests.post(
                UPSTOX_TOKEN_URL,
                headers=headers,
                data=data,
                timeout=10
            )
            
            if response.status_code == 200:
                token_data = response.json()
                return token_data.get("access_token"), token_data.get("refresh_token")
            else:
                st.error(f"Token refresh failed: {response.text}")
                return None
                
        except Exception as e:
            st.error(f"Token refresh error: {e}")
            return None
    
    @staticmethod
    def exchange_code_for_token(code: str) -> Optional[Dict]:
        """Exchange authorization code for tokens with full token data"""
        headers = {
            "Content-Type": "application/x-www-form-urlencoded",
            "Accept": "application/json"
        }
        
        data = {
            "code": code,
            "client_id": CLIENT_ID,
            "client_secret": CLIENT_SECRET,
            "redirect_uri": REDIRECT_URI,
            "grant_type": "authorization_code"
        }
        
        try:
            response = requests.post(
                UPSTOX_TOKEN_URL,
                headers=headers,
                data=data,
                timeout=10
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                st.error(f"Token exchange failed: {response.text}")
                return None
                
        except Exception as e:
            st.error(f"Token exchange error: {e}")
            return None
    
    # ==========================
    # USER PROFILE
    # ==========================
    
    @staticmethod
    def get_user_profile(access_token: str) -> Optional[Dict]:
        """Fetch user profile for session context"""
        headers = {
            "Accept": "application/json",
            "Authorization": f"Bearer {access_token}"
        }
        
        try:
            response = requests.get(UPSTOX_PROFILE_URL, headers=headers, timeout=10)
            if response.status_code == 200:
                return response.json().get("data", {})
            return None
        except Exception as e:
            st.warning(f"Profile fetch warning: {e}")
            return None
    
    # ==========================
    # MARKET STATE TRACKING (Research Integration)
    # ==========================
    
    @staticmethod
    def update_market_state(oi_velocity: float, gamma_exposure: float, 
                           wall_levels: Dict, spot_divergence: float):
        """
        Update comprehensive market state based on research concepts.
        
        Args:
            oi_velocity: Rate of change of Open Interest
            gamma_exposure: Net Gamma Exposure (positive/negative)
            wall_levels: Dict with 'call' and 'put' wall strike levels
            spot_divergence: Divergence between spot and derivative metrics
        """
        # Update funding liquidity state
        UpstoxSession._token_storage.update_funding_state(oi_velocity)
        
        # Determine gamma regime (research concept)
        if gamma_exposure > 0:
            gamma_regime = "POSITIVE"  # Stabilizing, pinning expected
        elif gamma_exposure < 0:
            gamma_regime = "NEGATIVE"  # Accelerating, squeezes possible
        else:
            gamma_regime = "NEUTRAL"
        
        # Detect potential traps (research concept)
        trap_zones = []
        current_price = 0  # Would be passed in production
        
        # Check if price is near walls with high OI unwinding
        for side, level in wall_levels.items():
            if level and abs(current_price - level) / level < 0.005:  # Within 0.5%
                # High probability of trap if OI velocity is negative (unwinding)
                if oi_velocity < -1.0:
                    trap_zones.append({
                        "side": side,
                        "level": level,
                        "type": "GAMMA_TRAP" if abs(gamma_exposure) > 0.5 else "OI_TRAP",
                        "confidence": min(abs(oi_velocity) / 3, 1.0)
                    })
        
        # Update market state
        UpstoxSession._market_state.update({
            "last_velocity": oi_velocity,
            "gamma_regime": gamma_regime,
            "wall_levels": wall_levels,
            "trap_zones": trap_zones,
            "spot_divergence": spot_divergence,
            "funding_regime": UpstoxSession._token_storage.get_funding_state()["liquidity_regime"],
            "timestamp": datetime.utcnow().isoformat()
        })
    
    @staticmethod
    def get_market_state() -> Dict:
        """Get current market state analysis"""
        return UpstoxSession._market_state.copy()
    
    # ==========================
    # STREAMLIT ENTRYPOINT (ENHANCED)
    # ==========================
    

    @staticmethod
    def authenticate() -> str:
        """
        Simplified authentication flow that ALWAYS shows login button when not authenticated
        """
        
        # DEBUG: Show what's in query params
        query_params = st.query_params
        st.sidebar.write(f"üîç Query params: {dict(query_params)}")
        
        # Check for OAuth callback FIRST (before checking anything else)
        if "code" in query_params:
            try:
                code = query_params["code"]
                st.sidebar.write(f"üîç Got OAuth code: {code[:20]}...")
                
                # Exchange code for tokens
                token_data = UpstoxSession.exchange_code_for_token(code)
                if token_data:
                    st.sidebar.success("‚úÖ Token exchange successful")
                    
                    # Save tokens
                    UpstoxSession._token_storage.save_tokens(
                        token_data["access_token"],
                        token_data.get("refresh_token", ""),
                        token_data.get("expires_in", 86400)
                    )
                    
                    # Store in session
                    st.session_state[UpstoxSession.SESSION_KEY] = token_data["access_token"]
                    if "refresh_token" in token_data:
                        st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = token_data["refresh_token"]
                    
                    # Load profile
                    profile = UpstoxSession.get_user_profile(token_data["access_token"])
                    if profile:
                        st.session_state[UpstoxSession.SESSION_PROFILE_KEY] = profile
                    
                    # Clean URL and rerun
                    st.query_params.clear()
                    st.rerun()
                else:
                    st.error("‚ùå Failed to exchange code for tokens")
                    
            except Exception as e:
                st.error(f"‚ùå Authentication failed: {str(e)}")
                import traceback
                st.error(traceback.format_exc())
        
        # Check if already authenticated
        if UpstoxSession.is_authenticated():
            token = UpstoxSession.get_access_token()
            return token
        
        # Check persistent storage
        stored_tokens = UpstoxSession._token_storage.load_tokens()
        if stored_tokens:
            # Use stored token
            st.session_state[UpstoxSession.SESSION_KEY] = stored_tokens["access_token"]
            st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = stored_tokens["refresh_token"]
            
            token = UpstoxSession.get_access_token()
            profile = UpstoxSession.get_user_profile(token)
            if profile:
                st.session_state[UpstoxSession.SESSION_PROFILE_KEY] = profile
            
            return token
        
        # ==============================================
        # SHOW LOGIN INTERFACE (only if not authenticated)
        # ==============================================
        st.markdown("""
        <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                    padding: 30px; border-radius: 15px; color: white;'>
            <h2 style='color: white;'>üîê Algorithmic Trading Platform Login</h2>
            <p style='font-size: 16px;'>
                Access real-time derivatives analytics
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("---")
        
        # Login button
        login_url = UpstoxSession.get_login_url()
        
        st.markdown(f"""
        <div style="text-align: center;">
            <a href="{login_url}">
                <button style="
                    background-color: #00d09c;
                    color: white;
                    padding: 15px 30px;
                    font-size: 18px;
                    font-weight: bold;
                    border: none;
                    border-radius: 10px;
                    cursor: pointer;
                    width: 80%;
                    margin: 20px 0;
                    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
                    transition: all 0.3s ease;
                ">
                üìà Login with Upstox
                </button>
            </a>
        </div>
        """, unsafe_allow_html=True)
        
        st.info("""
        **Login Instructions:**
        1. Click the "Login with Upstox" button above
        2. You'll be redirected to Upstox authorization page
        3. Log in with your Upstox credentials
        4. Authorize the application
        5. You'll be redirected back to this app
        """)
        
        # CRITICAL: Stop execution here
        st.stop()
        return None
    
    @staticmethod
    def get_login_url() -> str:
        """Generate OAuth login URL"""
        params = {
            "response_type": "code",
            "client_id": CLIENT_ID,
            "redirect_uri": REDIRECT_URI,
            "scope": "order placement portfolio"
        }
        return f"{UPSTOX_AUTH_URL}?{urlencode(params)}"
    
    @staticmethod
    def get_session_info() -> Dict:
        """Get comprehensive session information"""
        return {
            "authenticated": UpstoxSession.is_authenticated(),
            "has_profile": UpstoxSession.SESSION_PROFILE_KEY in st.session_state,
            "profile": st.session_state.get(UpstoxSession.SESSION_PROFILE_KEY, {}),
            "funding_state": UpstoxSession._token_storage.get_funding_state(),
            "market_state": UpstoxSession.get_market_state(),
            "storage_path": str(UpstoxSession._token_storage.storage_dir)
        }

# ==============================
# SESSION UTILITIES
# ==============================

def display_session_status():
    """Display session status in Streamlit sidebar"""
    if UpstoxSession.is_authenticated():
        with st.sidebar:
            st.markdown("---")
            st.markdown("### üß† Session Status")
            
            # User info
            profile = st.session_state.get(UpstoxSession.SESSION_PROFILE_KEY, {})
            if profile:
                st.markdown(f"**User:** {profile.get('user_name', 'N/A')}")
                st.markdown(f"**Email:** {profile.get('email', 'N/A')}")
            
            # Market state
            market_state = UpstoxSession.get_market_state()
            if market_state.get("funding_regime"):
                regime = market_state["funding_regime"]
                color = {
                    "EXPANSIVE": "üü¢",
                    "NORMAL": "üü°", 
                    "CONSTRICTED": "üî¥"
                }.get(regime, "‚ö™")
                st.markdown(f"**Funding:** {color} {regime}")
            
            if market_state.get("gamma_regime"):
                gamma = market_state["gamma_regime"]
                icon = "üìå" if gamma == "POSITIVE" else "üöÄ" if gamma == "NEGATIVE" else "‚öñÔ∏è"
                st.markdown(f"**Gamma:** {icon} {gamma}")
            
            # Logout button
            if st.button("üö™ Logout", type="secondary", use_container_width=True):
                UpstoxSession.logout()

# ==============================
# INITIALIZATION
# ==============================

def initialize_session():
    """
    Initialize session with enhanced capabilities.
    Call this at the start of your app.
    """
    # Ensure token storage directory exists
    Path("data/tokens").mkdir(parents=True, exist_ok=True)
    
    # Initialize session state keys
    session_keys = [
        UpstoxSession.SESSION_KEY,
        UpstoxSession.SESSION_REFRESH_KEY,
        UpstoxSession.SESSION_PROFILE_KEY,
        "market_state",
        "funding_analysis"
    ]
    
    for key in session_keys:
        if key not in st.session_state:
            st.session_state[key] = None if "state" in key else {}



====================================================================================================
FILE: .\core\signals\state_machine.py
====================================================================================================


File Name: state_machine.py
Full Path: G:\trading_app\core\signals\state_machine.py
Size: 38.59 KB
Last Modified: 01/28/2026 10:56:16
Extension: .py

"""
Enhanced Signal State Machine with Research-Based Decision Making.
Implements advanced signal generation using:
1. OI Velocity & Gamma Exposure analysis
2. Structural Walls & Traps detection
3. Spot Divergence analysis
4. Wyckoff pattern recognition
5. Market regime awareness
"""

from datetime import datetime, timedelta
import uuid
import pandas as pd
import numpy as np
from typing import Dict, Optional, Tuple, List
from dataclasses import dataclass
from enum import Enum
import json

# Import research features
from ml.feature_contract import FEATURE_VERSION, RESEARCH_FEATURES

# ==============================
# ENUMS & DATA CLASSES
# ==============================

class SignalStrength(Enum):
    """Signal strength classification"""
    WEAK = "WEAK"        # Low confidence, minor edge
    MODERATE = "MODERATE" # Decent confidence, clear edge
    STRONG = "STRONG"    # High confidence, strong edge
    VERY_STRONG = "VERY_STRONG" # Very high confidence, major edge

class MarketRegime(Enum):
    """Market regime classification"""
    ACCUMULATION = "ACCUMULATION"      # Wyckoff accumulation
    DISTRIBUTION = "DISTRIBUTION"      # Wyckoff distribution
    UPTREND = "UPTREND"                # Strong uptrend
    DOWNTREND = "DOWNTREND"            # Strong downtrend
    RANGING = "RANGING"                # Range-bound
    BREAKOUT = "BREAKOUT"              # Breakout from range
    SQUEEZE = "SQUEEZE"                # Gamma/OI squeeze
    REVERSAL = "REVERSAL"              # Trend reversal

class TrapType(Enum):
    """Types of market traps"""
    GAMMA_TRAP = "GAMMA_TRAP"          # Gamma-induced squeeze
    OI_TRAP = "OI_TRAP"                # OI unwinding trap
    WYCKOFF_SPRING = "WYCKOFF_SPRING"  # Wyckoff spring (bear trap)
    WYCKOFF_UPTHRUST = "WYCKOFF_UPTHRUST" # Wyckoff upthrust (bull trap)
    DIVERGENCE_TRAP = "DIVERGENCE_TRAP" # Divergence-based trap

@dataclass
class SignalComponents:
    """Components of signal decision"""
    # Core components
    trend_score: float                # -1 to 1 (bearish to bullish)
    momentum_score: float             # -1 to 1 (weak to strong)
    
    # Research components
    oi_velocity_score: float          # -1 to 1 (negative to positive velocity)
    gamma_score: float               # -1 to 1 (negative to positive gamma)
    wall_interaction_score: float    # -1 to 1 (wall defense vs trap)
    divergence_score: float          # -1 to 1 (bearish to bullish divergence)
    
    # Wyckoff components
    wyckoff_phase_score: float       # -1 to 1 (distribution to accumulation)
    pattern_score: float             # -1 to 1 (bearish to bullish patterns)
    
    # Composite scores
    composite_score: float           # Overall score (-1 to 1)
    confidence: float               # Signal confidence (0-1)
    
    # Regime classification
    market_regime: MarketRegime
    regime_confidence: float

@dataclass
class TrapAnalysis:
    """Analysis of potential trap"""
    trap_type: TrapType
    strike_level: float
    direction: str  # "BULLISH" or "BEARISH"
    confidence: float
    trigger_conditions: List[str]
    expected_move_pct: float

# ==============================
# ENHANCED SIGNAL STATE MACHINE
# ==============================

class SignalStateMachine:
    """
    Advanced signal generator with research-based decision making.
    Incorporates OI velocity, gamma exposure, walls/traps, divergence.
    """
    
    def __init__(
        self,
        signal_expiry_minutes: int = 5,
        confidence_threshold: float = 0.2,
        trap_confidence_threshold: float = 0.6
    ):
        self.signal_expiry_minutes = signal_expiry_minutes
        self.confidence_threshold = confidence_threshold
        self.trap_confidence_threshold = trap_confidence_threshold
        
        # State tracking
        self.signal_history = []
        self.regime_history = []
        self.trap_detections = []
        
        # Thresholds (configurable)
        self.thresholds = {
            "oi_velocity_high": 1.5,      # œÉ
            "oi_velocity_low": -1.5,      # œÉ
            "gamma_high": 500,           # Net gamma threshold
            "gamma_low": -500,           # Net gamma threshold
            "trap_prob_high": 0.7,       # High trap probability
            "divergence_high": 0.5,      # Significant divergence
            "wall_strength_high": 0.3,   # Strong wall
            "spring_detection_high": 0.6, # Strong spring pattern
            "upthrust_detection_high": 0.6 # Strong upthrust pattern
        }
    
    # ==============================
    # RESEARCH FEATURE EXTRACTION
    # ==============================
    
    def extract_research_features(self, feature_row: pd.Series) -> Dict:
        """
        Extract and normalize research features from feature row.
        """
        feats = {}
        
        # OI Velocity features
        feats["oi_velocity"] = feature_row.get("oi_velocity", 0.0)
        feats["oi_regime_expansive"] = feature_row.get("oi_regime_expansive", 0.0)
        feats["oi_regime_constricted"] = feature_row.get("oi_regime_constricted", 0.0)
        
        # Gamma Exposure features
        feats["net_gamma"] = feature_row.get("net_gamma", 0.0)
        feats["gamma_regime_positive"] = feature_row.get("gamma_regime_positive", 0.0)
        feats["gamma_regime_negative"] = feature_row.get("gamma_regime_negative", 0.0)
        feats["gamma_flip_distance"] = feature_row.get("gamma_flip_distance", 0.0)
        
        # Structural features
        feats["wall_strength"] = feature_row.get("wall_strength", 0.0)
        feats["wall_defense_score"] = feature_row.get("wall_defense_score", 0.0)
        feats["trap_probability"] = feature_row.get("trap_probability", 0.0)
        
        # Divergence features
        feats["price_oi_divergence"] = feature_row.get("price_oi_divergence", 0.0)
        feats["price_gamma_divergence"] = feature_row.get("price_gamma_divergence", 0.0)
        feats["divergence_score"] = feature_row.get("divergence_score", 0.0)
        feats["has_divergence"] = feature_row.get("has_divergence", 0.0)
        
        # Wyckoff features
        feats["spring_detection"] = feature_row.get("spring_detection", 0.0)
        feats["upthrust_detection"] = feature_row.get("upthrust_detection", 0.0)
        feats["accumulation_score"] = feature_row.get("accumulation_score", 0.0)
        
        # Composite features
        feats["gamma_wall_interaction"] = feature_row.get("gamma_wall_interaction", 0.0)
        feats["velocity_divergence_composite"] = feature_row.get("velocity_divergence_composite", 0.0)
        feats["trap_gamma_composite"] = feature_row.get("trap_gamma_composite", 0.0)
        
        # Base features (still important)
        feats["put_call_ratio"] = feature_row.get("put_call_ratio", 1.0)
        feats["price_momentum"] = feature_row.get("price_momentum", 0.0)
        feats["ccc_slope"] = feature_row.get("ccc_slope", 0.0)
        feats["vwap_distance"] = feature_row.get("vwap_distance", 0.0)
        
        return feats
    
    # ==============================
    # COMPONENT SCORING
    # ==============================
    
    def score_oi_velocity(self, feats: Dict) -> Tuple[float, str]:
        """
        Score OI velocity component.
        
        Returns:
            score (-1 to 1), analysis
        """
        velocity = feats.get("oi_velocity", 0.0)
        expansive = feats.get("oi_regime_expansive", 0.0)
        constricted = feats.get("oi_regime_constricted", 0.0)
        
        # Velocity scoring
        if velocity > self.thresholds["oi_velocity_high"]:
            score = 1.0  # Strong buildup
            analysis = "Strong OI buildup - initiative capital entering"
        elif velocity > 0.5:
            score = 0.5  # Moderate buildup
            analysis = "Moderate OI buildup"
        elif velocity < self.thresholds["oi_velocity_low"]:
            score = -1.0  # Strong unwinding
            analysis = "Strong OI unwinding - capital exiting"
        elif velocity < -0.5:
            score = -0.5  # Moderate unwinding
            analysis = "Moderate OI unwinding"
        else:
            score = 0.0  # Neutral
            analysis = "OI velocity neutral"
        
        # Regime adjustment
        if expansive > 0.5:
            score = max(score, 0.3)  # Bias bullish
            analysis += " (EXPANSIVE regime)"
        elif constricted > 0.5:
            score = min(score, -0.3)  # Bias bearish
            analysis += " (CONSTRICTED regime)"
        
        return score, analysis
    
    def score_gamma_exposure(self, feats: Dict) -> Tuple[float, str]:
        """
        Score Gamma Exposure component.
        
        Returns:
            score (-1 to 1), analysis
        """
        net_gamma = feats.get("net_gamma", 0.0)
        gamma_pos = feats.get("gamma_regime_positive", 0.0)
        gamma_neg = feats.get("gamma_regime_negative", 0.0)
        flip_distance = feats.get("gamma_flip_distance", 1.0)
        
        # Gamma scoring
        if gamma_neg > 0.5 and abs(net_gamma) > self.thresholds["gamma_high"]:
            score = -1.0  # Strong negative gamma (accelerating)
            analysis = "Strong negative gamma - volatility acceleration likely"
        elif gamma_neg > 0.5:
            score = -0.7  # Moderate negative gamma
            analysis = "Negative gamma regime - trending moves possible"
        elif gamma_pos > 0.5 and abs(net_gamma) > self.thresholds["gamma_high"]:
            score = 0.3  # Strong positive gamma (stabilizing)
            analysis = "Strong positive gamma - range-bound/pinning likely"
        elif gamma_pos > 0.5:
            score = 0.1  # Moderate positive gamma
            analysis = "Positive gamma regime - mean reversion favored"
        else:
            score = 0.0  # Neutral
            analysis = "Gamma exposure neutral"
        
        # Flip distance adjustment (closer to flip = more uncertainty)
        if flip_distance < 0.01:  # Very close to flip
            score *= 0.5  # Reduce confidence near flip
            analysis += " (Near gamma flip)"
        
        return score, analysis
    
    def score_structure(self, feats: Dict) -> Tuple[float, str, Optional[TrapAnalysis]]:
        """
        Score structural components (walls, traps, Wyckoff).
        
        Returns:
            score (-1 to 1), analysis, trap_analysis
        """
        wall_strength = feats.get("wall_strength", 0.0)
        wall_defense = feats.get("wall_defense_score", 0.0)
        trap_prob = feats.get("trap_probability", 0.0)
        spring = feats.get("spring_detection", 0.0)
        upthrust = feats.get("upthrust_detection", 0.0)
        accumulation = feats.get("accumulation_score", 0.0)
        
        score = 0.0
        analysis = []
        trap_analysis = None
        
        # Wall analysis
        if wall_strength > self.thresholds["wall_strength_high"]:
            if wall_defense > 0.7:
                score += 0.3  # Strong defense = continuation
                analysis.append(f"Strong wall defense ({wall_strength:.2f})")
            else:
                score -= 0.2  # Weak defense = potential break
                analysis.append(f"Weak wall defense ({wall_strength:.2f})")
        
        # Trap analysis
        if trap_prob > self.thresholds["trap_prob_high"]:
            # High trap probability
            trap_type = self._classify_trap_type(feats)
            if trap_type in [TrapType.GAMMA_TRAP, TrapType.WYCKOFF_SPRING]:
                # Bullish traps
                trap_analysis = TrapAnalysis(
                    trap_type=trap_type,
                    strike_level=0,  # Would be actual strike in production
                    direction="BULLISH",
                    confidence=trap_prob,
                    trigger_conditions=["Price breach", "OI unwinding"],
                    expected_move_pct=2.0  # Estimated move
                )
                score += 0.5  # Bullish bias
                analysis.append(f"{trap_type.value} detected (conf: {trap_prob:.2f})")
            elif trap_type in [TrapType.OI_TRAP, TrapType.WYCKOFF_UPTHRUST]:
                # Bearish traps
                trap_analysis = TrapAnalysis(
                    trap_type=trap_type,
                    strike_level=0,
                    direction="BEARISH",
                    confidence=trap_prob,
                    trigger_conditions=["Price rejection", "OI buildup"],
                    expected_move_pct=-2.0
                )
                score -= 0.5  # Bearish bias
                analysis.append(f"{trap_type.value} detected (conf: {trap_prob:.2f})")
        
        # Wyckoff pattern analysis
        if spring > self.thresholds["spring_detection_high"]:
            score += 0.4  # Spring = bullish
            analysis.append(f"Wyckoff Spring detected (strength: {spring:.2f})")
        
        if upthrust > self.thresholds["upthrust_detection_high"]:
            score -= 0.4  # Upthrust = bearish
            analysis.append(f"Wyckoff Upthrust detected (strength: {upthrust:.2f})")
        
        # Accumulation/distribution
        if accumulation > 0.7:
            score += 0.3  # Accumulation = bullish
            analysis.append(f"Accumulation phase (score: {accumulation:.2f})")
        elif accumulation < 0.3:
            score -= 0.3  # Distribution = bearish
            analysis.append(f"Distribution phase (score: {accumulation:.2f})")
        
        # Clamp score
        score = max(-1.0, min(1.0, score))
        
        return score, " | ".join(analysis) if analysis else "Structure neutral", trap_analysis
    
    def score_divergence(self, feats: Dict) -> Tuple[float, str]:
        """
        Score divergence components.
        
        Returns:
            score (-1 to 1), analysis
        """
        divergence_score = feats.get("divergence_score", 0.0)
        has_divergence = feats.get("has_divergence", 0.0)
        price_oi_div = feats.get("price_oi_divergence", 0.0)
        price_gamma_div = feats.get("price_gamma_divergence", 0.0)
        
        if has_divergence < 0.5:
            return 0.0, "No significant divergence"
        
        score = 0.0
        analysis = []
        
        # Price-OI divergence (research concept)
        if abs(price_oi_div) > 1.0:  # Significant divergence
            if price_oi_div > 0:  # Price up, OI down = bearish divergence
                score -= 0.4
                analysis.append("Bearish Price-OI divergence")
            else:  # Price down, OI up = bullish divergence
                score += 0.4
                analysis.append("Bullish Price-OI divergence")
        
        # Price-Gamma divergence
        if abs(price_gamma_div) > 0.5:
            if price_gamma_div > 0:  # Price moving against gamma regime
                score -= 0.3
                analysis.append("Price-Gamma regime divergence")
            else:
                score += 0.3
                analysis.append("Price-Gamma regime convergence")
        
        # Overall divergence score
        if divergence_score > self.thresholds["divergence_high"]:
            # High divergence confidence
            if divergence_score > 0:  # Bullish divergence
                score += 0.3
                analysis.append(f"Strong bullish divergence (conf: {divergence_score:.2f})")
            else:  # Bearish divergence
                score -= 0.3
                analysis.append(f"Strong bearish divergence (conf: {abs(divergence_score):.2f})")
        
        # Clamp score
        score = max(-1.0, min(1.0, score))
        
        return score, " | ".join(analysis) if analysis else "Divergence neutral"
    
    def score_trend_momentum(self, feats: Dict) -> Tuple[float, float, str]:
        """
        Score traditional trend and momentum components.
        
        Returns:
            trend_score, momentum_score, analysis
        """
        price_momentum = feats.get("price_momentum", 0.0)
        ccc_slope = feats.get("ccc_slope", 0.0)
        vwap_distance = feats.get("vwap_distance", 0.0)
        put_call_ratio = feats.get("put_call_ratio", 1.0)
        
        # Trend score (directional bias)
        trend_score = 0.0
        
        # Price momentum
        if price_momentum > 0.01:
            trend_score += 0.3
        elif price_momentum < -0.01:
            trend_score -= 0.3
        
        # Breadth momentum (CCC slope)
        if ccc_slope > 0.001:
            trend_score += 0.2
        elif ccc_slope < -0.001:
            trend_score -= 0.2
        
        # VWAP position
        if vwap_distance > 0.005:
            trend_score += 0.2  # Above VWAP = bullish
        elif vwap_distance < -0.005:
            trend_score -= 0.2  # Below VWAP = bearish
        
        # Put-Call ratio sentiment
        if put_call_ratio < 0.8:
            trend_score += 0.1  # Low PCR = bullish
        elif put_call_ratio > 1.2:
            trend_score -= 0.1  # High PCR = bearish
        
        # Momentum score (strength of move)
        momentum_score = abs(price_momentum) * 10  # Scale to 0-1 range
        momentum_score = min(momentum_score, 1.0)
        
        # Analysis
        analysis_parts = []
        if price_momentum > 0:
            analysis_parts.append(f"Price momentum: +{price_momentum*100:.1f}%")
        else:
            analysis_parts.append(f"Price momentum: {price_momentum*100:.1f}%")
        
        if ccc_slope > 0:
            analysis_parts.append(f"Breadth improving (CCC: +{ccc_slope:.3f})")
        elif ccc_slope < 0:
            analysis_parts.append(f"Breadth weakening (CCC: {ccc_slope:.3f})")
        
        analysis = " | ".join(analysis_parts) if analysis_parts else "Momentum neutral"
        
        return trend_score, momentum_score, analysis
    
    # ==============================
    # TRAP CLASSIFICATION
    # ==============================
    
    def _classify_trap_type(self, feats: Dict) -> TrapType:
        """
        Classify trap type based on feature combination.
        """
        trap_prob = feats.get("trap_probability", 0.0)
        gamma_neg = feats.get("gamma_regime_negative", 0.0)
        spring = feats.get("spring_detection", 0.0)
        upthrust = feats.get("upthrust_detection", 0.0)
        divergence = feats.get("has_divergence", 0.0)
        
        # Check for specific trap types
        if gamma_neg > 0.5 and trap_prob > 0.6:
            return TrapType.GAMMA_TRAP
        
        if trap_prob > 0.6 and gamma_neg < 0.5:
            return TrapType.OI_TRAP
        
        if spring > self.thresholds["spring_detection_high"]:
            return TrapType.WYCKOFF_SPRING
        
        if upthrust > self.thresholds["upthrust_detection_high"]:
            return TrapType.WYCKOFF_UPTHRUST
        
        if divergence > 0.5 and trap_prob > 0.5:
            return TrapType.DIVERGENCE_TRAP
        
        # Default
        return TrapType.OI_TRAP
    
    # ==============================
    # MARKET REGIME DETECTION
    # ==============================
    
    def detect_market_regime(self, feats: Dict, components: SignalComponents) -> Tuple[MarketRegime, float]:
        """
        Detect current market regime based on features.
        
        Returns:
            regime, confidence (0-1)
        """
        regime_scores = {
            MarketRegime.ACCUMULATION: 0.0,
            MarketRegime.DISTRIBUTION: 0.0,
            MarketRegime.UPTREND: 0.0,
            MarketRegime.DOWNTREND: 0.0,
            MarketRegime.RANGING: 0.0,
            MarketRegime.BREAKOUT: 0.0,
            MarketRegime.SQUEEZE: 0.0,
            MarketRegime.REVERSAL: 0.0
        }
        
        # Extract key features
        accumulation = feats.get("accumulation_score", 0.0)
        trap_prob = feats.get("trap_probability", 0.0)
        gamma_neg = feats.get("gamma_regime_negative", 0.0)
        divergence = feats.get("has_divergence", 0.0)
        price_momentum = feats.get("price_momentum", 0.0)
        
        # Score regimes
        # Accumulation/Distribution
        if accumulation > 0.7:
            regime_scores[MarketRegime.ACCUMULATION] = accumulation
        elif accumulation < 0.3:
            regime_scores[MarketRegime.DISTRIBUTION] = 1 - accumulation
        
        # Trend detection
        if abs(price_momentum) > 0.02:  # Strong trend
            if price_momentum > 0:
                regime_scores[MarketRegime.UPTREND] = abs(price_momentum) * 10
            else:
                regime_scores[MarketRegime.DOWNTREND] = abs(price_momentum) * 10
        else:
            regime_scores[MarketRegime.RANGING] = 0.7
        
        # Breakout/Squeeze
        if trap_prob > 0.6:
            if gamma_neg > 0.5:
                regime_scores[MarketRegime.SQUEEZE] = trap_prob
            else:
                regime_scores[MarketRegime.BREAKOUT] = trap_prob
        
        # Reversal
        if divergence > 0.5 and abs(price_momentum) > 0.01:
            regime_scores[MarketRegime.REVERSAL] = divergence
        
        # Find highest scoring regime
        best_regime_item = max(regime_scores.items(), key=lambda x: x[1])
        best_regime = best_regime_item[0]
        regime_confidence = best_regime_item[1]
        
        # If no clear regime, default to RANGING
        if regime_confidence < 0.3:
            return MarketRegime.RANGING, 0.3
        
        return best_regime, min(regime_confidence, 1.0)
    
    # ==============================
    # COMPOSITE SIGNAL GENERATION
    # ==============================
    
    def compute_composite_signal(self, components: SignalComponents) -> Tuple[str, float, SignalStrength]:
        """
        Compute final signal from components.
        
        Returns:
            signal_type, confidence, strength
        """
        composite = components.composite_score
        confidence = components.confidence
        
        # Determine signal type
        if composite > 0.3 and confidence > self.confidence_threshold:
            signal_type = "BUY"
            if composite > 0.6 and confidence > 0.7:
                strength = SignalStrength.VERY_STRONG
            elif composite > 0.4:
                strength = SignalStrength.STRONG
            elif composite > 0.3:
                strength = SignalStrength.MODERATE
            else:
                strength = SignalStrength.WEAK
                
        elif composite < -0.3 and confidence > self.confidence_threshold:
            signal_type = "SELL"
            if composite < -0.6 and confidence > 0.7:
                strength = SignalStrength.VERY_STRONG
            elif composite < -0.4:
                strength = SignalStrength.STRONG
            elif composite < -0.3:
                strength = SignalStrength.MODERATE
            else:
                strength = SignalStrength.WEAK
                
        else:
            signal_type = "NEUTRAL"
            strength = SignalStrength.WEAK
        
        return signal_type, confidence, strength
    
    # ==============================
    # MAIN DECISION ENGINE
    # ==============================
    
    def decide(self, feature_row: pd.Series) -> SignalComponents:
        """
        Main decision engine with research-based scoring.
        """
        # Extract research features
        feats = self.extract_research_features(feature_row)
        
        # Score all components
        trend_score, momentum_score, trend_analysis = self.score_trend_momentum(feats)
        oi_score, oi_analysis = self.score_oi_velocity(feats)
        gamma_score, gamma_analysis = self.score_gamma_exposure(feats)
        structure_score, structure_analysis, trap_analysis = self.score_structure(feats)
        divergence_score, divergence_analysis = self.score_divergence(feats)
        
        # Calculate Wyckoff phase score
        wyckoff_score = 0.0
        pattern_score = 0.0
        
        accumulation = feats.get("accumulation_score", 0.0)
        spring = feats.get("spring_detection", 0.0)
        upthrust = feats.get("upthrust_detection", 0.0)
        
        if accumulation > 0.5:
            wyckoff_score = accumulation - 0.5  # 0 to 0.5 range
        
        pattern_score = spring - upthrust  # Positive for springs, negative for upthrusts
        
        # Calculate composite score with weights
        weights = {
            "trend": 0.15,
            "momentum": 0.10,
            "oi_velocity": 0.20,     # High weight for OI velocity (research important)
            "gamma": 0.20,           # High weight for gamma (research important)
            "structure": 0.15,
            "divergence": 0.10,
            "wyckoff": 0.05,
            "pattern": 0.05
        }
        
        weighted_sum = (
            trend_score * weights["trend"] +
            momentum_score * np.sign(trend_score) * weights["momentum"] +  # Momentum amplifies trend
            oi_score * weights["oi_velocity"] +
            gamma_score * weights["gamma"] +
            structure_score * weights["structure"] +
            divergence_score * weights["divergence"] +
            wyckoff_score * weights["wyckoff"] +
            pattern_score * weights["pattern"]
        )
        
        # Calculate confidence
        component_scores = [
            abs(trend_score), abs(oi_score), abs(gamma_score),
            abs(structure_score), abs(divergence_score)
        ]
        confidence = np.mean([s for s in component_scores if s > 0.1]) if any(s > 0.1 for s in component_scores) else 0.0
        
        # Detect market regime
        market_regime, regime_confidence = self.detect_market_regime(feats, SignalComponents(
            trend_score=trend_score,
            momentum_score=momentum_score,
            oi_velocity_score=oi_score,
            gamma_score=gamma_score,
            wall_interaction_score=structure_score,
            divergence_score=divergence_score,
            wyckoff_phase_score=wyckoff_score,
            pattern_score=pattern_score,
            composite_score=weighted_sum,
            confidence=confidence,
            market_regime=MarketRegime.RANGING,  # Temporary placeholder
            regime_confidence=0.5  # Temporary placeholder
        ))
        
        # Store trap analysis if found
        if trap_analysis:
            self.trap_detections.append({
                "timestamp": feature_row.get("timestamp", ""),
                "trap_analysis": trap_analysis,
                "features": feats
            })
        
        return SignalComponents(
            trend_score=trend_score,
            momentum_score=momentum_score,
            oi_velocity_score=oi_score,
            gamma_score=gamma_score,
            wall_interaction_score=structure_score,
            divergence_score=divergence_score,
            wyckoff_phase_score=wyckoff_score,
            pattern_score=pattern_score,
            composite_score=weighted_sum,
            confidence=confidence,
            market_regime=market_regime,
            regime_confidence=regime_confidence
        )
    
    # ==============================
    # SIGNAL OBJECT GENERATION
    # ==============================
    
    def build_signal(
        self,
        feature_row: pd.Series,
        model_version: str = "research_v2"
    ) -> Dict:
        """
        Build complete signal record with research context.
        """
        # Run decision engine
        components = self.decide(feature_row)
        
        # Compute final signal
        signal_type, confidence, strength = self.compute_composite_signal(components)
        
        # Generate signal ID
        signal_id = str(uuid.uuid4())
        
        # Current time
        now = datetime.utcnow()
        
        # Build research context
        research_context = {
            "components": {
                "trend_score": round(components.trend_score, 3),
                "momentum_score": round(components.momentum_score, 3),
                "oi_velocity_score": round(components.oi_velocity_score, 3),
                "gamma_score": round(components.gamma_score, 3),
                "wall_interaction_score": round(components.wall_interaction_score, 3),
                "divergence_score": round(components.divergence_score, 3),
                "wyckoff_phase_score": round(components.wyckoff_phase_score, 3),
                "pattern_score": round(components.pattern_score, 3),
                "composite_score": round(components.composite_score, 3)
            },
            "market_regime": components.market_regime.value,
            "regime_confidence": round(components.regime_confidence, 3),
            "signal_strength": strength.value,
            "thresholds_used": self.thresholds
        }
        
        # Build analytics summary
        analytics_summary = {
            "oi_velocity": feature_row.get("oi_velocity", 0),
            "net_gamma": feature_row.get("net_gamma", 0),
            "trap_probability": feature_row.get("trap_probability", 0),
            "divergence_score": feature_row.get("divergence_score", 0),
            "wall_strength": feature_row.get("wall_strength", 0),
            "confidence_breakdown": {
                "component_confidence": round(components.confidence, 3),
                "regime_confidence": round(components.regime_confidence, 3),
                "composite_confidence": round(confidence, 3)
            }
        }
        
        # Build rationale
        rationale_parts = []

        # Extract values from feature_row (not from components)
        trap_probability = feature_row.get("trap_probability", 0.0)
        has_divergence = feature_row.get("has_divergence", 0.0)

        # Add component analysis
        if abs(components.oi_velocity_score) > 0.3:
            direction = "bullish" if components.oi_velocity_score > 0 else "bearish"
            rationale_parts.append(f"{direction.capitalize()} OI velocity")

        if abs(components.gamma_score) > 0.3:
            regime = "negative" if components.gamma_score < 0 else "positive"
            rationale_parts.append(f"{regime.capitalize()} gamma regime")

        if trap_probability > 0.5:
            rationale_parts.append(f"High trap probability ({trap_probability:.2f})")

        if has_divergence > 0.5:
            rationale_parts.append("Significant divergence detected")

        rationale = " | ".join(rationale_parts) if rationale_parts else "rule_based_research_v2"
        
        # Build complete signal
        signal = {
            "signal_id": signal_id,
            "timestamp": feature_row["timestamp"],
            "feature_version": feature_row.get("feature_version", FEATURE_VERSION),
            "model_version": model_version,
            
            "signal_type": signal_type,
            "confidence": round(confidence, 3),
            
            "market_state": components.market_regime.value,
            "rationale": rationale,
            
            "expiry_time": (now + timedelta(minutes=self.signal_expiry_minutes)).isoformat(),
            "status": "NEW",
            
            "created_at": now.isoformat(),
            
            # Enhanced fields
            "research_context": research_context,
            "analytics_summary": analytics_summary,
            "signal_strength": strength.value,
            
            # Additional metadata
            "spot_price": feature_row.get("spot_price", 0),
            "put_call_ratio": feature_row.get("put_call_ratio", 1.0),
            "oi_velocity": feature_row.get("oi_velocity", 0),
            "net_gamma": feature_row.get("net_gamma", 0)
        }
        
        # Store in history
        self.signal_history.append({
            "timestamp": now.isoformat(),
            "signal": signal,
            "components": components
        })
        
        # Keep history manageable
        if len(self.signal_history) > 1000:
            self.signal_history = self.signal_history[-1000:]
        
        return signal
    
    # ==============================
    # UTILITY METHODS
    # ==============================
    
    def get_signal_history(self, limit: int = 10) -> List[Dict]:
        """Get recent signal history."""
        return self.signal_history[-limit:] if self.signal_history else []
    
    def get_recent_trap_detections(self, limit: int = 5) -> List[Dict]:
        """Get recent trap detections."""
        return self.trap_detections[-limit:] if self.trap_detections else []
    
    def get_performance_metrics(self) -> Dict:
        """Calculate performance metrics for recent signals."""
        if not self.signal_history:
            return {}
        
        recent_signals = self.signal_history[-100:]  # Last 100 signals
        
        buy_signals = [s for s in recent_signals if s["signal"]["signal_type"] == "BUY"]
        sell_signals = [s for s in recent_signals if s["signal"]["signal_type"] == "SELL"]
        
        metrics = {
            "total_signals": len(recent_signals),
            "buy_signals": len(buy_signals),
            "sell_signals": len(sell_signals),
            "neutral_signals": len(recent_signals) - len(buy_signals) - len(sell_signals),
            "avg_confidence": np.mean([s["signal"]["confidence"] for s in recent_signals]) if recent_signals else 0,
            "recent_regimes": {},
            "trap_detections": len(self.trap_detections)
        }
        
        # Count recent regimes
        for signal in recent_signals[-20:]:  # Last 20 signals
            regime = signal["signal"]["market_state"]
            metrics["recent_regimes"][regime] = metrics["recent_regimes"].get(regime, 0) + 1
        
        return metrics
    
    def reset(self):
        """Reset state machine history."""
        self.signal_history = []
        self.regime_history = []
        self.trap_detections = []

# ==============================
# SIGNAL VALIDATION & FILTERING
# ==============================

class SignalValidator:
    """
    Validates signals based on research criteria.
    """
    
    @staticmethod
    def validate_signal(signal: Dict, feature_row: pd.Series) -> Tuple[bool, str]:
        """
        Validate signal against research criteria.
        
        Returns:
            is_valid, reason
        """
        # Check confidence threshold
        if signal.get("confidence", 0) < 0.2:
            return False, "Confidence below threshold"
        
        # Check for trap confirmation
        trap_prob = feature_row.get("trap_probability", 0)
        if trap_prob > 0.7 and signal.get("signal_type") != "NEUTRAL":
            # High trap probability requires careful validation
            gamma_neg = feature_row.get("gamma_regime_negative", 0)
            if gamma_neg > 0.5:
                # Negative gamma with high trap = likely valid
                return True, "Gamma trap confirmed"
            else:
                return False, "High trap probability without negative gamma"
        
        # Check divergence consistency
        has_divergence = feature_row.get("has_divergence", 0)
        if has_divergence > 0.5:
            # Signal should align with divergence direction
            divergence_score = feature_row.get("divergence_score", 0)
            signal_type = signal.get("signal_type")
            
            if divergence_score > 0 and signal_type != "BUY":
                return False, "Signal contradicts bullish divergence"
            elif divergence_score < 0 and signal_type != "SELL":
                return False, "Signal contradicts bearish divergence"
        
        # Check OI velocity consistency
        oi_velocity = feature_row.get("oi_velocity", 0)
        if abs(oi_velocity) > 1.5:  # Strong OI movement
            if oi_velocity > 0 and signal.get("signal_type") != "BUY":
                return False, "Signal contradicts strong OI buildup"
            elif oi_velocity < 0 and signal.get("signal_type") != "SELL":
                return False, "Signal contradicts strong OI unwinding"
        
        return True, "Signal validated"
    
    @staticmethod
    def filter_weak_signals(signals: List[Dict], min_strength: str = "MODERATE") -> List[Dict]:
        """
        Filter signals by strength.
        
        Args:
            signals: List of signals
            min_strength: Minimum strength required
        
        Returns:
            Filtered signals
        """
        strength_order = {
            "WEAK": 0,
            "MODERATE": 1,
            "STRONG": 2,
            "VERY_STRONG": 3
        }
        
        min_strength_value = strength_order.get(min_strength, 0)
        
        filtered = []
        for signal in signals:
            signal_strength = signal.get("signal_strength", "WEAK")
            if strength_order.get(signal_strength, 0) >= min_strength_value:
                filtered.append(signal)
        
        return filtered

# ==============================
# SIGNAL ANALYTICS
# ==============================

def analyze_signal_patterns(signals: List[Dict]) -> Dict:
    """
    Analyze patterns in signal generation.
    """
    if not signals:
        return {}
    
    # Convert to DataFrame for analysis
    df = pd.DataFrame(signals)
    
    analysis = {
        "total_signals": len(df),
        "signal_distribution": df["signal_type"].value_counts().to_dict(),
        "avg_confidence": df["confidence"].mean() if "confidence" in df.columns else 0,
        "strength_distribution": df["signal_strength"].value_counts().to_dict() if "signal_strength" in df.columns else {},
        "regime_distribution": df["market_state"].value_counts().to_dict() if "market_state" in df.columns else {}
    }
    
    # Calculate success rate if PNL data available
    if "pnl" in df.columns and not df["pnl"].isna().all():
        profitable = df[df["pnl"] > 0]
        analysis["profitable_signals"] = len(profitable)
        analysis["success_rate"] = len(profitable) / len(df) * 100 if len(df) > 0 else 0
        analysis["avg_pnl"] = df["pnl"].mean()
    
    return analysis

# ==============================
# INITIALIZATION
# ==============================

def create_signal_engine(
    signal_expiry_minutes: int = 5,
    confidence_threshold: float = 0.2
) -> SignalStateMachine:
    """
    Factory function to create signal engine.
    """
    return SignalStateMachine(
        signal_expiry_minutes=signal_expiry_minutes,
        confidence_threshold=confidence_threshold
    )



====================================================================================================
FILE: .\data\instrument_master.py
====================================================================================================


File Name: instrument_master.py
Full Path: G:\trading_app\data\instrument_master.py
Size: 18.18 KB
Last Modified: 01/28/2026 00:21:32
Extension: .py

"""
Enhanced Instrument Master with robust key generation.
Handles multiple underlying formats and expiry matching.
"""

from typing import List, Dict, Any, Optional
import gzip
import json
from datetime import datetime, timedelta
from pathlib import Path
import pandas as pd
import warnings

# ==============================
# INSTRUMENT LOADING
# ==============================

def load_instruments() -> List[Dict[str, Any]]:
    """
    Load instruments from compressed JSON file.
    
    Returns:
        List of instrument dictionaries
    """
    instrument_file = Path("data/instruments.json.gz")
    
    if not instrument_file.exists():
        warnings.warn(f"Instrument file not found: {instrument_file}")
        return []
    
    try:
        with gzip.open(instrument_file, 'rt', encoding='utf-8') as f:
            data = json.load(f)
            
        if isinstance(data, list):
            print(f"‚úì Loaded {len(data)} instruments")
            return data
        else:
            warnings.warn(f"Unexpected data format: {type(data)}")
            return []
            
    except Exception as e:
        warnings.warn(f"Error loading instruments: {e}")
        return []

def save_instruments(instruments: List[Dict[str, Any]]) -> bool:
    """Save instruments to compressed JSON file."""
    try:
        instrument_file = Path("data/instruments.json.gz")
        instrument_file.parent.mkdir(parents=True, exist_ok=True)
        
        with gzip.open(instrument_file, 'wt', encoding='utf-8') as f:
            json.dump(instruments, f)
            
        print(f"‚úì Saved {len(instruments)} instruments")
        return True
    except Exception as e:
        warnings.warn(f"Error saving instruments: {e}")
        return False

# ==============================
# OPTION KEY GENERATION
# ==============================

def get_option_keys(
    underlying: str, 
    expiry: str, 
    max_keys: int = 200,
    instrument_type: Optional[str] = None
) -> List[str]:
    """
    Get option instrument keys for given underlying and expiry.
    Fixed: Matches by DATE only, not exact timestamp.
    
    Args:
        underlying: Underlying symbol (e.g., 'NIFTY', 'BANKNIFTY')
        expiry: Expiry date in 'YYYY-MM-DD' format
        max_keys: Maximum number of keys to return
        instrument_type: Filter by 'CE', 'PE', or None for both
    
    Returns:
        List of instrument keys
    """
    instruments = load_instruments()
    
    if not instruments:
        print("‚ö†Ô∏è No instruments loaded")
        return []
    
    # Parse target expiry date
    try:
        target_date = datetime.strptime(expiry, "%Y-%m-%d").date()
    except ValueError as e:
        print(f"‚ùå Invalid expiry format: {expiry}. Expected YYYY-MM-DD")
        return []
    
    option_keys = []
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        # Check if it's an option
        inst_type = instrument.get('instrument_type')
        if inst_type not in ['CE', 'PE']:
            continue
        
        # Filter by instrument type if specified
        if instrument_type and inst_type != instrument_type:
            continue
        
        # EXACT match for name
        name = instrument.get('name', '')
        if name != underlying:
            continue
        
        # Check expiry - convert timestamp to date
        instrument_expiry_ts = instrument.get('expiry')
        if not instrument_expiry_ts:
            continue
        
        # Convert timestamp to date (ignoring time component)
        try:
            instrument_expiry_date = datetime.fromtimestamp(instrument_expiry_ts / 1000).date()
        except:
            continue
        
        # Match by date only (not exact timestamp)
        if instrument_expiry_date != target_date:
            continue
        
        instrument_key = instrument.get('instrument_key')
        if instrument_key:
            option_keys.append({
                'key': instrument_key,
                'strike_price': float(instrument.get('strike_price', 0)),
                'instrument_type': inst_type,
                'instrument': instrument
            })
    
    if not option_keys:
        print(f"‚ö†Ô∏è No {underlying} options found for expiry {expiry}")
        return []
    
    # Sort by strike price
    option_keys.sort(key=lambda x: x['strike_price'])
    
    print(f"‚úÖ Found {len(option_keys)} {underlying} options for {expiry}")
    
    # Return just the keys
    return [item['key'] for item in option_keys[:max_keys]]

def get_option_keys_around_price(
    underlying: str,
    expiry: str,
    spot_price: float,
    num_strikes: int = 10,
    max_keys: int = 100
) -> List[str]:
    """
    Get option keys around current spot price.
    Fixed: Matches by DATE only, not exact timestamp.
    
    Args:
        underlying: Underlying symbol
        expiry: Expiry date in 'YYYY-MM-DD' format
        spot_price: Current spot price
        num_strikes: Number of strikes to get on each side
        max_keys: Maximum total keys to return
    
    Returns:
        List of instrument keys sorted by proximity to spot
    """
    instruments = load_instruments()
    
    if not instruments:
        return []
    
    # Parse target expiry date
    try:
        target_date = datetime.strptime(expiry, "%Y-%m-%d").date()
    except ValueError as e:
        print(f"‚ùå Invalid expiry format: {expiry}. Expected YYYY-MM-DD")
        return []
    
    all_options = []
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        # Check if it's an option
        if instrument.get('instrument_type') not in ['CE', 'PE']:
            continue
        
        # Check underlying
        name = instrument.get('name', '')
        if name != underlying:
            continue
        
        # Check expiry - convert timestamp to date
        instrument_expiry_ts = instrument.get('expiry')
        if not instrument_expiry_ts:
            continue
        
        # Convert timestamp to date (ignoring time component)
        try:
            instrument_expiry_date = datetime.fromtimestamp(instrument_expiry_ts / 1000).date()
        except:
            continue
        
        # Match by date only (not exact timestamp)
        if instrument_expiry_date != target_date:
            continue
        
        # Calculate distance from spot
        strike_price = float(instrument.get('strike_price', 0))
        distance = abs(strike_price - spot_price)
        
        instrument_key = instrument.get('instrument_key')
        if instrument_key:
            all_options.append({
                'key': instrument_key,
                'strike_price': strike_price,
                'distance': distance,
                'instrument_type': instrument.get('instrument_type'),
                'instrument': instrument
            })
    
    if not all_options:
        print(f"‚ö†Ô∏è No {underlying} options found for expiry {expiry} around spot {spot_price}")
        return []
    
    # Sort by distance from spot
    all_options.sort(key=lambda x: x['distance'])
    
    # Get closest strikes
    closest_options = all_options[:num_strikes * 2]  # CE and PE for each strike
    
    # Sort by strike price for consistency
    closest_options.sort(key=lambda x: x['strike_price'])
    
    print(f"‚úÖ Found {len(closest_options[:max_keys])} {underlying} options around spot {spot_price} for {expiry}")
    
    return [item['key'] for item in closest_options[:max_keys]]


def get_available_expiries(underlying: str) -> List[str]:
    """
    Get all available expiry dates for an underlying.
    Fixed: Uses date-only matching.
    
    Args:
        underlying: Underlying symbol
    
    Returns:
        List of expiry dates in 'YYYY-MM-DD' format
    """
    instruments = load_instruments()
    
    if not instruments:
        return []
    
    expiry_dates = set()
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        # Check if it's the right underlying
        name = instrument.get('name', '')
        if name != underlying:
            continue
        
        # Check if it's an option
        if instrument.get('instrument_type') not in ['CE', 'PE']:
            continue
        
        expiry_ts = instrument.get('expiry')
        if expiry_ts:
            try:
                # Convert timestamp to date only
                expiry_date = datetime.fromtimestamp(expiry_ts / 1000).date()
                expiry_dates.add(expiry_date.strftime('%Y-%m-%d'))
            except:
                pass
    
    return sorted(expiry_dates)

def get_nearest_expiry(underlying: str, min_days: int = 0) -> str:
    """
    Get nearest expiry date for an underlying.
    Fixed: Uses date-only comparison.
    
    Args:
        underlying: Underlying symbol
        min_days: Minimum days until expiry
    
    Returns:
        Expiry date in 'YYYY-MM-DD' format, or empty string if none found
    """
    expiry_dates = get_available_expiries(underlying)
    
    if not expiry_dates:
        return ""
    
    # Get today's date (without time)
    today = datetime.now().date()
    
    # Add min_days
    from datetime import timedelta
    target_date = today + timedelta(days=min_days)
    target_str = target_date.strftime('%Y-%m-%d')
    
    # Find first expiry on or after target date
    for expiry in expiry_dates:
        if expiry >= target_str:
            return expiry
    
    # If no future expiry, return the last one
    return expiry_dates[-1]

def get_weekly_expiry(underlying: str) -> str:
    """
    Get nearest Thursday expiry (standard weekly expiry).
    
    Args:
        underlying: Underlying symbol
    
    Returns:
        Expiry date in 'YYYY-MM-DD' format
    """
    # Find the nearest Thursday
    today = datetime.now()
    
    # Thursday is weekday 3 (Monday=0)
    days_until_thursday = (3 - today.weekday()) % 7
    if days_until_thursday == 0:
        days_until_thursday = 7  # If today is Thursday, get next Thursday
    
    next_thursday = today + timedelta(days=days_until_thursday)
    
    # Check if this Thursday is available
    thursday_str = next_thursday.strftime('%Y-%m-%d')
    available_expiries = get_available_expiries(underlying)
    
    if thursday_str in available_expiries:
        return thursday_str
    
    # If not, get the nearest available expiry
    return get_nearest_expiry(underlying)

# ==============================
# INSTRUMENT LOOKUP
# ==============================

def get_instrument_by_key(instrument_key: str) -> Optional[Dict[str, Any]]:
    """Get instrument details by instrument key."""
    instruments = load_instruments()
    
    for instrument in instruments:
        if isinstance(instrument, dict) and instrument.get('instrument_key') == instrument_key:
            return instrument
    
    return None

def get_instruments_by_type(
    instrument_type: str,
    underlying: Optional[str] = None
) -> List[Dict[str, Any]]:
    """
    Get instruments by type.
    
    Args:
        instrument_type: 'EQ', 'CE', 'PE', 'FUT', etc.
        underlying: Optional underlying symbol filter
    
    Returns:
        List of matching instruments
    """
    instruments = load_instruments()
    
    filtered = []
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        if instrument.get('instrument_type') != instrument_type:
            continue
        
        if underlying:
            name = instrument.get('name', '')
            if name != underlying:
                continue
        
        filtered.append(instrument)
    
    return filtered

# ==============================
# BATCH OPERATIONS
# ==============================

def get_batch_option_keys(
    underlying: str,
    expiries: List[str],
    max_per_expiry: int = 50
) -> Dict[str, List[str]]:
    """
    Get option keys for multiple expiries.
    
    Args:
        underlying: Underlying symbol
        expiries: List of expiry dates
        max_per_expiry: Maximum keys per expiry
    
    Returns:
        Dictionary mapping expiry to list of keys
    """
    result = {}
    
    for expiry in expiries:
        keys = get_option_keys(underlying, expiry, max_keys=max_per_expiry)
        if keys:
            result[expiry] = keys
    
    return result

def get_instrument_keys_for_strikes(
    underlying: str,
    expiry: str,
    strikes: List[float],
    option_types: List[str] = ['CE', 'PE']
) -> List[str]:
    """
    Get instrument keys for specific strikes and option types.
    
    Args:
        underlying: Underlying symbol
        expiry: Expiry date
        strikes: List of strike prices
        option_types: List of option types
    
    Returns:
        List of instrument keys
    """
    instruments = load_instruments()
    
    keys = []
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        inst_type = instrument.get('instrument_type')
        if inst_type not in option_types:
            continue
        
        # Check underlying
        name = instrument.get('name', '')
        if name != underlying:
            continue
        
        # Check expiry
        instrument_expiry = instrument.get('expiry')
        if not instrument_expiry:
            continue
        
        # Convert expiry string to timestamp
        try:
            expiry_dt = datetime.strptime(expiry, "%Y-%m-%d")
            expiry_ts = int(expiry_dt.timestamp() * 1000)
        except:
            continue
        
        if instrument_expiry != expiry_ts:
            continue
        
        # Check strike
        strike_price = float(instrument.get('strike_price', 0))
        if strike_price not in strikes:
            continue
        
        instrument_key = instrument.get('instrument_key')
        if instrument_key:
            keys.append(instrument_key)
    
    return keys

# ==============================
# UTILITIES
# ==============================

def print_instrument_summary():
    """Print summary of loaded instruments."""
    instruments = load_instruments()
    
    if not instruments:
        print("No instruments loaded")
        return
    
    print(f"Total instruments: {len(instruments)}")
    
    # Count by type
    type_counts = {}
    underlying_counts = {}
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        inst_type = instrument.get('instrument_type', 'UNKNOWN')
        type_counts[inst_type] = type_counts.get(inst_type, 0) + 1
        
        name = instrument.get('name', 'UNKNOWN')
        underlying_counts[name] = underlying_counts.get(name, 0) + 1
    
    print("\nBy instrument type:")
    for inst_type, count in sorted(type_counts.items()):
        print(f"  {inst_type}: {count}")
    
    print("\nTop underlying symbols:")
    for name, count in sorted(underlying_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"  {name}: {count}")

# ==============================
# TEST FUNCTION
# ==============================

def test_instrument_master():
    """Test the instrument master functions."""
    print("üß™ Testing Instrument Master...")
    
    # Load instruments
    instruments = load_instruments()
    print(f"‚úì Loaded {len(instruments)} instruments")
    
    # Get available expiries for NIFTY
    nifty_expiries = get_available_expiries("NIFTY")
    print(f"‚úì NIFTY expiries: {nifty_expiries}")
    
    if nifty_expiries:
        # Get nearest expiry
        nearest = get_nearest_expiry("NIFTY")
        print(f"‚úì Nearest NIFTY expiry: {nearest}")
        
        # Get option keys for nearest expiry
        if nearest:
            keys = get_option_keys("NIFTY", nearest, max_keys=10)
            print(f"‚úì Got {len(keys)} option keys for {nearest}")
            
            if keys:
                print("  Sample keys:")
                for key in keys[:3]:
                    print(f"    - {key}")
                
                # Get instrument details
                instrument = get_instrument_by_key(keys[0])
                if instrument:
                    print(f"  First instrument: {instrument.get('trading_symbol', 'N/A')}")
    
    # Test BANKNIFTY
    banknifty_expiries = get_available_expiries("BANKNIFTY")
    print(f"‚úì BANKNIFTY expiries: {banknifty_expiries}")
    
    print("\n‚úÖ Instrument master test complete")
# ==============================
# EXPIRY MANAGEMENT
# ==============================

def get_next_available_expiry(underlying: str) -> str:
    """
    Get the next available expiry date (excluding today if market closed).
    
    Args:
        underlying: Underlying symbol (e.g., 'NIFTY', 'BANKNIFTY')
    
    Returns:
        Next available expiry date in 'YYYY-MM-DD' format
    """
    from datetime import datetime
    
    # Get all available expiries
    expiries = get_available_expiries(underlying)
    if not expiries:
        return ""
    
    # Get today's date
    today = datetime.now().strftime('%Y-%m-%d')
    
    # Check if market is open (for testing, you can hardcode this)
    # For now, let's assume if today is expiry day, we should use next expiry
    market_open = False  # You can set this based on actual market hours
    
    if market_open and today in expiries:
        # Market is open and today is an expiry - use it
        return today
    else:
        # Market closed or today not available - get next expiry
        for expiry in sorted(expiries):
            if expiry > today:
                return expiry
        
        # If no future expiry, return the last one
        return expiries[-1]
# Run test if executed directly
if __name__ == "__main__":
    test_instrument_master()



====================================================================================================
FILE: .\data\instrument_master_fixed.py
====================================================================================================


File Name: instrument_master_fixed.py
Full Path: G:\trading_app\data\instrument_master_fixed.py
Size: 5.45 KB
Last Modified: 01/28/2026 00:17:11
Extension: .py

"""
FIXED version - matches by date, not exact timestamp
"""
from typing import List, Dict, Any, Optional
import gzip
import json
from datetime import datetime, timedelta
from pathlib import Path

def load_instruments() -> List[Dict[str, Any]]:
    """Load instruments from compressed JSON file."""
    instrument_file = Path("data/instruments.json.gz")
    
    if not instrument_file.exists():
        print(f"‚ùå Instrument file not found: {instrument_file}")
        return []
    
    try:
        with gzip.open(instrument_file, 'rt', encoding='utf-8') as f:
            data = json.load(f)
            
        if isinstance(data, list):
            return data
        else:
            print(f"‚ö†Ô∏è Unexpected data format: {type(data)}")
            return []
            
    except Exception as e:
        print(f"‚ùå Error loading instruments: {e}")
        return []

def get_option_keys_fixed(
    underlying: str, 
    expiry: str, 
    max_keys: int = 200,
    debug: bool = False
) -> List[str]:
    """
    Get option instrument keys for given underlying and expiry.
    Matches by DATE only, not exact timestamp.
    
    Args:
        underlying: Underlying symbol (e.g., 'NIFTY', 'BANKNIFTY')
        expiry: Expiry date in 'YYYY-MM-DD' format
        max_keys: Maximum number of keys to return
        debug: Enable debug logging
    
    Returns:
        List of instrument keys
    """
    instruments = load_instruments()
    
    if not instruments:
        if debug: print("‚ö†Ô∏è No instruments loaded")
        return []
    
    # Parse target expiry date
    try:
        target_date = datetime.strptime(expiry, "%Y-%m-%d").date()
        if debug: print(f"Looking for options expiring on: {target_date}")
    except ValueError as e:
        print(f"‚ùå Invalid expiry format: {expiry}. Expected YYYY-MM-DD")
        return []
    
    option_keys = []
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        # Check if it's an option
        inst_type = instrument.get('instrument_type')
        if inst_type not in ['CE', 'PE']:
            continue
        
        # EXACT match for name
        name = instrument.get('name', '')
        if name != underlying:
            continue
        
        # Check expiry - convert timestamp to date
        instrument_expiry_ts = instrument.get('expiry')
        if not instrument_expiry_ts:
            continue
        
        # Convert timestamp to date (ignoring time component)
        try:
            instrument_expiry_date = datetime.fromtimestamp(instrument_expiry_ts / 1000).date()
        except:
            continue
        
        # Match by date only
        if instrument_expiry_date != target_date:
            continue
        
        instrument_key = instrument.get('instrument_key')
        if instrument_key:
            option_keys.append({
                'key': instrument_key,
                'strike_price': float(instrument.get('strike_price', 0)),
                'instrument_type': inst_type,
                'instrument': instrument
            })
    
    if not option_keys:
        if debug: print(f"‚ö†Ô∏è No {underlying} options found for expiry {expiry}")
        return []
    
    # Sort by strike price
    option_keys.sort(key=lambda x: x['strike_price'])
    
    if debug: print(f"‚úÖ Found {len(option_keys)} {underlying} options for {expiry}")
    
    # Return just the keys
    return [item['key'] for item in option_keys[:max_keys]]

def get_available_expiries_fixed(underlying: str) -> List[str]:
    """
    Get all available expiry dates for an underlying.
    Fixed version that works with date-only matching.
    """
    instruments = load_instruments()
    
    if not instruments:
        return []
    
    expiry_dates = set()
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        # Check if it's the right underlying
        name = instrument.get('name', '')
        if name != underlying:
            continue
        
        # Check if it's an option
        if instrument.get('instrument_type') not in ['CE', 'PE']:
            continue
        
        expiry_ts = instrument.get('expiry')
        if expiry_ts:
            try:
                expiry_date = datetime.fromtimestamp(expiry_ts / 1000).date()
                expiry_dates.add(expiry_date.strftime('%Y-%m-%d'))
            except:
                pass
    
    return sorted(expiry_dates)

def test_fixed_functions():
    """Test the fixed functions"""
    print("üß™ Testing FIXED functions...")
    
    # Test get_available_expiries_fixed
    print("\nüîç Available expiries (fixed):")
    expiries = get_available_expiries_fixed("NIFTY")
    print(f"NIFTY expiries: {expiries[:5]}...")
    
    # Test get_option_keys_fixed
    if expiries:
        test_expiry = expiries[0] if len(expiries) > 0 else "2026-02-03"
        print(f"\nüîç Testing option keys for {test_expiry} (fixed):")
        keys = get_option_keys_fixed("NIFTY", test_expiry, max_keys=10, debug=True)
        print(f"Found {len(keys)} keys")
        
        if keys:
            print("Sample keys:")
            for key in keys[:3]:
                print(f"  - {key}")
        else:
            print("‚ùå Still no keys found!")

if __name__ == "__main__":
    test_fixed_functions()



====================================================================================================
FILE: .\data\upstox_client.py
====================================================================================================


File Name: upstox_client.py
Full Path: G:\trading_app\data\upstox_client.py
Size: 41.99 KB
Last Modified: 01/28/2026 11:58:22
Extension: .py

"""
Enhanced Upstox Client with advanced derivatives analytics.
Implements research concepts:
1. OI Velocity - Rate of change of Open Interest
2. Gamma Exposure (GEX) - Dealer hedging pressure
3. Structural Walls vs Traps detection
4. Spot Divergence analysis
5. Market microstructure insights
"""

import requests
import pandas as pd
import numpy as np
from typing import List, Dict, Optional, Tuple
import json
from datetime import datetime, timedelta
from scipy.stats import linregress
from dataclasses import dataclass
from enum import Enum
import streamlit as st

# ==============================
# CONSTANTS & CONFIG
# ==============================

BASE_URL = "https://api.upstox.com/v2"

# Research-based constants
OI_VELOCITY_LOOKBACK = 5  # periods for velocity calculation
GAMMA_LOOKBACK = 10  # strikes for gamma calculation
WALL_THRESHOLD = 0.15  # Min OI concentration for wall detection
TRAP_CONFIRMATION_WINDOW = 3  # periods for trap confirmation

class MarketRegime(Enum):
    """Market regimes based on research"""
    NORMAL = "NORMAL"
    EXPANSIVE = "EXPANSIVE"  # High OI velocity, capital inflow
    CONSTRICTED = "CONSTRICTED"  # Negative OI velocity, capital outflow
    GAMMA_POSITIVE = "GAMMA_POSITIVE"  # Stabilizing, pinning
    GAMMA_NEGATIVE = "GAMMA_NEGATIVE"  # Accelerating, squeezes
    TRAP_FORMING = "TRAP_FORMING"  # Wall breach with unwinding
    DIVERGENCE = "DIVERGENCE"  # Spot vs derivatives divergence

@dataclass
class WallAnalysis:
    """Analysis of structural walls"""
    strike: float
    oi_concentration: float
    option_type: str  # CE or PE
    is_defended: bool
    unwinding_rate: float  # OI velocity at this strike
    gamma_contribution: float
    distance_to_spot: float  # % distance from current spot

@dataclass
class TrapAnalysis:
    """Analysis of potential traps"""
    wall_strike: float
    breach_direction: str  # "UP" or "DOWN"
    confidence: float
    trigger_time: datetime
    gamma_impact: float
    oi_unwinding: float

@dataclass
class GammaProfile:
    """Gamma Exposure profile"""
    net_gamma: float
    positive_gamma_strikes: List[float]
    negative_gamma_strikes: List[float]
    flip_levels: List[float]  # Where gamma changes sign
    max_gamma_strike: float
    regime: str

# ==============================
# ENHANCED UPSTOX CLIENT
# ==============================

class UpstoxClient:
    def __init__(self, access_token: str):
        self.access_token = access_token
        self.session = requests.Session()
        self.session.headers.update({
            "Accept": "application/json",
            "Authorization": f"Bearer {access_token}"
        })
        
        # State tracking for research concepts
        self.oi_history = {}  # symbol -> list of OI values
        self.price_history = {}  # symbol -> list of prices
        self.gamma_history = {}  # symbol -> list of gamma values
        self.velocity_history = {}  # symbol -> list of velocity values
        
        # Market structure tracking
        self.walls_cache = {}
        self.traps_cache = {}
        self.gex_cache = {}
        
        # Research-based analytics
        self.analytics = MarketAnalytics()
    
    # ==============================
    # CORE REQUEST HANDLER
    # ==============================
    
    def _make_request(self, method: str, endpoint: str, **kwargs):
        """Helper method to make API requests with error handling"""
        url = f"{BASE_URL}/{endpoint}"
        
        try:
            response = self.session.request(method, url, **kwargs)
            
            # Debug logging
            # print(f"Request: {method} {url}")
            # print(f"Params: {kwargs.get('params', {})}")
            # print(f"Status: {response.status_code}")
            
            if response.status_code != 200:
                print(f"Error Response: {response.text}")
            
            response.raise_for_status()
            return response.json()
            
        except requests.exceptions.HTTPError as e:
            print(f"HTTP Error: {e}")
            try:
                error_data = response.json()
                print(f"Error Details: {json.dumps(error_data, indent=2)}")
            except:
                pass
            raise
        except Exception as e:
            print(f"Request Error: {e}")
            raise
    
    # ==============================
    # RESEARCH IMPLEMENTATION: OI VELOCITY
    # ==============================
    
    def calculate_oi_velocity(self, symbol: str, current_oi: float) -> Tuple[float, str]:
        """
        Calculate Open Interest Velocity with regime classification.
        
        Research Concept: OI Velocity measures the kinetic energy of market trends.
        High positive velocity = Initiative capital entering (Buildup)
        High negative velocity = Capital exiting (Unwinding/Covering)
        
        Returns: (velocity_score, regime)
        """
        if symbol not in self.oi_history:
            self.oi_history[symbol] = []
        
        # Add current OI to history
        self.oi_history[symbol].append(current_oi)
        
        # Keep only last N periods
        if len(self.oi_history[symbol]) > OI_VELOCITY_LOOKBACK:
            self.oi_history[symbol] = self.oi_history[symbol][-OI_VELOCITY_LOOKBACK:]
        
        if len(self.oi_history[symbol]) < 2:
            return 0.0, "INSUFFICIENT_DATA"
        
        # Calculate velocity (rate of change)
        oi_series = pd.Series(self.oi_history[symbol])
        velocity = oi_series.pct_change().iloc[-1] * 100  # % change
        
        # Normalize velocity (research concept)
        if len(oi_series) >= 3:
            std_dev = oi_series.pct_change().std() * 100
            if std_dev > 0:
                normalized_velocity = velocity / std_dev
            else:
                normalized_velocity = velocity
        else:
            normalized_velocity = velocity
        
        # Classify regime
        if normalized_velocity > 1.5:
            regime = "EXPANSIVE"  # High capital inflow
        elif normalized_velocity < -1.5:
            regime = "CONSTRICTED"  # Capital outflow
        else:
            regime = "NORMAL"
        
        # Store for trend analysis
        if symbol not in self.velocity_history:
            self.velocity_history[symbol] = []
        self.velocity_history[symbol].append({
            "timestamp": datetime.utcnow(),
            "velocity": normalized_velocity,
            "regime": regime,
            "raw_oi": current_oi
        })
        
        return normalized_velocity, regime
    
    # ==============================
    # RESEARCH IMPLEMENTATION: GAMMA EXPOSURE
    # ==============================
    
    def calculate_gamma_exposure(self, option_chain_df: pd.DataFrame, 
                                 spot_price: float) -> GammaProfile:
        """
        Calculate Gamma Exposure (GEX) with dealer hedging analysis.
        
        Research Concept: GEX predicts volatility regimes.
        Positive GEX: Dealers long gamma ‚Üí Stabilizing, pinning expected
        Negative GEX: Dealers short gamma ‚Üí Accelerating, squeezes possible
        """
        if option_chain_df.empty:
            return GammaProfile(0.0, [], [], [], 0.0, "NEUTRAL")
        
        # Calculate gamma for each strike (simplified Black-Scholes gamma)
        gamma_values = []
        flip_levels = []
        
        for _, row in option_chain_df.iterrows():
            strike = row['strike']
            option_type = row['option_type']
            oi = row['oi']
            iv = row.get('iv', 0.3)  # Default IV if not available
            
            # Simplified gamma calculation
            # In production, use proper Black-Scholes or gather from API
            if iv > 0:
                # Approximate gamma: highest ATM, decays as move OTM
                distance = abs(strike - spot_price) / spot_price
                gamma = np.exp(-distance * 100) / (iv * np.sqrt(252/365))  # Simplified
                
                # Adjust for option type
                if option_type == "PE":
                    gamma = -gamma  # Short gamma for put writers
                
                # Weight by OI
                gamma_weighted = gamma * oi
                gamma_values.append((strike, gamma_weighted))
                
                # Track sign changes for flip levels
                if len(gamma_values) > 1:
                    prev_sign = np.sign(gamma_values[-2][1])
                    curr_sign = np.sign(gamma_weighted)
                    if prev_sign != curr_sign:
                        flip_level = (strike + gamma_values[-2][0]) / 2
                        flip_levels.append(flip_level)
        
        if not gamma_values:
            return GammaProfile(0.0, [], [], [], 0.0, "NEUTRAL")
        
        # Calculate net gamma
        strikes, gammas = zip(*gamma_values)
        net_gamma = sum(gammas)
        
        # Separate positive and negative gamma strikes
        positive_strikes = [s for s, g in gamma_values if g > 0]
        negative_strikes = [s for s, g in gamma_values if g < 0]
        
        # Find strike with maximum gamma impact
        max_gamma_idx = np.argmax(np.abs(gammas))
        max_gamma_strike = strikes[max_gamma_idx]
        
        # Determine regime
        if net_gamma > 0:
            regime = "GAMMA_POSITIVE"
        elif net_gamma < 0:
            regime = "GAMMA_NEGATIVE"
        else:
            regime = "NEUTRAL"
        
        return GammaProfile(
            net_gamma=net_gamma,
            positive_gamma_strikes=positive_strikes[:5],  # Top 5
            negative_gamma_strikes=negative_strikes[:5],
            flip_levels=flip_levels,
            max_gamma_strike=max_gamma_strike,
            regime=regime
        )
    
    # ==============================
    # RESEARCH IMPLEMENTATION: WALLS VS TRAPS
    # ==============================
    
    def analyze_structural_walls(self, option_chain_df: pd.DataFrame, 
                                 spot_price: float) -> List[WallAnalysis]:
        """
        Identify structural walls with defense analysis.
        
        Research Concept: Walls are high OI concentrations that act as barriers.
        Traps form when walls breach with OI unwinding.
        """
        if option_chain_df.empty:
            return []
        
        walls = []
        
        # Group by strike to find OI concentrations
        strike_groups = option_chain_df.groupby('strike')
        
        for strike, group in strike_groups:
            total_oi = group['oi'].sum()
            call_oi = group[group['option_type'] == 'CE']['oi'].sum()
            put_oi = group[group['option_type'] == 'PE']['oi'].sum()
            
            # Determine dominant option type at this strike
            if call_oi > put_oi:
                dominant_type = "CE"
                dominant_oi = call_oi
            else:
                dominant_type = "PE"
                dominant_oi = put_oi
            
            # Calculate OI concentration (research concept)
            total_all_oi = option_chain_df['oi'].sum()
            if total_all_oi > 0:
                oi_concentration = dominant_oi / total_all_oi
            else:
                oi_concentration = 0
            
            # Check if this is a wall (high concentration)
            if oi_concentration > WALL_THRESHOLD:
                # Calculate distance from spot
                distance_pct = abs(strike - spot_price) / spot_price * 100
                
                # Analyze defense strength
                # Defense is stronger if:
                # 1. High OI concentration
                # 2. Close to spot price
                # 3. Recent OI increase (build-up)
                
                # Calculate unwinding rate (OI velocity at this strike)
                strike_key = f"{strike}_{dominant_type}"
                if strike_key in self.oi_history:
                    strike_oi_series = pd.Series(self.oi_history[strike_key])
                    if len(strike_oi_series) > 1:
                        unwinding_rate = strike_oi_series.pct_change().iloc[-1] * 100
                    else:
                        unwinding_rate = 0
                else:
                    unwinding_rate = 0
                
                # Determine if wall is being defended
                # Negative unwinding = defending (adding positions)
                # Positive unwinding = abandoning (closing positions)
                is_defended = unwinding_rate < 0  # Adding OI = defending
                
                walls.append(WallAnalysis(
                    strike=strike,
                    oi_concentration=oi_concentration,
                    option_type=dominant_type,
                    is_defended=is_defended,
                    unwinding_rate=unwinding_rate,
                    gamma_contribution=0,  # Would calculate in production
                    distance_to_spot=distance_pct
                ))
        
        # Sort by OI concentration (highest first)
        walls.sort(key=lambda x: x.oi_concentration, reverse=True)
        return walls[:5]  # Return top 5 walls
    
    def detect_traps(self, walls: List[WallAnalysis], spot_price: float,
                     oi_velocity: float) -> List[TrapAnalysis]:
        """
        Detect potential traps forming at walls.
        
        Research Concept: Traps occur when:
        1. Price breaches a wall
        2. OI starts unwinding (velocity negative)
        3. Gamma is negative (accelerating)
        4. High confidence of squeeze
        """
        traps = []
        
        for wall in walls:
            # Check if price is near wall (¬±1%)
            price_to_wall_ratio = spot_price / wall.strike
            is_near_wall = 0.99 <= price_to_wall_ratio <= 1.01
            
            if is_near_wall:
                # Check for trap conditions
                trap_confidence = 0.0
                
                # Condition 1: OI unwinding (negative velocity)
                if wall.unwinding_rate > 1.0:  # Rapid unwinding
                    trap_confidence += 0.3
                
                # Condition 2: Overall OI velocity negative
                if oi_velocity < -1.0:
                    trap_confidence += 0.3
                
                # Condition 3: Wall not defended
                if not wall.is_defended:
                    trap_confidence += 0.2
                
                # Condition 4: Price already breached (for detection)
                if (wall.option_type == "CE" and spot_price > wall.strike) or \
                   (wall.option_type == "PE" and spot_price < wall.strike):
                    trap_confidence += 0.2
                
                if trap_confidence > 0.5:  # Minimum confidence threshold
                    breach_direction = "UP" if wall.option_type == "CE" else "DOWN"
                    
                    trap = TrapAnalysis(
                        wall_strike=wall.strike,
                        breach_direction=breach_direction,
                        confidence=trap_confidence,
                        trigger_time=datetime.utcnow(),
                        gamma_impact=0,  # Would calculate actual gamma
                        oi_unwinding=wall.unwinding_rate
                    )
                    traps.append(trap)
        
        return traps
    
    # ==============================
    # RESEARCH IMPLEMENTATION: SPOT DIVERGENCE
    # ==============================
    
    def analyze_spot_divergence(self, spot_price: float, spot_velocity: float,
                               oi_velocity: float, gamma_profile: GammaProfile) -> Dict:
        """
        Analyze divergence between spot price and derivatives metrics.
        
        Research Concept: Divergence reveals internal market weakness.
        Bullish divergence: Price down but OI/Gamma improving
        Bearish divergence: Price up but OI/Gamma deteriorating
        """
        divergence_analysis = {
            "has_divergence": False,
            "type": None,  # BULLISH/BEARISH
            "confidence": 0.0,
            "metrics": {}
        }
        
        # Price vs OI divergence
        if spot_velocity > 0 and oi_velocity < -0.5:
            # Price up but OI unwinding (bearish divergence)
            divergence_analysis["has_divergence"] = True
            divergence_analysis["type"] = "BEARISH"
            divergence_analysis["confidence"] = min(abs(oi_velocity) / 2, 1.0)
            divergence_analysis["metrics"]["price_oi_divergence"] = {
                "price_change": spot_velocity,
                "oi_change": oi_velocity,
                "interpretation": "Hollow rally - price rising on covering, not new buying"
            }
        
        elif spot_velocity < 0 and oi_velocity > 0.5:
            # Price down but OI building (bullish divergence)
            divergence_analysis["has_divergence"] = True
            divergence_analysis["type"] = "BULLISH"
            divergence_analysis["confidence"] = min(oi_velocity / 2, 1.0)
            divergence_analysis["metrics"]["price_oi_divergence"] = {
                "price_change": spot_velocity,
                "oi_change": oi_velocity,
                "interpretation": "Accumulation - smart money buying despite price drop"
            }
        
        # Price vs Gamma divergence
        if gamma_profile.regime == "GAMMA_POSITIVE" and spot_velocity > 1.0:
            # Positive gamma (stabilizing) but price moving fast
            divergence_analysis["has_divergence"] = True
            divergence_analysis["type"] = "BEARISH"
            divergence_analysis["confidence"] = max(divergence_analysis["confidence"], 0.3)
            divergence_analysis["metrics"]["price_gamma_divergence"] = {
                "gamma_regime": gamma_profile.regime,
                "price_velocity": spot_velocity,
                "interpretation": "Price moving against gamma regime - likely to revert"
            }
        
        return divergence_analysis
    
    # ==============================
    # ENHANCED OPTION CHAIN FETCH
    # ==============================
    
    def fetch_option_chain_with_analytics(self, option_keys: list[str], 
                                         spot_price: float) -> Dict:
        """
        Enhanced option chain fetch with full research analytics.
        Returns comprehensive analysis including OI velocity, GEX, walls, traps.
        """
        # Fetch raw option chain
        option_chain_df = self.fetch_option_chain(option_keys)
        
        if option_chain_df.empty:
            return {
                "raw_data": option_chain_df,
                "analytics": {},
                "warnings": ["No option data available"]
            }
        
        # Calculate total OI for velocity
        total_oi = option_chain_df['oi'].sum()
        
        # OI Velocity analysis
        oi_velocity, oi_regime = self.calculate_oi_velocity("INDEX", total_oi)
        
        # Gamma Exposure analysis
        gamma_profile = self.calculate_gamma_exposure(option_chain_df, spot_price)
        
        # Structural walls analysis
        walls = self.analyze_structural_walls(option_chain_df, spot_price)
        
        # Trap detection
        traps = self.detect_traps(walls, spot_price, oi_velocity)
        
        # Spot price velocity (approximate)
        if "INDEX" not in self.price_history:
            self.price_history["INDEX"] = []
        self.price_history["INDEX"].append(spot_price)
        if len(self.price_history["INDEX"]) > OI_VELOCITY_LOOKBACK:
            self.price_history["INDEX"] = self.price_history["INDEX"][-OI_VELOCITY_LOOKBACK:]
        
        spot_velocity = 0
        if len(self.price_history["INDEX"]) >= 2:
            price_series = pd.Series(self.price_history["INDEX"])
            spot_velocity = price_series.pct_change().iloc[-1] * 100
        
        # Spot divergence analysis
        divergence = self.analyze_spot_divergence(spot_price, spot_velocity, 
                                                 oi_velocity, gamma_profile)
        
        # Market regime synthesis
        market_regime = self._synthesize_market_regime(
            oi_regime, gamma_profile.regime, divergence
        )
        
        # Compile comprehensive analysis
        analytics = {
            "timestamp": datetime.utcnow().isoformat(),
            "spot_price": spot_price,
            
            # OI Analysis
            "oi_velocity": round(oi_velocity, 3),
            "oi_regime": oi_regime,
            "total_oi": int(total_oi),
            
            # Gamma Analysis
            "gamma_exposure": {
                "net_gamma": round(gamma_profile.net_gamma, 3),
                "regime": gamma_profile.regime,
                "flip_levels": [round(x, 2) for x in gamma_profile.flip_levels],
                "max_impact_strike": gamma_profile.max_gamma_strike
            },
            
            # Structure Analysis
            "structural_walls": [
                {
                    "strike": wall.strike,
                    "type": wall.option_type,
                    "concentration": round(wall.oi_concentration, 3),
                    "defended": wall.is_defended,
                    "distance_pct": round(wall.distance_to_spot, 2)
                }
                for wall in walls
            ],
            
            # Trap Analysis
            "potential_traps": [
                {
                    "strike": trap.wall_strike,
                    "direction": trap.breach_direction,
                    "confidence": round(trap.confidence, 3),
                    "unwinding_rate": round(trap.oi_unwinding, 2)
                }
                for trap in traps
            ],
            
            # Divergence Analysis
            "spot_divergence": divergence,
            
            # Market Regime
            "market_regime": market_regime,
            "regime_confidence": self._calculate_regime_confidence(
                oi_velocity, gamma_profile.net_gamma, divergence
            )
        }
        
        # Store in cache for trend analysis
        self.walls_cache[datetime.utcnow()] = walls
        self.traps_cache[datetime.utcnow()] = traps
        self.gex_cache[datetime.utcnow()] = gamma_profile
        
        return {
            "raw_data": option_chain_df,
            "analytics": analytics,
            "market_insights": self._generate_market_insights(analytics)
        }
    
    # ==============================
    # ORIGINAL METHODS (ENHANCED)
    # ==============================
    
    def fetch_option_chain(self, option_keys: list[str]) -> pd.DataFrame:
        """Original method - fetch and normalize option chain quotes."""
        if not option_keys:
            return pd.DataFrame(
                columns=["strike", "option_type", "oi", "oi_change", "iv", "ltp", "volume"]
            )
        
        # Take max 200 keys (API limit)
        keys_to_fetch = option_keys[:200]
        
        data = self._make_request(
            "GET",
            "market-quote/quotes",
            params={"instrument_key": ",".join(keys_to_fetch)}
        )
        
        rows = []
        for key, payload in data.get("data", {}).items():
            symbol = payload.get("trading_symbol", "")
            
            rows.append({
                "strike": payload.get("strike_price"),
                "option_type": "CE" if symbol.endswith("CE") else "PE",
                "oi": payload.get("oi", 0),
                "oi_change": payload.get("oi_day_high", 0) - payload.get("oi_day_low", 0),
                "iv": payload.get("implied_volatility", 0),
                "ltp": payload.get("last_price", 0),
                "volume": payload.get("volume", 0),
                "timestamp": payload.get("timestamp")
            })
        
        return pd.DataFrame(rows)
    
    def fetch_index_quote(self, symbol: str = "NSE_INDEX|Nifty 50") -> Optional[dict]:
        """Original method - fetch index quote."""
        try:
            data = self._make_request(
                "GET",
                "market-quote/quotes",
                params={"symbol": symbol}
            )
            
            if data.get("status") == "success":
                response_key = symbol.replace("|", ":")
                quote_data = data.get("data", {}).get(response_key)
                
                if not quote_data:
                    quote_data = data.get("data", {}).get(symbol)
                
                if not quote_data and data.get("data"):
                    first_key = list(data["data"].keys())[0]
                    quote_data = data["data"][first_key]
                
                if not quote_data:
                    return None
                
                ohlc = quote_data.get("ohlc", {})
                
                return {
                    "symbol": symbol,
                    "ltp": quote_data.get("last_price"),
                    "open": ohlc.get("open"),
                    "high": ohlc.get("high"),
                    "low": ohlc.get("low"),
                    "close": ohlc.get("close"),
                    "change": quote_data.get("change"),
                    "net_change": quote_data.get("net_change"),
                    "percent_change": quote_data.get("percent_change"),
                    "volume": quote_data.get("volume"),
                    "timestamp": quote_data.get("timestamp")
                }
            else:
                print(f"API Error: {data}")
                return None
                
        except Exception as e:
            print(f"Error in fetch_index_quote: {e}")
            return None
    
    def fetch_equity_quotes(self, symbols: list[str]) -> pd.DataFrame:
        """Fixed method - fetch equity quotes with proper error handling and symbol validation."""
        if not symbols:
            return pd.DataFrame(columns=["symbol", "ltp", "open"])
        
        # Clean symbols - fix problematic symbols
        cleaned_symbols = []
        for symbol in symbols:
            # Fix common issues
            if symbol == "SBI LIFE":
                symbol = "SBILIFE"  # Remove space
            elif "+" in symbol:
                symbol = symbol.replace("+", "")  # Remove plus signs
            elif "-" in symbol:
                symbol = symbol.replace("-", "")  # Remove hyphens
            cleaned_symbols.append(symbol)
        
        symbols = cleaned_symbols  # Use cleaned symbols
        
        # DEBUG: Print cleaned symbols
        print(f"Cleaned symbols for API call: {symbols[:5]}...")
        
        # FORMAT 1: NSE|SYMBOL (This is usually the correct format for Upstox)
        instrument_symbols = [f"NSE|{s}" for s in symbols]
        
        try:
            print(f"Trying format 1 (NSE|SYMBOL): {instrument_symbols[:3]}...")
            
            response = self._make_request(
                "GET",
                "https://api.upstox.com/v2/market-quote/quotes",
                params={"symbol": ",".join(instrument_symbols)}
            )
            
            # Check if response indicates error
            if response.get("status") == "error":
                print(f"API Error in format 1: {response.get('errors', [{}])[0].get('message', 'Unknown error')}")
                raise Exception(f"API Error: {response.get('errors', [{}])[0].get('message')}")
            
            print("‚úì Format 1 succeeded")
            data = response
            
        except Exception as e:
            print(f"Format 1 failed: {e}")
            
            # FORMAT 2: Try with instrument_key instead of symbol
            instrument_symbols = [f"NSE_EQ|{s}" for s in symbols]
            print(f"Trying format 2 (NSE_EQ|SYMBOL): {instrument_symbols[:3]}...")
            
            try:
                response = self._make_request(
                    "GET",
                    "https://api.upstox.com/v2/market-quote/quotes",
                    params={"instrument_key": ",".join(instrument_symbols)}
                )
                
                # Check if response indicates error
                if response.get("status") == "error":
                    print(f"API Error in format 2: {response.get('errors', [{}])[0].get('message', 'Unknown error')}")
                    raise Exception(f"API Error: {response.get('errors', [{}])[0].get('message')}")
                
                print("‚úì Format 2 succeeded")
                data = response
                
            except Exception as e2:
                print(f"Format 2 also failed: {e2}")
                
                # FORMAT 3: Try individual symbol-by-symbol with smaller batches
                print("Trying format 3: Individual symbol requests...")
                all_data = {"data": {}}
                success_count = 0
                
                # Try each symbol individually first to identify problematic ones
                problem_symbols = []
                good_symbols = []
                
                for symbol in symbols:
                    try:
                        # Try NSE| format first
                        response = self._make_request(
                            "GET",
                            "https://api.upstox.com/v2/market-quote/quotes",
                            params={"symbol": f"NSE|{symbol}"}
                        )
                        
                        if response.get("status") == "error":
                            # Try NSE_EQ| format
                            response = self._make_request(
                                "GET",
                                "https://api.upstox.com/v2/market-quote/quotes",
                                params={"instrument_key": f"NSE_EQ|{symbol}"}
                            )
                        
                        if response.get("status") == "success" and "data" in response:
                            all_data["data"].update(response["data"])
                            success_count += 1
                            good_symbols.append(symbol)
                        else:
                            problem_symbols.append(symbol)
                            
                    except Exception as sym_e:
                        problem_symbols.append(symbol)
                        print(f"  Failed for symbol {symbol}: {sym_e}")
                        continue
                
                if success_count > 0:
                    print(f"‚úì Format 3 succeeded for {success_count}/{len(symbols)} symbols")
                    print(f"  Problem symbols: {problem_symbols}")
                    print(f"  Good symbols: {good_symbols[:5]}...")
                    data = all_data
                else:
                    print("All formats failed, returning empty dataframe")
                    return pd.DataFrame(columns=["symbol", "ltp", "open"])
        
        # Process the response - with validation
        if "data" not in data:
            print("Warning: No 'data' key in response")
            print(f"Response keys: {list(data.keys())}")
            return pd.DataFrame(columns=["symbol", "ltp", "open"])
        
        rows = []
        for key, payload in data.get("data", {}).items():
            try:
                # Extract symbol from instrument_key
                if "|" in key:
                    symbol = key.split("|")[-1]
                else:
                    symbol = key
                
                ohlc = payload.get("ohlc", {})
                
                rows.append({
                    "symbol": symbol,
                    "ltp": payload.get("last_price"),
                    "open": ohlc.get("open"),
                    "high": ohlc.get("high"),
                    "low": ohlc.get("low"),
                    "close": ohlc.get("close"),
                    "change": payload.get("change"),
                    "percent_change": payload.get("percent_change"),
                    "volume": payload.get("volume", 0),
                    "timestamp": payload.get("timestamp")
                })
            except Exception as e:
                print(f"Error processing symbol {key}: {e}")
                continue
        
        if not rows:
            print("Warning: No data processed from API response")
            return pd.DataFrame(columns=["symbol", "ltp", "open"])
        
        print(f"‚úì Successfully processed {len(rows)} equity quotes")
        return pd.DataFrame(rows)
    
    # ==============================
    # HELPER METHODS
    # ==============================
    
    def _synthesize_market_regime(self, oi_regime: str, gamma_regime: str, 
                                 divergence: Dict) -> str:
        """Synthesize overall market regime from multiple indicators."""
        regimes = []
        
        # OI-based regime
        if oi_regime == "EXPANSIVE":
            regimes.append("CAPITAL_INFLOW")
        elif oi_regime == "CONSTRICTED":
            regimes.append("CAPITAL_OUTFLOW")
        
        # Gamma-based regime
        if "POSITIVE" in gamma_regime:
            regimes.append("STABILIZING")
        elif "NEGATIVE" in gamma_regime:
            regimes.append("ACCELERATING")
        
        # Divergence-based
        if divergence["has_divergence"]:
            regimes.append(f"DIVERGENCE_{divergence['type']}")
        
        if not regimes:
            return "NEUTRAL"
        
        # Combine regimes
        if len(regimes) == 1:
            return regimes[0]
        else:
            return f"{regimes[0]}_{regimes[1]}"
    
    def _calculate_regime_confidence(self, oi_velocity: float, 
                                    net_gamma: float, divergence: Dict) -> float:
        """Calculate confidence score for market regime."""
        confidence = 0.5  # Base confidence
        
        # OI velocity confidence
        oi_confidence = min(abs(oi_velocity) / 3, 1.0)
        confidence = 0.7 * confidence + 0.3 * oi_confidence
        
        # Gamma confidence
        gamma_confidence = min(abs(net_gamma) / 1000, 1.0)  # Scale appropriately
        confidence = 0.7 * confidence + 0.3 * gamma_confidence
        
        # Divergence confidence
        if divergence["has_divergence"]:
            div_confidence = divergence["confidence"]
            confidence = 0.8 * confidence + 0.2 * div_confidence
        
        return round(confidence, 3)
    
    def _generate_market_insights(self, analytics: Dict) -> List[str]:
        """Generate human-readable market insights."""
        insights = []
        
        # OI insights
        oi_vel = analytics["oi_velocity"]
        if oi_vel > 1.5:
            insights.append("üìà Strong OI buildup - New capital entering market")
        elif oi_vel < -1.5:
            insights.append("üìâ OI unwinding - Positions being closed, watch for reversals")
        
        # Gamma insights
        gamma_regime = analytics["gamma_exposure"]["regime"]
        if gamma_regime == "GAMMA_POSITIVE":
            insights.append("üìå Positive Gamma - Market likely to pin/stabilize")
        elif gamma_regime == "GAMMA_NEGATIVE":
            insights.append("üöÄ Negative Gamma - Accelerating moves possible, watch for squeezes")
        
        # Wall insights
        walls = analytics["structural_walls"]
        if walls:
            top_wall = walls[0]
            if top_wall["defended"]:
                insights.append(f"üõ°Ô∏è Strong wall at {top_wall['strike']} ({top_wall['type']}) - Being defended")
            else:
                insights.append(f"‚ö†Ô∏è Wall at {top_wall['strike']} weakening - Monitor for breach")
        
        # Trap insights
        traps = analytics["potential_traps"]
        if traps:
            trap = traps[0]
            insights.append(f"üéØ Potential {trap['direction']} trap at {trap['strike']} - Confidence: {trap['confidence']*100:.0f}%")
        
        # Divergence insights
        if analytics["spot_divergence"]["has_divergence"]:
            div_type = analytics["spot_divergence"]["type"]
            if div_type == "BULLISH":
                insights.append("üîç Bullish divergence detected - Price down but smart money accumulating")
            else:
                insights.append("üîç Bearish divergence detected - Price up but internal weakness")
        
        return insights
    
    def get_market_analytics_summary(self) -> Dict:
        """Get summary of all market analytics."""
        return {
            "oi_velocity_history": self.velocity_history,
            "gamma_history": self.gex_cache,
            "walls_history": self.walls_cache,
            "traps_history": self.traps_cache,
            "current_regime": getattr(self, 'current_regime', 'UNKNOWN'),
            "last_update": datetime.utcnow().isoformat()
        }


# ==============================
# MARKET ANALYTICS HELPER CLASS
# ==============================

class MarketAnalytics:
    """Helper class for advanced market analytics."""
    
    @staticmethod
    def calculate_put_call_ratio(option_chain_df: pd.DataFrame) -> Dict:
        """Calculate Put-Call Ratio with breakdown."""
        put_oi = option_chain_df[option_chain_df["option_type"] == "PE"]["oi"].sum()
        call_oi = option_chain_df[option_chain_df["option_type"] == "CE"]["oi"].sum()
        put_volume = option_chain_df[option_chain_df["option_type"] == "PE"]["volume"].sum()
        call_volume = option_chain_df[option_chain_df["option_type"] == "CE"]["volume"].sum()
        
        pcr_oi = put_oi / call_oi if call_oi > 0 else 0
        pcr_volume = put_volume / call_volume if call_volume > 0 else 0
        
        return {
            "pcr_oi": round(pcr_oi, 3),
            "pcr_volume": round(pcr_volume, 3),
            "sentiment": "BEARISH" if pcr_oi > 1.2 else "BULLISH" if pcr_oi < 0.8 else "NEUTRAL",
            "put_oi": int(put_oi),
            "call_oi": int(call_oi),
            "total_oi": int(put_oi + call_oi)
        }
    
    @staticmethod
    def detect_max_pain(option_chain_df: pd.DataFrame) -> Dict:
        """Calculate Max Pain strike."""
        if option_chain_df.empty:
            return {"max_pain_strike": 0, "total_pain": 0}
        
        strikes = sorted(option_chain_df['strike'].unique())
        pain_values = []
        
        for strike in strikes:
            total_pain = 0
            
            # Calculate pain from puts
            puts = option_chain_df[(option_chain_df['strike'] == strike) & 
                                  (option_chain_df['option_type'] == 'PE')]
            for _, put in puts.iterrows():
                if strike < put['strike']:  # ITM
                    total_pain += (put['strike'] - strike) * put['oi']
            
            # Calculate pain from calls
            calls = option_chain_df[(option_chain_df['strike'] == strike) & 
                                   (option_chain_df['option_type'] == 'CE')]
            for _, call in calls.iterrows():
                if strike > call['strike']:  # ITM
                    total_pain += (strike - call['strike']) * call['oi']
            
            pain_values.append((strike, total_pain))
        
        if pain_values:
            max_pain_strike, min_pain = min(pain_values, key=lambda x: x[1])
            return {
                "max_pain_strike": max_pain_strike,
                "total_pain": int(min_pain),
                "all_pain_levels": pain_values[:10]  # Top 10
            }
        
        return {"max_pain_strike": 0, "total_pain": 0}


# ==============================
# UTILITY FUNCTIONS
# ==============================

def display_market_analytics(analytics: Dict):
    """Display market analytics in Streamlit."""
    if not analytics:
        st.info("No analytics available yet")
        return
    
    st.markdown("### üìä Advanced Market Analytics")
    
    # OI Velocity
    col1, col2, col3 = st.columns(3)
    with col1:
        oi_vel = analytics.get("oi_velocity", 0)
        regime = analytics.get("oi_regime", "N/A")
        color = "green" if oi_vel > 0 else "red"
        st.metric("OI Velocity", f"{oi_vel:.2f}œÉ", regime, delta_color="off")
    
    # Gamma Exposure
    with col2:
        gamma = analytics.get("gamma_exposure", {}).get("net_gamma", 0)
        gamma_regime = analytics.get("gamma_exposure", {}).get("regime", "N/A")
        icon = "üìå" if "POSITIVE" in gamma_regime else "üöÄ" if "NEGATIVE" in gamma_regime else "‚öñÔ∏è"
        st.metric("Gamma Exposure", f"{icon} {gamma_regime}", f"{gamma:.2f}")
    
    # Market Regime
    with col3:
        regime = analytics.get("market_regime", "N/A")
        confidence = analytics.get("regime_confidence", 0) * 100
        st.metric("Market Regime", regime, f"{confidence:.0f}% confidence")
    
    # Insights
    insights = analytics.get("market_insights", [])
    if insights:
        st.markdown("#### üí° Market Insights")
        for insight in insights[:5]:  # Top 5 insights
            st.info(insight)
    
    # Structural Walls
    walls = analytics.get("structural_walls", [])
    if walls:
        st.markdown("#### üß± Structural Walls")
        wall_df = pd.DataFrame(walls)
        st.dataframe(wall_df, use_container_width=True)
    
    # Traps
    traps = analytics.get("potential_traps", [])
    if traps:
        st.markdown("#### üéØ Potential Traps")
        for trap in traps:
            direction = trap.get("direction", "")
            strike = trap.get("strike", 0)
            confidence = trap.get("confidence", 0) * 100
            st.warning(f"{direction} trap at {strike} ({confidence:.0f}% confidence)")



====================================================================================================
FILE: .\features\breadth.py
====================================================================================================


File Name: breadth.py
Full Path: G:\trading_app\features\breadth.py
Size: 17.69 KB
Last Modified: 01/27/2026 19:42:19
Extension: .py

"""
Enhanced Breadth Features with Research Calculations.
Includes CCC analysis, market breadth, and constituent analysis.
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
from scipy import stats

# ==============================
# BREADTH FEATURE CALCULATIONS
# ==============================

def compute_breadth_features(
    constituents_df: pd.DataFrame,
    ccc_history: pd.Series
) -> Dict[str, float]:
    """
    Compute breadth-based features including research metrics.
    """
    # Base features
    ccc_value = compute_ccc_value(constituents_df)
    ccc_slope = compute_ccc_slope(ccc_history)
    
    # Advanced breadth features
    if not constituents_df.empty:
        breadth_metrics = compute_market_breadth(constituents_df)
        weight_distribution = analyze_weight_distribution(constituents_df)
        sector_analysis = analyze_sector_concentration(constituents_df)
        momentum_breadth = compute_momentum_breadth(constituents_df)
    else:
        breadth_metrics = {
            "advance_decline_ratio": 1.0,
            "percent_above_ma": 0.5,
            "breadth_momentum": 0.0
        }
        weight_distribution = {
            "top5_concentration": 0.3,
            "herfindahl_index": 0.1,
            "weight_skew": 0.0
        }
        sector_analysis = {
            "sector_concentration": 0.5,
            "dominant_sector_strength": 0.0
        }
        momentum_breadth = {
            "positive_momentum_ratio": 0.5,
            "momentum_dispersion": 0.0
        }
    
    # Combine all features
    features = {
        # Base CCC features
        "ccc_value": ccc_value,
        "ccc_slope": ccc_slope,
        
        # Market breadth
        "advance_decline_ratio": breadth_metrics["advance_decline_ratio"],
        "percent_above_ma": breadth_metrics["percent_above_ma"],
        "breadth_momentum": breadth_metrics["breadth_momentum"],
        
        # Weight distribution
        "top5_concentration": weight_distribution["top5_concentration"],
        "herfindahl_index": weight_distribution["herfindahl_index"],
        "weight_skew": weight_distribution["weight_skew"],
        
        # Sector analysis
        "sector_concentration": sector_analysis["sector_concentration"],
        "dominant_sector_strength": sector_analysis["dominant_sector_strength"],
        
        # Momentum breadth
        "positive_momentum_ratio": momentum_breadth["positive_momentum_ratio"],
        "momentum_dispersion": momentum_breadth["momentum_dispersion"],
        
        # Derived features
        "breadth_health": compute_breadth_health_score(
            ccc_value, ccc_slope, breadth_metrics
        ),
        "market_participation": compute_market_participation(constituents_df)
    }
    
    return features

# ==============================
# CCC CALCULATIONS
# ==============================

def compute_ccc_value(
    constituents_df: pd.DataFrame,
    weight_col: str = "weight",
    price_change_col: str = "price_change"
) -> float:
    """
    Compute Cumulative Constituent Contribution (CCC).
    """
    if constituents_df.empty:
        return 0.0
    
    required_cols = {weight_col, price_change_col}
    if not required_cols.issubset(constituents_df.columns):
        raise ValueError(
            f"Missing required columns: {required_cols - set(constituents_df.columns)}"
        )
    
    ccc = (constituents_df[weight_col] * constituents_df[price_change_col]).sum()
    return float(ccc)

def build_constituents_df(
    equity_quotes_df: pd.DataFrame,
    weights: Dict[str, float]
) -> pd.DataFrame:
    """
    Build constituents DataFrame with weight and price change.
    """
    rows = []
    
    for _, row in equity_quotes_df.iterrows():
        symbol = row["symbol"]
        if symbol not in weights:
            continue
        
        open_price = row["open"]
        ltp = row["ltp"]
        
        if not open_price or open_price == 0:
            continue
        
        price_change = (ltp - open_price) / open_price
        
        rows.append({
            "symbol": symbol,
            "weight": weights[symbol],
            "price_change": price_change,
            "ltp": ltp,
            "open": open_price
        })
    
    return pd.DataFrame(rows)

def compute_ccc_slope(
    ccc_series: pd.Series,
    lookback: int = 5
) -> float:
    """
    Compute slope of CCC over last N points.
    """
    if len(ccc_series) <= lookback:
        return 0.0
    
    y = ccc_series.iloc[-lookback:]
    x = np.arange(len(y))
    
    slope = np.polyfit(x, y, 1)[0]
    return float(slope)

# ==============================
# MARKET BREADTH ANALYSIS
# ==============================

def compute_market_breadth(constituents_df: pd.DataFrame) -> Dict[str, float]:
    """
    Compute comprehensive market breadth metrics.
    """
    if constituents_df.empty:
        return {
            "advance_decline_ratio": 1.0,
            "percent_above_ma": 0.5,
            "breadth_momentum": 0.0,
            "new_highs_lows": 0.0,
            "breadth_thrust": 0.0
        }
    
    # Advance-Decline ratio
    advances = constituents_df[constituents_df["price_change"] > 0]
    declines = constituents_df[constituents_df["price_change"] < 0]
    
    advance_count = len(advances)
    decline_count = len(declines)
    
    if decline_count == 0:
        advance_decline_ratio = float(advance_count) if advance_count > 0 else 1.0
    else:
        advance_decline_ratio = advance_count / decline_count
    
    # Percent above "moving average" (simplified as above open)
    above_open = constituents_df[constituents_df["price_change"] > 0]
    percent_above = len(above_open) / len(constituents_df)
    
    # Breadth momentum (weighted average of price changes)
    if constituents_df["weight"].sum() > 0:
        breadth_momentum = (constituents_df["weight"] * constituents_df["price_change"]).sum()
    else:
        breadth_momentum = constituents_df["price_change"].mean()
    
    # New highs/lows (simplified as extreme moves)
    price_change_std = constituents_df["price_change"].std()
    if price_change_std > 0:
        extreme_positives = constituents_df[
            constituents_df["price_change"] > 2 * price_change_std
        ]
        extreme_negatives = constituents_df[
            constituents_df["price_change"] < -2 * price_change_std
        ]
        new_highs_lows = (len(extreme_positives) - len(extreme_negatives)) / len(constituents_df)
    else:
        new_highs_lows = 0.0
    
    # Breadth thrust (rapid improvement in breadth)
    # Simplified as acceleration in advance-decline ratio
    breadth_thrust = 0.0  # Would require historical data
    
    return {
        "advance_decline_ratio": float(advance_decline_ratio),
        "percent_above_ma": float(percent_above),
        "breadth_momentum": float(breadth_momentum),
        "new_highs_lows": float(new_highs_lows),
        "breadth_thrust": float(breadth_thrust)
    }

# ==============================
# WEIGHT DISTRIBUTION ANALYSIS
# ==============================

def analyze_weight_distribution(constituents_df: pd.DataFrame) -> Dict[str, float]:
    """
    Analyze weight distribution of index constituents.
    """
    if constituents_df.empty:
        return {
            "top5_concentration": 0.3,
            "herfindahl_index": 0.1,
            "weight_skew": 0.0,
            "weight_gini": 0.5
        }
    
    weights = constituents_df["weight"].values
    
    # Top 5 concentration
    weights_sorted = np.sort(weights)[::-1]
    top5_concentration = np.sum(weights_sorted[:5]) if len(weights_sorted) >= 5 else np.sum(weights_sorted)
    
    # Herfindahl-Hirschman Index (concentration measure)
    herfindahl_index = np.sum(weights ** 2)
    
    # Weight skewness
    if len(weights) >= 3:
        weight_skew = stats.skew(weights)
    else:
        weight_skew = 0.0
    
    # Gini coefficient (inequality measure)
    if len(weights) >= 2:
        sorted_weights = np.sort(weights)
        n = len(sorted_weights)
        cumulative = np.cumsum(sorted_weights)
        gini = (n + 1 - 2 * np.sum(cumulative) / cumulative[-1]) / n
    else:
        gini = 0.5
    
    return {
        "top5_concentration": float(top5_concentration),
        "herfindahl_index": float(herfindahl_index),
        "weight_skew": float(weight_skew),
        "weight_gini": float(gini)
    }

# ==============================
# SECTOR ANALYSIS
# ==============================

def analyze_sector_concentration(constituents_df: pd.DataFrame) -> Dict[str, float]:
    """
    Analyze sector concentration (simplified version).
    In production, this would use actual sector data.
    """
    if constituents_df.empty:
        return {
            "sector_concentration": 0.5,
            "dominant_sector_strength": 0.0,
            "sector_correlation": 0.0
        }
    
    # Simplified: group by weight quartiles
    if len(constituents_df) >= 4:
        quartiles = pd.qcut(constituents_df["weight"], 4, labels=False)
        sector_concentration = 1.0 - len(set(quartiles)) / 4  # Higher = more concentrated
    else:
        sector_concentration = 0.5
    
    # Dominant sector strength (weight of top quartile)
    if len(constituents_df) >= 4:
        top_quartile = constituents_df.nlargest(len(constituents_df) // 4, "weight")
        dominant_sector_strength = top_quartile["weight"].sum()
    else:
        dominant_sector_strength = constituents_df["weight"].max()
    
    # Sector correlation (simplified as correlation of top weights)
    if len(constituents_df) >= 5:
        top_5 = constituents_df.nlargest(5, "weight")
        # Use price changes as proxy for sector performance
        if "price_change" in top_5.columns:
            sector_correlation = top_5["price_change"].std() / (abs(top_5["price_change"].mean()) + 1e-10)
            sector_correlation = min(sector_correlation, 1.0)
        else:
            sector_correlation = 0.5
    else:
        sector_correlation = 0.5
    
    return {
        "sector_concentration": float(sector_concentration),
        "dominant_sector_strength": float(dominant_sector_strength),
        "sector_correlation": float(sector_correlation)
    }

# ==============================
# MOMENTUM BREADTH
# ==============================

def compute_momentum_breadth(constituents_df: pd.DataFrame) -> Dict[str, float]:
    """
    Compute momentum breadth across constituents.
    """
    if constituents_df.empty or "price_change" not in constituents_df.columns:
        return {
            "positive_momentum_ratio": 0.5,
            "momentum_dispersion": 0.0,
            "momentum_trend": 0.0
        }
    
    price_changes = constituents_df["price_change"].values
    weights = constituents_df["weight"].values
    
    # Positive momentum ratio
    positive_momentum_ratio = np.sum(price_changes > 0) / len(price_changes)
    
    # Momentum dispersion (standard deviation of momentum)
    if len(price_changes) >= 2:
        momentum_dispersion = np.std(price_changes)
    else:
        momentum_dispersion = 0.0
    
    # Weighted momentum trend
    if np.sum(weights) > 0:
        weighted_momentum = np.sum(weights * price_changes) / np.sum(weights)
    else:
        weighted_momentum = np.mean(price_changes)
    
    return {
        "positive_momentum_ratio": float(positive_momentum_ratio),
        "momentum_dispersion": float(momentum_dispersion),
        "momentum_trend": float(weighted_momentum)
    }

# ==============================
# DERIVED FEATURES
# ==============================

def compute_breadth_health_score(
    ccc_value: float,
    ccc_slope: float,
    breadth_metrics: Dict[str, float]
) -> float:
    """
    Compute composite breadth health score (0-1).
    """
    scores = []
    
    # CCC value score
    ccc_score = min(abs(ccc_value) * 10, 1.0)
    scores.append(ccc_score * 0.3)
    
    # CCC slope score (positive slope is healthy)
    if ccc_slope > 0:
        slope_score = min(ccc_slope * 100, 1.0)
    else:
        slope_score = max(ccc_slope * 50, -1.0)  # Negative slope penalized
    scores.append(max(slope_score, 0) * 0.2)
    
    # Advance-decline ratio score
    adr = breadth_metrics.get("advance_decline_ratio", 1.0)
    if adr > 1.0:
        adr_score = min((adr - 1.0) * 2, 1.0)  # Cap at 1.0
    else:
        adr_score = max(adr - 0.5, 0) * 2  # Below 0.5 is bad
    scores.append(adr_score * 0.2)
    
    # Percent above MA score
    percent_above = breadth_metrics.get("percent_above_ma", 0.5)
    percent_score = abs(percent_above - 0.5) * 2  # Distance from 50%
    scores.append(percent_score * 0.15)
    
    # Breadth momentum score
    breadth_momentum = breadth_metrics.get("breadth_momentum", 0.0)
    momentum_score = min(abs(breadth_momentum) * 100, 1.0)
    scores.append(momentum_score * 0.15)
    
    return float(np.sum(scores))

def compute_market_participation(constituents_df: pd.DataFrame) -> float:
    """
    Compute market participation score (0-1).
    Higher = broader market participation in moves.
    """
    if constituents_df.empty or "price_change" not in constituents_df.columns:
        return 0.5
    
    price_changes = constituents_df["price_change"].values
    
    if len(price_changes) < 2:
        return 0.5
    
    # Participation = 1 - (fraction of stocks with near-zero changes)
    near_zero = np.sum(np.abs(price_changes) < 0.001) / len(price_changes)
    participation = 1.0 - near_zero
    
    # Adjust for direction consistency
    positive_fraction = np.sum(price_changes > 0) / len(price_changes)
    direction_consistency = max(positive_fraction, 1 - positive_fraction)
    
    participation = participation * direction_consistency
    
    return float(participation)

# ==============================
# DIVERGENCE DETECTION
# ==============================

def detect_breadth_divergence(
    price_change: float,
    breadth_metrics: Dict[str, float],
    historical_breadth: List[Dict]
) -> Tuple[bool, str, float]:
    """
    Detect divergence between price and breadth indicators.
    
    Returns:
        has_divergence, direction, confidence
    """
    if len(historical_breadth) < 5:
        return False, "NEUTRAL", 0.0
    
    # Get recent breadth values
    recent_breadth = historical_breadth[-5:]
    
    # Calculate breadth momentum
    breadth_values = [b.get("advance_decline_ratio", 1.0) for b in recent_breadth]
    breadth_momentum = np.polyfit(range(len(breadth_values)), breadth_values, 1)[0]
    
    # Price momentum (simplified)
    price_momentum = price_change
    
    # Detect divergence
    if price_momentum > 0.01 and breadth_momentum < -0.1:
        # Price up but breadth deteriorating (bearish divergence)
        confidence = min(abs(breadth_momentum) * 10, 1.0)
        return True, "BEARISH", confidence
    
    elif price_momentum < -0.01 and breadth_momentum > 0.1:
        # Price down but breadth improving (bullish divergence)
        confidence = min(abs(breadth_momentum) * 10, 1.0)
        return True, "BULLISH", confidence
    
    return False, "NEUTRAL", 0.0

# ==============================
# UTILITY FUNCTIONS
# ==============================

def calculate_breadth_indicators(constituents_df: pd.DataFrame) -> Dict[str, float]:
    """
    Calculate traditional breadth indicators.
    """
    if constituents_df.empty:
        return {}
    
    indicators = {}
    
    # McClellan Oscillator components
    advances = len(constituents_df[constituents_df["price_change"] > 0])
    declines = len(constituents_df[constituents_df["price_change"] < 0])
    
    indicators["advance_decline_line"] = advances - declines
    indicators["advance_decline_ratio"] = advances / declines if declines > 0 else advances
    
    # Arms Index (TRIN)
    if declines > 0:
        advance_volume = 1  # Simplified
        decline_volume = 1  # Simplified
        indicators["arms_index"] = (advances / declines) / (advance_volume / decline_volume)
    else:
        indicators["arms_index"] = 0.0
    
    # Percent above moving average (simplified)
    avg_change = constituents_df["price_change"].mean()
    indicators["percent_above_average"] = len(
        constituents_df[constituents_df["price_change"] > avg_change]
    ) / len(constituents_df)
    
    return indicators

def get_breadth_alerts(breadth_features: Dict[str, float]) -> List[str]:
    """
    Generate breadth-based alerts.
    """
    alerts = []
    
    # Check CCC value
    ccc_value = breadth_features.get("ccc_value", 0.0)
    if ccc_value > 0.01:
        alerts.append("Strong positive breadth (CCC > 1%)")
    elif ccc_value < -0.01:
        alerts.append("Strong negative breadth (CCC < -1%)")
    
    # Check advance-decline ratio
    adr = breadth_features.get("advance_decline_ratio", 1.0)
    if adr > 2.0:
        alerts.append("Extreme breadth: Advances >> Declines")
    elif adr < 0.5:
        alerts.append("Extreme breadth: Declines >> Advances")
    
    # Check breadth health
    health = breadth_features.get("breadth_health", 0.5)
    if health > 0.8:
        alerts.append("Excellent breadth health")
    elif health < 0.3:
        alerts.append("Poor breadth health")
    
    # Check market participation
    participation = breadth_features.get("market_participation", 0.5)
    if participation > 0.8:
        alerts.append("Broad market participation")
    elif participation < 0.3:
        alerts.append("Narrow market participation")
    
    return alerts[:3]  # Return top 3 alerts



====================================================================================================
FILE: .\features\option_features.py
====================================================================================================


File Name: option_features.py
Full Path: G:\trading_app\features\option_features.py
Size: 18.24 KB
Last Modified: 01/28/2026 10:33:55
Extension: .py

"""
Enhanced Option Features with Research Calculations.
Includes Gamma Exposure, OI Velocity, and structural analysis.
"""

import pandas as pd
import numpy as np
from datetime import datetime
from scipy.stats import norm
from typing import List, Dict, Tuple, Optional, Any, Union

# ==============================
# BLACK-SCHOLES CALCULATIONS
# ==============================

def black_scholes_greeks(
    S: float,           # Spot price
    K: float,           # Strike price
    T: float,           # Time to expiry (years)
    r: float,           # Risk-free rate
    sigma: float,       # Implied volatility
    option_type: str    # 'CE' or 'PE'
) -> Dict[str, float]:
    """
    Calculate Black-Scholes Greeks for options.
    """
    if T <= 0 or sigma <= 0:
        return {
            "delta": 0.5 if option_type == "CE" else -0.5,
            "gamma": 0.0,
            "theta": 0.0,
            "vega": 0.0
        }
    
    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))
    d2 = d1 - sigma * np.sqrt(T)
    
    if option_type == "CE":
        delta = norm.cdf(d1)
        theta = (-S * norm.pdf(d1) * sigma / (2 * np.sqrt(T)) 
                 - r * K * np.exp(-r * T) * norm.cdf(d2))
    else:  # PE
        delta = norm.cdf(d1) - 1
        theta = (-S * norm.pdf(d1) * sigma / (2 * np.sqrt(T))
                 + r * K * np.exp(-r * T) * norm.cdf(-d2))
    
    gamma = norm.pdf(d1) / (S * sigma * np.sqrt(T))
    vega = S * norm.pdf(d1) * np.sqrt(T)
    
    return {
        "delta": float(delta),
        "gamma": float(gamma),
        "theta": float(theta),
        "vega": float(vega)
    }

# ==============================
# OPTION CHAIN FEATURES
# ==============================

def compute_option_features(
    option_chain_df: pd.DataFrame,
    spot_price: float,
    expiry_datetime: datetime
) -> Dict[str, float]:
    """
    Compute option chain features including research metrics.
    """
    if option_chain_df.empty:
        return get_default_features()
    
    # Base features
    put_call_ratio = compute_put_call_ratio(option_chain_df)
    oi_delta = compute_oi_delta(option_chain_df)
    oi_concentration = compute_oi_concentration(option_chain_df)
    atm_iv = compute_atm_iv(option_chain_df, spot_price)
    iv_skew = compute_iv_skew(option_chain_df, spot_price)
    
    # Advanced features
    gamma_profile = compute_gamma_profile(option_chain_df, spot_price, expiry_datetime)
    oi_velocity = compute_oi_velocity(option_chain_df)
    max_pain = compute_max_pain(option_chain_df)
    vix_smile = compute_vix_smile(option_chain_df, spot_price)
    
    # Combine all features
    features = {
        # Base features
        "put_call_ratio": put_call_ratio,
        "oi_delta": oi_delta,
        "oi_concentration": oi_concentration,
        "atm_iv": atm_iv,
        "iv_skew": iv_skew,
        
        # Time feature
        "time_to_expiry_minutes": compute_time_to_expiry_minutes(expiry_datetime),
        
        # Research features
        "net_gamma": gamma_profile["net_gamma"],
        "gamma_skew": gamma_profile["skew"],
        "max_gamma_strike": gamma_profile["max_strike"],
        "oi_velocity": oi_velocity,
        "max_pain_strike": max_pain,
        "vix_smile": vix_smile,
        
        # Volume and OI ratios
        "call_oi_ratio": compute_call_oi_ratio(option_chain_df),
        "put_oi_ratio": compute_put_oi_ratio(option_chain_df),
        "volume_put_call_ratio": compute_volume_pcr(option_chain_df)
    }
    
    return features

# ==============================
# BASE FEATURE CALCULATIONS
# ==============================

def compute_put_call_ratio(option_chain_df: pd.DataFrame) -> float:
    """Calculate Put-Call Ratio (OI-based)."""
    if option_chain_df.empty:
        return 1.0
    
    put_oi = option_chain_df.loc[
        option_chain_df["option_type"] == "PE", "oi"
    ].sum()
    
    call_oi = option_chain_df.loc[
        option_chain_df["option_type"] == "CE", "oi"
    ].sum()
    
    if call_oi == 0:
        return 0.0
    
    return float(put_oi / call_oi)

def compute_oi_delta(option_chain_df: pd.DataFrame) -> float:
    """Calculate OI Delta (Put OI change - Call OI change)."""
    if option_chain_df.empty:
        return 0.0
    
    put_delta = option_chain_df.loc[
        option_chain_df["option_type"] == "PE", "oi_change"
    ].sum()
    
    call_delta = option_chain_df.loc[
        option_chain_df["option_type"] == "CE", "oi_change"
    ].sum()
    
    return float(put_delta - call_delta)

def compute_oi_concentration(option_chain_df: pd.DataFrame) -> float:
    """Calculate OI concentration at maximum OI strike."""
    if option_chain_df.empty:
        return 0.0
    
    total_oi = option_chain_df["oi"].sum()
    if total_oi == 0:
        return 0.0
    
    max_strike_oi = (
        option_chain_df
        .groupby("strike")["oi"]
        .sum()
        .max()
    )
    
    return float(max_strike_oi / total_oi)

def compute_atm_iv(option_chain_df: pd.DataFrame, spot_price: float) -> float:
    """Calculate At-The-Money Implied Volatility."""
    if option_chain_df.empty:
        return 0.3  # Default IV
    
    option_chain_df = option_chain_df.copy()
    option_chain_df["dist"] = abs(option_chain_df["strike"] - spot_price)
    
    atm_row = option_chain_df.sort_values("dist").iloc[0]
    iv = atm_row["iv"]
    
    return float(iv) if not np.isnan(iv) else 0.3

def compute_iv_skew(option_chain_df: pd.DataFrame, spot_price: float) -> float:
    """Calculate IV Skew (Call IV - Put IV)."""
    if option_chain_df.empty:
        return 0.0
    
    option_chain_df = option_chain_df.copy()
    option_chain_df["dist"] = abs(option_chain_df["strike"] - spot_price)
    
    # Get nearest strikes
    nearest_strikes = option_chain_df.sort_values("dist").head(10)
    
    ce_iv = nearest_strikes.loc[
        nearest_strikes["option_type"] == "CE", "iv"
    ].mean()
    
    pe_iv = nearest_strikes.loc[
        nearest_strikes["option_type"] == "PE", "iv"
    ].mean()
    
    if np.isnan(ce_iv) or np.isnan(pe_iv):
        return 0.0
    
    return float(ce_iv - pe_iv)

def compute_time_to_expiry_minutes(expiry_datetime: datetime) -> int:
    """Calculate minutes to expiry."""
    now = datetime.utcnow()
    delta = expiry_datetime - now
    
    minutes = int(delta.total_seconds() / 60)
    return max(minutes, 0)

# ==============================
# RESEARCH FEATURE CALCULATIONS
# ==============================

def compute_gamma_profile(
    option_chain_df: pd.DataFrame,
    spot_price: float,
    expiry_datetime: datetime
) -> Dict[str, float]:
    """
    Compute Gamma Exposure profile for research.
    """
    if option_chain_df.empty:
        return {
            "net_gamma": 0.0,
            "positive_gamma": 0.0,
            "negative_gamma": 0.0,
            "skew": 0.0,
            "max_strike": 0.0
        }
    
    
    # Calculate time to expiry in years
    now = datetime.utcnow()
    T = max((expiry_datetime - now).total_seconds() / (365 * 24 * 3600), 0.001)
    r = 0.05  # Risk-free rate approximation
    
    total_gamma = 0.0
    positive_gamma = 0.0
    negative_gamma = 0.0
    gamma_strikes = []
    
    for _, row in option_chain_df.iterrows():
        strike = row['strike']
        option_type = row['option_type']
        oi = row['oi']
        iv = row.get('iv', 0.3)
        
        # Calculate gamma for this strike
        greeks = black_scholes_greeks(
            S=spot_price,
            K=strike,
            T=T,
            r=r,
            sigma=iv,
            option_type=option_type
        )
        
        gamma = greeks["gamma"]
        
        # Adjust sign based on market maker position
        # Market makers are typically short options
        if option_type == "CE":
            gamma_contribution = -gamma * oi * 100  # Negative for short calls
        else:  # PE
            gamma_contribution = gamma * oi * 100   # Positive for short puts
        
        total_gamma += gamma_contribution
        
        if gamma_contribution > 0:
            positive_gamma += gamma_contribution
        else:
            negative_gamma += gamma_contribution
        
        gamma_strikes.append((strike, gamma_contribution))
    
    # Calculate gamma skew and max_strike
    if gamma_strikes:
        strikes, gammas = zip(*gamma_strikes)
        
        # Calculate skew
        if len(gammas) >= 10:
            gamma_skew = np.mean(gammas[:5]) - np.mean(gammas[-5:])
        else:
            gamma_skew = 0.0
        
        # Find max gamma strike safely
        if len(strikes) > 0 and len(gammas) > 0:
            max_idx = np.argmax(np.abs(gammas))
            max_gamma_strike = strikes[max_idx]
            # Ensure it's not None
            if max_gamma_strike is None or pd.isnull(max_gamma_strike):
                max_gamma_strike = 0.0
        else:
            max_gamma_strike = 0.0
            gamma_skew = 0.0
    else:
        gamma_skew = 0.0
        max_gamma_strike = 0.0
    
    return {
        "net_gamma": float(total_gamma),
        "positive_gamma": float(positive_gamma),
        "negative_gamma": float(negative_gamma),
        "skew": float(gamma_skew),
        "max_strike": float(max_gamma_strike)
    }

def compute_oi_velocity(option_chain_df: pd.DataFrame) -> float:
    """
    Calculate OI Velocity (rate of change of OI).
    In production, this would use historical data.
    """
    if option_chain_df.empty:
        return 0.0
    
    # Simplified velocity calculation
    # In real implementation, compare with previous snapshot
    total_oi = option_chain_df["oi"].sum()
    oi_change = option_chain_df["oi_change"].sum()
    
    if total_oi == 0:
        return 0.0
    
    velocity = oi_change / total_oi * 100
    return float(velocity)

def compute_max_pain(option_chain_df: pd.DataFrame) -> float:
    """Calculate Max Pain strike."""
    if option_chain_df.empty:
        return 0.0
    
    # Check if we have strikes
    if 'strike' not in option_chain_df.columns:
        return 0.0
    
    strikes = sorted(option_chain_df['strike'].unique())
    if not strikes:
        return 0.0
    
    pain_values = []
    
    for strike in strikes:
        total_pain = 0
        
        # Calculate pain from puts
        puts = option_chain_df[(option_chain_df['strike'] == strike) & 
                              (option_chain_df['option_type'] == 'PE')]
        for _, put in puts.iterrows():
            # Check if put is ITM (strike > spot for put pain calculation)
            # Actually for max pain, we calculate loss for option writers
            # For puts: writers lose when spot < strike
            put_strike = put['strike']
            if isinstance(put_strike, (int, float)):
                if strike < put_strike:  # ITM puts cause pain
                    put_oi = put['oi'] if pd.notnull(put['oi']) else 0
                    total_pain += (put_strike - strike) * put_oi
        
        # Calculate pain from calls
        calls = option_chain_df[(option_chain_df['strike'] == strike) & 
                               (option_chain_df['option_type'] == 'CE')]
        for _, call in calls.iterrows():
            # For calls: writers lose when spot > strike
            call_strike = call['strike']
            if isinstance(call_strike, (int, float)):
                if strike > call_strike:  # ITM calls cause pain
                    call_oi = call['oi'] if pd.notnull(call['oi']) else 0
                    total_pain += (strike - call_strike) * call_oi
        
        pain_values.append((strike, total_pain))
    
    if pain_values:
        # Find strike with minimum total pain
        min_pain_item = min(pain_values, key=lambda x: x[1])
        max_pain_strike = min_pain_item[0]
        
        # Ensure it's a valid number
        if max_pain_strike is None or pd.isnull(max_pain_strike):
            return 0.0
        return float(max_pain_strike)
    
    return 0.0

def compute_vix_smile(option_chain_df: pd.DataFrame, spot_price: float) -> float:
    """Calculate VIX smile curvature."""
    if option_chain_df.empty or spot_price <= 0:
        return 0.0
    
    # Group by distance from spot
    option_chain_df = option_chain_df.copy()
    option_chain_df['distance_pct'] = abs(option_chain_df['strike'] - spot_price) / spot_price * 100
    
    # Get IVs at different distances
    atm_iv = option_chain_df[
        option_chain_df['distance_pct'] < 2
    ]['iv'].mean()
    
    otm_iv = option_chain_df[
        (option_chain_df['distance_pct'] >= 5) & 
        (option_chain_df['distance_pct'] < 10)
    ]['iv'].mean()
    
    if np.isnan(atm_iv) or np.isnan(otm_iv):
        return 0.0
    
    # Smile = OTM IV - ATM IV (positive = smile, negative = smirk)
    return float(otm_iv - atm_iv)

# ==============================
# SUPPORTING CALCULATIONS
# ==============================

def compute_call_oi_ratio(option_chain_df: pd.DataFrame) -> float:
    """Calculate Call OI as percentage of total OI."""
    if option_chain_df.empty:
        return 0.5
    
    total_oi = option_chain_df["oi"].sum()
    call_oi = option_chain_df[option_chain_df["option_type"] == "CE"]["oi"].sum()
    
    if total_oi == 0:
        return 0.5
    
    return float(call_oi / total_oi)

def compute_put_oi_ratio(option_chain_df: pd.DataFrame) -> float:
    """Calculate Put OI as percentage of total OI."""
    if option_chain_df.empty:
        return 0.5
    
    total_oi = option_chain_df["oi"].sum()
    put_oi = option_chain_df[option_chain_df["option_type"] == "PE"]["oi"].sum()
    
    if total_oi == 0:
        return 0.5
    
    return float(put_oi / total_oi)

def compute_volume_pcr(option_chain_df: pd.DataFrame) -> float:
    """Calculate Volume-based Put-Call Ratio."""
    if option_chain_df.empty:
        return 1.0
    
    put_volume = option_chain_df.loc[
        option_chain_df["option_type"] == "PE", "volume"
    ].sum()
    
    call_volume = option_chain_df.loc[
        option_chain_df["option_type"] == "CE", "volume"
    ].sum()
    
    if call_volume == 0:
        return 0.0
    
    return float(put_volume / call_volume)

def get_default_features() -> Dict[str, float]:
    """Return default feature values when no data is available."""
    return {
        "put_call_ratio": 1.0,
        "oi_delta": 0.0,
        "oi_concentration": 0.0,
        "atm_iv": 0.3,
        "iv_skew": 0.0,
        "time_to_expiry_minutes": 0,
        "net_gamma": 0.0,
        "gamma_skew": 0.0,
        "max_gamma_strike": 0.0,
        "oi_velocity": 0.0,
        "max_pain_strike": 0.0,
        "vix_smile": 0.0,
        "call_oi_ratio": 0.5,
        "put_oi_ratio": 0.5,
        "volume_put_call_ratio": 1.0
    }

# ==============================
# UTILITY FUNCTIONS
# ==============================

def analyze_option_skew(option_chain_df: pd.DataFrame, spot_price: float) -> Dict[str, float]:
    """
    Analyze option skew across strikes.
    """
    if option_chain_df.empty:
        return {
            "call_skew": 0.0,
            "put_skew": 0.0,
            "total_skew": 0.0
        }
    
    # Separate calls and puts
    calls = option_chain_df[option_chain_df["option_type"] == "CE"].copy()
    puts = option_chain_df[option_chain_df["option_type"] == "PE"].copy()
    
    # Calculate distance from spot
    calls["distance_pct"] = (calls["strike"] - spot_price) / spot_price * 100
    puts["distance_pct"] = (spot_price - puts["strike"]) / spot_price * 100
    
    # Calculate skew (slope of IV vs distance)
    call_skew = 0.0
    put_skew = 0.0
    
    if len(calls) >= 5:
        # Sort by distance and take top 5 OTM calls
        otm_calls = calls[calls["distance_pct"] > 0].nlargest(5, "distance_pct")
        if len(otm_calls) >= 2:
            call_skew = np.polyfit(otm_calls["distance_pct"], otm_calls["iv"], 1)[0]
    
    if len(puts) >= 5:
        # Sort by distance and take top 5 OTM puts
        otm_puts = puts[puts["distance_pct"] > 0].nlargest(5, "distance_pct")
        if len(otm_puts) >= 2:
            put_skew = np.polyfit(otm_puts["distance_pct"], otm_puts["iv"], 1)[0]
    
    return {
        "call_skew": float(call_skew),
        "put_skew": float(put_skew),
        "total_skew": float(call_skew + put_skew)
    }

def detect_gamma_flip_levels(
    option_chain_df: pd.DataFrame,
    spot_price: float,
    expiry_datetime: datetime
) -> List[float]:
    """
    Detect price levels where gamma flips sign.
    """
    if option_chain_df.empty:
        return []
    
    # Calculate gamma at different price levels
    price_levels = np.linspace(spot_price * 0.95, spot_price * 1.05, 21)
    gamma_levels = []
    
    T = max((expiry_datetime - datetime.utcnow()).total_seconds() / (365 * 24 * 3600), 0.001)
    r = 0.05
    
    for price in price_levels:
        total_gamma = 0.0
        
        for _, row in option_chain_df.iterrows():
            strike = row['strike']
            option_type = row['option_type']
            oi = row['oi']
            iv = row.get('iv', 0.3)
            
            greeks = black_scholes_greeks(
                S=price,
                K=strike,
                T=T,
                r=r,
                sigma=iv,
                option_type=option_type
            )
            
            gamma = greeks["gamma"]
            
            if option_type == "CE":
                gamma_contribution = -gamma * oi
            else:
                gamma_contribution = gamma * oi
            
            total_gamma += gamma_contribution
        
        gamma_levels.append((price, total_gamma))
    
    # Find sign changes
    flip_levels = []
    for i in range(1, len(gamma_levels)):
        prev_sign = np.sign(gamma_levels[i-1][1])
        curr_sign = np.sign(gamma_levels[i][1])
        
        if prev_sign != curr_sign and prev_sign != 0 and curr_sign != 0:
            flip_price = (gamma_levels[i-1][0] + gamma_levels[i][0]) / 2
            flip_levels.append(float(flip_price))
    
    return flip_levels



====================================================================================================
FILE: .\features\price_features.py
====================================================================================================


File Name: price_features.py
Full Path: G:\trading_app\features\price_features.py
Size: 12.15 KB
Last Modified: 01/27/2026 19:39:48
Extension: .py

"""
Enhanced Price Features with Research Calculations.
Includes VWAP distance, momentum, volume analysis, and divergence detection.
"""

import pandas as pd
import numpy as np
from typing import Dict, Tuple, List
from scipy import stats

# ==============================
# PRICE FEATURE CALCULATIONS
# ==============================

def compute_price_features(
    ltp: float,
    vwap: float,
    price_series: pd.Series,
    volume_series: pd.Series
) -> Dict[str, float]:
    """
    Compute price-based features including research metrics.
    """
    # Base features
    vwap_distance = compute_vwap_distance(ltp, vwap)
    price_momentum = compute_price_momentum(price_series)
    volume_ratio = compute_volume_ratio(volume_series)
    
    # Advanced features
    trend_strength = compute_trend_strength(price_series)
    volatility = compute_volatility(price_series)
    rsi = compute_rsi(price_series)
    volume_profile = compute_volume_profile(price_series, volume_series)
    price_efficiency = compute_price_efficiency(price_series)
    
    # Divergence detection
    volume_price_divergence = compute_volume_price_divergence(price_series, volume_series)
    
    # Combine all features
    features = {
        # Base features
        "vwap_distance": vwap_distance,
        "price_momentum": price_momentum,
        "volume_ratio": volume_ratio,
        
        # Trend and momentum
        "trend_strength": trend_strength,
        "price_volatility": volatility,
        "rsi": rsi,
        "price_efficiency": price_efficiency,
        
        # Volume analysis
        "volume_trend": volume_profile["trend"],
        "volume_volatility": volume_profile["volatility"],
        "volume_clustering": volume_profile["clustering"],
        
        # Divergence
        "volume_price_divergence": volume_price_divergence,
        "has_volume_divergence": 1.0 if abs(volume_price_divergence) > 0.5 else 0.0
    }
    
    return features

# ==============================
# BASE FEATURE CALCULATIONS
# ==============================

def compute_vwap_distance(ltp: float, vwap: float) -> float:
    """Calculate distance from VWAP."""
    if vwap == 0:
        return 0.0
    
    return float((ltp - vwap) / vwap)

def compute_price_momentum(
    price_series: pd.Series,
    lookback: int = 5
) -> float:
    """Calculate price momentum over lookback period."""
    if len(price_series) <= lookback:
        return 0.0
    
    past_price = price_series.iloc[-lookback - 1]
    current_price = price_series.iloc[-1]
    
    if past_price == 0:
        return 0.0
    
    return float((current_price - past_price) / past_price)

def compute_volume_ratio(
    volume_series: pd.Series,
    lookback: int = 20
) -> float:
    """Calculate current volume relative to average."""
    if len(volume_series) < lookback:
        return 0.0
    
    current_volume = volume_series.iloc[-1]
    avg_volume = volume_series.iloc[-lookback:].mean()
    
    if avg_volume == 0:
        return 0.0
    
    return float(current_volume / avg_volume)

# ==============================
# ADVANCED FEATURE CALCULATIONS
# ==============================

def compute_trend_strength(price_series: pd.Series, lookback: int = 20) -> float:
    """
    Calculate trend strength using linear regression.
    Returns R¬≤ value (0-1) indicating trend strength.
    """
    if len(price_series) < lookback:
        return 0.0
    
    prices = price_series.iloc[-lookback:].values
    x = np.arange(len(prices))
    
    # Linear regression
    slope, intercept, r_value, p_value, std_err = stats.linregress(x, prices)
    
    # R¬≤ value indicates trend strength
    r_squared = r_value ** 2
    
    return float(r_squared)

def compute_volatility(price_series: pd.Series, lookback: int = 20) -> float:
    """Calculate price volatility (annualized)."""
    if len(price_series) <= 1:
        return 0.0
    
    returns = price_series.pct_change().dropna()
    
    if len(returns) < lookback:
        sample_returns = returns
    else:
        sample_returns = returns.iloc[-lookback:]
    
    if len(sample_returns) <= 1:
        return 0.0
    
    # Annualized volatility (assuming daily data)
    daily_vol = sample_returns.std()
    annual_vol = daily_vol * np.sqrt(252)
    
    return float(annual_vol)

def compute_rsi(price_series: pd.Series, period: int = 14) -> float:
    """Calculate Relative Strength Index."""
    if len(price_series) <= period:
        return 50.0  # Neutral
    
    delta = price_series.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
    
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    
    return float(rsi.iloc[-1]) if not pd.isna(rsi.iloc[-1]) else 50.0

def compute_price_efficiency(price_series: pd.Series) -> float:
    """
    Calculate price efficiency (random walk index).
    Higher values indicate more efficient/trending markets.
    """
    if len(price_series) < 10:
        return 0.5
    
    # Calculate Hurst exponent approximation
    n = min(len(price_series), 100)
    lags = range(2, min(n // 2, 20))
    
    if len(lags) < 2:
        return 0.5
    
    tau = []
    for lag in lags:
        # Calculate variance of lagged differences
        price_diff = np.diff(price_series.iloc[-n:], lag)
        if len(price_diff) > 1:
            tau.append(np.std(price_diff))
        else:
            tau.append(0)
    
    tau = np.array(tau)
    lags = np.array(lags)
    
    # Remove zeros
    mask = (tau > 0) & (lags > 0)
    if np.sum(mask) < 2:
        return 0.5
    
    # Linear regression in log space
    try:
        hurst = np.polyfit(np.log(lags[mask]), np.log(tau[mask]), 1)[0]
        efficiency = hurst  # H ‚âà 0.5 random walk, >0.5 trending, <0.5 mean-reverting
    except:
        efficiency = 0.5
    
    return float(efficiency)

# ==============================
# VOLUME ANALYSIS
# ==============================

def compute_volume_profile(
    price_series: pd.Series,
    volume_series: pd.Series,
    lookback: int = 20
) -> Dict[str, float]:
    """
    Compute volume profile features.
    """
    if len(volume_series) < lookback or len(price_series) < lookback:
        return {
            "trend": 0.0,
            "volatility": 0.0,
            "clustering": 0.0
        }
    
    recent_volume = volume_series.iloc[-lookback:]
    recent_prices = price_series.iloc[-lookback:]
    
    # Volume trend
    x = np.arange(len(recent_volume))
    volume_trend = np.polyfit(x, recent_volume.values, 1)[0]
    volume_trend_normalized = volume_trend / (recent_volume.mean() + 1e-10)
    
    # Volume volatility
    volume_volatility = recent_volume.std() / (recent_volume.mean() + 1e-10)
    
    # Volume clustering (autocorrelation)
    if len(recent_volume) >= 5:
        volume_clustering = recent_volume.autocorr(lag=1)
        if pd.isna(volume_clustering):
            volume_clustering = 0.0
    else:
        volume_clustering = 0.0
    
    return {
        "trend": float(volume_trend_normalized),
        "volatility": float(volume_volatility),
        "clustering": float(volume_clustering)
    }

def compute_volume_price_divergence(
    price_series: pd.Series,
    volume_series: pd.Series,
    lookback: int = 10
) -> float:
    """
    Detect divergence between price and volume.
    Positive = price up, volume down (bearish divergence)
    Negative = price down, volume up (bullish divergence)
    """
    if len(price_series) < lookback or len(volume_series) < lookback:
        return 0.0
    
    # Calculate price and volume changes
    price_change = (price_series.iloc[-1] - price_series.iloc[-lookback]) / price_series.iloc[-lookback]
    volume_change = (volume_series.iloc[-1] - volume_series.iloc[-lookback]) / (volume_series.iloc[-lookback] + 1e-10)
    
    # Normalize
    price_norm = price_change / (np.std(price_series.iloc[-lookback:].pct_change().dropna()) + 1e-10)
    volume_norm = volume_change / (np.std(volume_series.iloc[-lookback:].pct_change().dropna()) + 1e-10)
    
    # Divergence score
    divergence = price_norm - volume_norm
    
    return float(divergence)

# ==============================
# SUPPORTING CALCULATIONS
# ==============================

def detect_price_patterns(price_series: pd.Series) -> Dict[str, float]:
    """
    Detect common price patterns.
    """
    if len(price_series) < 20:
        return {
            "double_top": 0.0,
            "double_bottom": 0.0,
            "head_shoulders": 0.0,
            "triangle": 0.0
        }
    
    # Simplified pattern detection
    prices = price_series.iloc[-20:].values
    
    # Calculate peaks and troughs
    from scipy.signal import find_peaks
    
    peaks, _ = find_peaks(prices, prominence=np.std(prices) * 0.5)
    troughs, _ = find_peaks(-prices, prominence=np.std(prices) * 0.5)
    
    pattern_scores = {
        "double_top": 0.0,
        "double_bottom": 0.0,
        "head_shoulders": 0.0,
        "triangle": 0.0
    }
    
    # Double top detection
    if len(peaks) >= 2:
        peak1 = prices[peaks[-2]]
        peak2 = prices[peaks[-1]]
        if abs(peak1 - peak2) / peak1 < 0.02:  # Within 2%
            pattern_scores["double_top"] = 0.8
    
    # Double bottom detection
    if len(troughs) >= 2:
        trough1 = prices[troughs[-2]]
        trough2 = prices[troughs[-1]]
        if abs(trough1 - trough2) / trough1 < 0.02:
            pattern_scores["double_bottom"] = 0.8
    
    return pattern_scores

def calculate_support_resistance(
    price_series: pd.Series,
    window: int = 20
) -> Dict[str, float]:
    """
    Calculate support and resistance levels.
    """
    if len(price_series) < window:
        return {
            "support": 0.0,
            "resistance": 0.0,
            "current_position": 0.5
        }
    
    recent_prices = price_series.iloc[-window:]
    
    support = recent_prices.min()
    resistance = recent_prices.max()
    current = price_series.iloc[-1]
    
    if resistance - support == 0:
        position = 0.5
    else:
        position = (current - support) / (resistance - support)
    
    return {
        "support": float(support),
        "resistance": float(resistance),
        "current_position": float(position)
    }

# ==============================
# DIVERGENCE DETECTION
# ==============================

def detect_momentum_divergence(
    price_series: pd.Series,
    momentum_series: pd.Series
) -> Tuple[bool, str, float]:
    """
    Detect divergence between price and momentum oscillator.
    
    Returns:
        has_divergence, direction, confidence
    """
    if len(price_series) < 10 or len(momentum_series) < 10:
        return False, "NEUTRAL", 0.0
    
    # Get recent peaks in price and momentum
    price_peaks = []
    momentum_peaks = []
    
    # Simplified peak detection
    for i in range(5, len(price_series) - 5):
        if price_series.iloc[i] == price_series.iloc[i-5:i+5].max():
            price_peaks.append((i, price_series.iloc[i]))
        if momentum_series.iloc[i] == momentum_series.iloc[i-5:i+5].max():
            momentum_peaks.append((i, momentum_series.iloc[i]))
    
    if len(price_peaks) < 2 or len(momentum_peaks) < 2:
        return False, "NEUTRAL", 0.0
    
    # Check for divergence
    price_trend = price_peaks[-1][1] - price_peaks[-2][1]
    momentum_trend = momentum_peaks[-1][1] - momentum_peaks[-2][1]
    
    # Bullish divergence: price makes lower low, momentum makes higher low
    # Bearish divergence: price makes higher high, momentum makes lower high
    
    if price_trend > 0 and momentum_trend < 0:
        # Bearish divergence
        confidence = min(abs(momentum_trend) / 10, 1.0)
        return True, "BEARISH", confidence
    
    elif price_trend < 0 and momentum_trend > 0:
        # Bullish divergence
        confidence = min(abs(momentum_trend) / 10, 1.0)
        return True, "BULLISH", confidence
    
    return False, "NEUTRAL", 0.0



====================================================================================================
FILE: .\intelligence\market_state.py
====================================================================================================


File Name: market_state.py
Full Path: G:\trading_app\intelligence\market_state.py
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .py




====================================================================================================
FILE: .\intelligence\signal_logic.py
====================================================================================================


File Name: signal_logic.py
Full Path: G:\trading_app\intelligence\signal_logic.py
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .py




====================================================================================================
FILE: .\ml\feature_contract.py
====================================================================================================


File Name: feature_contract.py
Full Path: G:\trading_app\ml\feature_contract.py
Size: 16.29 KB
Last Modified: 01/27/2026 19:13:41
Extension: .py

"""
ENHANCED FEATURE CONTRACT - SINGLE SOURCE OF TRUTH
Incorporates research concepts: OI Velocity, Gamma Exposure, Walls/Traps, Spot Divergence

Used by:
- Live data collection
- ML training
- ML inference
- Backtesting
- Signal generation
"""

# ==============================
# FEATURE VERSION
# ==============================

FEATURE_VERSION = "v2.0"  # Updated for research features

# ==============================
# FEATURE CATEGORIES
# ==============================

# Base features (original set)
BASE_FEATURES = [
    "timestamp",
    
    # Option structure (original)
    "put_call_ratio",
    "oi_delta",
    "oi_concentration",
    "atm_iv",
    "iv_skew",
    
    # Price & flow (original)
    "vwap_distance",
    "price_momentum",
    "volume_ratio",
    
    # Breadth (original)
    "ccc_value",
    "ccc_slope",
    
    # Time context
    "time_to_expiry_minutes"
]

# Research features (enhanced)
RESEARCH_FEATURES = [
    # OI Velocity features
    "oi_velocity",
    "oi_velocity_ma",
    "oi_velocity_std",
    "oi_regime_expansive",      # 1.0 if EXPANSIVE, else 0.0
    "oi_regime_constricted",    # 1.0 if CONSTRICTED, else 0.0
    
    # Gamma Exposure features
    "net_gamma",
    "gamma_regime_positive",    # 1.0 if POSITIVE, else 0.0
    "gamma_regime_negative",    # 1.0 if NEGATIVE, else 0.0
    "gamma_flip_distance",      # Distance to nearest gamma flip (normalized)
    "max_gamma_strike_distance",# Distance to max gamma strike (normalized)
    
    # Structural features (Walls & Traps)
    "wall_strength",            # Combined strength of top walls (0-1)
    "wall_defense_score",       # How well walls are defended (0-1)
    "trap_probability",         # Probability of trap formation (0-1)
    
    # Divergence features
    "price_oi_divergence",      # Divergence between price and OI
    "price_gamma_divergence",   # Divergence between price and gamma
    "divergence_score",         # Combined divergence score (0-1)
    "has_divergence",           # 1.0 if significant divergence, else 0.0
    
    # Market microstructure
    "max_pain_distance",        # Distance to max pain (normalized)
    "vix_smile",                # Volatility smile curvature
    "skewness",                 # Option skew (put IV - call IV)
    
    # Wyckoff-inspired features
    "spring_detection",         # Bear trap probability (0-1)
    "upthrust_detection",       # Bull trap probability (0-1)
    "accumulation_score",       # Accumulation phase score (0-1)
    
    # Derived composite features
    "gamma_wall_interaction",   # wall_strength * abs(net_gamma)
    "velocity_divergence_composite",  # oi_velocity * divergence_score
    "trap_gamma_composite"      # trap_probability * gamma_regime_negative
]

# ==============================
# COMPLETE FEATURE SET
# ==============================

# All features (base + research)
FEATURE_COLUMNS = BASE_FEATURES + RESEARCH_FEATURES

# Target variable
TARGET_COLUMN = "future_return_5m"

# Metadata columns
METADATA_COLUMNS = [
    "feature_version",
    "timestamp"
]

# Primary keys for database
PRIMARY_KEYS = ["timestamp"]

# ==============================
# FEATURE GROUPS FOR ANALYSIS
# ==============================

FEATURE_GROUPS = {
    "option_structure": [
        "put_call_ratio",
        "oi_delta", 
        "oi_concentration",
        "atm_iv",
        "iv_skew",
        "skewness",
        "vix_smile"
    ],
    
    "price_momentum": [
        "vwap_distance",
        "price_momentum",
        "volume_ratio",
        "ccc_value",
        "ccc_slope"
    ],
    
    "oi_analysis": [
        "oi_velocity",
        "oi_velocity_ma",
        "oi_velocity_std",
        "oi_regime_expansive",
        "oi_regime_constricted"
    ],
    
    "gamma_exposure": [
        "net_gamma",
        "gamma_regime_positive",
        "gamma_regime_negative",
        "gamma_flip_distance",
        "max_gamma_strike_distance"
    ],
    
    "structure_analysis": [
        "wall_strength",
        "wall_defense_score",
        "trap_probability",
        "spring_detection",
        "upthrust_detection",
        "accumulation_score"
    ],
    
    "divergence_analysis": [
        "price_oi_divergence",
        "price_gamma_divergence",
        "divergence_score",
        "has_divergence"
    ],
    
    "composite_features": [
        "gamma_wall_interaction",
        "velocity_divergence_composite",
        "trap_gamma_composite"
    ],
    
    "context_features": [
        "time_to_expiry_minutes",
        "max_pain_distance"
    ]
}

# ==============================
# FEATURE DESCRIPTIONS
# ==============================

FEATURE_DESCRIPTIONS = {
    # Base features
    "put_call_ratio": "Put OI / Call OI ratio. >1.0 = bearish sentiment",
    "oi_delta": "Net OI change (Put OI change - Call OI change)",
    "oi_concentration": "Maximum OI concentration at any single strike",
    "atm_iv": "At-the-money implied volatility",
    "iv_skew": "Call IV - Put IV (positive = call skew, negative = put skew)",
    "vwap_distance": "(LTP - VWAP) / VWAP. Positive = above VWAP (bullish)",
    "price_momentum": "Normalized price momentum over lookback period",
    "volume_ratio": "Current volume / average volume",
    "ccc_value": "Cumulative Constituent Contribution - breadth indicator",
    "ccc_slope": "Slope of CCC over lookback period",
    "time_to_expiry_minutes": "Minutes remaining until option expiry",
    
    # OI Velocity features
    "oi_velocity": "Normalized rate of change of Open Interest (œÉ)",
    "oi_velocity_ma": "Moving average of OI velocity",
    "oi_velocity_std": "Standard deviation of OI velocity",
    "oi_regime_expansive": "1.0 if OI velocity > 1.5œÉ (capital inflow), else 0.0",
    "oi_regime_constricted": "1.0 if OI velocity < -1.5œÉ (capital outflow), else 0.0",
    
    # Gamma Exposure features
    "net_gamma": "Net Gamma Exposure of market makers",
    "gamma_regime_positive": "1.0 if net_gamma > 0 (stabilizing/pinning), else 0.0",
    "gamma_regime_negative": "1.0 if net_gamma < 0 (accelerating), else 0.0",
    "gamma_flip_distance": "Distance to nearest gamma flip level (normalized)",
    "max_gamma_strike_distance": "Distance to strike with maximum gamma impact",
    
    # Structural features
    "wall_strength": "Combined strength of structural walls (0-1 scale)",
    "wall_defense_score": "How strongly walls are being defended (0-1)",
    "trap_probability": "Probability of trap/squeeze formation (0-1)",
    "spring_detection": "Wyckoff spring pattern detection (0-1)",
    "upthrust_detection": "Wyckoff upthrust pattern detection (0-1)",
    "accumulation_score": "Accumulation phase score (0-1)",
    
    # Divergence features
    "price_oi_divergence": "Divergence between price change and OI change",
    "price_gamma_divergence": "Divergence between price change and gamma change",
    "divergence_score": "Combined divergence confidence (0-1)",
    "has_divergence": "1.0 if significant divergence detected, else 0.0",
    
    # Market microstructure
    "max_pain_distance": "Distance to max pain strike (normalized)",
    "vix_smile": "Volatility smile curvature (ATM IV - OTM IV)",
    "skewness": "Option skew (Put IV - Call IV)",
    
    # Composite features
    "gamma_wall_interaction": "Interaction between gamma and wall strength",
    "velocity_divergence_composite": "OI velocity multiplied by divergence score",
    "trap_gamma_composite": "Trap probability weighted by negative gamma regime"
}

# ==============================
# FEATURE IMPORTANCE GUIDELINES
# ==============================

# Expected impact on returns (for initial model weighting)
FEATURE_IMPACT = {
    "high_impact": [
        "oi_velocity",
        "net_gamma",
        "trap_probability",
        "divergence_score",
        "wall_strength"
    ],
    
    "medium_impact": [
        "put_call_ratio",
        "price_momentum",
        "ccc_slope",
        "gamma_regime_negative",
        "has_divergence",
        "spring_detection"
    ],
    
    "low_impact": [
        "oi_concentration",
        "vwap_distance",
        "volume_ratio",
        "max_pain_distance",
        "skewness"
    ],
    
    "contextual": [
        "time_to_expiry_minutes",
        "atm_iv",
        "vix_smile",
        "accumulation_score"
    ]
}

# ==============================
# FEATURE VALIDATION RULES
# ==============================

FEATURE_VALIDATION = {
    "value_ranges": {
        "put_call_ratio": (0, 5),
        "oi_velocity": (-10, 10),
        "net_gamma": (-1e6, 1e6),
        "trap_probability": (0, 1),
        "divergence_score": (0, 1),
        "wall_strength": (0, 1)
    },
    
    "required_features": [
        "timestamp",
        "feature_version",
        "put_call_ratio",
        "vwap_distance",
        "price_momentum",
        "ccc_value"
    ],
    
    "derived_features": [
        "oi_regime_expansive",
        "oi_regime_constricted",
        "gamma_regime_positive",
        "gamma_regime_negative",
        "has_divergence"
    ]
}

# ==============================
# MODEL INPUT CONFIGURATION
# ==============================

# Features for different model types
MODEL_FEATURE_SETS = {
    "full_model": FEATURE_COLUMNS,
    
    "research_model": RESEARCH_FEATURES,
    
    "momentum_model": [
        "price_momentum",
        "volume_ratio",
        "ccc_slope",
        "oi_velocity",
        "net_gamma"
    ],
    
    "divergence_model": [
        "price_oi_divergence",
        "price_gamma_divergence",
        "divergence_score",
        "has_divergence",
        "trap_probability"
    ],
    
    "structure_model": [
        "wall_strength",
        "wall_defense_score",
        "trap_probability",
        "spring_detection",
        "upthrust_detection"
    ],
    
    "quick_model": [
        "put_call_ratio",
        "oi_velocity",
        "net_gamma",
        "trap_probability",
        "divergence_score"
    ]
}

# ==============================
# UTILITY FUNCTIONS
# ==============================

def get_feature_group(feature_name: str) -> str:
    """Get the group name for a feature."""
    for group_name, features in FEATURE_GROUPS.items():
        if feature_name in features:
            return group_name
    return "unknown"

def validate_feature_name(feature_name: str) -> bool:
    """Check if a feature name is valid."""
    return feature_name in FEATURE_COLUMNS or feature_name in METADATA_COLUMNS

def get_feature_description(feature_name: str) -> str:
    """Get description for a feature."""
    return FEATURE_DESCRIPTIONS.get(feature_name, "No description available")

def get_features_by_impact(impact_level: str) -> list:
    """Get features by impact level."""
    return FEATURE_IMPACT.get(impact_level, [])

def get_model_features(model_type: str = "full_model") -> list:
    """Get feature set for specific model type."""
    return MODEL_FEATURE_SETS.get(model_type, FEATURE_COLUMNS)

def print_feature_summary():
    """Print summary of all features."""
    print(f"=== FEATURE CONTRACT v{FEATURE_VERSION} ===")
    print(f"Total features: {len(FEATURE_COLUMNS)}")
    print(f"Base features: {len(BASE_FEATURES)}")
    print(f"Research features: {len(RESEARCH_FEATURES)}")
    print()
    
    print("Feature Groups:")
    for group_name, features in FEATURE_GROUPS.items():
        print(f"  {group_name}: {len(features)} features")
    
    print()
    print("Top Impact Features:")
    for feature in FEATURE_IMPACT["high_impact"][:5]:
        print(f"  ‚Ä¢ {feature}: {FEATURE_DESCRIPTIONS.get(feature, '')}")

# ==============================
# DATABASE SCHEMA
# ==============================

# Schema for market_features table
MARKET_FEATURES_SCHEMA = {
    "table_name": "market_features",
    "columns": {
        "timestamp": "TEXT PRIMARY KEY",
        "feature_version": "TEXT",
        "future_return_5m": "REAL",
        
        # Base features
        "put_call_ratio": "REAL",
        "oi_delta": "REAL",
        "oi_concentration": "REAL",
        "atm_iv": "REAL",
        "iv_skew": "REAL",
        "vwap_distance": "REAL",
        "price_momentum": "REAL",
        "volume_ratio": "REAL",
        "ccc_value": "REAL",
        "ccc_slope": "REAL",
        "time_to_expiry_minutes": "INTEGER",
        
        # Research features (added in v2.0)
        "oi_velocity": "REAL",
        "oi_velocity_ma": "REAL",
        "oi_velocity_std": "REAL",
        "oi_regime_expansive": "REAL",
        "oi_regime_constricted": "REAL",
        "net_gamma": "REAL",
        "gamma_regime_positive": "REAL",
        "gamma_regime_negative": "REAL",
        "gamma_flip_distance": "REAL",
        "max_gamma_strike_distance": "REAL",
        "wall_strength": "REAL",
        "wall_defense_score": "REAL",
        "trap_probability": "REAL",
        "price_oi_divergence": "REAL",
        "price_gamma_divergence": "REAL",
        "divergence_score": "REAL",
        "has_divergence": "REAL",
        "max_pain_distance": "REAL",
        "vix_smile": "REAL",
        "skewness": "REAL",
        "spring_detection": "REAL",
        "upthrust_detection": "REAL",
        "accumulation_score": "REAL",
        "gamma_wall_interaction": "REAL",
        "velocity_divergence_composite": "REAL",
        "trap_gamma_composite": "REAL"
    },
    "indexes": [
        "CREATE INDEX idx_timestamp ON market_features(timestamp)",
        "CREATE INDEX idx_feature_version ON market_features(feature_version)",
        "CREATE INDEX idx_oi_velocity ON market_features(oi_velocity)",
        "CREATE INDEX idx_net_gamma ON market_features(net_gamma)"
    ]
}

# ==============================
# MIGRATION UTILITIES
# ==============================

def get_migration_sql(from_version: str, to_version: str) -> list:
    """
    Get SQL migration statements for feature version updates.
    
    Args:
        from_version: Current feature version
        to_version: Target feature version
    
    Returns:
        List of SQL statements to migrate
    """
    migrations = []
    
    if from_version == "v1.0" and to_version == "v2.0":
        # Add research feature columns
        research_columns = [
            "oi_velocity REAL DEFAULT 0.0",
            "oi_velocity_ma REAL DEFAULT 0.0",
            "oi_velocity_std REAL DEFAULT 0.0",
            "oi_regime_expansive REAL DEFAULT 0.0",
            "oi_regime_constricted REAL DEFAULT 0.0",
            "net_gamma REAL DEFAULT 0.0",
            "gamma_regime_positive REAL DEFAULT 0.0",
            "gamma_regime_negative REAL DEFAULT 0.0",
            "gamma_flip_distance REAL DEFAULT 0.0",
            "max_gamma_strike_distance REAL DEFAULT 0.0",
            "wall_strength REAL DEFAULT 0.0",
            "wall_defense_score REAL DEFAULT 0.0",
            "trap_probability REAL DEFAULT 0.0",
            "price_oi_divergence REAL DEFAULT 0.0",
            "price_gamma_divergence REAL DEFAULT 0.0",
            "divergence_score REAL DEFAULT 0.0",
            "has_divergence REAL DEFAULT 0.0",
            "max_pain_distance REAL DEFAULT 0.0",
            "vix_smile REAL DEFAULT 0.0",
            "skewness REAL DEFAULT 0.0",
            "spring_detection REAL DEFAULT 0.0",
            "upthrust_detection REAL DEFAULT 0.0",
            "accumulation_score REAL DEFAULT 0.0",
            "gamma_wall_interaction REAL DEFAULT 0.0",
            "velocity_divergence_composite REAL DEFAULT 0.0",
            "trap_gamma_composite REAL DEFAULT 0.0"
        ]
        
        for column_def in research_columns:
            column_name = column_def.split()[0]
            migrations.append(f"ALTER TABLE market_features ADD COLUMN {column_def}")
        
        # Add indexes for new features
        migrations.append("CREATE INDEX idx_oi_velocity ON market_features(oi_velocity)")
        migrations.append("CREATE INDEX idx_net_gamma ON market_features(net_gamma)")
        migrations.append("CREATE INDEX idx_trap_probability ON market_features(trap_probability)")
    
    return migrations

# ==============================
# INITIALIZATION
# ==============================

if __name__ == "__main__":
    print_feature_summary()



====================================================================================================
FILE: .\ml\inference.py
====================================================================================================


File Name: inference.py
Full Path: G:\trading_app\ml\inference.py
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .py




====================================================================================================
FILE: .\ml\training.py
====================================================================================================


File Name: training.py
Full Path: G:\trading_app\ml\training.py
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .py




====================================================================================================
FILE: .\risk\position_sizing.py
====================================================================================================


File Name: position_sizing.py
Full Path: G:\trading_app\risk\position_sizing.py
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .py




====================================================================================================
FILE: .\risk\stoploss.py
====================================================================================================


File Name: stoploss.py
Full Path: G:\trading_app\risk\stoploss.py
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .py




====================================================================================================
FILE: .\scripts\fix_database.py
====================================================================================================


File Name: fix_database.py
Full Path: G:\trading_app\scripts\fix_database.py
Size: 4.7 KB
Last Modified: 01/27/2026 20:46:32
Extension: .py

"""
Database Fix Script - Run this to fix migration issues.
"""

import sys
from pathlib import Path
import sqlite3

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))


# Add this function to scripts/fix_database.py or create a new script

def add_details_json_column():
    """Add missing details_json column to system_health table."""
    try:
        with get_connection() as conn:
            # Check if column exists
            cur = conn.execute("PRAGMA table_info(system_health)")
            columns = [row[1] for row in cur.fetchall()]
            
            if "details_json" not in columns:
                print("Adding details_json column to system_health table...")
                conn.execute("""
                    ALTER TABLE system_health 
                    ADD COLUMN details_json TEXT DEFAULT '{}'
                """)
                print("‚úì Added details_json column")
            else:
                print("‚úì details_json column already exists")
    except Exception as e:
        print(f"‚ùå Error adding details_json column: {e}")
        
def fix_database():
    """Fix database migration issues."""
    print("=" * 60)
    print("DATABASE FIX SCRIPT")
    print("=" * 60)
    
    db_path = Path("G:/trading_app/storage/trading.db")
    
    if not db_path.exists():
        print("Database file not found. Creating new database...")
        from storage.repository import initialize_storage
        initialize_storage()
        return
    
    print(f"Fixing database at: {db_path}")
    
    # Connect to database
    conn = sqlite3.connect(str(db_path))
    
    try:
        # Check if market_features table exists
        cur = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='market_features'")
        if not cur.fetchone():
            print("market_features table doesn't exist. Creating...")
            conn.close()
            from storage.repository import initialize_storage
            initialize_storage()
            return
        
        # Get current columns
        cur = conn.execute("PRAGMA table_info(market_features)")
        columns = [row[1] for row in cur.fetchall()]
        print(f"Current columns in market_features: {columns}")
        
        # Research columns that should exist
        research_columns = [
            "oi_velocity", "oi_velocity_ma", "oi_velocity_std",
            "oi_regime_expansive", "oi_regime_constricted",
            "net_gamma", "gamma_regime_positive", "gamma_regime_negative",
            "gamma_flip_distance", "max_gamma_strike_distance",
            "wall_strength", "wall_defense_score", "trap_probability",
            "price_oi_divergence", "price_gamma_divergence", "divergence_score",
            "has_divergence", "max_pain_distance", "vix_smile", "skewness",
            "spring_detection", "upthrust_detection", "accumulation_score",
            "gamma_wall_interaction", "velocity_divergence_composite", "trap_gamma_composite"
        ]
        
        # Add missing columns
        for column in research_columns:
            if column not in columns:
                print(f"Adding column: {column}")
                try:
                    conn.execute(f"ALTER TABLE market_features ADD COLUMN {column} REAL DEFAULT 0.0")
                    print(f"  ‚úì Added {column}")
                except sqlite3.OperationalError as e:
                    if "duplicate column" in str(e):
                        print(f"  ‚ö†Ô∏è Column {column} already exists")
                    else:
                        print(f"  ‚ùå Error adding {column}: {e}")
        
        # Check database_info table
        cur = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='database_info'")
        if not cur.fetchone():
            print("Creating database_info table...")
            conn.execute("""
                CREATE TABLE database_info (
                    key TEXT PRIMARY KEY,
                    value TEXT,
                    updated_at TEXT
                )
            """)
        
        # Set feature version to v2.0
        from datetime import datetime
        conn.execute(
            "INSERT OR REPLACE INTO database_info (key, value, updated_at) VALUES (?, ?, ?)",
            ("feature_version", "v2.0", datetime.utcnow().isoformat())
        )
        
        conn.commit()
        print("‚úì Database fixed successfully!")
        
    except Exception as e:
        print(f"‚ùå Error fixing database: {e}")
        import traceback
        traceback.print_exc()
    finally:
        conn.close()

if __name__ == "__main__":
    fix_database()



====================================================================================================
FILE: .\scripts\initialize_database.py
====================================================================================================


File Name: initialize_database.py
Full Path: G:\trading_app\scripts\initialize_database.py
Size: 1.33 KB
Last Modified: 01/27/2026 19:46:53
Extension: .py

"""
Database Initialization Script.
Run this once to set up the database with research features.
"""

import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from storage.repository import initialize_database, migrate_database
from ml.feature_contract import FEATURE_VERSION

def main():
    """Initialize and migrate database."""
    print("=" * 60)
    print("DATABASE INITIALIZATION SCRIPT")
    print("=" * 60)
    
    try:
        # Initialize database
        print("\n1. Initializing database...")
        initialize_database()
        
        # Migrate to current feature version
        print(f"\n2. Migrating to feature version {FEATURE_VERSION}...")
        success = migrate_database(FEATURE_VERSION)
        
        if success:
            print(f"\n‚úÖ Database initialized successfully!")
            print(f"   Feature Version: {FEATURE_VERSION}")
            print(f"   Database Path: storage/trading.db")
        else:
            print("\n‚ùå Database migration failed!")
            sys.exit(1)
            
    except Exception as e:
        print(f"\n‚ùå Error during initialization: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()



====================================================================================================
FILE: .\storage\repository - Copy.py
====================================================================================================


File Name: repository - Copy.py
Full Path: G:\trading_app\storage\repository - Copy.py
Size: 37.96 KB
Last Modified: 01/27/2026 19:19:21
Extension: .py

"""
Enhanced Storage Repository with Research Features Support.
Handles database operations for market features, signals, and analytics.
Includes migration system for feature version updates.
"""

import sqlite3
import pandas as pd
from typing import Dict, Optional, List, Any, Tuple
from pathlib import Path
from datetime import datetime, timedelta
import json
import numpy as np
from contextlib import contextmanager

# Import feature contract for schema
from ml.feature_contract import (
    FEATURE_VERSION, 
    FEATURE_COLUMNS, 
    METADATA_COLUMNS,
    MARKET_FEATURES_SCHEMA,
    get_migration_sql
)

# =========================
# DATABASE CONFIG
# =========================

DB_PATH = Path("G:/trading_app/storage/trading.db")

# Ensure directory exists
DB_PATH.parent.mkdir(parents=True, exist_ok=True)

# =========================
# CONNECTION MANAGEMENT
# =========================

@contextmanager
def get_connection() -> sqlite3.Connection:
    """
    Context manager for database connections with automatic cleanup.
    """
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    
    # Enable WAL mode for better concurrency
    conn.execute("PRAGMA journal_mode=WAL")
    conn.execute("PRAGMA synchronous=NORMAL")
    conn.execute("PRAGMA foreign_keys=ON")
    
    try:
        yield conn
        conn.commit()
    except Exception as e:
        conn.rollback()
        raise e
    finally:
        conn.close()

def initialize_database():
    """
    Initialize database with all required tables and indexes.
    """
    with get_connection() as conn:
        # Create market_features table
        columns = MARKET_FEATURES_SCHEMA["columns"]
        column_defs = [f"{name} {dtype}" for name, dtype in columns.items()]
        
        create_table_sql = f"""
            CREATE TABLE IF NOT EXISTS market_features (
                {', '.join(column_defs)}
            )
        """
        
        conn.execute(create_table_sql)
        
        # Create indexes
        for index_sql in MARKET_FEATURES_SCHEMA.get("indexes", []):
            try:
                conn.execute(index_sql)
            except sqlite3.OperationalError as e:
                if "already exists" not in str(e):
                    raise
        
        # Create signals table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS signals (
                signal_id TEXT PRIMARY KEY,
                timestamp TEXT NOT NULL,
                feature_version TEXT NOT NULL,
                model_version TEXT NOT NULL,
                signal_type TEXT NOT NULL,
                confidence REAL,
                market_state TEXT,
                rationale TEXT,
                expiry_time TEXT,
                status TEXT,
                created_at TEXT,
                research_context TEXT,
                analytics_summary TEXT,
                pnl REAL DEFAULT NULL,
                exit_time TEXT,
                exit_price REAL,
                stop_loss_hit INTEGER DEFAULT 0,
                take_profit_hit INTEGER DEFAULT 0,
                trade_duration_seconds INTEGER
            )
        """)
        
        # Create signals indexes
        conn.execute("CREATE INDEX IF NOT EXISTS idx_signals_timestamp ON signals(timestamp)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_signals_status ON signals(status)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_signals_type ON signals(signal_type)")
        
        # Create model_registry table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS model_registry (
                model_version TEXT PRIMARY KEY,
                feature_version TEXT NOT NULL,
                algorithm TEXT NOT NULL,
                trained_on_start TEXT,
                trained_on_end TEXT,
                metrics_json TEXT,
                feature_importance_json TEXT,
                created_at TEXT,
                is_active INTEGER DEFAULT 0
            )
        """)
        
        # Create system_health table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS system_health (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                component TEXT NOT NULL,
                status TEXT NOT NULL,
                message TEXT,
                details_json TEXT
            )
        """)
        
        # Create research_analytics table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS research_analytics (
                timestamp TEXT PRIMARY KEY,
                oi_velocity REAL,
                oi_regime TEXT,
                net_gamma REAL,
                gamma_regime TEXT,
                wall_strength REAL,
                trap_probability REAL,
                divergence_score REAL,
                market_regime TEXT,
                confidence REAL,
                insights_json TEXT,
                walls_json TEXT,
                traps_json TEXT,
                created_at TEXT
            )
        """)
        
        # Create analytics indexes
        conn.execute("CREATE INDEX IF NOT EXISTS idx_analytics_timestamp ON research_analytics(timestamp)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_analytics_regime ON research_analytics(market_regime)")
        
        # Create feature_history table for tracking feature changes
        conn.execute("""
            CREATE TABLE IF NOT EXISTS feature_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                feature_name TEXT NOT NULL,
                old_value REAL,
                new_value REAL,
                change_pct REAL,
                created_at TEXT
            )
        """)
        
        # Create database_info table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS database_info (
                key TEXT PRIMARY KEY,
                value TEXT,
                updated_at TEXT
            )
        """)
        
        # Initialize database info
        conn.execute(
            "INSERT OR REPLACE INTO database_info (key, value, updated_at) VALUES (?, ?, ?)",
            ("feature_version", FEATURE_VERSION, datetime.utcnow().isoformat())
        )
        
        # Log initialization
        log_system_health(
            "database",
            "INITIALIZED",
            f"Database initialized with feature version {FEATURE_VERSION}"
        )
        
        print(f"‚úì Database initialized at {DB_PATH}")
        print(f"‚úì Feature version: {FEATURE_VERSION}")

# =========================
# DATABASE MIGRATION
# =========================

def migrate_database(target_version: str = FEATURE_VERSION) -> bool:
    """
    Migrate database to target feature version.
    
    Args:
        target_version: Target feature version
    
    Returns:
        True if migration successful
    """
    try:
        with get_connection() as conn:
            # Get current version
            cur = conn.execute("SELECT value FROM database_info WHERE key = 'feature_version'")
            row = cur.fetchone()
            current_version = row["value"] if row else "v1.0"
            
            if current_version == target_version:
                print(f"Database already at version {target_version}")
                return True
            
            print(f"Migrating database from {current_version} to {target_version}")
            
            # Get migration SQL
            migrations = get_migration_sql(current_version, target_version)
            
            if not migrations:
                print("No migration required")
                return True
            
            # Execute migrations
            for migration_sql in migrations:
                try:
                    conn.execute(migration_sql)
                    print(f"‚úì Executed: {migration_sql[:50]}...")
                except sqlite3.OperationalError as e:
                    if "already exists" in str(e) or "duplicate column" in str(e):
                        print(f"‚ö†Ô∏è Skipping (already exists): {migration_sql[:50]}...")
                    else:
                        raise
            
            # Update version
            conn.execute(
                "UPDATE database_info SET value = ?, updated_at = ? WHERE key = 'feature_version'",
                (target_version, datetime.utcnow().isoformat())
            )
            
            # Log migration
            log_system_health(
                "database",
                "MIGRATED",
                f"Migrated from {current_version} to {target_version}",
                json.dumps({"migrations_applied": len(migrations)})
            )
            
            print(f"‚úì Database migrated to version {target_version}")
            return True
            
    except Exception as e:
        print(f"‚ùå Migration failed: {e}")
        log_system_health(
            "database",
            "MIGRATION_FAILED",
            f"Failed to migrate: {str(e)}"
        )
        return False

# =========================
# MARKET FEATURES OPERATIONS
# =========================

def insert_market_features(df: pd.DataFrame) -> None:
    """
    Insert feature rows into market_features table.
    
    Args:
        df: DataFrame with feature columns matching MARKET_FEATURES_SCHEMA
    """
    if df.empty:
        return
    
    # Ensure feature_version is set
    if "feature_version" not in df.columns:
        df["feature_version"] = FEATURE_VERSION
    
    # Ensure timestamp is string
    df["timestamp"] = df["timestamp"].astype(str)
    
    # Ensure all required columns exist
    required_columns = list(MARKET_FEATURES_SCHEMA["columns"].keys())
    for col in required_columns:
        if col not in df.columns:
            df[col] = None  # Add missing columns with NULL
    
    # Select only columns that exist in schema
    df_to_insert = df[required_columns].copy()
    
    # Convert NaN to None for SQL
    df_to_insert = df_to_insert.replace({np.nan: None})
    
    try:
        with get_connection() as conn:
            # Insert data
            df_to_insert.to_sql(
                name="market_features",
                con=conn,
                if_exists="append",
                index=False
            )
            
            # Log successful insert
            log_system_health(
                "features",
                "INSERTED",
                f"Inserted {len(df)} feature rows",
                json.dumps({
                    "timestamp": df["timestamp"].iloc[0],
                    "feature_version": df["feature_version"].iloc[0],
                    "row_count": len(df)
                })
            )
            
            # Track feature changes for important features
            track_feature_changes(df_to_insert)
            
    except Exception as e:
        print(f"‚ùå Error inserting features: {e}")
        log_system_health(
            "features",
            "INSERT_FAILED",
            f"Failed to insert features: {str(e)}",
            json.dumps({"error": str(e)})
        )
        raise

def fetch_latest_features(feature_version: str = FEATURE_VERSION) -> Optional[pd.DataFrame]:
    """
    Fetch the most recent feature row for inference.
    
    Args:
        feature_version: Feature version to fetch
    
    Returns:
        DataFrame with latest features or None
    """
    query = """
        SELECT *
        FROM market_features
        WHERE feature_version = ?
        ORDER BY timestamp DESC
        LIMIT 1
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(feature_version,))
            
            if not df.empty:
                log_system_health(
                    "features",
                    "FETCHED",
                    f"Fetched latest features for version {feature_version}"
                )
            
            return df if not df.empty else None
            
    except Exception as e:
        print(f"‚ùå Error fetching features: {e}")
        return None

def fetch_features_by_time_range(
    start_time: str,
    end_time: str,
    feature_version: str = FEATURE_VERSION
) -> pd.DataFrame:
    """
    Fetch features within a time range.
    
    Args:
        start_time: Start timestamp (inclusive)
        end_time: End timestamp (inclusive)
        feature_version: Feature version filter
    
    Returns:
        DataFrame with features in range
    """
    query = """
        SELECT *
        FROM market_features
        WHERE feature_version = ?
          AND timestamp BETWEEN ? AND ?
        ORDER BY timestamp ASC
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(
                query,
                conn,
                params=(feature_version, start_time, end_time)
            )
            
            log_system_health(
                "features",
                "FETCHED_RANGE",
                f"Fetched {len(df)} features from {start_time} to {end_time}"
            )
            
            return df
            
    except Exception as e:
        print(f"‚ùå Error fetching features by range: {e}")
        return pd.DataFrame()

def fetch_training_data(
    feature_version: str = FEATURE_VERSION,
    start_ts: Optional[str] = None,
    end_ts: Optional[str] = None,
    min_confidence: float = 0.0
) -> pd.DataFrame:
    """
    Fetch labeled data for ML training with research features.
    
    Args:
        feature_version: Feature version
        start_ts: Start timestamp
        end_ts: End timestamp
        min_confidence: Minimum signal confidence for training
    
    Returns:
        DataFrame with features and labels
    """
    # Build WHERE clause
    conditions = ["feature_version = ?", "future_return_5m IS NOT NULL"]
    params = [feature_version]
    
    if start_ts:
        conditions.append("timestamp >= ?")
        params.append(start_ts)
    
    if end_ts:
        conditions.append("timestamp <= ?")
        params.append(end_ts)
    
    where_clause = " AND ".join(conditions)
    
    query = f"""
        SELECT mf.*, s.confidence as signal_confidence
        FROM market_features mf
        LEFT JOIN signals s ON mf.timestamp = s.timestamp AND s.status = 'VALIDATED'
        WHERE {where_clause}
        ORDER BY mf.timestamp ASC
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=params)
            
            # Filter by confidence if needed
            if min_confidence > 0 and 'signal_confidence' in df.columns:
                df = df[df['signal_confidence'] >= min_confidence]
            
            log_system_health(
                "training",
                "FETCHED",
                f"Fetched {len(df)} training samples"
            )
            
            return df
            
    except Exception as e:
        print(f"‚ùå Error fetching training data: {e}")
        return pd.DataFrame()

# =========================
# RESEARCH ANALYTICS OPERATIONS
# =========================

def insert_research_analytics(analytics: Dict) -> None:
    """
    Insert research analytics for tracking and visualization.
    
    Args:
        analytics: Dictionary with research analytics data
    """
    if not analytics:
        return
    
    # Prepare data
    timestamp = analytics.get("timestamp", datetime.utcnow().isoformat())
    
    data = {
        "timestamp": timestamp,
        "oi_velocity": analytics.get("oi_velocity"),
        "oi_regime": analytics.get("oi_regime"),
        "net_gamma": analytics.get("gamma_exposure", {}).get("net_gamma"),
        "gamma_regime": analytics.get("gamma_exposure", {}).get("regime"),
        "wall_strength": analytics.get("structural_walls", [{}])[0].get("concentration", 0) if analytics.get("structural_walls") else 0,
        "trap_probability": analytics.get("potential_traps", [{}])[0].get("confidence", 0) if analytics.get("potential_traps") else 0,
        "divergence_score": analytics.get("spot_divergence", {}).get("confidence", 0),
        "market_regime": analytics.get("market_regime"),
        "confidence": analytics.get("regime_confidence"),
        "insights_json": json.dumps(analytics.get("market_insights", [])),
        "walls_json": json.dumps(analytics.get("structural_walls", [])),
        "traps_json": json.dumps(analytics.get("potential_traps", [])),
        "created_at": datetime.utcnow().isoformat()
    }
    
    # Clean NaN values
    for key, value in data.items():
        if isinstance(value, float) and np.isnan(value):
            data[key] = None
    
    try:
        with get_connection() as conn:
            columns = ", ".join(data.keys())
            placeholders = ", ".join(["?"] * len(data))
            values = list(data.values())
            
            conn.execute(
                f"INSERT OR REPLACE INTO research_analytics ({columns}) VALUES ({placeholders})",
                values
            )
            
    except Exception as e:
        print(f"‚ùå Error inserting research analytics: {e}")

def fetch_recent_analytics(limit: int = 100) -> pd.DataFrame:
    """
    Fetch recent research analytics for visualization.
    
    Args:
        limit: Number of rows to fetch
    
    Returns:
        DataFrame with analytics
    """
    query = """
        SELECT *
        FROM research_analytics
        ORDER BY timestamp DESC
        LIMIT ?
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(limit,))
            return df
    except Exception as e:
        print(f"‚ùå Error fetching analytics: {e}")
        return pd.DataFrame()

# =========================
# SIGNAL STORE OPERATIONS (ENHANCED)
# =========================

def insert_signal(signal: Dict) -> None:
    """
    Insert a trading signal with research context.
    
    Args:
        signal: Signal dictionary with research context
    """
    if not signal:
        return
    
    # Add research context if available
    research_context = signal.get("research_context", {})
    if research_context and not isinstance(research_context, str):
        signal["research_context"] = json.dumps(research_context)
    
    # Add analytics summary
    analytics_summary = signal.get("analytics_summary", {})
    if analytics_summary and not isinstance(analytics_summary, str):
        signal["analytics_summary"] = json.dumps(analytics_summary)
    
    # Ensure all required fields
    required_fields = [
        "signal_id", "timestamp", "feature_version", "model_version",
        "signal_type", "confidence", "status", "created_at"
    ]
    
    for field in required_fields:
        if field not in signal:
            print(f"‚ö†Ô∏è Missing required field in signal: {field}")
            signal[field] = ""
    
    try:
        with get_connection() as conn:
            columns = ", ".join(signal.keys())
            placeholders = ", ".join(["?"] * len(signal))
            values = list(signal.values())
            
            conn.execute(
                f"INSERT INTO signals ({columns}) VALUES ({placeholders})",
                values
            )
            
            log_system_health(
                "signals",
                "INSERTED",
                f"Signal {signal['signal_id'][:8]} inserted: {signal['signal_type']} (conf: {signal['confidence']})",
                json.dumps({
                    "signal_id": signal["signal_id"],
                    "type": signal["signal_type"],
                    "confidence": signal["confidence"]
                })
            )
            
    except Exception as e:
        print(f"‚ùå Error inserting signal: {e}")
        log_system_health(
            "signals",
            "INSERT_FAILED",
            f"Failed to insert signal: {str(e)}"
        )
        raise

def signal_exists(feature_timestamp: str) -> bool:
    """
    Check if a signal already exists for given timestamp.
    
    Args:
        feature_timestamp: Feature timestamp
    
    Returns:
        True if signal exists
    """
    query = """
        SELECT 1 FROM signals WHERE timestamp = ? LIMIT 1
    """
    
    try:
        with get_connection() as conn:
            cur = conn.execute(query, (feature_timestamp,))
            return cur.fetchone() is not None
    except Exception as e:
        print(f"‚ùå Error checking signal existence: {e}")
        return False

def validate_new_signals(confidence_threshold: float = 0.2):
    """
    Promote NEW signals to VALIDATED if confidence is sufficient.
    
    Args:
        confidence_threshold: Minimum confidence for validation
    """
    query = """
        UPDATE signals
        SET status = 'VALIDATED'
        WHERE status = 'NEW'
          AND confidence >= ?
    """
    
    try:
        with get_connection() as conn:
            result = conn.execute(query, (confidence_threshold,))
            updated_count = result.rowcount
            
            if updated_count > 0:
                log_system_health(
                    "signals",
                    "VALIDATED",
                    f"Validated {updated_count} signals (conf >= {confidence_threshold})"
                )
                
    except Exception as e:
        print(f"‚ùå Error validating signals: {e}")
        log_system_health(
            "signals",
            "VALIDATION_FAILED",
            f"Signal validation failed: {str(e)}"
        )

def expire_old_signals():
    """
    Expire VALIDATED signals whose expiry_time has passed.
    """
    query = """
        UPDATE signals
        SET status = 'EXPIRED'
        WHERE status = 'VALIDATED'
          AND expiry_time < ?
    """
    
    now = datetime.utcnow().isoformat()
    
    try:
        with get_connection() as conn:
            result = conn.execute(query, (now,))
            expired_count = result.rowcount
            
            if expired_count > 0:
                log_system_health(
                    "signals",
                    "EXPIRED",
                    f"Expired {expired_count} signals"
                )
                
    except Exception as e:
        print(f"‚ùå Error expiring signals: {e}")

def update_signal_status(signal_id: str, new_status: str, 
                        pnl: Optional[float] = None,
                        exit_time: Optional[str] = None,
                        exit_price: Optional[float] = None) -> None:
    """
    Update signal lifecycle status with trade results.
    
    Args:
        signal_id: Signal ID
        new_status: New status
        pnl: Profit/Loss if applicable
        exit_time: Exit timestamp
        exit_price: Exit price
    """
    # Build update query
    updates = ["status = ?"]
    params = [new_status]
    
    if pnl is not None:
        updates.append("pnl = ?")
        params.append(pnl)
    
    if exit_time:
        updates.append("exit_time = ?")
        params.append(exit_time)
    
    if exit_price is not None:
        updates.append("exit_price = ?")
        params.append(exit_price)
    
    # Calculate trade duration if exit_time is provided
    if exit_time:
        updates.append("""
            trade_duration_seconds = (
                CAST((julianday(?) - julianday(created_at)) * 86400 AS INTEGER)
            )
        """)
        params.append(exit_time)
    
    query = f"""
        UPDATE signals
        SET {', '.join(updates)}
        WHERE signal_id = ?
    """
    
    params.append(signal_id)
    
    try:
        with get_connection() as conn:
            conn.execute(query, params)
            
            log_system_health(
                "signals",
                "UPDATED",
                f"Signal {signal_id[:8]} updated to {new_status}"
            )
            
    except Exception as e:
        print(f"‚ùå Error updating signal: {e}")
        log_system_health(
            "signals",
            "UPDATE_FAILED",
            f"Failed to update signal {signal_id}: {str(e)}"
        )

def fetch_active_signals(limit: int = 10) -> pd.DataFrame:
    """
    Fetch active signals for display.
    
    Args:
        limit: Maximum number of signals to fetch
    
    Returns:
        DataFrame with active signals
    """
    query = """
        SELECT *
        FROM signals
        WHERE status IN ('NEW','VALIDATED')
        ORDER BY created_at DESC
        LIMIT ?
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(limit,))
            
            # Parse JSON fields
            for col in ["research_context", "analytics_summary"]:
                if col in df.columns:
                    df[col] = df[col].apply(
                        lambda x: json.loads(x) if x and isinstance(x, str) else {}
                    )
            
            return df
            
    except Exception as e:
        print(f"‚ùå Error fetching active signals: {e}")
        return pd.DataFrame()

def fetch_signal_performance(days: int = 30) -> Dict:
    """
    Fetch signal performance metrics.
    
    Args:
        days: Number of days to analyze
    
    Returns:
        Dictionary with performance metrics
    """
    cutoff_date = (datetime.utcnow() - timedelta(days=days)).isoformat()
    
    query = """
        SELECT 
            signal_type,
            status,
            COUNT(*) as count,
            AVG(confidence) as avg_confidence,
            AVG(pnl) as avg_pnl,
            SUM(CASE WHEN pnl > 0 THEN 1 ELSE 0 END) as winners,
            SUM(CASE WHEN pnl <= 0 THEN 1 ELSE 0 END) as losers
        FROM signals
        WHERE created_at >= ?
        GROUP BY signal_type, status
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(cutoff_date,))
            
            if df.empty:
                return {}
            
            # Calculate overall metrics
            total_signals = df['count'].sum()
            profitable_signals = df[df['signal_type'] == 'BUY']['winners'].sum() + \
                                df[df['signal_type'] == 'SELL']['winners'].sum()
            
            performance = {
                "total_signals": int(total_signals),
                "profitable_signals": int(profitable_signals),
                "win_rate": round(profitable_signals / total_signals * 100, 1) if total_signals > 0 else 0,
                "breakdown": df.to_dict('records'),
                "period_days": days
            }
            
            return performance
            
    except Exception as e:
        print(f"‚ùå Error fetching performance: {e}")
        return {}

# =========================
# MODEL REGISTRY OPERATIONS
# =========================

def register_model(model_info: Dict) -> None:
    """
    Register trained ML model metadata.
    
    Args:
        model_info: Model information dictionary
    """
    required_keys = [
        "model_version", "feature_version", "algorithm",
        "trained_on_start", "trained_on_end", "metrics_json"
    ]
    
    for key in required_keys:
        if key not in model_info:
            raise ValueError(f"Missing required key: {key}")
    
    # Set created_at if not provided
    if "created_at" not in model_info:
        model_info["created_at"] = datetime.utcnow().isoformat()
    
    try:
        with get_connection() as conn:
            # Deactivate other models for this feature version
            conn.execute("""
                UPDATE model_registry
                SET is_active = 0
                WHERE feature_version = ?
            """, (model_info["feature_version"],))
            
            # Insert new model as active
            columns = ", ".join(model_info.keys())
            placeholders = ", ".join(["?"] * len(model_info))
            values = list(model_info.values())
            
            conn.execute(
                f"INSERT INTO model_registry ({columns}) VALUES ({placeholders})",
                values
            )
            
            log_system_health(
                "models",
                "REGISTERED",
                f"Model {model_info['model_version']} registered"
            )
            
    except Exception as e:
        print(f"‚ùå Error registering model: {e}")
        raise

def fetch_active_model(feature_version: str = FEATURE_VERSION) -> Optional[Dict]:
    """
    Fetch the active model for a given feature version.
    
    Args:
        feature_version: Feature version
    
    Returns:
        Model info dictionary or None
    """
    query = """
        SELECT *
        FROM model_registry
        WHERE feature_version = ?
          AND is_active = 1
        ORDER BY created_at DESC
        LIMIT 1
    """
    
    try:
        with get_connection() as conn:
            row = conn.execute(query, (feature_version,)).fetchone()
            return dict(row) if row else None
    except Exception as e:
        print(f"‚ùå Error fetching active model: {e}")
        return None

# =========================
# SYSTEM HEALTH LOGGING
# =========================

def log_system_health(
    component: str,
    status: str,
    message: Optional[str] = None,
    details_json: Optional[str] = None
) -> None:
    """
    Log system health status.
    
    Args:
        component: System component
        status: Status (OK, ERROR, WARNING, etc.)
        message: Optional message
        details_json: Optional JSON details
    """
    query = """
        INSERT INTO system_health (timestamp, component, status, message, details_json)
        VALUES (?, ?, ?, ?, ?)
    """
    
    params = (
        datetime.utcnow().isoformat(),
        component,
        status,
        message,
        details_json
    )
    
    try:
        with get_connection() as conn:
            conn.execute(query, params)
    except Exception as e:
        print(f"‚ùå Error logging system health: {e}")

def fetch_recent_health_logs(limit: int = 50) -> pd.DataFrame:
    """
    Fetch recent system health logs.
    
    Args:
        limit: Number of logs to fetch
    
    Returns:
        DataFrame with health logs
    """
    query = """
        SELECT *
        FROM system_health
        ORDER BY timestamp DESC
        LIMIT ?
    """
    
    try:
        with get_connection() as conn:
            return pd.read_sql_query(query, conn, params=(limit,))
    except Exception as e:
        print(f"‚ùå Error fetching health logs: {e}")
        return pd.DataFrame()

# =========================
# FEATURE HISTORY TRACKING
# =========================

def track_feature_changes(feature_df: pd.DataFrame) -> None:
    """
    Track significant feature changes for monitoring.
    
    Args:
        feature_df: DataFrame with features
    """
    if feature_df.empty or len(feature_df) < 2:
        return
    
    # Only track important features
    important_features = [
        "oi_velocity", "net_gamma", "trap_probability",
        "divergence_score", "wall_strength", "put_call_ratio"
    ]
    
    current = feature_df.iloc[-1]
    previous = feature_df.iloc[-2] if len(feature_df) > 1 else current
    
    changes = []
    for feature in important_features:
        if feature in current and feature in previous:
            old_val = previous[feature]
            new_val = current[feature]
            
            if old_val is not None and new_val is not None and old_val != 0:
                change_pct = ((new_val - old_val) / abs(old_val)) * 100
                
                # Only track significant changes (>10%)
                if abs(change_pct) > 10:
                    changes.append({
                        "timestamp": current["timestamp"],
                        "feature_name": feature,
                        "old_value": old_val,
                        "new_value": new_val,
                        "change_pct": change_pct,
                        "created_at": datetime.utcnow().isoformat()
                    })
    
    if changes:
        try:
            with get_connection() as conn:
                for change in changes:
                    conn.execute("""
                        INSERT INTO feature_history 
                        (timestamp, feature_name, old_value, new_value, change_pct, created_at)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (
                        change["timestamp"], change["feature_name"],
                        change["old_value"], change["new_value"],
                        change["change_pct"], change["created_at"]
                    ))
        except Exception as e:
            print(f"‚ùå Error tracking feature changes: {e}")

# =========================
# DATABASE MAINTENANCE
# =========================

def cleanup_old_data(days_to_keep: int = 30) -> Dict:
    """
    Clean up old data from database.
    
    Args:
        days_to_keep: Number of days of data to keep
    
    Returns:
        Dictionary with cleanup statistics
    """
    cutoff_date = (datetime.utcnow() - timedelta(days=days_to_keep)).isoformat()
    
    cleanup_stats = {}
    
    try:
        with get_connection() as conn:
            # Clean old market features
            result = conn.execute(
                "DELETE FROM market_features WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["market_features"] = result.rowcount
            
            # Clean old signals
            result = conn.execute(
                "DELETE FROM signals WHERE created_at < ? AND status = 'EXPIRED'",
                (cutoff_date,)
            )
            cleanup_stats["signals"] = result.rowcount
            
            # Clean old system health logs
            result = conn.execute(
                "DELETE FROM system_health WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["system_health"] = result.rowcount
            
            # Clean old research analytics
            result = conn.execute(
                "DELETE FROM research_analytics WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["research_analytics"] = result.rowcount
            
            # Clean old feature history
            result = conn.execute(
                "DELETE FROM feature_history WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["feature_history"] = result.rowcount
            
            # Vacuum database
            conn.execute("VACUUM")
            
            log_system_health(
                "database",
                "CLEANED",
                f"Cleaned up {sum(cleanup_stats.values())} old records",
                json.dumps(cleanup_stats)
            )
            
            return cleanup_stats
            
    except Exception as e:
        print(f"‚ùå Error during cleanup: {e}")
        log_system_health(
            "database",
            "CLEANUP_FAILED",
            f"Cleanup failed: {str(e)}"
        )
        return {}

# =========================
# DATABASE INFO & STATS
# =========================

def get_database_stats() -> Dict:
    """
    Get database statistics.
    
    Returns:
        Dictionary with database statistics
    """
    stats = {}
    
    try:
        with get_connection() as conn:
            # Table counts
            tables = [
                "market_features", "signals", "model_registry",
                "system_health", "research_analytics", "feature_history"
            ]
            
            for table in tables:
                cur = conn.execute(f"SELECT COUNT(*) as count FROM {table}")
                row = cur.fetchone()
                stats[f"{table}_count"] = row["count"] if row else 0
            
            # Latest feature timestamp
            cur = conn.execute("SELECT MAX(timestamp) as latest FROM market_features")
            row = cur.fetchone()
            stats["latest_feature"] = row["latest"] if row else None
            
            # Feature version
            cur = conn.execute("SELECT value FROM database_info WHERE key = 'feature_version'")
            row = cur.fetchone()
            stats["feature_version"] = row["value"] if row else "unknown"
            
            # Active signals
            cur = conn.execute("""
                SELECT COUNT(*) as count 
                FROM signals 
                WHERE status IN ('NEW','VALIDATED')
            """)
            row = cur.fetchone()
            stats["active_signals"] = row["count"] if row else 0
            
            # Database size
            db_size = DB_PATH.stat().st_size if DB_PATH.exists() else 0
            stats["database_size_mb"] = round(db_size / (1024 * 1024), 2)
            
            return stats
            
    except Exception as e:
        print(f"‚ùå Error getting database stats: {e}")
        return {}

# =========================
# INITIALIZATION
# =========================

def initialize_storage():
    """
    Initialize storage system.
    """
    print("Initializing storage system...")
    initialize_database()
    
    # Check and run migrations if needed
    current_version = get_database_stats().get("feature_version", "v1.0")
    if current_version != FEATURE_VERSION:
        print(f"Migrating from {current_version} to {FEATURE_VERSION}")
        migrate_database(FEATURE_VERSION)
    
    print("‚úì Storage system initialized")

# Run initialization when module is imported
initialize_storage()



====================================================================================================
FILE: .\storage\repository.py
====================================================================================================


File Name: repository.py
Full Path: G:\trading_app\storage\repository.py
Size: 40.25 KB
Last Modified: 01/27/2026 20:06:38
Extension: .py

"""
Enhanced Storage Repository with Research Features Support - FIXED VERSION
Handles database operations for market features, signals, and analytics.
Includes migration system for feature version updates.
"""

import sqlite3
import pandas as pd
from typing import Dict, Optional, List, Any, Tuple
from pathlib import Path
from datetime import datetime, timedelta
import json
import numpy as np
from contextlib import contextmanager

# Import feature contract for schema
from ml.feature_contract import (
    FEATURE_VERSION, 
    FEATURE_COLUMNS, 
    METADATA_COLUMNS,
    MARKET_FEATURES_SCHEMA,
    get_migration_sql
)

# =========================
# DATABASE CONFIG
# =========================

DB_PATH = Path("G:/trading_app/storage/trading.db")

# Ensure directory exists
DB_PATH.parent.mkdir(parents=True, exist_ok=True)

# =========================
# CONNECTION MANAGEMENT
# =========================

@contextmanager
def get_connection() -> sqlite3.Connection:
    """
    Context manager for database connections with automatic cleanup.
    """
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    
    # Enable WAL mode for better concurrency
    conn.execute("PRAGMA journal_mode=WAL")
    conn.execute("PRAGMA synchronous=NORMAL")
    conn.execute("PRAGMA foreign_keys=ON")
    
    try:
        yield conn
        conn.commit()
    except Exception as e:
        conn.rollback()
        raise e
    finally:
        conn.close()

def initialize_database():
    """
    Initialize database with all required tables and indexes.
    """
    with get_connection() as conn:
        # Check if market_features table exists and get its columns
        cur = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='market_features'")
        table_exists = cur.fetchone() is not None
        
        if table_exists:
            # Get existing columns
            cur = conn.execute("PRAGMA table_info(market_features)")
            existing_columns = {row[1] for row in cur.fetchall()}
            print(f"Existing columns in market_features: {existing_columns}")
        else:
            existing_columns = set()
        
        # Get expected columns from schema
        expected_columns = set(MARKET_FEATURES_SCHEMA["columns"].keys())
        print(f"Expected columns in market_features: {expected_columns}")
        
        # Create table if it doesn't exist
        if not table_exists:
            print("Creating market_features table from scratch...")
            columns = MARKET_FEATURES_SCHEMA["columns"]
            column_defs = [f"{name} {dtype}" for name, dtype in columns.items()]
            
            create_table_sql = f"""
                CREATE TABLE IF NOT EXISTS market_features (
                    {', '.join(column_defs)}
                )
            """
            
            conn.execute(create_table_sql)
            print("market_features table created successfully")
        
        # Check for missing columns and add them
        missing_columns = expected_columns - existing_columns
        if missing_columns:
            print(f"Adding missing columns: {missing_columns}")
            
            for column in missing_columns:
                dtype = MARKET_FEATURES_SCHEMA["columns"][column]
                try:
                    conn.execute(f"ALTER TABLE market_features ADD COLUMN {column} {dtype}")
                    print(f"  ‚úì Added column: {column}")
                except sqlite3.OperationalError as e:
                    if "duplicate column" in str(e):
                        print(f"  ‚ö†Ô∏è Column {column} already exists")
                    else:
                        raise
        
        # Create other tables
        print("Creating other tables...")
        
        # Create signals table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS signals (
                signal_id TEXT PRIMARY KEY,
                timestamp TEXT NOT NULL,
                feature_version TEXT NOT NULL,
                model_version TEXT NOT NULL,
                signal_type TEXT NOT NULL,
                confidence REAL,
                market_state TEXT,
                rationale TEXT,
                expiry_time TEXT,
                status TEXT,
                created_at TEXT,
                research_context TEXT,
                analytics_summary TEXT,
                pnl REAL DEFAULT NULL,
                exit_time TEXT,
                exit_price REAL,
                stop_loss_hit INTEGER DEFAULT 0,
                take_profit_hit INTEGER DEFAULT 0,
                trade_duration_seconds INTEGER
            )
        """)
        
        # Create model_registry table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS model_registry (
                model_version TEXT PRIMARY KEY,
                feature_version TEXT NOT NULL,
                algorithm TEXT NOT NULL,
                trained_on_start TEXT,
                trained_on_end TEXT,
                metrics_json TEXT,
                feature_importance_json TEXT,
                created_at TEXT,
                is_active INTEGER DEFAULT 0
            )
        """)
        
        # Create system_health table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS system_health (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                component TEXT NOT NULL,
                status TEXT NOT NULL,
                message TEXT,
                details_json TEXT
            )
        """)
        
        # Create research_analytics table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS research_analytics (
                timestamp TEXT PRIMARY KEY,
                oi_velocity REAL,
                oi_regime TEXT,
                net_gamma REAL,
                gamma_regime TEXT,
                wall_strength REAL,
                trap_probability REAL,
                divergence_score REAL,
                market_regime TEXT,
                confidence REAL,
                insights_json TEXT,
                walls_json TEXT,
                traps_json TEXT,
                created_at TEXT
            )
        """)
        
        # Create feature_history table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS feature_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                feature_name TEXT NOT NULL,
                old_value REAL,
                new_value REAL,
                change_pct REAL,
                created_at TEXT
            )
        """)
        
        # Create database_info table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS database_info (
                key TEXT PRIMARY KEY,
                value TEXT,
                updated_at TEXT
            )
        """)
        
        # Create indexes (skip if they already exist)
        print("Creating indexes...")
        index_sqls = [
            # market_features indexes
            "CREATE INDEX IF NOT EXISTS idx_timestamp ON market_features(timestamp)",
            "CREATE INDEX IF NOT EXISTS idx_feature_version ON market_features(feature_version)",
            "CREATE INDEX IF NOT EXISTS idx_oi_velocity ON market_features(oi_velocity)",
            "CREATE INDEX IF NOT EXISTS idx_net_gamma ON market_features(net_gamma)",
            
            # signals indexes
            "CREATE INDEX IF NOT EXISTS idx_signals_timestamp ON signals(timestamp)",
            "CREATE INDEX IF NOT EXISTS idx_signals_status ON signals(status)",
            "CREATE INDEX IF NOT EXISTS idx_signals_type ON signals(signal_type)",
            
            # research_analytics indexes
            "CREATE INDEX IF NOT EXISTS idx_analytics_timestamp ON research_analytics(timestamp)",
            "CREATE INDEX IF NOT EXISTS idx_analytics_regime ON research_analytics(market_regime)"
        ]
        
        for index_sql in index_sqls:
            try:
                conn.execute(index_sql)
            except sqlite3.OperationalError as e:
                if "already exists" not in str(e):
                    print(f"  ‚ö†Ô∏è Could not create index: {e}")
        
        # Initialize database info
        current_version = get_current_feature_version(conn)
        if not current_version:
            conn.execute(
                "INSERT INTO database_info (key, value, updated_at) VALUES (?, ?, ?)",
                ("feature_version", FEATURE_VERSION, datetime.utcnow().isoformat())
            )
            print(f"‚úì Set feature version to {FEATURE_VERSION}")
        else:
            print(f"‚úì Current feature version: {current_version}")
        
        # Log initialization
        _log_system_health_conn(
            conn,
            "database",
            "INITIALIZED",
            f"Database initialized with feature version {FEATURE_VERSION}"
        )
        
        print(f"‚úì Database initialized at {DB_PATH}")

def get_current_feature_version(conn: sqlite3.Connection) -> Optional[str]:
    """Get current feature version from database."""
    try:
        cur = conn.execute("SELECT value FROM database_info WHERE key = 'feature_version'")
        row = cur.fetchone()
        return row["value"] if row else None
    except:
        return None

# =========================
# DATABASE MIGRATION
# =========================

def migrate_database(target_version: str = FEATURE_VERSION) -> bool:
    """
    Migrate database to target feature version.
    """
    try:
        with get_connection() as conn:
            # Get current version
            current_version = get_current_feature_version(conn)
            if not current_version:
                current_version = "v1.0"
            
            if current_version == target_version:
                print(f"Database already at version {target_version}")
                return True
            
            print(f"Migrating database from {current_version} to {target_version}")
            
            # Get migration SQL
            migrations = get_migration_sql(current_version, target_version)
            
            if not migrations:
                print("No migration required")
                return True
            
            # Execute migrations
            for migration_sql in migrations:
                try:
                    print(f"Executing: {migration_sql[:80]}...")
                    conn.execute(migration_sql)
                except sqlite3.OperationalError as e:
                    if "already exists" in str(e) or "duplicate column" in str(e):
                        print(f"  ‚ö†Ô∏è Skipping (already exists): {migration_sql[:50]}...")
                    else:
                        print(f"  ‚ùå Error: {e}")
                        raise
            
            # Update version
            conn.execute(
                "INSERT OR REPLACE INTO database_info (key, value, updated_at) VALUES (?, ?, ?)",
                (target_version, target_version, datetime.utcnow().isoformat())
            )
            
            # Log migration
            _log_system_health_conn(
                conn,
                "database",
                "MIGRATED",
                f"Migrated from {current_version} to {target_version}",
                json.dumps({"migrations_applied": len(migrations)})
            )
            
            print(f"‚úì Database migrated to version {target_version}")
            return True
            
    except Exception as e:
        print(f"‚ùå Migration failed: {e}")
        import traceback
        traceback.print_exc()
        return False

# =========================
# SYSTEM HEALTH LOGGING (PRIVATE HELPER)
# =========================

def _log_system_health_conn(
    conn: sqlite3.Connection,
    component: str,
    status: str,
    message: Optional[str] = None,
    details_json: Optional[str] = None
) -> None:
    """
    PRIVATE: Log system health status (requires connection).
    """
    query = """
        INSERT INTO system_health (timestamp, component, status, message, details_json)
        VALUES (?, ?, ?, ?, ?)
    """
    
    params = (
        datetime.utcnow().isoformat(),
        component,
        status,
        message,
        details_json
    )
    
    try:
        conn.execute(query, params)
    except Exception as e:
        print(f"‚ùå Error logging system health: {e}")

# =========================
# PUBLIC SYSTEM HEALTH LOGGING
# =========================

def log_system_health(
    component: str,
    status: str,
    message: Optional[str] = None,
    details_json: Optional[str] = None
) -> None:
    """
    PUBLIC: Log system health status.
    """
    with get_connection() as conn:
        _log_system_health_conn(conn, component, status, message, details_json)

# =========================
# INITIALIZATION (MODIFIED)
# =========================

def initialize_storage():
    """
    Initialize storage system.
    """
    print("Initializing storage system...")
    
    try:
        # Initialize database structure
        initialize_database()
        
        # Get current version
        with get_connection() as conn:
            current_version = get_current_feature_version(conn)
        
        # Check and run migrations if needed
        if current_version != FEATURE_VERSION:
            print(f"Current version: {current_version}, Target: {FEATURE_VERSION}")
            print(f"Migrating from {current_version} to {FEATURE_VERSION}")
            success = migrate_database(FEATURE_VERSION)
            
            if not success:
                print("‚ö†Ô∏è Migration may have issues, but continuing...")
        
        print("‚úì Storage system initialized")
        
    except Exception as e:
        print(f"‚ùå Error initializing storage: {e}")
        import traceback
        traceback.print_exc()
        raise

# =========================
# MARKET FEATURES OPERATIONS
# =========================

def insert_market_features(df: pd.DataFrame) -> None:
    """
    Insert feature rows into market_features table.
    """
    if df.empty:
        return
    
    # Ensure feature_version is set
    if "feature_version" not in df.columns:
        df["feature_version"] = FEATURE_VERSION
    
    # Ensure timestamp is string
    df["timestamp"] = df["timestamp"].astype(str)
    
    # Get expected columns from schema
    expected_columns = set(MARKET_FEATURES_SCHEMA["columns"].keys())
    
    # Add any missing columns with NULL
    for col in expected_columns:
        if col not in df.columns:
            df[col] = None
    
    # Select only columns that exist in schema
    df_to_insert = df[list(expected_columns)].copy()
    
    # Convert NaN to None for SQL
    df_to_insert = df_to_insert.replace({np.nan: None})
    
    try:
        with get_connection() as conn:
            # Insert data
            df_to_insert.to_sql(
                name="market_features",
                con=conn,
                if_exists="append",
                index=False
            )
            
            print(f"‚úì Inserted {len(df)} feature rows")
            
    except Exception as e:
        print(f"‚ùå Error inserting features: {e}")
        raise

def fetch_latest_features(feature_version: str = FEATURE_VERSION) -> Optional[pd.DataFrame]:
    """
    Fetch the most recent feature row for inference.
    """
    query = """
        SELECT *
        FROM market_features
        WHERE feature_version = ?
        ORDER BY timestamp DESC
        LIMIT 1
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(feature_version,))
            return df if not df.empty else None
            
    except Exception as e:
        print(f"‚ùå Error fetching features: {e}")
        return None

def fetch_features_by_time_range(
    start_time: str,
    end_time: str,
    feature_version: str = FEATURE_VERSION
) -> pd.DataFrame:
    """
    Fetch features within a time range.
    
    Args:
        start_time: Start timestamp (inclusive)
        end_time: End timestamp (inclusive)
        feature_version: Feature version filter
    
    Returns:
        DataFrame with features in range
    """
    query = """
        SELECT *
        FROM market_features
        WHERE feature_version = ?
          AND timestamp BETWEEN ? AND ?
        ORDER BY timestamp ASC
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(
                query,
                conn,
                params=(feature_version, start_time, end_time)
            )
            
            log_system_health(
                "features",
                "FETCHED_RANGE",
                f"Fetched {len(df)} features from {start_time} to {end_time}"
            )
            
            return df
            
    except Exception as e:
        print(f"‚ùå Error fetching features by range: {e}")
        return pd.DataFrame()

def fetch_training_data(
    feature_version: str = FEATURE_VERSION,
    start_ts: Optional[str] = None,
    end_ts: Optional[str] = None,
    min_confidence: float = 0.0
) -> pd.DataFrame:
    """
    Fetch labeled data for ML training with research features.
    
    Args:
        feature_version: Feature version
        start_ts: Start timestamp
        end_ts: End timestamp
        min_confidence: Minimum signal confidence for training
    
    Returns:
        DataFrame with features and labels
    """
    # Build WHERE clause
    conditions = ["feature_version = ?", "future_return_5m IS NOT NULL"]
    params = [feature_version]
    
    if start_ts:
        conditions.append("timestamp >= ?")
        params.append(start_ts)
    
    if end_ts:
        conditions.append("timestamp <= ?")
        params.append(end_ts)
    
    where_clause = " AND ".join(conditions)
    
    query = f"""
        SELECT mf.*, s.confidence as signal_confidence
        FROM market_features mf
        LEFT JOIN signals s ON mf.timestamp = s.timestamp AND s.status = 'VALIDATED'
        WHERE {where_clause}
        ORDER BY mf.timestamp ASC
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=params)
            
            # Filter by confidence if needed
            if min_confidence > 0 and 'signal_confidence' in df.columns:
                df = df[df['signal_confidence'] >= min_confidence]
            
            log_system_health(
                "training",
                "FETCHED",
                f"Fetched {len(df)} training samples"
            )
            
            return df
            
    except Exception as e:
        print(f"‚ùå Error fetching training data: {e}")
        return pd.DataFrame()

# =========================
# RESEARCH ANALYTICS OPERATIONS
# =========================

def insert_research_analytics(analytics: Dict) -> None:
    """
    Insert research analytics for tracking and visualization.
    
    Args:
        analytics: Dictionary with research analytics data
    """
    if not analytics:
        return
    
    # Prepare data
    timestamp = analytics.get("timestamp", datetime.utcnow().isoformat())
    
    data = {
        "timestamp": timestamp,
        "oi_velocity": analytics.get("oi_velocity"),
        "oi_regime": analytics.get("oi_regime"),
        "net_gamma": analytics.get("gamma_exposure", {}).get("net_gamma"),
        "gamma_regime": analytics.get("gamma_exposure", {}).get("regime"),
        "wall_strength": analytics.get("structural_walls", [{}])[0].get("concentration", 0) if analytics.get("structural_walls") else 0,
        "trap_probability": analytics.get("potential_traps", [{}])[0].get("confidence", 0) if analytics.get("potential_traps") else 0,
        "divergence_score": analytics.get("spot_divergence", {}).get("confidence", 0),
        "market_regime": analytics.get("market_regime"),
        "confidence": analytics.get("regime_confidence"),
        "insights_json": json.dumps(analytics.get("market_insights", [])),
        "walls_json": json.dumps(analytics.get("structural_walls", [])),
        "traps_json": json.dumps(analytics.get("potential_traps", [])),
        "created_at": datetime.utcnow().isoformat()
    }
    
    # Clean NaN values
    for key, value in data.items():
        if isinstance(value, float) and np.isnan(value):
            data[key] = None
    
    try:
        with get_connection() as conn:
            columns = ", ".join(data.keys())
            placeholders = ", ".join(["?"] * len(data))
            values = list(data.values())
            
            conn.execute(
                f"INSERT OR REPLACE INTO research_analytics ({columns}) VALUES ({placeholders})",
                values
            )
            
    except Exception as e:
        print(f"‚ùå Error inserting research analytics: {e}")

def fetch_recent_analytics(limit: int = 100) -> pd.DataFrame:
    """
    Fetch recent research analytics for visualization.
    
    Args:
        limit: Number of rows to fetch
    
    Returns:
        DataFrame with analytics
    """
    query = """
        SELECT *
        FROM research_analytics
        ORDER BY timestamp DESC
        LIMIT ?
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(limit,))
            return df
    except Exception as e:
        print(f"‚ùå Error fetching analytics: {e}")
        return pd.DataFrame()

# =========================
# SIGNAL STORE OPERATIONS (ENHANCED)
# =========================

def insert_signal(signal: Dict) -> None:
    """
    Insert a trading signal with research context.
    
    Args:
        signal: Signal dictionary with research context
    """
    if not signal:
        return
    
    # Add research context if available
    research_context = signal.get("research_context", {})
    if research_context and not isinstance(research_context, str):
        signal["research_context"] = json.dumps(research_context)
    
    # Add analytics summary
    analytics_summary = signal.get("analytics_summary", {})
    if analytics_summary and not isinstance(analytics_summary, str):
        signal["analytics_summary"] = json.dumps(analytics_summary)
    
    # Ensure all required fields
    required_fields = [
        "signal_id", "timestamp", "feature_version", "model_version",
        "signal_type", "confidence", "status", "created_at"
    ]
    
    for field in required_fields:
        if field not in signal:
            print(f"‚ö†Ô∏è Missing required field in signal: {field}")
            signal[field] = ""
    
    try:
        with get_connection() as conn:
            columns = ", ".join(signal.keys())
            placeholders = ", ".join(["?"] * len(signal))
            values = list(signal.values())
            
            conn.execute(
                f"INSERT INTO signals ({columns}) VALUES ({placeholders})",
                values
            )
            
            log_system_health(
                "signals",
                "INSERTED",
                f"Signal {signal['signal_id'][:8]} inserted: {signal['signal_type']} (conf: {signal['confidence']})",
                json.dumps({
                    "signal_id": signal["signal_id"],
                    "type": signal["signal_type"],
                    "confidence": signal["confidence"]
                })
            )
            
    except Exception as e:
        print(f"‚ùå Error inserting signal: {e}")
        log_system_health(
            "signals",
            "INSERT_FAILED",
            f"Failed to insert signal: {str(e)}"
        )
        raise

def signal_exists(feature_timestamp: str) -> bool:
    """
    Check if a signal already exists for given timestamp.
    
    Args:
        feature_timestamp: Feature timestamp
    
    Returns:
        True if signal exists
    """
    query = """
        SELECT 1 FROM signals WHERE timestamp = ? LIMIT 1
    """
    
    try:
        with get_connection() as conn:
            cur = conn.execute(query, (feature_timestamp,))
            return cur.fetchone() is not None
    except Exception as e:
        print(f"‚ùå Error checking signal existence: {e}")
        return False

def validate_new_signals(confidence_threshold: float = 0.2):
    """
    Promote NEW signals to VALIDATED if confidence is sufficient.
    
    Args:
        confidence_threshold: Minimum confidence for validation
    """
    query = """
        UPDATE signals
        SET status = 'VALIDATED'
        WHERE status = 'NEW'
          AND confidence >= ?
    """
    
    try:
        with get_connection() as conn:
            result = conn.execute(query, (confidence_threshold,))
            updated_count = result.rowcount
            
            if updated_count > 0:
                log_system_health(
                    "signals",
                    "VALIDATED",
                    f"Validated {updated_count} signals (conf >= {confidence_threshold})"
                )
                
    except Exception as e:
        print(f"‚ùå Error validating signals: {e}")
        log_system_health(
            "signals",
            "VALIDATION_FAILED",
            f"Signal validation failed: {str(e)}"
        )

def expire_old_signals():
    """
    Expire VALIDATED signals whose expiry_time has passed.
    """
    query = """
        UPDATE signals
        SET status = 'EXPIRED'
        WHERE status = 'VALIDATED'
          AND expiry_time < ?
    """
    
    now = datetime.utcnow().isoformat()
    
    try:
        with get_connection() as conn:
            result = conn.execute(query, (now,))
            expired_count = result.rowcount
            
            if expired_count > 0:
                log_system_health(
                    "signals",
                    "EXPIRED",
                    f"Expired {expired_count} signals"
                )
                
    except Exception as e:
        print(f"‚ùå Error expiring signals: {e}")

def update_signal_status(signal_id: str, new_status: str, 
                        pnl: Optional[float] = None,
                        exit_time: Optional[str] = None,
                        exit_price: Optional[float] = None) -> None:
    """
    Update signal lifecycle status with trade results.
    
    Args:
        signal_id: Signal ID
        new_status: New status
        pnl: Profit/Loss if applicable
        exit_time: Exit timestamp
        exit_price: Exit price
    """
    # Build update query
    updates = ["status = ?"]
    params = [new_status]
    
    if pnl is not None:
        updates.append("pnl = ?")
        params.append(pnl)
    
    if exit_time:
        updates.append("exit_time = ?")
        params.append(exit_time)
    
    if exit_price is not None:
        updates.append("exit_price = ?")
        params.append(exit_price)
    
    # Calculate trade duration if exit_time is provided
    if exit_time:
        updates.append("""
            trade_duration_seconds = (
                CAST((julianday(?) - julianday(created_at)) * 86400 AS INTEGER)
            )
        """)
        params.append(exit_time)
    
    query = f"""
        UPDATE signals
        SET {', '.join(updates)}
        WHERE signal_id = ?
    """
    
    params.append(signal_id)
    
    try:
        with get_connection() as conn:
            conn.execute(query, params)
            
            log_system_health(
                "signals",
                "UPDATED",
                f"Signal {signal_id[:8]} updated to {new_status}"
            )
            
    except Exception as e:
        print(f"‚ùå Error updating signal: {e}")
        log_system_health(
            "signals",
            "UPDATE_FAILED",
            f"Failed to update signal {signal_id}: {str(e)}"
        )

def fetch_active_signals(limit: int = 10) -> pd.DataFrame:
    """Fetch active signals for display."""
    query = """
        SELECT *
        FROM signals
        WHERE status IN ('NEW','VALIDATED')
        ORDER BY created_at DESC
        LIMIT ?
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(limit,))
            return df
    except Exception as e:
        print(f"‚ùå Error fetching active signals: {e}")
        return pd.DataFrame()

def fetch_signal_performance(days: int = 30) -> Dict:
    """
    Fetch signal performance metrics.
    
    Args:
        days: Number of days to analyze
    
    Returns:
        Dictionary with performance metrics
    """
    cutoff_date = (datetime.utcnow() - timedelta(days=days)).isoformat()
    
    query = """
        SELECT 
            signal_type,
            status,
            COUNT(*) as count,
            AVG(confidence) as avg_confidence,
            AVG(pnl) as avg_pnl,
            SUM(CASE WHEN pnl > 0 THEN 1 ELSE 0 END) as winners,
            SUM(CASE WHEN pnl <= 0 THEN 1 ELSE 0 END) as losers
        FROM signals
        WHERE created_at >= ?
        GROUP BY signal_type, status
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(cutoff_date,))
            
            if df.empty:
                return {}
            
            # Calculate overall metrics
            total_signals = df['count'].sum()
            profitable_signals = df[df['signal_type'] == 'BUY']['winners'].sum() + \
                                df[df['signal_type'] == 'SELL']['winners'].sum()
            
            performance = {
                "total_signals": int(total_signals),
                "profitable_signals": int(profitable_signals),
                "win_rate": round(profitable_signals / total_signals * 100, 1) if total_signals > 0 else 0,
                "breakdown": df.to_dict('records'),
                "period_days": days
            }
            
            return performance
            
    except Exception as e:
        print(f"‚ùå Error fetching performance: {e}")
        return {}

# =========================
# MODEL REGISTRY OPERATIONS
# =========================

def register_model(model_info: Dict) -> None:
    """
    Register trained ML model metadata.
    
    Args:
        model_info: Model information dictionary
    """
    required_keys = [
        "model_version", "feature_version", "algorithm",
        "trained_on_start", "trained_on_end", "metrics_json"
    ]
    
    for key in required_keys:
        if key not in model_info:
            raise ValueError(f"Missing required key: {key}")
    
    # Set created_at if not provided
    if "created_at" not in model_info:
        model_info["created_at"] = datetime.utcnow().isoformat()
    
    try:
        with get_connection() as conn:
            # Deactivate other models for this feature version
            conn.execute("""
                UPDATE model_registry
                SET is_active = 0
                WHERE feature_version = ?
            """, (model_info["feature_version"],))
            
            # Insert new model as active
            columns = ", ".join(model_info.keys())
            placeholders = ", ".join(["?"] * len(model_info))
            values = list(model_info.values())
            
            conn.execute(
                f"INSERT INTO model_registry ({columns}) VALUES ({placeholders})",
                values
            )
            
            log_system_health(
                "models",
                "REGISTERED",
                f"Model {model_info['model_version']} registered"
            )
            
    except Exception as e:
        print(f"‚ùå Error registering model: {e}")
        raise

def fetch_active_model(feature_version: str = FEATURE_VERSION) -> Optional[Dict]:
    """
    Fetch the active model for a given feature version.
    
    Args:
        feature_version: Feature version
    
    Returns:
        Model info dictionary or None
    """
    query = """
        SELECT *
        FROM model_registry
        WHERE feature_version = ?
          AND is_active = 1
        ORDER BY created_at DESC
        LIMIT 1
    """
    
    try:
        with get_connection() as conn:
            row = conn.execute(query, (feature_version,)).fetchone()
            return dict(row) if row else None
    except Exception as e:
        print(f"‚ùå Error fetching active model: {e}")
        return None

# =========================
# SYSTEM HEALTH LOGGING (PUBLIC)
# =========================

def fetch_recent_health_logs(limit: int = 50) -> pd.DataFrame:
    """
    Fetch recent system health logs.
    
    Args:
        limit: Number of logs to fetch
    
    Returns:
        DataFrame with health logs
    """
    query = """
        SELECT *
        FROM system_health
        ORDER BY timestamp DESC
        LIMIT ?
    """
    
    try:
        with get_connection() as conn:
            return pd.read_sql_query(query, conn, params=(limit,))
    except Exception as e:
        print(f"‚ùå Error fetching health logs: {e}")
        return pd.DataFrame()

# =========================
# FEATURE HISTORY TRACKING
# =========================

def track_feature_changes(feature_df: pd.DataFrame) -> None:
    """
    Track significant feature changes for monitoring.
    
    Args:
        feature_df: DataFrame with features
    """
    if feature_df.empty or len(feature_df) < 2:
        return
    
    # Only track important features
    important_features = [
        "oi_velocity", "net_gamma", "trap_probability",
        "divergence_score", "wall_strength", "put_call_ratio"
    ]
    
    current = feature_df.iloc[-1]
    previous = feature_df.iloc[-2] if len(feature_df) > 1 else current
    
    changes = []
    for feature in important_features:
        if feature in current and feature in previous:
            old_val = previous[feature]
            new_val = current[feature]
            
            if old_val is not None and new_val is not None and old_val != 0:
                change_pct = ((new_val - old_val) / abs(old_val)) * 100
                
                # Only track significant changes (>10%)
                if abs(change_pct) > 10:
                    changes.append({
                        "timestamp": current["timestamp"],
                        "feature_name": feature,
                        "old_value": old_val,
                        "new_value": new_val,
                        "change_pct": change_pct,
                        "created_at": datetime.utcnow().isoformat()
                    })
    
    if changes:
        try:
            with get_connection() as conn:
                for change in changes:
                    conn.execute("""
                        INSERT INTO feature_history 
                        (timestamp, feature_name, old_value, new_value, change_pct, created_at)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (
                        change["timestamp"], change["feature_name"],
                        change["old_value"], change["new_value"],
                        change["change_pct"], change["created_at"]
                    ))
        except Exception as e:
            print(f"‚ùå Error tracking feature changes: {e}")

# =========================
# DATABASE MAINTENANCE
# =========================

def cleanup_old_data(days_to_keep: int = 30) -> Dict:
    """
    Clean up old data from database.
    
    Args:
        days_to_keep: Number of days of data to keep
    
    Returns:
        Dictionary with cleanup statistics
    """
    cutoff_date = (datetime.utcnow() - timedelta(days=days_to_keep)).isoformat()
    
    cleanup_stats = {}
    
    try:
        with get_connection() as conn:
            # Clean old market features
            result = conn.execute(
                "DELETE FROM market_features WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["market_features"] = result.rowcount
            
            # Clean old signals
            result = conn.execute(
                "DELETE FROM signals WHERE created_at < ? AND status = 'EXPIRED'",
                (cutoff_date,)
            )
            cleanup_stats["signals"] = result.rowcount
            
            # Clean old system health logs
            result = conn.execute(
                "DELETE FROM system_health WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["system_health"] = result.rowcount
            
            # Clean old research analytics
            result = conn.execute(
                "DELETE FROM research_analytics WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["research_analytics"] = result.rowcount
            
            # Clean old feature history
            result = conn.execute(
                "DELETE FROM feature_history WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["feature_history"] = result.rowcount
            
            # Vacuum database
            conn.execute("VACUUM")
            
            log_system_health(
                "database",
                "CLEANED",
                f"Cleaned up {sum(cleanup_stats.values())} old records",
                json.dumps(cleanup_stats)
            )
            
            return cleanup_stats
            
    except Exception as e:
        print(f"‚ùå Error during cleanup: {e}")
        log_system_health(
            "database",
            "CLEANUP_FAILED",
            f"Cleanup failed: {str(e)}"
        )
        return {}
# =========================
# DATABASE INFO & STATS
# =========================

def get_database_stats() -> Dict:
    """Get database statistics."""
    stats = {}
    
    try:
        with get_connection() as conn:
            # Table counts
            tables = [
                "market_features", "signals", "model_registry",
                "system_health", "research_analytics", "feature_history"
            ]
            
            for table in tables:
                try:
                    cur = conn.execute(f"SELECT COUNT(*) as count FROM {table}")
                    row = cur.fetchone()
                    stats[f"{table}_count"] = row["count"] if row else 0
                except:
                    stats[f"{table}_count"] = 0
            
            # Latest feature timestamp
            try:
                cur = conn.execute("SELECT MAX(timestamp) as latest FROM market_features")
                row = cur.fetchone()
                stats["latest_feature"] = row["latest"] if row else None
            except:
                stats["latest_feature"] = None
            
            # Feature version
            stats["feature_version"] = get_current_feature_version(conn) or "unknown"
            
            # Database size
            if DB_PATH.exists():
                db_size = DB_PATH.stat().st_size
                stats["database_size_mb"] = round(db_size / (1024 * 1024), 2)
            else:
                stats["database_size_mb"] = 0
            
            return stats
            
    except Exception as e:
        print(f"‚ùå Error getting database stats: {e}")
        return {}


# =========================
# INITIALIZATION
# =========================

def initialize_storage():
    """
    Initialize storage system.
    """
    print("Initializing storage system...")
    initialize_database()
    
    # Check and run migrations if needed
    current_version = get_database_stats().get("feature_version", "v1.0")
    if current_version != FEATURE_VERSION:
        print(f"Migrating from {current_version} to {FEATURE_VERSION}")
        migrate_database(FEATURE_VERSION)
    
    print("‚úì Storage system initialized")

# Run initialization when module is imported
initialize_storage()



====================================================================================================
FILE: .\ui\charts.py
====================================================================================================


File Name: charts.py
Full Path: G:\trading_app\ui\charts.py
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .py




====================================================================================================
FILE: .\ui\heatmap.py
====================================================================================================


File Name: heatmap.py
Full Path: G:\trading_app\ui\heatmap.py
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .py




====================================================================================================
FILE: .\ui\option_chain.py
====================================================================================================


File Name: option_chain.py
Full Path: G:\trading_app\ui\option_chain.py
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .py




====================================================================================================
FILE: .\ui\signals.py
====================================================================================================


File Name: signals.py
Full Path: G:\trading_app\ui\signals.py
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .py




====================================================================================================
FILE: .\utils\expiry_utils.py
====================================================================================================


File Name: expiry_utils.py
Full Path: G:\trading_app\utils\expiry_utils.py
Size: 2.46 KB
Last Modified: 01/27/2026 23:57:39
Extension: .py

"""
Utility functions for expiry date selection
"""
from datetime import datetime, time
from data.instrument_master import get_available_expiries

def is_market_open() -> bool:
    """
    Check if market is currently open.
    NSE market hours: 9:15 AM to 3:30 PM IST, Monday to Friday
    """
    now = datetime.now()
    
    # Check if it's a weekday
    if now.weekday() >= 5:  # Saturday (5) or Sunday (6)
        return False
    
    # Check time
    market_open = time(9, 15)  # 9:15 AM IST
    market_close = time(15, 30)  # 3:30 PM IST
    
    current_time = now.time()
    return market_open <= current_time <= market_close

def get_trading_expiry(underlying: str) -> str:
    """
    Get appropriate expiry date for trading.
    - If market is open and today is expiry: use today
    - If market is closed and today is expiry: use next expiry
    - Otherwise: use nearest expiry
    """
    expiries = get_available_expiries(underlying)
    if not expiries:
        return ""
    
    today = datetime.now().strftime('%Y-%m-%d')
    
    # Check if today is an expiry day
    if today in expiries:
        if is_market_open():
            # Market is open on expiry day - use today
            return today
        else:
            # Market closed on expiry day - use next expiry
            for expiry in sorted(expiries):
                if expiry > today:
                    return expiry
            return expiries[-1]  # Fallback to last expiry
    else:
        # Today is not expiry day - get nearest future expiry
        for expiry in sorted(expiries):
            if expiry >= today:
                return expiry
        return expiries[-1]  # Fallback to last expiry

def get_expiry_for_backtesting(underlying: str, reference_date: str = None) -> str:
    """
    Get expiry for backtesting (can use historical expiries).
    
    Args:
        underlying: Underlying symbol
        reference_date: Reference date in 'YYYY-MM-DD' format (default: today)
    
    Returns:
        Expiry date for backtesting
    """
    expiries = get_available_expiries(underlying)
    if not expiries:
        return ""
    
    if reference_date:
        ref_date = reference_date
    else:
        ref_date = datetime.now().strftime('%Y-%m-%d')
    
    # Find the nearest expiry on or after reference date
    for expiry in sorted(expiries):
        if expiry >= ref_date:
            return expiry
    
    return expiries[-1]



****************************************************************************************************
CATEGORY: Documentation (1 files)
****************************************************************************************************


====================================================================================================
FILE: .\codebase_export.txt
====================================================================================================


File Name: codebase_export.txt
Full Path: G:\trading_app\codebase_export.txt
Size: 1026.53 KB
Last Modified: 01/28/2026 15:51:25
Extension: .txt





****************************************************************************************************
CATEGORY: Configuration Files (2 files)
****************************************************************************************************


====================================================================================================
FILE: .\config\nifty_weights.json
====================================================================================================


File Name: nifty_weights.json
Full Path: G:\trading_app\config\nifty_weights.json
Size: 1.1 KB
Last Modified: 01/28/2026 11:59:05
Extension: .json

{
  "HDFCBANK": 0.1272,
  "RELIANCE": 0.0823,
  "ICICIBANK": 0.0840,
  "INFY": 0.0498,
  "BHARTIARTL": 0.0469,
  "LT": 0.0386,
  "SBIN": 0.0375,
  "AXISBANK": 0.0358,
  "TCS": 0.0287,
  "ITC": 0.0259,
  "KOTAKBANK": 0.0261,
  "BAJFINANCE": 0.0212,
  "HINDUNILVR": 0.0186,
  "ULTRACEMCO": 0.0184,
  "TITAN": 0.0176,
  "MARUTI": 0.0169,
  "NTPC": 0.0166,
  "HCLTECH": 0.0158,
  "ADANIPORTS": 0.0156,
  "SUNPHARMA": 0.0152,
  "BEL": 0.0151,
  "BAJAJFINSV": 0.0152,
  "JSWSTEEL": 0.0148,
  "TATASTEEL": 0.0140,
  "ONGC": 0.0154,
  "BAJAJ-AUTO": 0.0131,
  "COALINDIA": 0.0129,
  "ADANIENT": 0.0125,
  "ASIANPAINT": 0.0125,
  "NESTLEIND": 0.0124,
  "WIPRO": 0.0122,
  "ZOMATO": 0.0121,
  "POWERGRID": 0.0117,
  "HINDALCO": 0.0107,
  "SBILIFE": 0.0101,  # CHANGED FROM "SBI LIFE"
  "EICHERMOT": 0.0097,
  "GRASIM": 0.0096,
  "SHRIRAMFIN": 0.0093,
  "INDIGO": 0.0091,
  "TECHM": 0.0085,
  "JIOFIN": 0.0081,
  "HDFCLIFE": 0.0077,
  "TRENT": 0.0067,
  "APOLLOHOSP": 0.0063,
  "TATAMOTORS": 0.0062,
  "MAXHEALTH": 0.0061,
  "TATACONSUM": 0.0058,
  "CIPLA": 0.0053,
  "DRREDDY": 0.0051
}



====================================================================================================
FILE: .\config\settings.json
====================================================================================================


File Name: settings.json
Full Path: G:\trading_app\config\settings.json
Size: 0 KB
Last Modified: 01/25/2026 16:00:50
Extension: .json




****************************************************************************************************
CATEGORY: Shell Scripts (1 files)
****************************************************************************************************


====================================================================================================
FILE: .\launch_app.bat
====================================================================================================


File Name: launch_app.bat
Full Path: G:\trading_app\launch_app.bat
Size: 0.66 KB
Last Modified: 01/28/2026 00:35:41
Extension: .bat

@echo off
echo ===========================================
echo üöÄ LAUNCHING TRADING RESEARCH PLATFORM v2.0
echo ===========================================
echo.
echo Environment: trading_tool
echo Features:
echo ‚Ä¢ OI Velocity Analytics
echo ‚Ä¢ Gamma Exposure (GEX) Tracking  
echo ‚Ä¢ Structural Walls/Traps Detection
echo ‚Ä¢ Spot Divergence Analysis
echo.
echo Market Hours: 9:15 AM - 3:30 PM IST
echo.
echo Starting app...
echo URL: http://localhost:8501
echo Press Ctrl+C to stop
echo.

REM Activate conda environment
call conda activate trading_tool

REM Set Streamlit config
set STREAMLIT_SERVER_FILE_WATCHER_TYPE=none

REM Run the app
streamlit run app.py

pause



****************************************************************************************************
CATEGORY: Database/SQL (2 files)
****************************************************************************************************


====================================================================================================
FILE: .\storage\schema.sql
====================================================================================================


File Name: schema.sql
Full Path: G:\trading_app\storage\schema.sql
Size: 2.44 KB
Last Modified: 01/25/2026 16:05:19
Extension: .sql

-- ================================
-- FEATURE STORE (IMMUTABLE)
-- ================================
CREATE TABLE IF NOT EXISTS market_features (
    timestamp TEXT NOT NULL,
    feature_version TEXT NOT NULL,

    -- Option structure
    put_call_ratio REAL NOT NULL,
    oi_delta REAL NOT NULL,
    oi_concentration REAL NOT NULL,
    atm_iv REAL NOT NULL,
    iv_skew REAL NOT NULL,

    -- Price & flow
    vwap_distance REAL NOT NULL,
    price_momentum REAL NOT NULL,
    volume_ratio REAL NOT NULL,

    -- Breadth
    ccc_value REAL NOT NULL,
    ccc_slope REAL NOT NULL,

    -- Time context
    time_to_expiry_minutes INTEGER NOT NULL,

    -- ML target (NULL in live mode)
    future_return_5m REAL,

    PRIMARY KEY (timestamp, feature_version)
);

-- ================================
-- SIGNAL STATE MACHINE
-- ================================
CREATE TABLE IF NOT EXISTS signals (
    signal_id TEXT PRIMARY KEY,
    timestamp TEXT NOT NULL,

    feature_version TEXT NOT NULL,
    model_version TEXT NOT NULL,

    signal_type TEXT CHECK(signal_type IN ('BUY','SELL','NEUTRAL')) NOT NULL,
    confidence REAL NOT NULL,

    market_state TEXT NOT NULL,
    rationale TEXT,

    expiry_time TEXT NOT NULL,
    status TEXT CHECK(status IN ('NEW','VALIDATED','EXPIRED','EVALUATED')) NOT NULL,

    created_at TEXT NOT NULL
);

-- ================================
-- TRADE EXECUTION (FUTURE)
-- ================================
CREATE TABLE IF NOT EXISTS trades (
    trade_id TEXT PRIMARY KEY,
    signal_id TEXT NOT NULL,

    entry_price REAL NOT NULL,
    exit_price REAL,
    quantity INTEGER NOT NULL,

    stop_loss REAL,
    take_profit REAL,

    pnl REAL,
    executed_at TEXT,
    closed_at TEXT,

    FOREIGN KEY(signal_id) REFERENCES signals(signal_id)
);

-- ================================
-- MODEL REGISTRY
-- ================================
CREATE TABLE IF NOT EXISTS model_registry (
    model_version TEXT PRIMARY KEY,
    feature_version TEXT NOT NULL,

    algorithm TEXT NOT NULL,
    trained_on_start TEXT NOT NULL,
    trained_on_end TEXT NOT NULL,

    metrics_json TEXT NOT NULL,
    created_at TEXT NOT NULL
);

-- ================================
-- SYSTEM HEALTH & DEBUG
-- ================================
CREATE TABLE IF NOT EXISTS system_health (
    timestamp TEXT PRIMARY KEY,
    component TEXT NOT NULL,
    status TEXT NOT NULL,
    message TEXT
);




====================================================================================================
FILE: .\storage\trading.db
====================================================================================================


File Name: trading.db
Full Path: G:\trading_app\storage\trading.db
Size: 164 KB
Last Modified: 01/28/2026 15:45:25
Extension: .db

SQLite format 3   @        )           -                                                  .ç¯   ˚    ˚íx
¶⁄	ª
k	Y‰z@55                                                                                                                                                                         CW1 indexsqlite_autoindex_research_analytics_1research_analyticsÅC''ÇEtabledatabase_infodatabase_infoCREATE TABLE database_info (
                    key TEXT PRIMARY KEY,
                    value TEXT,
                    updated_at TEXT
                )9M' indexsqlite_autoindex_database_info_1database_info       å^++òstablemarket_featuresmarket_featuresCREATE TABLE market_features (
    timestamp TEXT NOT NULL,
    feature_version TEXT NOT NULL,

    -- Option structure
    put_call_ratio REAL NOT NULL,
    oi_delta REAL NOT NULL,
    oi_concentration REAL NOT NULL,
    atm_iv REAL NOT NULL,
    iv_skew REAL NOT NULL,

    -- Price & flow
    vwap_distance REAL NOT NULL,
    price_momentum REAL NOT NULL,
    volume_ratio REAL NOT NULL,

    -- Breadth
    ccc_value REAL NOT NULL,
    ccc_slope REAL NOT NULL,

    -- Time context
    time_to_expiry_minutes INTEGER NOT NULL,

    -- ML target (NULL in live mode)
    future_return_5m REAL, oi_velocity REAL DEFAULT 0.0, oi_velocity_ma REAL DEFAULT 0.0, oi_velocity_std REAL DEFAULT 0.0, oi_regime_expansive REAL DEFAULT 0.0, oi_regime_constricted REAL DEFAULT 0.0, net_gamma REAL DEFAULT 0.0, gamma_regime_positive REAL DEFAULT 0.0, gamma_regime_negative REAL DEFAULT 0.0, gamma_flip_distance REAL DEFAULT 0.0, max_gamma_strike_distance REAL DEFAULT 0.0, wall_strength REAL DEFAULT 0.0, wall_defense_score REAL DEFAULT 0.0, trap_probability REAL DEFAULT 0.0, price_oi_divergence REAL DEFAULT 0.0, price_gamma_divergence REAL DEFAULT 0.0, divergence_score REAL DEFAULT 0.0, has_divergence REAL DEFAULT 0.0, max_pain_distance REAL DEFAULT 0.0, vix_smile REAL DEFAULT 0.0, skewness REAL DEFAULT 0.0, spring_detection REAL DEFAULT 0.0, upthrust_detection REAL DEFAULT 0.0, accumulation_score REAL DEFAULT 0.0, gamma_wall_interaction REAL DEFAULT 0.0, velocity_divergence_composite REAL DEFAULT 0.0, trap_gamma_composite REAL DEFAULT 0.0,

    PRIMARY KEY (timestamp, feature_version)
)s3+Åindexidx_feature_versionmarket_featuresCREATE INDEX idx_feature_version ON market_features(feature_version)`'+}indexidx_timestampmarket_featuresCREATE INDEX idx_timestamp ON market_features(timestamp)Å-	''Çtablesystem_healthsystem_health
CREATE TABLE system_health (
    timestamp TEXT PRIMARY KEY,
    component TEXT NOT NULL,
    status TEXT NOT NULL,
    message TEXT
)9
M' indexsqlite_autoindex_system_health_1system_healthÇ1))Ñtablemodel_registrymodel_registryCREATE TABLE model_registry (
    model_version TEXT PRIMARY KEY,
    feature_version TEXT NOT NULL,

    algorithm TEXT NOT NULL,
    trained_on_start TEXT NOT NULL,
    trained_on_end TEXT NOT NULL,

    metrics_json TEXT NOT NULL,
    created_at TEXT NOT NULL
);O) indexsqlite_autoindex_model_registry_1model_registry	Ç^ÖtabletradestradesCREATE TABLE trades (
    trade_id TEXT PRIMARY KEY,
    signal_id TEXT NOT NULL,

    entry_price REAL NOT NULL,
    exit_price REAL,
    quantity INTEGER NOT NULL,

    stop_loss REAL,
    take_profit REAL,

    pnl REAL,
    executed_at TEXT,
    closed_at TEXT,

    FOREIGN KEY(signal_id) REFERENCES signals(signal_id)
)+? indexsqlite_autoindex_trades_1tradesÉjá+tablesignalssignalsCREATE TABLE signals (
    signal_id TEXT PRIMARY KEY,
    timestamp TEXT NOT NULL,

    feature_version TEXT NOT NULL,
    model_version TEXT NOT NULL,

    signal_type TEXT CHECK(signal_type IN ('BUY','SELL','NEUTRAL')) NOT NULL,
    confidence REAL NOT NULL,

    market_state TEXT NOT NULL,
    rationale TEXT,

    expiry_time TEXT NOT NULL,
    status TEXT CHECK(status IN ('NEW','VALIDATED','EXPIRED','EVALUATED')) NOT NULL,

    created_at TEXT NOT NULL
)-A indexsqlite_autoindex_signals_1signals=Q+ indexsqlite_autoindex_market_features_1market_feat      Ò    &˚ˆÒÆKË∑X˘ö;
‹
}
	ø	`	¢C‰Ö&«h	™KÏç.œp≤SÙï6 ◊ p                    e*)A 2026-01-28T06:40:21.292000v2.0ñˆä?a,?…ΩÆ#ø>ÀﬂŸWƒ) /@J›`› ‹]))A 2026-01-28T06:40:21.021000v2.0ñˆä?a,?…ΩÆ# /@J›`› ‹]()A 2026-01-28T06:31:50.779000v2.0ìÛm?fxÚÄ≠ 8@J’†ùê¸7]')A 2026-01-28T06:31:50.494000v2.0ìÛm?fxÚÄ≠ 8@J’a>†ù”]&)A 2026-01-28T06:31:05.488000v2.0ík¶?eº‘Y: 8@J≈ì´±ßS]%)A 2026-01-28T06:30:40.508000v2.0ík¶?eC|¬◊l 9@J≈ì´±ßS]$)A 2026-01-28T06:22:21.102000v2.0ä±6?d(üoÏ A@J‡˘∏˛º]#)A 2026-01-28T06:22:17.461000v2.0ä±6?dA \∞ê A@J‡˘∏˛º]")A 2026-01-28T06:21:37.495000v2.0ä∞ı?b:≈ÌWc B@J‡˜˜¶ó]!)A 2026-01-28T06:16:35.233000v2.0ÅÊU?^ù±»\è9 G@J≈f YÍﬂ] )A 2026-01-28T06:16:15.146000v2.0ÅÊU?_;q’‚√n G@J≈gYÜ≥ˆ])A 2026-01-28T06:15:36.853000v2.0ÅÊU?VP7Œ≈°s H@J≈coM±Ô])A 2026-01-28T06:08:09.844000v2.0iƒä?aÉmJz^¶ O@Jmü†Í!Í])A 2026-01-28T06:07:56.401000v2.0iƒä?b°ﬁ:ù6 P@Jm» ü‚])A 2026-01-28T06:07:52.779000v2.0iƒä?cÙ)F¨^ P@JmŒ!])A 2026-01-28T06:06:20.388000v2.0`àÌ?fÌ˜‹K‘\ Q@J7v3›,$])A 2026-01-28T05:52:07.962000v2.0=Å6?eïhñU›Ç _@I¨N¡RøW])A 2026-01-28T05:52:07.278000v2.0=Å6?eïhñU›Ç _@I¨N¡RøW])A 2026-01-28T05:51:41.390000v2.0=Å6?fd˘µö« `@I¨N¡RøW])A 2026-01-28T05:51:37.694000v2.0=Å6?f3(£‹j `@I¨N¡RøW])A 2026-01-28T05:51:10.961000v2.0=Äı?ecó∫|S% `@I¨N?6Í3])A 2026-01-28T05:46:59.473000v2.00VZ?e≤w¡?m? e@IÖ59îõú])A 2026-01-28T05:46:05.515000v2.0&l?f~ÑˇRé e@IuÉËÖ1])A 2026-01-28T05:36:29.142000v2.0£?fW»áY o@I>úJg¨V])A 2026-01-28T05:36:25.501000v2.0£?e◊‘f"î≤ o@I>ù‰ùD‡])A 2026-01-28T05:34:29.323000v2.0Mè?c˛î=è˘a q@I<a•Q])A 2026-01-28T05:29:35.610000v2.0w?YuìW–¥ v@Ië‚ı≥])A 2026-01-28T05:29:32.065000v2.0w?X|~ª « v@Ië‚ı≥])A 2026-01-28T05:27:20.977000v2.0˛=K?YÒN§æí x@IP–¿c])A 2026-01-28T05:27:17.464000v2.0˛=
?XæÍä‰ŸC x@IPI.&-])A 2026-01-28T05:26:46.561000v2.0˛4'?U°‹ÕL@l y@I^€ ])A 2026-01-28T05:19:21.892000v2.0Ût?@Sîo≈ Ä@Iöe∑ïÃ]
)A 2026-01-28T05:14:34.190000v2.0ÈZH?LGÁz*÷ Ö@HÔámü†/	A 2099-01-01T00:00:00.000000v_testaA 2026-01-25T11:42:21.599026v1.0?‡      ˛¢?‹q«q«@0333333øÊfffff`øw?0µíç3?∑ŒŸá+ﬂaA 2026-01-25T11:42:10.621540v1.0?‡      ˛¢?‹q«q«@0333333øÊfffff`øw?0µíç3?∑ŒŸá+ﬂaA 2026-01-25T11:41:59.638980v1.0?‡      ˛¢?‹q«q«@0333333øÊfffff`øw?0µíç3?∑ŒŸá+ﬂaA 2026-01-25T11:41:48.577341v1.0?‡      ˛¢?‹q«q«@0333333øÊfffff`øw?0µíç3?∑ŒŸá+ﬂaA 2026-01-25T11:41:37.555290v1.0?‡      ˛¢?‹q«q«@0333333øÊfffff`øw?0µíç3?∑ŒŸá+ﬂaA 2026-01-25T11:41:26.568723v1.0?‡      ˛¢?‹q«q«@0333333øÊfffff`øw?0µíç3?∑ŒŸá+ﬂaA 2026-01-25T11:41:15.550080v1.0?‡      ˛¢?‹q«q«@0333333øÊfffff`øw?0µíç3?∑ŒŸá+ﬂaA 2026-01-25T11:41:04.576288v1.0?‡      ˛¢?‹q«q«@0333333øÊfffff`øw?   #z   "R   !*   ÿ    %ÿqM)·ósO+„øõwS/Á√ü{W3Î«£[7
Ô
À
ß
É
_
;
	Û	œ	´	á	c	?	˜”ØãgC˚◊≥èkG#ˇ€∑ìoK'ﬂªósO+„øõwS/Á√ü{W3Î«£[7ÔÀßÉ_; Ûª                     #A2026-01-28T09:35:43.555000v2.0k#A2026-01-28T09:35:41.275000v2.0j#A2026-01-28T09:35:39.165000v2.0i#A2026-01-28T09:35:36.646000v2.0h#A2026-01-28T09:35:33.885000v2.0g#A2026-01-28T09:35:30.083000v2.0f#A2026-01-28T09:35:28.003000v2.0e#A2026-01-28T09:35:25.801000v2.0d#A2026-01-28T09:35:23.275000v2.0c#A2026-01-28T09:35:22.380000v2.0b#A2026-01-28T09:35:19.816000v2.0a#A2026-01-28T09:35:17.748000v2.0`#A2026-01-28T09:35:14.221000v2.0_#A2026-01-28T09:35:10.416000v2.0^#A2026-01-28T09:35:07.399000v2.0]#A2026-01-28T09:35:05.000000v2.0\#A2026-01-28T09:35:01.038000v2.0[#A2026-01-28T09:33:23.167000v2.0Z#A2026-01-28T09:33:21.012000v2.0Y#A2026-01-28T09:33:16.484000v2.0X#A2026-01-28T09:33:11.088000v2.0W#A2026-01-28T09:33:08.934000v2.0V#A2026-01-28T09:33:06.400000v2.0U#A2026-01-28T09:33:03.557000v2.0T#A2026-01-28T09:33:00.465000v2.0S#A2026-01-28T09:32:57.886000v2.0R#A2026-01-28T09:32:55.730000v2.0Q#A2026-01-28T09:32:53.557000v2.0P#A2026-01-28T09:32:51.239000v2.0O#A2026-01-28T09:32:48.486000v2.0N#A2026-01-28T09:32:44.011000v2.0M#A2026-01-28T09:31:27.140000v2.0L#A2026-01-28T09:31:19.729000v2.0K#A2026-01-28T09:31:07.825000v2.0J#A2026-01-28T09:31:04.356000v2.0I#A2026-01-28T09:31:01.901000v2.0H#A2026-01-28T09:30:59.784000v2.0G#A2026-01-28T09:30:57.436000v2.0F#A2026-01-28T09:30:55.169000v2.0E#A2026-01-28T09:30:53.000000v2.0D#A2026-01-28T09:30:43.588000v2.0C#A2026-01-28T09:28:30.647000v2.0B#A2026-01-28T09:24:43.058000v2.0A#A2026-01-28T08:59:42.530000v2.0@#A2026-01-28T08:48:39.919000v2.0?#A2026-01-28T08:48:36.899000v2.0>#A2026-01-28T08:48:36.233000v2.0=#A2026-01-28T08:47:59.438000v2.0<#A2026-01-28T08:47:56.415000v2.0;#A2026-01-28T08:47:55.705000v2.0:#A2026-01-28T08:43:51.980000v2.09#A2026-01-28T08:36:52.342000v2.08#A2026-01-28T08:27:46.438000v2.07#A2026-01-28T08:23:17.226000v2.06#A2026-01-28T08:04:46.718000v2.05#A2026-01-28T08:00:50.649000v2.04#A2026-01-28T07:53:02.967000v2.03#A2026-01-28T07:03:27.140000v2.02#A2026-01-28T06:55:09.932000v2.01#A2026-01-28T06:50:55.215000v2.00#A2026-01-28T06:50:52.103000v2.0/#A2026-01-28T06:49:41.724000v2.0.#A2026-01-28T06:49:30.564000v2.0-#A2026-01-28T06:41:34.642000v2.0,#A2026-01-28T06:41:34.374000v2.0+#A2026-01-28T06:40:21.292000v2.0*#A2026-01-28T06:40:21.021000v2.0)#A2026-01-28T06:31:50.779000v2.0(#A2026-01-28T06:31:50.494000v2.0'#A2026-01-28T06:31:05.488000v2.0&#A2026-01-28T06:30:40.508000v2.0%#A2026-01-28T06:22:21.102000v2.0$#A2026-01-28T06:22:17.461000v2.0##A2026-01-28T06:21:37.495000v2.0"#A2026-01-28T06:16:35.233000v2.0!#A2026-01-28T06:16:15.146000v2.0 #A2026-01-28T06:15:36.853000v2.0#A2026-01-28T06:08:09.844000v2.0#A2026-01-28T06:07:56.401000v2.0#A2026-01-28T06:07:52.779000v2.0#A2026-01-28T06:06:20.388000v2.0#A2026-01-28T05:52:07.962000v2.0#A2026-01-28T05:52:07.278000v2.0#A2026-01-28T05:51:41.390000v2.0#A2026-01-28T05:51:37.694000v2.0#A2026-01-28T05:51:10.961000v2.0#A2026-01-28T05:46:59.473000v2.0#A2026-01-28T05:46:05.515000v2.0#A2026-01-28T05:36:29.142000v2.0#A2026-01-28T05:36:25.501000v2.0#A2026-01-28T05:34:29.323000v2.0#A2026-01-28T05:29:35.610000v2.0#A2026-01-28T05:29:32.065000v2.0#A2026-01-28T05:27:20.977000v2.0#A2026-01-28T05:27:17.464000v2.0#A2026-01-28T05:26:46.561000v2.0#A2026-01-28T05:19:21.892000v2.0#A2026-01-28T05:14:34.190000v2.0
%A2099-01-01T00:00:00.000000v_test	#A2026-01-25T11:42:21.599026v1.0#A2026-01-25T11:42:10.621540v1.0#A2026-01-25T11:41:59.638980v1.0#A2026-01-25T11:41:48.577341v1.0#A2026-01-25T11:41:37.555290v1.0#A2026-01-25T11:41:26.568723v1.0#A2026-01-25T11:41:15.550080   $#A2026-01-28T08:36:52.342000v2.08                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ˚    ˚tÆKËÖ"ø\
˘
ñ
3	–	m	
ßD·~∏UÚè,…f†=⁄w±N Î à                                                a(A#i 2026-01-28T05:14:22.402138databaseINITIALIZEDDatabase initialized with feature version v2.0a'A#i 2026-01-28T05:05:41.987734databaseINITIALIZEDDatabase initialized with feature version v2.0a&A#i 2026-01-28T04:57:50.503194databaseINITIALIZEDDatabase initialized with feature version v2.0a%A#i 2026-01-28T04:52:50.763142databaseINITIALIZEDDatabase initialized with feature version v2.0a$A#i 2026-01-28T04:52:42.786021databaseINITIALIZEDDatabase initialized with feature version v2.0a#A#i 2026-01-28T04:47:37.468804databaseINITIALIZEDDatabase initialized with feature version v2.0a"A#i 2026-01-28T04:46:32.033460databaseINITIALIZEDDatabase initialized with feature version v2.0a!A#i 2026-01-28T04:37:17.459125databaseINITIALIZEDDatabase initialized with feature version v2.0a A#i 2026-01-27T19:13:02.317899databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T19:08:58.298867databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T19:03:31.232029databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T18:59:18.947210databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T18:56:27.828253databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T18:53:12.447633databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T18:31:42.035891databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T18:01:21.708968databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T17:46:23.163126databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T17:39:29.862988databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T17:32:40.051619databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T17:28:04.747800databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T17:20:29.518387databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T17:09:43.478375databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T17:04:27.059563databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T16:57:01.347666databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T16:36:56.110338databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T16:30:17.737576databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T16:25:13.922547databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T16:22:42.520595databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T16:17:12.712443databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T16:15:47.816936databaseINITIALIZEDDatabase initialized with feature version v2.0a
A#i 2026-01-27T16:10:03.058102databaseINITIALIZEDDatabase initialized with feature version v2.0a	A#i 2026-01-27T16:02:24.908087databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T15:56:49.321836databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T15:55:12.043326databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T15:53:00.003044databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T15:47:49.632080databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T15:45:10.629384databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T15:23:05.514201databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T15:23:00.226092databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T15:23:00.219092databaseINITIALIZEDDatabase initialized with feature version   (
   Fá ‚√§ÖfG(	ÍÀ¨çnO0Ú”¥ïvW8˙€ºù~_@!„ƒ•ÜgH)

Î
Ã
≠
é
o
P
1
	Û	‘	µ	ñ	w	X	9	˚‹Ωû`A"‰≈¶á                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   A2026-01-28T10:12:24.670789FA2026-01-28T10:08:36.360500EA2026-01-28T10:06:41.408696DA2026-01-28T09:37:19.092608CA2026-01-28T09:23:50.994343BA2026-01-28T09:13:12.097786AA2026-01-28T08:59:30.571202@A2026-01-28T08:43:00.104506?A2026-01-28T08:35:57.079832>A2026-01-28T08:27:34.322972=A2026-01-28T08:23:04.095066<A2026-01-28T07:59:20.262151;A2026-01-28T07:52:51.594754:A2026-01-28T07:49:14.6050069A2026-01-28T07:37:58.9903288A2026-01-28T07:25:00.3461427A2026-01-28T07:20:18.9556026A2026-01-28T07:03:16.1812805A2026-01-28T07:01:19.4119364A2026-01-28T06:54:59.4666273A2026-01-28T06:49:19.8820662A2026-01-28T06:30:26.2659111A2026-01-28T06:21:25.5712480A2026-01-28T06:15:25.403421/A2026-01-28T06:13:57.765125.A2026-01-28T06:06:08.121486-A2026-01-28T05:45:52.410455,A2026-01-28T05:34:18.194931+A2026-01-28T05:26:30.404089*A2026-01-28T05:19:10.353460)A2026-01-28T05:14:22.402138(A2026-01-28T05:05:41.987734'A2026-01-28T04:57:50.503194&A2026-01-28T04:52:50.763142%A2026-01-28T04:52:42.786021$A2026-01-28T04:47:37.468804#A2026-01-28T04:46:32.033460"A2026-01-28T04:37:17.459125!A2026-01-27T19:13:02.317899 A2026-01-27T19:08:58.298867A2026-01-27T19:03:31.232029A2026-01-27T18:59:18.947210A2026-01-27T18:56:27.828253A2026-01-27T18:53:12.447633A2026-01-27T18:31:42.035891A2026-01-27T18:01:21.708968A2026-01-27T17:46:23.163126A2026-01-27T17:39:29.862988A2026-01-27T17:32:40.051619A2026-01-27T17:28:04.747800A2026-01-27T17:20:29.518387A2026-01-27T17:09:43.478375A2026-01-27T17:04:27.059563A2026-01-27T16:57:01.347666A2026-01-27T16:36:56.110338A2026-01-27T16:30:17.737576A2026-01-27T16:25:13.922547A2026-01-27T16:22:42.520595A2026-01-27T16:17:12.712443A2026-01-27T16:15:47.816936A2026-01-27T16:10:03.058102
A2026-01-27T16:02:24.908087	A2026-01-27T15:56:49.321836A2026-01-27T15:55:12.043326A2026-01-27T15:53:00.003044A2026-01-27T15:47:49.632080A2026-01-27T15:45:10.629384A2026-01-27T15:23:05.514201A2026-01-27T15:23:00.226092A	2026-01-27T15:23:00.219092   ›    (›ÖfG(	À¨çnO0Ú”¥ïvW8˙€ºù~_@!„ƒ•ÜgH)

Î
Ã
≠
é
o
P
1
	Û	‘	µ	ñ	w	X	9	˚‹Ωû`A"‰≈¶áhI*ÏÕÆèpQ2Ù’∂óxY:¸›æüÄaB#Â∆ßàiJ+ÌŒØêqR3ı÷∑òyZ;Í                              A2026-01-28T10:14:07.814000{A2026-01-28T10:12:34.883000zA2026-01-28T10:08:46.837000yA2026-01-28T10:07:46.753000xA2026-01-28T10:07:45.361000wA2026-01-28T10:07:43.838000vA2026-01-28T10:07:42.087000uA2026-01-28T10:07:38.811000tA2026-01-28T10:07:33.381000sA2026-01-28T10:06:55.029000rA2026-01-28T09:42:33.373000qA2026-01-28T09:42:26.609000pA2026-01-28T09:42:22.440000oA2026-01-28T09:42:06.302000nA2026-01-28T09:42:03.034000mA2026-01-28T09:41:39.700000lA2026-01-28T09:35:43.555000kA2026-01-28T09:35:41.275000jA2026-01-28T09:35:39.165000iA2026-01-28T09:35:36.646000hA2026-01-28T09:35:33.885000gA2026-01-28T09:35:30.083000fA2026-01-28T09:35:28.003000eA2026-01-28T09:35:25.801000dA2026-01-28T09:35:23.275000cA2026-01-28T09:35:22.380000bA2026-01-28T09:35:19.816000aA2026-01-28T09:35:17.748000`A2026-01-28T09:35:14.221000_A2026-01-28T09:35:10.416000^A2026-01-28T09:35:07.399000]A2026-01-28T09:35:05.000000\A2026-01-28T09:35:01.038000[A2026-01-28T09:33:23.167000ZA2026-01-28T09:33:21.012000YA2026-01-28T09:33:16.484000XA2026-01-28T09:33:11.088000WA2026-01-28T09:33:08.934000VA2026-01-28T09:33:06.400000UA2026-01-28T09:33:03.557000TA2026-01-28T09:33:00.465000SA2026-01-28T09:32:57.886000RA2026-01-28T09:32:55.730000QA2026-01-28T09:32:53.557000PA2026-01-28T09:32:51.239000OA2026-01-28T09:32:48.486000NA2026-01-28T09:32:44.011000MA2026-01-28T09:31:27.140000LA2026-01-28T09:31:19.729000KA2026-01-28T09:31:07.825000JA2026-01-28T09:31:04.356000IA2026-01-28T09:31:01.901000HA2026-01-28T09:30:59.784000GA2026-01-28T09:30:57.436000FA2026-01-28T09:30:55.169000EA2026-01-28T09:30:53.000000DA2026-01-28T09:30:43.588000CA2026-01-28T09:28:30.647000BA2026-01-28T09:24:43.058000AA2026-01-28T08:59:42.530000@A2026-01-28T08:48:39.919000?A2026-01-28T08:48:36.899000>A2026-01-28T08:48:36.233000=A2026-01-28T08:47:59.438000<A2026-01-28T08:47:56.415000;A2026-01-28T08:47:55.705000:A2026-01-28T08:43:51.9800009A2026-01-28T08:36:52.3420008A2026-01-28T08:27:46.4380007A2026-01-28T08:23:17.2260006A2026-01-28T08:04:46.7180005A2026-01-28T08:00:50.6490004A2026-01-28T07:53:02.9670003A2026-01-28T07:03:27.1400002A2026-01-28T06:55:09.9320001A2026-01-28T06:50:55.2150000A2026-01-28T06:50:52.103000/A2026-01-28T06:49:41.724000.A2026-01-28T06:49:30.564000-A2026-01-28T06:41:34.642000,A2026-01-28T06:41:34.374000+A2026-01-28T06:40:21.292000*A2026-01-28T06:40:21.021000)A2026-01-28T06:31:50.779000(A2026-01-28T06:31:50.494000'A2026-01-28T06:31:05.488000&A2026-01-28T06:30:40.508000%A2026-01-28T06:22:21.102000$A2026-01-28T06:22:17.461000#A2026-01-28T06:21:37.495000"A2026-01-28T06:16:35.233000!A2026-01-28T06:16:15.146000 A2026-01-28T06:15:36.853000A2026-01-28T06:08:09.844000A2026-01-28T06:07:56.401000A2026-01-28T06:07:52.779000A2026-01-28T06:06:20.388000A2026-01-28T05:52:07.962000A2026-01-28T05:52:07.278000A2026-01-28T05:51:41.390000A2026-01-28T05:51:37.694000A2026-01-28T05:51:10.961000A2026-01-28T05:46:59.473000A2026-01-28T05:46:05.515000A2026-01-28T05:36:29.142000A2026-01-28T05:36:25.501000A2026-01-28T05:34:29.323000A2026-01-28T05:29:35.610000A2026-01-28T05:29:32.065000A2026-01-28T05:27:20.977000A2026-01-28T05:27:17.464000A2026-01-28T05:26:46.561000A2026-01-28T05:19:21.892000A2026-01-28T05:14:34.190000
A2099-01-01T00:00:00.000000	A2026-01-25T11:42:21.599026A2026-01-25T11:42:10.621540A2026-01-25T11:41:59.638980A2026-01-25T11:41:48.577341A2026-01-25T11:41:37.555290A2026-01-25T11:41:26.568723A2026-01-25T11:41:15.55   'A2026-01-28T08:59:42.530000@
   É` ¯ÔÊ›‘À¬π•úìäÅxof]TKB90'˙ÒËﬂ÷Õƒª≤©†óéÖ|sjaXOF=4+"˛ıÏ„⁄—»ø∂≠§õíâÄwne\SJA8/&˘Áﬁ’Ã√∫±®üñçÑ{ri`WNE<3*!˝ÙÎ‚Ÿ–«æµ¨£öëà~tj`Æ                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  	v2.0 É	v2.0 Ç	v2.0 Å	v2.0 Äv2.0v2.0~v2.0}v2.0|v2.0{v2.0zv2.0yv2.0xv2.0wv2.0vv2.0uv2.0tv2.0sv2.0rv2.0qv2.0pv2.0ov2.0nv2.0mv2.0lv2.0kv2.0jv2.0iv2.0hv2.0gv2.0fv2.0ev2.0dv2.0cv2.0bv2.0av2.0`v2.0_v2.0^v2.0]v2.0\v2.0[v2.0Zv2.0Yv2.0Xv2.0Wv2.0Vv2.0Uv2.0Tv2.0Sv2.0Rv2.0Qv2.0Pv2.0Ov2.0Nv2.0Mv2.0Lv2.0Kv2.0Jv2.0Iv2.0Hv2.0Gv2.0Fv2.0Ev2.0Dv2.0Cv2.0Bv2.0Av2.0@v2.0?v2.0>v2.0=v2.0<v2.0;v2.0:v2.09v2.08v2.07v2.06v2.05v2.04v2.03v2.02v2.01v2.00v2.0/v2.0.v2.0-v2.0,v2.0+v2.0*v2.0)v2.0(v2.0'v2.0&v2.0%v2.0$v2.0#v2.0"v2.0!v2.0 v2.0v2.0v2.0v2.0v2.0v2.0v2.0v2.0v2.0v2.0v2.0v2.0v2.0v2.0v2.0v2.0v2.0v2.0v2.0v2.0v2.0v2.0

v_test	v1.0v1.0v1.0v1.0v1.0v1.0v1.0	v1.0   Õ Õ                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   1+Afeature_versionv2.02026-01-27T15:22:59.899613
   Ï Ï                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  +feature_version   ˆ    )˚ˆ∆T‚p˛å
®
6	ƒ	R‡n¸ä¶4Õ[Èwì!Ø=ÀY Á u                                       p#A/A2026-01-28T06:31:05.482547INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:31:05.482808p"A/A2026-01-28T06:30:40.503400INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:30:40.503651p!A/A2026-01-28T06:22:21.097751INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:22:21.098095p A/A2026-01-28T06:22:17.455766INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:22:17.456034pA/A2026-01-28T06:21:37.491333INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:21:37.491586pA/A2026-01-28T06:16:35.227884INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:16:35.228144pA/A2026-01-28T06:16:15.140016INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:16:15.140190pA/A2026-01-28T06:15:36.849235INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:15:36.849381pA/A2026-01-28T06:08:09.839092INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:08:09.840028pA/A2026-01-28T06:07:56.395340INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:07:56.395814pA/A2026-01-28T06:07:52.773021INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:07:52.773440pA/A2026-01-28T06:06:20.382836INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:06:20.383282eAA2026-01-28T05:52:07.956548NORMALNEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:52:07.956974pA/A2026-01-28T05:52:07.272290INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:52:07.272815pA/A2026-01-28T05:51:41.386812INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:51:41.387039pA/A2026-01-28T05:51:37.688929INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:51:37.689420pA/A2026-01-28T05:51:10.956144INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:51:10.957109pA/A2026-01-28T05:46:59.469416INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:46:59.469846pA/A2026-01-28T05:46:05.509568INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:46:05.510585pA/A2026-01-28T05:36:29.135997INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:36:29.136656pA/A2026-01-28T05:36:25.496810INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:36:25.497118pA/A2026-01-28T05:34:29.319276INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:34:29.319789pA/A2026-01-28T05:29:35.606801INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:29:35.607071pA/A2026-01-28T05:29:32.060421INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:29:32.060701pA/A2026-01-28T05:27:20.972676INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:27:20.972921p
A/A2026-01-28T05:27:17.460019INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:27:17.460262p	A/A2026-01-28T05:26:46.556677INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:26:46.557134pA/A2026-01-28T05:19:21.887777INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:19:21.888094pA/A2026-01-28T05:14:34.186492INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:14:34.186948pA/A2026-01-28T05:06:22.347257INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:06:22.347694pA/A2026-01-28T05:05:53.247688INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:05:53.248079pA/A2026-01-28T04:58:10.268492INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T04:58:10.268805pA/A2026-01-28T04:54:41.121020INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T04:54:41.121020pA/A2026-01-28T04:54:36.976297INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T04:54:36.976297pA/A2026-01-28T04:52:54.680286INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T04:52    F   #
   LÕ ‚√§ÖfG(	ÍÀ¨çnO0Ú”¥ïvW8˙€ºù~_@!„ƒ•ÜgH)

Î
Ã
≠
é
o
P
1
	Û	‘	µ	ñ	w	X	9	˚‹Ωû`A"‰≈¶áhI*ÏÕ                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             A2026-01-28T10:15:24.904453LA2026-01-28T10:15:21.821432KA2026-01-28T10:14:54.845844JA2026-01-28T10:14:51.746194IA2026-01-28T10:14:51.000798HA2026-01-28T10:14:35.767075GA2026-01-28T10:14:11.705391FA2026-01-28T10:14:08.605170EA2026-01-28T10:14:07.808860DA2026-01-28T10:12:34.877996CA2026-01-28T09:42:33.366206BA2026-01-28T09:42:26.603947AA2026-01-28T09:42:22.435366@A2026-01-28T09:42:06.296875?A2026-01-28T09:42:03.029383>A2026-01-28T09:41:39.695115=A2026-01-28T08:48:39.914133<A2026-01-28T08:48:36.895613;A2026-01-28T08:48:36.227696:A2026-01-28T08:47:59.4322779A2026-01-28T08:47:56.4079268A2026-01-28T08:47:55.6993077A2026-01-28T08:43:51.9757216A2026-01-28T08:36:52.3366055A2026-01-28T08:27:46.4332874A2026-01-28T08:23:17.2211193A2026-01-28T08:04:46.7129532A2026-01-28T08:00:50.6452511A2026-01-28T07:53:02.9618340A2026-01-28T07:03:27.134329/A2026-01-28T06:55:09.927529.A2026-01-28T06:50:55.209495-A2026-01-28T06:50:52.097380,A2026-01-28T06:49:41.719288+A2026-01-28T06:49:30.559257*A2026-01-28T06:41:34.638653)A2026-01-28T06:41:34.368604(A2026-01-28T06:40:21.288492'A2026-01-28T06:40:21.015789&A2026-01-28T06:31:50.774209%A2026-01-28T06:31:50.490038$A2026-01-28T06:31:05.482547#A2026-01-28T06:30:40.503400"A2026-01-28T06:22:21.097751!A2026-01-28T06:22:17.455766 A2026-01-28T06:21:37.491333A2026-01-28T06:16:35.227884A2026-01-28T06:16:15.140016A2026-01-28T06:15:36.849235A2026-01-28T06:08:09.839092A2026-01-28T06:07:56.395340A2026-01-28T06:07:52.773021A2026-01-28T06:06:20.382836A2026-01-28T05:52:07.956548A2026-01-28T05:52:07.272290A2026-01-28T05:51:41.386812A2026-01-28T05:51:37.688929A2026-01-28T05:51:10.956144A2026-01-28T05:46:59.469416A2026-01-28T05:46:05.509568A2026-01-28T05:36:29.135997A2026-01-28T05:36:25.496810A2026-01-28T05:34:29.319276A2026-01-28T05:29:35.606801A2026-01-28T05:29:32.060421A2026-01-28T05:27:20.972676A2026-01-28T05:27:17.460019
A2026-01-28T05:26:46.556677	A2026-01-28T05:19:21.887777A2026-01-28T05:14:34.186492A2026-01-28T05:06:22.347257A2026-01-28T05:05:53.247688A2026-01-28T04:58:10.268492A2026-01-28T04:54:41.121020A2026-01-28T04:54:36.976297A	2026-01-28T04:52:54.680286‰ É É¡èíx
¶⁄	ª
k	Y‰z@55                                                                                                                                                                                                                                                                             CW1 indexsqlite_autoindex_research_analytics_1research_analyticsÅC''ÇEtabledatabase_infodatabase_infoCREATE TABLE database_info (
                    key TEXT PRIMARY KEY,
                    value TEXT,
                    updated_at TEXT
                )9M' indexsqlite_autoindex_database_info_1database_info‰     å^++òstablemarket_featuresmarket_featuresCREATE TABLE market_features (
    timestamp TEXT NOT NULL,
    feature_version TEXT NOT NULL,

    -- Option structure
    put_call_ratio REAL NOT NULL,
    oi_delta REAL NOT NULL,
    oi_concentration REAL NOT NULL,
    atm_iv REAL NOT NULL,
    iv_skew REAL NOT NULL,

    -- Price & flow
    vwap_distance REAL NOT NULL,
    price_momentum REAL NOT NULL,
    volume_ratio REAL NOT NULL,

    -- Breadth
    ccc_value REAL NOT NULL,
    ccc_slope REAL NOT NULL,

    -- Time context
    time_to_expiry_minutes INTEGER NOT NULL,

    -- ML target (NULL in live mode)
    future_return_5m REAL, oi_velocity REAL DEFAULT 0.0, oi_velocity_ma REAL DEFAULT 0.0, oi_velocity_std REAL DEFAULT 0.0, oi_regime_expansive REAL DEFAULT 0.0, oi_regime_constricted REAL DEFAULT 0.0, net_gamma REAL DEFAULT 0.0, gamma_regime_positive REAL DEFAULT 0.0, gamma_regime_negative REAL DEFAULT 0.0, gamma_flip_distance REAL DEFAULT 0.0, max_gamma_strike_distance REAL DEFAULT 0.0, wall_strength REAL DEFAULT 0.0, wall_defense_score REAL DEFAULT 0.0, trap_probability REAL DEFAULT 0.0, price_oi_divergence REAL DEFAULT 0.0, price_gamma_divergence REAL DEFAULT 0.0, divergence_score REAL DEFAULT 0.0, has_divergence REAL DEFAULT 0.0, max_pain_distance REAL DEFAULT 0.0, vix_smile REAL DEFAULT 0.0, skewness REAL DEFAULT 0.0, spring_detection REAL DEFAULT 0.0, upthrust_detection REAL DEFAULT 0.0, accumulation_score REAL DEFAULT 0.0, gamma_wall_interaction REAL DEFAULT 0.0, velocity_divergence_composite REAL DEFAULT 0.0, trap_gamma_composite REAL DEFAULT 0.0,

    PRIMARY KEY (timestamp, feature_version)
)  ´3+Åindexidx_feature_versionmarket_featuresCREATE INDEX idx_feature_version ON market_features(feature_version)`'+}indexidx_timestampmarket_featuresCREATE INDEX idx_timestamp ON market_features(timestamp)Å-	''Çtablesystem_healthsystem_health
CREATE TABLE system_health (
    timestamp TEXT PRIMARY KEY,
    component TEXT NOT NULL,
    status TEXT NOT NULL,
    message TEXT
)  :M' indexsqlite_autoindex_system_health_1system_healthÇ1))Ñtablemodel_registrymodel_registryCREATE TABLE model_registry (
    model_version TEXT PRIMARY KEY,
    feature_version TEXT NOT NULL,

    algorithm TEXT NOT NULL,
    trained_on_start TEXT NOT NULL,
    trained_on_end TEXT NOT NULL,

    metrics_json TEXT NOT NULL,
    created_at TEXT NOT NULL
)  ÀO) indexsqlite_autoindex_model_registry_1model_registry	Ç^ÖtabletradestradesCREATE TABLE trades (
    trade_id TEXT PRIMARY KEY,
    signal_id TEXT NOT NULL,

    entry_price REAL NOT NULL,
    exit_price REAL,
    quantity INTEGER NOT NULL,

    stop_loss REAL,
    take_profit REAL,

    pnl REAL,
    executed_at TEXT,
    closed_at TEXT,

    FOREIGN KEY(signal_id) REFERENCES signals(signal_id)
)+? indexsqlite_autÑ áWtablesignalssignalsCREATE TABLE signals (
    signal_id TEXT PRIMARY KEY,
    timestamp TEXT NOT NULL,

    feature_version TEXT NOT NULL,
    model_version TEXT NOT NULL,

    signal_type TEXT CHECK(signal_type IN ('BUY','SELL','NEUTRAL')) NOT NULL,
    confidence REAL NOT NULL,

    market_state TEXT NOT NULL,
    rationale TEXT,

    expiry_time TEXT NOT NULL,
    status TEXT CHECK(status IN ('NEW','VALIDATED','EXPIRED','EVALUATED')) NOT NULL,

    created_at TEXT NOT NULL
, pnl REAL DEFAULT 0.0)-A indexsqlite_autoindex_signals_1signals=Q+ indexsqlite_autoindex_market_features_1market_featuresQ H ür>H¥?
y
>Ì®Bá%√jì                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ÅM	''ÇYtablesystem_healthsystem_health
CREATE TABLE system_health (
    timestamp TEXT PRIMARY KEY,
    component TEXT NOT NULL,
    status TEXT NOT NULL,
    message TEXT
, details_json TEXT DEFAULT '{}')y51Åindexidx_analytics_regimeresearch_analyticsCREATE INDEX idx_analytics_regime ON research_analytics(market_regime){;1Åindexidx_analytics_timestampresearch_analyticsCREATE INDEX idx_analytics_timestamp ON research_analytics(timestamp)X-windexidx_signals_typesignalsCREATE INDEX idx_signals_type ON signals(signal_type)W1qindexidx_signals_statussignalsCREATE INDEX idx_signals_status ON signals(status)`7}indexidx_signals_timestampsignalsCREATE INDEX idx_signals_timestamp ON signals(timestamp)`'+}indexidx_net_gammamarket_featuresCREATE INDEX idx_net_gamma ON market_features(net_gamma)g++Åindexidx_oi_velocitymarket_featuresCREATE INDEX idx_oi_velocity ON market_features(oi_velocity)P++Ytablesqlite_sequencesqlite_sequenceCREATE TABLE sqlite_sequence(name,seq)Çc++Ñ}tablefeature_historyfeature_historyCREATE TABLE feature_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                feature_name TEXT NOT NULL,
                old_value REAL,
                new_value REAL,
                change_pct REAL,
                created_at TEXT
            )CW1 indexsqlite_autoindex_research_analytics_1research_analyticsÑN11àGtableresearch_analyticsresearch_analyticsCREATE TABLE research_analytics (
                timestamp TEXT PRIMARY KEY,
                oi_velocity REAL,
                oi_regime TEXT,
                net_gamma REAL,
                gamma_regime TEXT,
                wall_strength REAL,
                trap_probability REAL,
                divergence_score REAL,
                market_regime TEXT,
                confidence REAL,
                insights_json TEXT,
                walls_json TEXT,
                traps_json TEXT,
                created_at TEXT
            )9M' indexsqlite_autoindex_database_info_1database_infoÅC''ÇEtabledatabase_infodatabase_infoCREATE TABLE database_info (
                    key TEXT PRIMARY KEY,
                    value TEXT,
                    updated_at TEXT
                )s3+Åindexidx_feature_versionmarket_featuresCREATE INDEX idx_feature_version ON market_features(feature_version)`'+}indexidx_timestampmarket_featuresCREATE INDEX idx_timestamp ON market_features(timestamp)9
M' indexsqlite_autoindex_system_health_1system_health   ∞''Çtablesystem_healthsystem_health
CREATE TABLE system_health (
    timestamp TEXT PRIMARY KEY,
    component TEXT NOT NULL,
    status TEXT NOT NULL,
    message TEXT
);O) indexsqlite_autoindex_model_registry_1model_registry	Ç1))Ñtablemodel_registrymodel_registryCREATE TABLE model_registry (
    model_version TEXT PRIMARY KEY,
    feature_version TEXT NOT NULL,

    algorithm TEXT NOT NULL,
    trained_on_start TEXT NOT NULL,
    trained_on_end TEXT NOT NULL,

    metrics_json TEXT NOT NULL,
    created_at TEXT NOT NULL
)+? indexsqlite_autoindex_trades_1tradesÇ^ÖtabletradestradesCREATE TABLE trades (
    trade_id TEXT PRIMARY KEY,
    signal_id TEXT NOT NULL,

    entry_price REAL NOT NULL,
    exit_price REAL,
    quantity INTEGER NOT NULL,

    stop_loss REAL,
    take_profit REAL,

    pnl REAL,
    executed_at TEXT,
    closed_at TEXT,

    FOREIGN KEY(signal_id) REFERENCES signals(signal_id)
)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
   É	û ¸˜ÚÌË„ﬁŸ‘«∫†ì≠ÜylR_E8+˜Í√–›∂ú©h[NA4' ÛèÇuÊŸÃø≤•òã~qd0#WJ=	¸Ôzm‚’»ªÆ°îá`SF9,¯Îﬁ—ƒ∑vi\OB5(™ùêÉ
Ù
Á
⁄
Õ
¿
≥
¶
ô
å

r
e
X
K
>
1
$


	˝		„	÷	»	∫	¨	û                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                @R¶ÌíB1ß É@R¶ÌíB1ß Ç@R¶ÌíB1ß Å@R¶ÌíB1ß Ä@R¶ÌíB1ß@R¶ÌíB1ß~@R¶ÌíB1ß}@R¶ÌíB1ß|@R¶ÌíB1ß{@R¶ÌíB1ßz@R¶ÌíB1ßy@R¶ÌíB1ßx@R¶ÌíB1ßw@R¶ÌíB1ßv@R¶ÌíB1ßu@R¶ÌíB1ßt@R¶ÌíB1ßs@R¶ÌíB1ßr@P≈¶duÎˇq@P≈¶duÎˇp@P≈¶duÎˇo@P≈°]Cn@P≈°]Cm@P√ŒÛ¿Êl@P/\@µÍsk@P.úI/Dqj@P-A√%≤i@P$:;cxëh@PÕQÿö†g@P2˜GT„f@P2˜GT„e@P2˜GT„d@P2˜GT„c@P2˜GT„b@P2˜GT„a@P2˜GT„`@P2˜GT„_@P3Löc_^@P3Löc_]@P3Löc_\@P3Löc_[@P*±Ï{GZ@P*±Ï{GY@P*FhGX@P*FhGW@P%\ΩoÓV@P%\ΩoÓU@P%\ΩoÓT@PÛƒ&S@PˇG ÔR@PFÍYbQ@PƒÙ˝Ì_P@P1ëºO@PÌ ˛N@PÜZ˜¥lM@O±Î3mQNL@O±Î3mQNK@O±ÎŸZFJ@O±ÎŸZFI@O±ÎŸZFH@O±ÎŸZFG@O±ÎŸZFF@O±ÎŸZFE@O±ÎŸZFD@O±ÎŸZFC@O`Y∆Ú∑€B@O	ıko∂cA@Mº\tΩ@@MÅÄŸ_Ô,?@MÅÅoã≈>@MÅÅoã≈=@MÅ´ªòI<@MÅ´ªòI;@MÅ™E û+:@Mx˙Ú‡≤9@M_®‘<Í8@M,]∂ŒeÃ7@L˜U<–“6@L+%XwTû5@L I”àöc4@Kü›¶Sc;3@K1¯‹â2@J˘ŒÛZı1@JıOh¸{0@JıOh¸{/@J‡B¡W.@J‡B¡W-@JÏÂ#˚Á,@JÏÂ#˚Á+@J›`› ‹*@J›`› ‹)@J’†ùê¸7(@J’a>†ù”'@J≈ì´±ßS&@J≈ì´±ßS%@J‡˘∏˛º$@J‡˘∏˛º#@J‡˜˜¶ó"@J≈f YÍﬂ!@J≈gYÜ≥ˆ @J≈coM±Ô@Jmü†Í!Í@Jm» ü‚@JmŒ!@J7v3›,$@I¨N¡RøW@I¨N¡RøW@I¨N¡RøW@I¨N¡RøW@I¨N?6Í3@IÖ59îõú@IuÉËÖ1@I>úJg¨V@I>ù‰ùD‡@I<a•Q@Ië‚ı≥@Ië‚ı≥@IP–¿c@IPI.&-@I^€ @Iöe∑ïÃ@HÔámü†
		
   Én ¸˜ÚÌË„ﬁŸ‘œ ≈¿ª∂±¨ß¢ùòìéâÑzupkfa\WRMHC>94/*% ˝¯ÛÓÈ‰ﬂ⁄’–À∆¡º∑≤≠®£ûôîèäÖÄ{vqlgb]XSNID?:50+&!˛˘ÙÔÍÂ‡€÷—Ã«¬Ω∏≥Æ©§üöïêãÜÄztn                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 É Ç Å Ä~}|{zyxwvutsrqponmlkjihgfedcba`_^]\[ZYXWVUTSRQPONMLKJIHGFEDCBA@?>=<;:9876543210/.-,+*)('&%$#"! 
		
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              
   LÕ ‚√§ÖfG(	ÍÀ¨çnO0Ú”¥ïvW8˙€ºù~_@!„ƒ•ÜgH)

Î
Ã
≠
é
o
P
1
	Û	‘	µ	ñ	w	X	9	˚‹Ωû`A"‰≈¶áhI*ÏÕ                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             A2026-01-28T10:15:24.904453LA2026-01-28T10:15:21.821432KA2026-01-28T10:14:54.845844JA2026-01-28T10:14:51.746194IA2026-01-28T10:14:51.000798HA2026-01-28T10:14:35.767075GA2026-01-28T10:14:11.705391FA2026-01-28T10:14:08.605170EA2026-01-28T10:14:07.808860DA2026-01-28T10:12:34.877996CA2026-01-28T09:42:33.366206BA2026-01-28T09:42:26.603947AA2026-01-28T09:42:22.435366@A2026-01-28T09:42:06.296875?A2026-01-28T09:42:03.029383>A2026-01-28T09:41:39.695115=A2026-01-28T08:48:39.914133<A2026-01-28T08:48:36.895613;A2026-01-28T08:48:36.227696:A2026-01-28T08:47:59.4322779A2026-01-28T08:47:56.4079268A2026-01-28T08:47:55.6993077A2026-01-28T08:43:51.9757216A2026-01-28T08:36:52.3366055A2026-01-28T08:27:46.4332874A2026-01-28T08:23:17.2211193A2026-01-28T08:04:46.7129532A2026-01-28T08:00:50.6452511A2026-01-28T07:53:02.9618340A2026-01-28T07:03:27.134329/A2026-01-28T06:55:09.927529.A2026-01-28T06:50:55.209495-A2026-01-28T06:50:52.097380,A2026-01-28T06:49:41.719288+A2026-01-28T06:49:30.559257*A2026-01-28T06:41:34.638653)A2026-01-28T06:41:34.368604(A2026-01-28T06:40:21.288492'A2026-01-28T06:40:21.015789&A2026-01-28T06:31:50.774209%A2026-01-28T06:31:50.490038$A2026-01-28T06:31:05.482547#A2026-01-28T06:30:40.503400"A2026-01-28T06:22:21.097751!A2026-01-28T06:22:17.455766 A2026-01-28T06:21:37.491333A2026-01-28T06:16:35.227884A2026-01-28T06:16:15.140016A2026-01-28T06:15:36.849235A2026-01-28T06:08:09.839092A2026-01-28T06:07:56.395340A2026-01-28T06:07:52.773021A2026-01-28T06:06:20.382836A2026-01-28T05:52:07.956548A2026-01-28T05:52:07.272290A2026-01-28T05:51:41.386812A2026-01-28T05:51:37.688929A2026-01-28T05:51:10.956144A2026-01-28T05:46:59.469416A2026-01-28T05:46:05.509568A2026-01-28T05:36:29.135997A2026-01-28T05:36:25.496810A2026-01-28T05:34:29.319276A2026-01-28T05:29:35.606801A2026-01-28T05:29:32.060421A2026-01-28T05:27:20.972676A2026-01-28T05:27:17.460019
A2026-01-28T05:26:46.556677	A2026-01-28T05:19:21.887777A2026-01-28T05:14:34.186492A2026-01-28T05:06:22.347257A2026-01-28T05:05:53.247688A2026-01-28T04:58:10.268492A2026-01-28T04:54:41.121020A2026-01-28T04:54:36.976297A	2026-01-28T04:52:54.680286
   Lf bıÈ›—≈π≠°ïâ}qeYMA5)˘Ì·’…Ω±•ôçÅui]QE9-!	˝ÒÂŸÕ¡µ©ùëÖyVJ>2&ˆÍﬁ“∆∫Æ¢ñä~rf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NEUTRALLNEUTRALKNEUTRALJNEUTRALINEUTRALHNEUTRALGNEUTRALFNEUTRALENEUTRALDNEUTRALCNEUTRALBNEUTRALANEUTRAL@NEUTRAL?NEUTRAL>NEUTRAL=NEUTRAL<NEUTRAL;NEUTRAL:NEUTRAL9NEUTRAL81DIVERGENCE_BULLISH7NEUTRAL6NEUTRAL5NEUTRAL4NEUTRAL3NEUTRAL2NEUTRAL1NEUTRAL0NEUTRAL/NEUTRAL.NEUTRAL-NEUTRAL,NEUTRAL+NEUTRAL*NEUTRAL)NEUTRAL(NEUTRAL'NEUTRAL&NEUTRAL%NEUTRAL$NEUTRAL#NEUTRAL"NEUTRAL!NEUTRAL NEUTRALNEUTRALNEUTRALNEUTRALNEUTRALNEUTRALNEUTRALNEUTRALNEUTRALNEUTRALNEUTRALNEUTRALNEUTRALNEUTRALNEUTRALNEUTRALNEUTRALNEUTRALNEUTRALNEUTRALNEUTRALNEUTRAL
NEUTRAL	NEUTRALNEUTRALNEUTRALNEUTRALNEUTRALNEUTRALNEUTRAL
	NEUTRAL   ( à ù:◊tÆKËÖ"ø\
˘
ñ
3	–	m	
ßD·~∏UÚè,…f†=⁄w±N Î à                                                a(A#i 2026-01-28T05:14:22.402138databaseINITIALIZEDDatabase initialized with feature version v2.0a'A#i 2026-01-28T05:05:41.987734databaseINITIALIZEDDatabase initialized with feature version v2.0a&A#i 2026-01-28T04:57:50.503194databaseINITIALIZEDDatabase initialized with feature version v2.0a%A#i 2026-01-28T04:52:50.763142databaseINITIALIZEDDatabase initialized with feature version v2.0a$A#i 2026-01-28T04:52:42.786021databaseINITIALIZEDDatabase initialized with feature version v2.0a#A#i 2026-01-28T04:47:37.468804databaseINITIALIZEDDatabase initialized with feature version v2.0a"A#i 2026-01-28T04:46:32.033460databaseINITIALIZEDDatabase initialized with feature version v2.0a!A#i 2026-01-28T04:37:17.459125databaseINITIALIZEDDatabase initialized with feature version v2.0a A#i 2026-01-27T19:13:02.317899databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T19:08:58.298867databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T19:03:31.232029databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T18:59:18.947210databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T18:56:27.828253databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T18:53:12.447633databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T18:31:42.035891databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T18:01:21.708968databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T17:46:23.163126databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T17:39:29.862988databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T17:32:40.051619databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T17:28:04.747800databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T17:20:29.518387databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T17:09:43.478375databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T17:04:27.059563databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T16:57:01.347666databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T16:36:56.110338databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T16:30:17.737576databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T16:25:13.922547databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T16:22:42.520595databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T16:17:12.712443databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T16:15:47.816936databaseINITIALIZEDDatabase initialized with feature version v2.0a
A#i 2026-01-27T16:10:03.058102databaseINITIALIZEDDatabase initialized with feature version v2.0a	A#i 2026-01-27T16:02:24.908087databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T15:56:49.321836databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T15:55:12.043326databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T15:53:00.003044databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T15:47:49.632080databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T15:45:10.629384databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T15:23:05.514201databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T15:23:00.226092databaseINITIALIZEDDatabase initialized with feature version v2.0aA#i 2026-01-27T15:23:00.219092databaseINITIALIZEDDatabase initialized with feature version v2.0   f ù:◊tÆKËÖ"ø\
˘
ñ
3	–	m	
ßD·~∏UÚè,…f                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  aFA#i 2026-01-28T10:12:24.670789databaseINITIALIZEDDatabase initialized with feature version v2.0aEA#i 2026-01-28T10:08:36.360500databaseINITIALIZEDDatabase initialized with feature version v2.0aDA#i 2026-01-28T10:06:41.408696databaseINITIALIZEDDatabase initialized with feature version v2.0aCA#i 2026-01-28T09:37:19.092608databaseINITIALIZEDDatabase initialized with feature version v2.0aBA#i 2026-01-28T09:23:50.994343databaseINITIALIZEDDatabase initialized with feature version v2.0aAA#i 2026-01-28T09:13:12.097786databaseINITIALIZEDDatabase initialized with feature version v2.0a@A#i 2026-01-28T08:59:30.571202databaseINITIALIZEDDatabase initialized with feature version v2.0a?A#i 2026-01-28T08:43:00.104506databaseINITIALIZEDDatabase initialized with feature version v2.0a>A#i 2026-01-28T08:35:57.079832databaseINITIALIZEDDatabase initialized with feature version v2.0a=A#i 2026-01-28T08:27:34.322972databaseINITIALIZEDDatabase initialized with feature version v2.0a<A#i 2026-01-28T08:23:04.095066databaseINITIALIZEDDatabase initialized with feature version v2.0a;A#i 2026-01-28T07:59:20.262151databaseINITIALIZEDDatabase initialized with feature version v2.0a:A#i 2026-01-28T07:52:51.594754databaseINITIALIZEDDatabase initialized with feature version v2.0a9A#i 2026-01-28T07:49:14.605006databaseINITIALIZEDDatabase initialized with feature version v2.0a8A#i 2026-01-28T07:37:58.990328databaseINITIALIZEDDatabase initialized with feature version v2.0a7A#i 2026-01-28T07:25:00.346142databaseINITIALIZEDDatabase initialized with feature version v2.0a6A#i 2026-01-28T07:20:18.955602databaseINITIALIZEDDatabase initialized with feature version v2.0a5A#i 2026-01-28T07:03:16.181280databaseINITIALIZEDDatabase initialized with feature version v2.0a4A#i 2026-01-28T07:01:19.411936databaseINITIALIZEDDatabase initialized with feature version v2.0a3A#i 2026-01-28T06:54:59.466627databaseINITIALIZEDDatabase initialized with feature version v2.0a2A#i 2026-01-28T06:49:19.882066databaseINITIALIZEDDatabase initialized with feature version v2.0a1A#i 2026-01-28T06:30:26.265911databaseINITIALIZEDDatabase initialized with feature version v2.0a0A#i 2026-01-28T06:21:25.571248databaseINITIALIZEDDatabase initialized with feature version v2.0a/A#i 2026-01-28T06:15:25.403421databaseINITIALIZEDDatabase initialized with feature version v2.0a.A#i 2026-01-28T06:13:57.765125databaseINITIALIZEDDatabase initialized with feature version v2.0a-A#i 2026-01-28T06:06:08.121486databaseINITIALIZEDDatabase initialized with feature version v2.0a,A#i 2026-01-28T05:45:52.410455databaseINITIALIZEDDatabase initialized with feature version v2.0a+A#i 2026-01-28T05:34:18.194931databaseINITIALIZEDDatabase initialized with feature version v2.0a*A#i 2026-01-28T05:26:30.404089databaseINITIALIZEDDatabase initialized with feature version v2.0a)A#i 2026-01-28T05:19:10.353460databaseINITIALIZEDDatabase initialized with feature version v2.0   # u é™8∆T‚p˛å
®
6	ƒ	R‡n¸ä¶4Õ[Èwì!Ø=ÀY Á u                                       p#A/A2026-01-28T06:31:05.482547INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:31:05.482808p"A/A2026-01-28T06:30:40.503400INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:30:40.503651p!A/A2026-01-28T06:22:21.097751INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:22:21.098095p A/A2026-01-28T06:22:17.455766INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:22:17.456034pA/A2026-01-28T06:21:37.491333INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:21:37.491586pA/A2026-01-28T06:16:35.227884INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:16:35.228144pA/A2026-01-28T06:16:15.140016INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:16:15.140190pA/A2026-01-28T06:15:36.849235INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:15:36.849381pA/A2026-01-28T06:08:09.839092INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:08:09.840028pA/A2026-01-28T06:07:56.395340INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:07:56.395814pA/A2026-01-28T06:07:52.773021INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:07:52.773440pA/A2026-01-28T06:06:20.382836INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:06:20.383282eAA2026-01-28T05:52:07.956548NORMALNEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:52:07.956974pA/A2026-01-28T05:52:07.272290INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:52:07.272815pA/A2026-01-28T05:51:41.386812INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:51:41.387039pA/A2026-01-28T05:51:37.688929INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:51:37.689420pA/A2026-01-28T05:51:10.956144INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:51:10.957109pA/A2026-01-28T05:46:59.469416INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:46:59.469846pA/A2026-01-28T05:46:05.509568INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:46:05.510585pA/A2026-01-28T05:36:29.135997INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:36:29.136656pA/A2026-01-28T05:36:25.496810INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:36:25.497118pA/A2026-01-28T05:34:29.319276INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:34:29.319789pA/A2026-01-28T05:29:35.606801INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:29:35.607071pA/A2026-01-28T05:29:32.060421INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:29:32.060701pA/A2026-01-28T05:27:20.972676INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:27:20.972921p
A/A2026-01-28T05:27:17.460019INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:27:17.460262p	A/A2026-01-28T05:26:46.556677INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:26:46.557134pA/A2026-01-28T05:19:21.887777INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:19:21.888094pA/A2026-01-28T05:14:34.186492INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:14:34.186948pA/A2026-01-28T05:06:22.347257INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:06:22.347694pA/A2026-01-28T05:05:53.247688INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T05:05:53.248079pA/A2026-01-28T04:58:10.268492INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T04:58:10.268805pA/A2026-01-28T04:54:41.121020INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T04:54:41.121020pA/A2026-01-28T04:54:36.976297INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T04:54:36.976297pA/A2026-01-28T04:52:54.680286INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T04:52:54.680286   # Ä é≠F‘m˚â•3
¡
O	›	k˘á£ Æ<Õ[Èwì!Ø=Àd Ú Ä                                                  pFA/A2026-01-28T10:14:11.705391INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T10:14:11.705862pEA/A2026-01-28T10:14:08.605170INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T10:14:08.605709eDAA2026-01-28T10:14:07.808860NORMALNEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T10:14:07.809371pCA/A2026-01-28T10:12:34.877996INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T10:12:34.878484pBA/A2026-01-28T09:42:33.366206INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T09:42:33.366461pAA/A2026-01-28T09:42:26.603947INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T09:42:26.604437p@A/A2026-01-28T09:42:22.435366INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T09:42:22.435845p?A/A2026-01-28T09:42:06.296875INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T09:42:06.297321p>A/A2026-01-28T09:42:03.029383INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T09:42:03.029628p=A/A2026-01-28T09:41:39.695115INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T09:41:39.695664p<A/A2026-01-28T08:48:39.914133INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T08:48:39.914587p;A/A2026-01-28T08:48:36.895613INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T08:48:36.895957m:AA2026-01-28T08:48:36.227696?öüæv»¥9NORMALNEUTRALNEUTRAL?œù≤-V[][][]2026-01-28T08:48:36.228225p9A/A2026-01-28T08:47:59.432277INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T08:47:59.432698p8A/A2026-01-28T08:47:56.407926INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T08:47:56.408995Å 7A1A2026-01-28T08:47:55.699307?ˆüæv»¥9NORMALNEUTRAL?Ê†ûf;ÕDIVERGENCE_BULLISH?⁄∞ ƒõ•„[][][]2026-01-28T08:47:55.699568p6A/A2026-01-28T08:43:51.975721INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T08:43:51.976227p5A/A2026-01-28T08:36:52.336605INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T08:36:52.337049p4A/A2026-01-28T08:27:46.433287INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T08:27:46.433754p3A/A2026-01-28T08:23:17.221119INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T08:23:17.221580p2A/A2026-01-28T08:04:46.712953INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T08:04:46.713194p1A/A2026-01-28T08:00:50.645251INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T08:00:50.645738p0A/A2026-01-28T07:53:02.961834INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T07:53:02.962316p/A/A2026-01-28T07:03:27.134329INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T07:03:27.134566p.A/A2026-01-28T06:55:09.927529INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:55:09.927697p-A/A2026-01-28T06:50:55.209495INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:50:55.209746p,A/A2026-01-28T06:50:52.097380INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:50:52.097630p+A/A2026-01-28T06:49:41.719288INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:49:41.719514p*A/A2026-01-28T06:49:30.559257INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:49:30.559508e)AA2026-01-28T06:41:34.638653NORMALNEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:41:34.638915p(A/A2026-01-28T06:41:34.368604INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:41:34.368851e'AA2026-01-28T06:40:21.288492NORMALNEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:40:21.288754p&A/A2026-01-28T06:40:21.015789INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:40:21.016034m%AA2026-01-28T06:31:50.774209øˆüæv»¥9NORMALNEUTRALNEUTRAL?÷ìtºj[][][]2026-01-28T06:31:50.774479p$A/A2026-01-28T06:31:50.490038INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T06:31:50.490194   * p ù:◊tÆKË∑X˘ö;
‹
}
	ø	`	¢C‰Ö&«h	™KÏç.œp≤SÙï6 ◊ p                    e*)A 2026-01-28T06:40:21.292000v2.0ñˆä?a,?…ΩÆ#ø>ÀﬂŸWƒ) /@J›`› ‹]))A 2026-01-28T06:40:21.021000v2.0ñˆä?a,?…ΩÆ# /@J›`› ‹]()A 2026-01-28T06:31:50.779000v2.0ìÛm?fxÚÄ≠ 8@J’†ùê¸7]')A 2026-01-28T06:31:50.494000v2.0ìÛm?fxÚÄ≠ 8@J’a>†ù”]&)A 2026-01-28T06:31:05.488000v2.0ík¶?eº‘Y: 8@J≈ì´±ßS]%)A 2026-01-28T06:30:40.508000v2.0ík¶?eC|¬◊l 9@J≈ì´±ßS]$)A 2026-01-28T06:22:21.102000v2.0ä±6?d(üoÏ A@J‡˘∏˛º]#)A 2026-01-28T06:22:17.461000v2.0ä±6?dA \∞ê A@J‡˘∏˛º]")A 2026-01-28T06:21:37.495000v2.0ä∞ı?b:≈ÌWc B@J‡˜˜¶ó]!)A 2026-01-28T06:16:35.233000v2.0ÅÊU?^ù±»\è9 G@J≈f YÍﬂ] )A 2026-01-28T06:16:15.146000v2.0ÅÊU?_;q’‚√n G@J≈gYÜ≥ˆ])A 2026-01-28T06:15:36.853000v2.0ÅÊU?VP7Œ≈°s H@J≈coM±Ô])A 2026-01-28T06:08:09.844000v2.0iƒä?aÉmJz^¶ O@Jmü†Í!Í])A 2026-01-28T06:07:56.401000v2.0iƒä?b°ﬁ:ù6 P@Jm» ü‚])A 2026-01-28T06:07:52.779000v2.0iƒä?cÙ)F¨^ P@JmŒ!])A 2026-01-28T06:06:20.388000v2.0`àÌ?fÌ˜‹K‘\ Q@J7v3›,$])A 2026-01-28T05:52:07.962000v2.0=Å6?eïhñU›Ç _@I¨N¡RøW])A 2026-01-28T05:52:07.278000v2.0=Å6?eïhñU›Ç _@I¨N¡RøW])A 2026-01-28T05:51:41.390000v2.0=Å6?fd˘µö« `@I¨N¡RøW])A 2026-01-28T05:51:37.694000v2.0=Å6?f3(£‹j `@I¨N¡RøW])A 2026-01-28T05:51:10.961000v2.0=Äı?ecó∫|S% `@I¨N?6Í3])A 2026-01-28T05:46:59.473000v2.00VZ?e≤w¡?m? e@IÖ59îõú])A 2026-01-28T05:46:05.515000v2.0&l?f~ÑˇRé e@IuÉËÖ1])A 2026-01-28T05:36:29.142000v2.0£?fW»áY o@I>úJg¨V])A 2026-01-28T05:36:25.501000v2.0£?e◊‘f"î≤ o@I>ù‰ùD‡])A 2026-01-28T05:34:29.323000v2.0Mè?c˛î=è˘a q@I<a•Q])A 2026-01-28T05:29:35.610000v2.0w?YuìW–¥ v@Ië‚ı≥])A 2026-01-28T05:29:32.065000v2.0w?X|~ª « v@Ië‚ı≥])A 2026-01-28T05:27:20.977000v2.0˛=K?YÒN§æí x@IP–¿c])A 2026-01-28T05:27:17.464000v2.0˛=
?XæÍä‰ŸC x@IPI.&-])A 2026-01-28T05:26:46.561000v2.0˛4'?U°‹ÕL@l y@I^€ ])A 2026-01-28T05:19:21.892000v2.0Ût?@Sîo≈ Ä@Iöe∑ïÃ]
)A 2026-01-28T05:14:34.190000v2.0ÈZH?LGÁz*÷ Ö@HÔámü†/	A 2099-01-01T00:00:00.000000v_testaA 2026-01-25T11:42:21.599026v1.0?‡      ˛¢?‹q«q«@0333333øÊfffff`øw?0µíç3?∑ŒŸá+ﬂaA 2026-01-25T11:42:10.621540v1.0?‡      ˛¢?‹q«q«@0333333øÊfffff`øw?0µíç3?∑ŒŸá+ﬂaA 2026-01-25T11:41:59.638980v1.0?‡      ˛¢?‹q«q«@0333333øÊfffff`øw?0µíç3?∑ŒŸá+ﬂaA 2026-01-25T11:41:48.577341v1.0?‡      ˛¢?‹q«q«@0333333øÊfffff`øw?0µíç3?∑ŒŸá+ﬂaA 2026-01-25T11:41:37.555290v1.0?‡      ˛¢?‹q«q«@0333333øÊfffff`øw?0µíç3?∑ŒŸá+ﬂaA 2026-01-25T11:41:26.568723v1.0?‡      ˛¢?‹q«q«@0333333øÊfffff`øw?0µíç3?∑ŒŸá+ﬂaA 2026-01-25T11:41:15.550080v1.0?‡      ˛¢?‹q«q«@0333333øÊfffff`øw?0µíç3?∑ŒŸá+ﬂaA 2026-01-25T11:41:04.576288v1.0?‡      ˛¢?‹q«q«@0333333øÊfffff`øw?0µíç3?∑ŒŸá+ﬂ   ( ® ô2”t∂W¯ô:€|
æ
_
 	°	B„|æ_ °B„|ÆG‡y´D›v ®                                                                                eR)A 2026-01-28T09:32:57.886000v2.0€…?jò*≥wsHæˇÇàòA¡É@PˇG ÔeQ)A 2026-01-28T09:32:55.730000v2.0€…?jb3°ø ç,§ßÊÉ@PFÍYbeP)A 2026-01-28T09:32:53.557000v2.0€…?j⁄ñÉD+ƒ?*Ê§5ı“É@PƒÙ˝Ì_eO)A 2026-01-28T09:32:51.239000v2.0€≈H?k¥ŸK??2üÎcr–É@P1ëºeN)A 2026-01-28T09:32:48.486000v2.0€≈H?k@¢ È°?6¢ºDkÓÉ@PÌ ˛eM)A 2026-01-28T09:32:44.011000v2.0€≈H?j÷o∆G_D?<3^MíÓ>É@PÜZ˜¥leL)A 2026-01-28T09:31:27.140000v2.0◊À·?j§ûÍm÷3?@+ÁÿTÑ@O±Î3mQNeK)A 2026-01-28T09:31:19.729000v2.0◊À·?i*ŸºëÉ?/NAí‡mÑ@O±Î3mQNeJ)A 2026-01-28T09:31:07.825000v2.0◊À·?hæÍä‰ŸC?2_0bSÑ@O±ÎŸZFeI)A 2026-01-28T09:31:04.356000v2.0◊À·?h1≈qQ”.>¸˘w^„˛¬Ñ@O±ÎŸZFeH)A 2026-01-28T09:31:01.901000v2.0◊À·?gMr◊ïø/–¶6iÅjÑ@O±ÎŸZFeG)A 2026-01-28T09:30:59.784000v2.0◊À·?fñ [è#ÿ?!ò“qù8ßÖ@O±ÎŸZFeF)A 2026-01-28T09:30:57.436000v2.0◊À·?g4äiX?QZXïÌã›Ö@O±ÎŸZF]E)A 2026-01-28T09:30:55.169000v2.0◊À·?fqm∂´¸eÖ@O±ÎŸZF]D)A 2026-01-28T09:30:53.000000v2.0◊À·?g˜ß~≤gÖ@O±ÎŸZF]C)A 2026-01-28T09:30:43.588000v2.0◊À·?iL§wﬁÖ@O±ÎŸZF]B)A 2026-01-28T09:28:30.647000v2.0”øÒ?e|Ä(i≠á@O`Y∆Ú∑€]A)A 2026-01-28T09:24:43.058000v2.0º˜∑?]›oñ´ã@O	ıko∂cU@)A 2026-01-28T08:59:42.530000v2.0™.ª§@Mº\tΩe?)A 2026-01-28T08:48:39.919000v2.0âe‹ø`·Ü˜_=ø$À—ïyØ@MÅÄŸ_Ô,e>)A 2026-01-28T08:48:36.899000v2.0âe‹ø`Øµ§‘‡ø21≠ü	Ø@MÅÅoã≈]=)A 2026-01-28T08:48:36.233000v2.0âe‹ø`∏lñØ@MÅÅoã≈]<)A 2026-01-28T08:47:59.438000v2.0âe‹ø^R¯~ñ@T∞@MÅ´ªòI];)A 2026-01-28T08:47:56.415000v2.0âe‹ø_SÄ°[∞@MÅ´ªòI]:)A 2026-01-28T08:47:55.705000v2.0âeZø_*÷·Ôî∞@MÅ™E û+]9)A 2026-01-28T08:43:51.980000v2.0É‘-ø\’ìΩo¥@Mx˙Ú‡≤]8)A 2026-01-28T08:36:52.342000v2.0yæ˜øUÄ¶Âe·ñª@M_®‘<Í]7)A 2026-01-28T08:27:46.438000v2.0Zû_øY;t∞ÑÆ°ƒ@M,]∂ŒeÃ]6)A 2026-01-28T08:23:17.226000v2.0GtÁøa~Õ≥É»@L˜U<–“]5)A 2026-01-28T08:04:46.718000v2.0B”øP?üµt_‘€@L+%XwTû]4)A 2026-01-28T08:00:50.649000v2.0	ãëø9	£‘´ `ﬂ@L I”àöc]3)A 2026-01-28T07:53:02.967000v2.0#eøK”™ŒÉ÷ÅÊ@Kü›¶Sc;]2)A 2026-01-28T07:03:27.140000v2.0√¸??»ñÔu»7 @K1¯‹â]1)A 2026-01-28T06:55:09.932000v2.0∞Á˜?P’I ˝ü  @J˘ŒÛZı]0)A 2026-01-28T06:50:55.215000v2.0™°?[≤tÊùs %@JıOh¸{]/)A 2026-01-28T06:50:52.103000v2.0™°?[p	–…è %@JıOh¸{].)A 2026-01-28T06:49:41.724000v2.0§Zá?[ë>˛∑#5 &@J‡B¡W]-)A 2026-01-28T06:49:30.564000v2.0§Zá?ZÎ1w7Xó &@J‡B¡We,)A 2026-01-28T06:41:34.642000v2.0ôl{?b:≈ÌWcø> ‹‹WO .@JÏÂ#˚Áe+)A 2026-01-28T06:41:34.374000v2.0ôl{?b:≈ÌWcø=†ÁÛ7'} .@JÏÂ#˚Á   ( X ô2Àd˝ñ/»a˙ì,
≈
^	˜	ê	)¬[Ùç&øXÒí3‘uØPÒí3‘u ∑ X]z)A 2026-01-28T10:12:34.883000v2.0¡4?k5Í¿˝ß}[@R¶ÌíB1ß]y)A 2026-01-28T10:08:46.837000v2.0¡4?k5Í¿˝ß}_@R¶ÌíB1ß]x)A 2026-01-28T10:07:46.753000v2.0¡4?k5Í¿˝ß}`@R¶ÌíB1ß]w)A 2026-01-28T10:07:45.361000v2.0¡4?k5Í¿˝ß}`@R¶ÌíB1ß]v)A 2026-01-28T10:07:43.838000v2.0¡4?k5Í¿˝ß}`@R¶ÌíB1ß]u)A 2026-01-28T10:07:42.087000v2.0¡4?k5Í¿˝ß}`@R¶ÌíB1ß]t)A 2026-01-28T10:07:38.811000v2.0¡4?k5Í¿˝ß}`@R¶ÌíB1ß]s)A 2026-01-28T10:07:33.381000v2.0¡4?k5Í¿˝ß}`@R¶ÌíB1ß]r)A 2026-01-28T10:06:55.029000v2.0¡4?k5Í¿˝ß}a@R¶ÌíB1ßeq)A 2026-01-28T09:42:33.373000v2.0Ùß?gò, »j.?++°ƒFJy@P≈¶duÎˇ]p)A 2026-01-28T09:42:26.609000v2.0Ùß?gã∑È“êy@P≈¶duÎˇ]o)A 2026-01-28T09:42:22.440000v2.0Ùß?gÉjoÿp⁄y@P≈¶duÎˇ]n)A 2026-01-28T09:42:06.302000v2.0Ùß?g^ ıJ¥y@P≈°]C]m)A 2026-01-28T09:42:03.034000v2.0Ùß?gE%]Ñﬂy@P≈°]C]l)A 2026-01-28T09:41:39.700000v2.0ÚûÖ?e‰HùˆPz@P√ŒÛ¿Êek)A 2026-01-28T09:35:43.555000v2.0›‚?jMqi±%ØæÏ˜Aú”o€Ä@P/\@µÍsej)A 2026-01-28T09:35:41.275000v2.0‹ﬂ†?júQpt?…? çR†7≥ñÄ@P.úI/Dqei)A 2026-01-28T09:35:39.165000v2.0‹Ñy?jBEäÆ¿>Úû¸Ù>ÄßÄ@P-A√%≤eh)A 2026-01-28T09:35:36.646000v2.0‹{ñ?jQò&≠‰æÙ∞ë‡ÙèÄ@P$:;cxëeg)A 2026-01-28T09:35:33.885000v2.0€€_?jUæ„™ºæÚûÈ∞‹;ÅÄ@PÕQÿö†ef)A 2026-01-28T09:35:30.083000v2.0€€_?jjÄîöµlæˆ¬%w]Ü¸Ä@P2˜GT„ee)A 2026-01-28T09:35:28.003000v2.0€€_?jYÂ†ßáMæË”Ë¨ÏÚ`Ä@P2˜GT„ed)A 2026-01-28T09:35:25.801000v2.0€€_?jYÂ†ßáMøπAtO>FÄ@P2˜GT„ec)A 2026-01-28T09:35:23.275000v2.0€€_?j{àç„ã>‡çJy`Ä@P2˜GT„eb)A 2026-01-28T09:35:22.380000v2.0€€_?j{àç„ã?Àõ≥…⁄≤Ä@P2˜GT„ea)A 2026-01-28T09:35:19.816000v2.0€€_?jrŒîM"?'Ã/lÍ
Ä@P2˜GT„e`)A 2026-01-28T09:35:17.748000v2.0€€_?j±!d9?-øZîøXÄ@P2˜GT„e_)A 2026-01-28T09:35:14.221000v2.0€€_?jjÄîöµlø`Ä,≥πÄ@P2˜GT„e^)A 2026-01-28T09:35:10.416000v2.0€€_?iº%ì!Teø*1û_-˘Ä@P3Löc_e])A 2026-01-28T09:35:07.399000v2.0€€_?i&≤ˇî∑Êø1◊…i¢VæÄ@P3Löc_e\)A 2026-01-28T09:35:05.000000v2.0€€_?hÙ‚#ª-âø4+Üíz•Ä@P3Löc_e[)A 2026-01-28T09:35:01.038000v2.0€€_?h”¨;‘“óø4+ö¸ôÄ@P3Löc_eZ)A 2026-01-28T09:33:23.167000v2.0€”??k>8:˜=Êæ‡å‘[Çm˝Ç@P*±Ï{GeY)A 2026-01-28T09:33:21.012000v2.0€”??k_n"›ö$>Ïˆ~‹+—ÒÇ@P*±Ï{GeX)A 2026-01-28T09:33:16.484000v2.0€”??kcîﬂ⁄eY?”{ÜÇ!Ç@P*FhGeW)A 2026-01-28T09:33:11.088000v2.0€”??k|}M«)·?rÆ’5Ç@P*FhGeV)A 2026-01-28T09:33:08.934000v2.0€”??k[Ge‡ŒÔ?ÚÂ£r·Ç@P%\ΩoÓeU)A 2026-01-28T09:33:06.400000v2.0€”??kN”.Íl?ˆ‹|pÍ„Ç@P%\ΩoÓeT)A 2026-01-28T09:33:03.557000v2.0€”??kB^˜Ù
g>ˆ¡Õ/Çª3Ç@P%\ΩoÓeS)A 2026-01-28T09:33:00.465000v2.0€…?jˇÛ('QÎæ–å¯º§E/Ç@PÛƒ&
ª 6C ›πïqM)·ósO+„øõwS/Á√ü{W3Î«£[7
Ô
À
ß
É
_
;
	Û	œ	´	á	c	?	˜”ØãgC˚◊≥èkG#ˇ€∑ìoK'ﬂªósO+„øõwS/Á√ü{W3Î«£[7ÔÀßÉ_; Ûª                     #A2026-01-28T09:35:43.555000v2.0k#A2026-01-28T09:35:41.275000v2.0j#A2026-01-28T09:35:39.165000v2.0i#A2026-01-28T09:35:36.646000v2.0h#A2026-01-28T09:35:33.885000v2.0g#A2026-01-28T09:35:30.083000v2.0f#A2026-01-28T09:35:28.003000v2.0e#A2026-01-28T09:35:25.801000v2.0d#A2026-01-28T09:35:23.275000v2.0c#A2026-01-28T09:35:22.380000v2.0b#A2026-01-28T09:35:19.816000v2.0a#A2026-01-28T09:35:17.748000v2.0`#A2026-01-28T09:35:14.221000v2.0_#A2026-01-28T09:35:10.416000v2.0^#A2026-01-28T09:35:07.399000v2.0]#A2026-01-28T09:35:05.000000v2.0\#A2026-01-28T09:35:01.038000v2.0[#A2026-01-28T09:33:23.167000v2.0Z#A2026-01-28T09:33:21.012000v2.0Y#A2026-01-28T09:33:16.484000v2.0X#A2026-01-28T09:33:11.088000v2.0W#A2026-01-28T09:33:08.934000v2.0V#A2026-01-28T09:33:06.400000v2.0U#A2026-01-28T09:33:03.557000v2.0T#A2026-01-28T09:33:00.465000v2.0S#A2026-01-28T09:32:57.886000v2.0R#A2026-01-28T09:32:55.730000v2.0Q#A2026-01-28T09:32:53.557000v2.0P#A2026-01-28T09:32:51.239000v2.0O#A2026-01-28T09:32:48.486000v2.0N#A2026-01-28T09:32:44.011000v2.0M#A2026-01-28T09:31:27.140000v2.0L#A2026-01-28T09:31:19.729000v2.0K#A2026-01-28T09:31:07.825000v2.0J#A2026-01-28T09:31:04.356000v2.0I#A2026-01-28T09:31:01.901000v2.0H#A2026-01-28T09:30:59.784000v2.0G#A2026-01-28T09:30:57.436000v2.0F#A2026-01-28T09:30:55.169000v2.0E#A2026-01-28T09:30:53.000000v2.0D#A2026-01-28T09:30:43.588000v2.0C#A2026-01-28T09:28:30.647000v2.0B#A2026-01-28T09:24:43.058000v2.0A#A2026-01-28T08:59:42.530000v2.0@#A2026-01-28T08:48:39.919000v2.0?#A2026-01-28T08:48:36.899000v2.0>#A2026-01-28T08:48:36.233000v2.0=#A2026-01-28T08:47:59.438000v2.0<#A2026-01-28T08:47:56.415000v2.0;#A2026-01-28T08:47:55.705000v2.0:#A2026-01-28T08:43:51.980000v2.09#A2026-01-28T08:36:52.342000v2.08#A2026-01-28T08:27:46.438000v2.07#A2026-01-28T08:23:17.226000v2.06#A2026-01-28T08:04:46.718000v2.05#A2026-01-28T08:00:50.649000v2.04#A2026-01-28T07:53:02.967000v2.03#A2026-01-28T07:03:27.140000v2.02#A2026-01-28T06:55:09.932000v2.01#A2026-01-28T06:50:55.215000v2.00#A2026-01-28T06:50:52.103000v2.0/#A2026-01-28T06:49:41.724000v2.0.#A2026-01-28T06:49:30.564000v2.0-#A2026-01-28T06:41:34.642000v2.0,#A2026-01-28T06:41:34.374000v2.0+#A2026-01-28T06:40:21.292000v2.0*#A2026-01-28T06:40:21.021000v2.0)#A2026-01-28T06:31:50.779000v2.0(#A2026-01-28T06:31:50.494000v2.0'#A2026-01-28T06:31:05.488000v2.0&#A2026-01-28T06:30:40.508000v2.0%#A2026-01-28T06:22:21.102000v2.0$#A2026-01-28T06:22:17.461000v2.0##A2026-01-28T06:21:37.495000v2.0"#A2026-01-28T06:16:35.233000v2.0!#A2026-01-28T06:16:15.146000v2.0 #A2026-01-28T06:15:36.853000v2.0#A2026-01-28T06:08:09.844000v2.0#A2026-01-28T06:07:56.401000v2.0#A2026-01-28T06:07:52.779000v2.0#A2026-01-28T06:06:20.388000v2.0#A2026-01-28T05:52:07.962000v2.0#A2026-01-28T05:52:07.278000v2.0#A2026-01-28T05:51:41.390000v2.0#A2026-01-28T05:51:37.694000v2.0#A2026-01-28T05:51:10.961000v2.0#A2026-01-28T05:46:59.473000v2.0#A2026-01-28T05:46:05.515000v2.0#A2026-01-28T05:36:29.142000v2.0#A2026-01-28T05:36:25.501000v2.0#A2026-01-28T05:34:29.323000v2.0#A2026-01-28T05:29:35.610000v2.0#A2026-01-28T05:29:32.065000v2.0#A2026-01-28T05:27:20.977000v2.0#A2026-01-28T05:27:17.464000v2.0#A2026-01-28T05:26:46.561000v2.0#A2026-01-28T05:19:21.892000v2.0#A2026-01-28T05:14:34.190000v2.0
   &2099-01-01T00:00:00.000000v_test	#A2026-01-25T11:42:21.599026v1.0#A2026-01-25T11:42:10.621540v1.0#A2026-01-25T11:41:59.638980v1.0#A2026-01-25T11:41:48.577341v1.0#A2026-01-25T11:41:37.555290v1.0#A2026-01-25T11:41:26.568723v1.0#A2026-01-25T11:41:15.550080v1.0"A	2026-01-25T11:41:04.576288v1.0
   LJ ‹∏îpL(‡ºòtP,‰¿úxT0Ëƒ†|X4Ï»§Ä\8

Ã
®
Ñ
`
<
	Ù	–	¨	à	d	@	¯‘∞fB˙÷≤éjF"˛⁄∂ínJ&ﬁπîoJä                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          $A2026-01-28T10:15:24.908000v2.0 É$A2026-01-28T10:15:21.825000v2.0 Ç$A2026-01-28T10:14:54.851000v2.0 Å$A2026-01-28T10:14:51.752000v2.0 Ä#A2026-01-28T10:14:51.006000v2.0#A2026-01-28T10:14:35.772000v2.0~#A2026-01-28T10:14:11.710000v2.0}#A2026-01-28T10:14:08.611000v2.0|#A2026-01-28T10:14:07.814000v2.0{#A2026-01-28T10:12:34.883000v2.0z#A2026-01-28T10:08:46.837000v2.0y#A2026-01-28T10:07:46.753000v2.0x#A2026-01-28T10:07:45.361000v2.0w#A2026-01-28T10:07:43.838000v2.0v#A2026-01-28T10:07:42.087000v2.0u#A2026-01-28T10:07:38.811000v2.0t#A2026-01-28T10:07:33.381000v2.0s#A2026-01-28T10:06:55.029000v2.0r#A2026-01-28T09:42:33.373000v2.0q#A2026-01-28T09:42:26.609000v2.0p#A2026-01-28T09:42:22.440000v2.0o#A2026-01-28T09:42:06.302000v2.0n#A2026-01-28T09:42:03.034000v2.0m%A2099-01-01T00:00:00.000000v_test	#A2026-01-28T09:41:39.700000v2.0l#A2026-01-28T09:35:43.555000v2.0k#A2026-01-28T09:35:41.275000v2.0j#A2026-01-28T09:35:39.165000v2.0i#A2026-01-28T09:35:36.646000v2.0h#A2026-01-28T09:35:33.885000v2.0g#A2026-01-28T09:35:30.083000v2.0f#A2026-01-28T09:35:28.003000v2.0e#A2026-01-28T09:35:25.801000v2.0d#A2026-01-28T09:35:23.275000v2.0c#A2026-01-28T09:35:22.380000v2.0b#A2026-01-28T09:35:19.816000v2.0a#A2026-01-28T09:35:17.748000v2.0`#A2026-01-28T09:35:14.221000v2.0_#A2026-01-28T09:35:10.416000v2.0^#A2026-01-28T09:35:07.399000v2.0]#A2026-01-28T09:35:05.000000v2.0\#A2026-01-28T09:35:01.038000v2.0[#A2026-01-28T09:33:23.167000v2.0Z#A2026-01-28T09:33:21.012000v2.0Y#A2026-01-28T09:33:16.484000v2.0X#A2026-01-28T09:33:11.088000v2.0W#A2026-01-28T09:33:08.934000v2.0V#A2026-01-28T09:33:06.400000v2.0U#A2026-01-28T09:33:03.557000v2.0T#A2026-01-28T09:33:00.465000v2.0S#A2026-01-28T09:32:57.886000v2.0R#A2026-01-28T09:32:55.730000v2.0Q#A2026-01-28T09:32:53.557000v2.0P#A2026-01-28T09:32:51.239000v2.0O#A2026-01-28T09:32:48.486000v2.0N#A2026-01-28T09:32:44.011000v2.0M#A2026-01-28T09:31:27.140000v2.0L#A2026-01-28T09:31:19.729000v2.0K#A2026-01-28T09:31:07.825000v2.0J#A2026-01-28T09:31:04.356000v2.0I#A2026-01-28T09:31:01.901000v2.0H#A2026-01-28T09:30:59.784000v2.0G#A2026-01-28T09:30:57.436000v2.0F#A2026-01-28T09:30:55.169000v2.0E#A2026-01-28T09:30:53.000000v2.0D#A2026-01-28T09:30:43.588000v2.0C#A2026-01-28T09:28:30.647000v2.0B#A2026-01-28T09:24:43.058000v2.0A#A2026-01-28T08:59:42.530000v2.0@#A2026-01-28T08:48:39.919000v2.0?#A2026-01-28T08:48:36.899000v2.0>#A2026-01-28T08:48:36.233000v2.0=#A2026-01-28T08:47:59.438000v2.0<#A2026-01-28T08:47:56.415000v2.0;#A2026-01-28T08:47:55.705000v2.0:#A2026-01-28T08:43:51.980000v2.09   	• °B„Ñ%≈e•                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ]Å)A 2026-01-28T10:15:24.908000v2.0¡4?k5Í¿˝ß}X@R¶ÌíB1ß]Å)A 2026-01-28T10:15:21.825000v2.0¡4?k5Í¿˝ß}X@R¶ÌíB1ß]Å)A 2026-01-28T10:14:54.851000v2.0¡4?k5Í¿˝ß}Y@R¶ÌíB1ß]Å )A 2026-01-28T10:14:51.752000v2.0¡4?k5Í¿˝ß}Y@R¶ÌíB1ß])A 2026-01-28T10:14:51.006000v2.0¡4?k5Í¿˝ß}Y@R¶ÌíB1ß]~)A 2026-01-28T10:14:35.772000v2.0¡4?k5Í¿˝ß}Y@R¶ÌíB1ß]})A 2026-01-28T10:14:11.710000v2.0¡4?k5Í¿˝ß}Y@R¶ÌíB1ß]|)A 2026-01-28T10:14:08.611000v2.0¡4?k5Í¿˝ß}Y@R¶ÌíB1ß]{)A 2026-01-28T10:14:07.814000v2.0¡4?k5Í¿˝ß}Y@R¶ÌíB1ß
Í >` ‚√§ÖfG(	À¨çnO0Ú”¥ïvW8˙€ºù~_@!„ƒ•ÜgH)

Î
Ã
≠
é
o
P
1
	Û	‘	µ	ñ	w	X	9	˚‹Ωû`A"‰≈¶áhI*ÏÕÆèpQ2Ù’∂óxY:¸›æüÄaB#Â∆ßàiJ+ÌŒØêqR3ı÷∑òyZ;Í                              A2026-01-28T10:14:07.814000{A2026-01-28T10:12:34.883000zA2026-01-28T10:08:46.837000yA2026-01-28T10:07:46.753000xA2026-01-28T10:07:45.361000wA2026-01-28T10:07:43.838000vA2026-01-28T10:07:42.087000uA2026-01-28T10:07:38.811000tA2026-01-28T10:07:33.381000sA2026-01-28T10:06:55.029000rA2026-01-28T09:42:33.373000qA2026-01-28T09:42:26.609000pA2026-01-28T09:42:22.440000oA2026-01-28T09:42:06.302000nA2026-01-28T09:42:03.034000mA2026-01-28T09:41:39.700000lA2026-01-28T09:35:43.555000kA2026-01-28T09:35:41.275000jA2026-01-28T09:35:39.165000iA2026-01-28T09:35:36.646000hA2026-01-28T09:35:33.885000gA2026-01-28T09:35:30.083000fA2026-01-28T09:35:28.003000eA2026-01-28T09:35:25.801000dA2026-01-28T09:35:23.275000cA2026-01-28T09:35:22.380000bA2026-01-28T09:35:19.816000aA2026-01-28T09:35:17.748000`A2026-01-28T09:35:14.221000_A2026-01-28T09:35:10.416000^A2026-01-28T09:35:07.399000]A2026-01-28T09:35:05.000000\A2026-01-28T09:35:01.038000[A2026-01-28T09:33:23.167000ZA2026-01-28T09:33:21.012000YA2026-01-28T09:33:16.484000XA2026-01-28T09:33:11.088000WA2026-01-28T09:33:08.934000VA2026-01-28T09:33:06.400000UA2026-01-28T09:33:03.557000TA2026-01-28T09:33:00.465000SA2026-01-28T09:32:57.886000RA2026-01-28T09:32:55.730000QA2026-01-28T09:32:53.557000PA2026-01-28T09:32:51.239000OA2026-01-28T09:32:48.486000NA2026-01-28T09:32:44.011000MA2026-01-28T09:31:27.140000LA2026-01-28T09:31:19.729000KA2026-01-28T09:31:07.825000JA2026-01-28T09:31:04.356000IA2026-01-28T09:31:01.901000HA2026-01-28T09:30:59.784000GA2026-01-28T09:30:57.436000FA2026-01-28T09:30:55.169000EA2026-01-28T09:30:53.000000DA2026-01-28T09:30:43.588000CA2026-01-28T09:28:30.647000BA2026-01-28T09:24:43.058000AA2026-01-28T08:59:42.530000@A2026-01-28T08:48:39.919000?A2026-01-28T08:48:36.899000>A2026-01-28T08:48:36.233000=A2026-01-28T08:47:59.438000<A2026-01-28T08:47:56.415000;A2026-01-28T08:47:55.705000:A2026-01-28T08:43:51.9800009A2026-01-28T08:36:52.3420008A2026-01-28T08:27:46.4380007A2026-01-28T08:23:17.2260006A2026-01-28T08:04:46.7180005A2026-01-28T08:00:50.6490004A2026-01-28T07:53:02.9670003A2026-01-28T07:03:27.1400002A2026-01-28T06:55:09.9320001A2026-01-28T06:50:55.2150000A2026-01-28T06:50:52.103000/A2026-01-28T06:49:41.724000.A2026-01-28T06:49:30.564000-A2026-01-28T06:41:34.642000,A2026-01-28T06:41:34.374000+A2026-01-28T06:40:21.292000*A2026-01-28T06:40:21.021000)A2026-01-28T06:31:50.779000(A2026-01-28T06:31:50.494000'A2026-01-28T06:31:05.488000&A2026-01-28T06:30:40.508000%A2026-01-28T06:22:21.102000$A2026-01-28T06:22:17.461000#A2026-01-28T06:21:37.495000"A2026-01-28T06:16:35.233000!A2026-01-28T06:16:15.146000 A2026-01-28T06:15:36.853000A2026-01-28T06:08:09.844000A2026-01-28T06:07:56.401000A2026-01-28T06:07:52.779000A2026-01-28T06:06:20.388000A2026-01-28T05:52:07.962000A2026-01-28T05:52:07.278000A2026-01-28T05:51:41.390000A2026-01-28T05:51:37.694000A2026-01-28T05:51:10.961000A2026-01-28T05:46:59.473000A2026-01-28T05:46:05.515000A2026-01-28T05:36:29.142000A2026-01-28T05:36:25.501000A2026-01-28T05:34:29.323000A2026-01-28T05:29:35.610000A2026-01-28T05:29:32.065000A2026-01-28T05:27:20.977000A2026-01-28T05:27:17.464000A2026-01-28T05:26:46.561000A2026-01-28T05:19:21.892000A2026-01-28T05:14:34.190000
   2099-01-01T00:00:00.000000	A2026-01-25T11:42:21.599026A2026-01-25T11:42:10.621540A2026-01-25T11:41:59.638980A2026-01-25T11:41:48.577341A2026-01-25T11:41:37.555290A2026-01-25T11:41:26.568723A2026-01-25T11:41:15.550080A	2026-01-25T11:41:04.576288
   D¿ ·¬£ÑeF'È ´åmN/Ò“≥îuV7˘⁄ªú}^? ‚√§ÖfG(	
Í
À
¨
ç
n
O
0
	Ú	”	¥	ï	v	W	8	˙€º~_@  ‡¿ù                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                A2026-01-28T10:15:24.908000 ÉA2026-01-28T10:15:21.825000 ÇA2026-01-28T10:14:54.851000 ÅA2026-01-28T10:14:51.752000 ÄA2026-01-28T10:14:51.006000A2026-01-28T10:14:35.772000~A2026-01-28T10:14:11.710000}A2099-01-01T00:00:00.000000	A2026-01-28T10:14:08.611000|A2026-01-28T10:14:07.814000{A2026-01-28T10:12:34.883000zA2026-01-28T10:08:46.837000yA2026-01-28T10:07:46.753000xA2026-01-28T10:07:45.361000wA2026-01-28T10:07:43.838000vA2026-01-28T10:07:42.087000uA2026-01-28T10:07:38.811000tA2026-01-28T10:07:33.381000sA2026-01-28T10:06:55.029000rA2026-01-28T09:42:33.373000qA2026-01-28T09:42:26.609000pA2026-01-28T09:42:22.440000oA2026-01-28T09:42:06.302000nA2026-01-28T09:42:03.034000mA2026-01-28T09:41:39.700000lA2026-01-28T09:35:43.555000kA2026-01-28T09:35:41.275000jA2026-01-28T09:35:39.165000iA2026-01-28T09:35:36.646000hA2026-01-28T09:35:33.885000gA2026-01-28T09:35:30.083000fA2026-01-28T09:35:28.003000eA2026-01-28T09:35:25.801000dA2026-01-28T09:35:23.275000cA2026-01-28T09:35:22.380000bA2026-01-28T09:35:19.816000aA2026-01-28T09:35:17.748000`A2026-01-28T09:35:14.221000_A2026-01-28T09:35:10.416000^A2026-01-28T09:35:07.399000]A2026-01-28T09:35:05.000000\A2026-01-28T09:35:01.038000[A2026-01-28T09:33:23.167000ZA2026-01-28T09:33:21.012000YA2026-01-28T09:33:16.484000XA2026-01-28T09:33:11.088000WA2026-01-28T09:33:08.934000VA2026-01-28T09:33:06.400000UA2026-01-28T09:33:03.557000TA2026-01-28T09:33:00.465000SA2026-01-28T09:32:57.886000RA2026-01-28T09:32:55.730000QA2026-01-28T09:32:53.557000PA2026-01-28T09:32:51.239000OA2026-01-28T09:32:48.486000NA2026-01-28T09:32:44.011000MA2026-01-28T09:31:27.140000LA2026-01-28T09:31:19.729000KA2026-01-28T09:31:07.825000JA2026-01-28T09:31:04.356000IA2026-01-28T09:31:01.901000HA2026-01-28T09:30:59.784000GA2026-01-28T09:30:57.436000FA2026-01-28T09:30:55.169000EA2026-01-28T09:30:53.000000DA2026-01-28T09:30:43.588000CA2026-01-28T09:28:30.647000BA2026-01-28T09:24:43.058000A   _ é'µC—_                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           pLA/A2026-01-28T10:15:24.904453INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T10:15:24.904907pKA/A2026-01-28T10:15:21.821432INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T10:15:21.821657pJA/A2026-01-28T10:14:54.845844INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T10:14:54.846570pIA/A2026-01-28T10:14:51.746194INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T10:14:51.746658eHAA2026-01-28T10:14:51.000798NORMALNEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T10:14:51.001185pGA/A2026-01-28T10:14:35.767075INSUFFICIENT_DATANEUTRALNEUTRAL?œ\(ı¬è\[][][]2026-01-28T10:14:35.767317



####################################################################################################
SUMMARY
####################################################################################################

Export completed: 2026-01-28 15:51:26
Total files exported: 42
Output file: G:\trading_app\codebase_export.txt
File size: 2.34 MB



--------------------------------------------------
FILE PATH : G:\trading_app\config\nifty_weights.json
SIZE      : 1098 bytes
--------------------------------------------------

{
  "HDFCBANK": 0.1272,
  "RELIANCE": 0.0823,
  "ICICIBANK": 0.0840,
  "INFY": 0.0498,
  "BHARTIARTL": 0.0469,
  "LT": 0.0386,
  "SBIN": 0.0375,
  "AXISBANK": 0.0358,
  "TCS": 0.0287,
  "ITC": 0.0259,
  "KOTAKBANK": 0.0261,
  "BAJFINANCE": 0.0212,
  "HINDUNILVR": 0.0186,
  "ULTRACEMCO": 0.0184,
  "TITAN": 0.0176,
  "MARUTI": 0.0169,
  "NTPC": 0.0166,
  "HCLTECH": 0.0158,
  "ADANIPORTS": 0.0156,
  "SUNPHARMA": 0.0152,
  "BEL": 0.0151,
  "BAJAJFINSV": 0.0152,
  "JSWSTEEL": 0.0148,
  "TATASTEEL": 0.0140,
  "ONGC": 0.0154,
  "BAJAJ-AUTO": 0.0131,
  "COALINDIA": 0.0129,
  "ADANIENT": 0.0125,
  "ASIANPAINT": 0.0125,
  "NESTLEIND": 0.0124,
  "WIPRO": 0.0122,
  "ZOMATO": 0.0121,
  "POWERGRID": 0.0117,
  "HINDALCO": 0.0107,
  "SBILIFE": 0.0101,
  "EICHERMOT": 0.0097,
  "GRASIM": 0.0096,
  "SHRIRAMFIN": 0.0093,
  "INDIGO": 0.0091,
  "TECHM": 0.0085,
  "JIOFIN": 0.0081,
  "HDFCLIFE": 0.0077,
  "TRENT": 0.0067,
  "APOLLOHOSP": 0.0063,
  "TATAMOTORS": 0.0062,
  "MAXHEALTH": 0.0061,
  "TATACONSUM": 0.0058,
  "CIPLA": 0.0053,
  "DRREDDY": 0.0051
}


--------------------------------------------------
FILE PATH : G:\trading_app\config\settings.json
SIZE      : 0 bytes
--------------------------------------------------



--------------------------------------------------
FILE PATH : G:\trading_app\core\debug_session.py
SIZE      : 2128 bytes
--------------------------------------------------

import streamlit as st
from pathlib import Path

st.set_page_config(layout="wide", page_title="Debug Auth")

st.title("üîç Authentication Debug")

# Check token file
token_path = Path("data/tokens/upstox_tokens.enc")
st.write(f"Token path: {token_path}")
st.write(f"Token exists: {token_path.exists()}")

# Simulate what authenticate() should do
if not token_path.exists():
    st.success("‚úÖ Perfect! Token file doesn't exist - should show login button")
    
    # Show what the login button should look like
    st.markdown("---")
    st.subheader("This is what should appear:")
    
    login_url = "https://api.upstox.com/v2/login/authorization/dialog?response_type=code&client_id=e9bc6b14-447a-4d2f-aa32-2a4aecbafe56&redirect_uri=http://127.0.0.1:8501/callback&scope=order placement portfolio"
    
    st.markdown(f"""
    <div style="text-align: center;">
        <a href="{login_url}" target="_blank">
            <button style="
                background-color: #00d09c;
                color: white;
                padding: 15px 30px;
                font-size: 18px;
                font-weight: bold;
                border: none;
                border-radius: 10px;
                cursor: pointer;
                width: 80%;
                margin: 20px 0;
                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            ">
            üìà Login with Upstox
            </button>
        </a>
    </div>
    """, unsafe_allow_html=True)
    
    st.warning("If you don't see this in the main app, the issue is in how authenticate() is being called.")
else:
    st.warning("Token file exists - try deleting it with: `rm data/tokens/upstox_tokens.enc`")

# Check how authenticate is called in app.py
st.markdown("---")
st.subheader("How authenticate() is called in app.py:")
st.code("""
# This should be in your app.py:
from core.session import UpstoxSession

# Early in the app initialization:
token = UpstoxSession.authenticate()

# If token is None, authenticate() should have shown login button
# and stopped execution with st.stop()
""")


--------------------------------------------------
FILE PATH : G:\trading_app\core\feature_pipeline.py
SIZE      : 30876 bytes
--------------------------------------------------

"""
Enhanced Feature Pipeline with Research-Based Feature Engineering.
Implements all research concepts:
1. OI Velocity & Gamma Exposure features
2. Structural Walls & Traps features
3. Spot Divergence features
4. Market microstructure features
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, Optional, Tuple
from dataclasses import dataclass
from scipy import stats
from datetime import timezone

from ml.feature_contract import FEATURE_VERSION, FEATURE_COLUMNS, TARGET_COLUMN
from storage.repository import insert_market_features
from data.upstox_client import UpstoxClient, MarketAnalytics

# Import feature computation modules
from features.option_features import compute_option_features
from features.price_features import compute_price_features
from features.breadth import compute_breadth_features


from enum import Enum

class PipelineMode(str, Enum):
    RESEARCH = "research"
    EXECUTION = "execution"

# ==============================
# RESEARCH-BASED FEATURE ENGINEERING
# ==============================

@dataclass
class ResearchFeatures:
    """Container for research-based features"""
    # OI Velocity features
    oi_velocity: float
    oi_velocity_ma: float  # Moving average
    oi_velocity_std: float  # Standard deviation
    oi_regime: str  # EXPANSIVE/NORMAL/CONSTRICTED
    
    # Gamma Exposure features
    net_gamma: float
    gamma_regime: str  # POSITIVE/NEGATIVE/NEUTRAL
    gamma_flip_distance: float  # Distance to nearest gamma flip
    max_gamma_strike: float
    
    # Structural features
    wall_strength: float  # Combined strength of top walls
    wall_defense_score: float  # Defense score (0-1)
    trap_probability: float  # Probability of trap formation
    
    # Divergence features
    price_oi_divergence: float  # Divergence between price and OI
    price_gamma_divergence: float  # Divergence between price and gamma
    divergence_score: float  # Combined divergence score
    
    # Market microstructure
    put_call_ratio: float
    max_pain_distance: float  # Distance to max pain
    vix_smile: float  # Volatility smile curvature
    skewness: float  # Option skew
    
    # Wyckoff-inspired features
    spring_detection: float  # Bear trap probability
    upthrust_detection: float  # Bull trap probability
    accumulation_score: float  # Accumulation phase score

class EnhancedFeatureEngine:
    """
    Enhanced feature engineering engine incorporating research concepts.
    Transforms raw market data into research-based predictive features.
    """
    @staticmethod
    def _safe_float(value, default: float = 0.0) -> float:
        """
        Convert None / NaN / invalid values into a safe float.
        """
        if isinstance(value, (int, float)) and not np.isnan(value):
            return float(value)
        return default

    def __init__(self, lookback_periods: int = 20):
        self.lookback = lookback_periods
        self.feature_history = []
        self.oi_velocity_history = []
        self.gamma_history = []
        
    def extract_research_features(self, 
                                option_chain_analytics: Dict,
                                spot_price: float,
                                price_series: pd.Series,
                                volume_series: pd.Series,
                                constituents_df: pd.DataFrame) -> ResearchFeatures:
        """
        Extract research-based features from market analytics.
        """
        analytics = option_chain_analytics.get("analytics", {})
        
        # Add null checks
        if not analytics:
            # Return default/empty features
            return ResearchFeatures(
                oi_velocity=0.0,
                oi_velocity_ma=0.0,
                oi_velocity_std=0.0,
                oi_regime="NORMAL",
                net_gamma=0.0,
                gamma_regime="NEUTRAL",
                gamma_flip_distance=0.0,
                max_gamma_strike=0.0,
                wall_strength=0.0,
                wall_defense_score=0.0,
                trap_probability=0.0,
                price_oi_divergence=0.0,
                price_gamma_divergence=0.0,
                divergence_score=0.0,
                put_call_ratio=1.0,
                max_pain_distance=0.0,
                vix_smile=0.0,
                skewness=0.0,
                spring_detection=0.0,
                upthrust_detection=0.0,
                accumulation_score=0.0
            )
        
        # OI Velocity features
        oi_velocity = self._safe_float(analytics.get("oi_velocity"))
        oi_regime = analytics.get("oi_regime", "NORMAL")
        
        # Update history and calculate statistics
        self.oi_velocity_history.append(oi_velocity)
        if len(self.oi_velocity_history) > self.lookback:
            self.oi_velocity_history = self.oi_velocity_history[-self.lookback:]
        
        oi_velocity_ma = np.mean(self.oi_velocity_history) if self.oi_velocity_history else 0.0
        oi_velocity_std = np.std(self.oi_velocity_history) if len(self.oi_velocity_history) > 1 else 1.0
        
        # Gamma Exposure features
        gamma_data = analytics.get("gamma_exposure", {})
        net_gamma = gamma_data.get("net_gamma", 0.0)
        gamma_regime = gamma_data.get("regime", "NEUTRAL")
        flip_levels = gamma_data.get("flip_levels", [])
        max_gamma_strike = gamma_data.get("max_impact_strike", 0.0)
        
        # Calculate distance to nearest gamma flip
        gamma_flip_distance = 0.0
        if flip_levels:
            valid_levels = [
                self._safe_float(level)
                for level in flip_levels
                if isinstance(level, (int, float))
            ]

            if valid_levels and spot_price > 0:
                distances = [
                    abs(level - spot_price) / spot_price
                    for level in valid_levels
                ]
                gamma_flip_distance = min(distances) if distances else 0.0
        
        # Update gamma history
        self.gamma_history.append(self._safe_float(net_gamma))
        if len(self.gamma_history) > self.lookback:
            self.gamma_history = self.gamma_history[-self.lookback:]
        
        # Structural features
        walls = analytics.get("structural_walls", [])
        traps = analytics.get("potential_traps", [])
        
        # Wall strength (weighted by concentration and defense)
        wall_strength = 0.0
        wall_defense_score = 0.0
        if walls:
            for wall in walls:
                concentration = wall.get("concentration", 0.0)
                defended = 1.0 if wall.get("defended", False) else 0.0
                distance = wall.get("distance_pct", 100) / 100  # Normalize
                
                # Walls closer to spot have more impact
                proximity_factor = 1.0 / (1.0 + distance)
                wall_strength += concentration * proximity_factor
                wall_defense_score += defended * concentration
        
        # Normalize wall features
        wall_strength = min(wall_strength, 1.0)
        wall_defense_score = min(wall_defense_score, 1.0)
        
        # Trap probability
        trap_probability = 0.0
        if traps:
            trap_confidences = [trap.get("confidence", 0.0) for trap in traps]
            trap_probability = max(trap_confidences) if trap_confidences else 0.0
        
        # Divergence features
        divergence_data = analytics.get("spot_divergence", {})
        has_divergence = divergence_data.get("has_divergence", False)
        divergence_type = divergence_data.get("type", "")
        divergence_confidence = divergence_data.get("confidence", 0.0)
        
        # Calculate price-OI divergence
        price_change = 0.0
        if len(price_series) >= 2:
            prev_price = self._safe_float(price_series.iloc[-2])
            curr_price = self._safe_float(price_series.iloc[-1])

            if prev_price > 0:
                price_change = (curr_price - prev_price) / prev_price * 100

        
        price_oi_divergence = abs(
            (price_change or 0.0) -
            (oi_velocity or 0.0)
        )
        
        # Calculate price-gamma divergence
        price_gamma_divergence = 0.0
        if self.gamma_history and len(self.gamma_history) >= 2:
            gamma_change = (
                    self._safe_float(self.gamma_history[-1]) -
                    self._safe_float(self.gamma_history[-2])
                )
            price_gamma_divergence = abs(
                        self._safe_float(price_change) -
                        self._safe_float(gamma_change) * 100
                    )
        
        divergence_score = divergence_confidence if has_divergence else 0.0
        
        # Market microstructure features
        pcr_data = MarketAnalytics.calculate_put_call_ratio(option_chain_analytics.get("raw_data", pd.DataFrame()))
        put_call_ratio = pcr_data.get("pcr_oi")
        put_call_ratio = float(put_call_ratio) if put_call_ratio is not None else 0.0

        
        max_pain_data = MarketAnalytics.detect_max_pain(option_chain_analytics.get("raw_data", pd.DataFrame()))
        max_pain_strike = max_pain_data.get("max_pain_strike")
        if max_pain_strike is None or spot_price <= 0:
            max_pain_distance = 0.0
        else:
            max_pain_distance = abs(spot_price - max_pain_strike) / spot_price

        
        # Calculate VIX smile (simplified)
        vix_smile = self._calculate_vix_smile(option_chain_analytics.get("raw_data", pd.DataFrame()), spot_price)
        
        # Calculate skewness
        skewness = self._calculate_option_skew(option_chain_analytics.get("raw_data", pd.DataFrame()))
        
        # Wyckoff-inspired features
        spring_detection = self._detect_spring_pattern(price_series, volume_series, constituents_df)
        upthrust_detection = self._detect_upthrust_pattern(price_series, volume_series, constituents_df)
        accumulation_score = self._calculate_accumulation_score(price_series, volume_series, oi_velocity)
        
        return ResearchFeatures(
            oi_velocity=oi_velocity,
            oi_velocity_ma=oi_velocity_ma,
            oi_velocity_std=oi_velocity_std,
            oi_regime=oi_regime,
            
            net_gamma=net_gamma,
            gamma_regime=gamma_regime,
            gamma_flip_distance=gamma_flip_distance,
            max_gamma_strike=max_gamma_strike,
            
            wall_strength=wall_strength,
            wall_defense_score=wall_defense_score,
            trap_probability=trap_probability,
            
            price_oi_divergence=price_oi_divergence,
            price_gamma_divergence=price_gamma_divergence,
            divergence_score=divergence_score,
            
            put_call_ratio=put_call_ratio,
            max_pain_distance=max_pain_distance,
            vix_smile=vix_smile,
            skewness=skewness,
            
            spring_detection=spring_detection,
            upthrust_detection=upthrust_detection,
            accumulation_score=accumulation_score
        )
    
    def _calculate_vix_smile(self, option_chain_df: pd.DataFrame, spot_price: float) -> float:
        """Calculate volatility smile curvature."""
        if option_chain_df.empty or spot_price <= 0:
            return 0.0
        
        # Group by distance from spot
        option_chain_df = option_chain_df.copy()
        option_chain_df['distance_pct'] = abs(option_chain_df['strike'] - spot_price) / spot_price * 100
        
        # Bin by distance
        bins = [0, 2, 5, 10, 20]
        smiles = []
        
        for i in range(len(bins) - 1):
            lower = bins[i]
            upper = bins[i + 1]
            
            mask = (option_chain_df['distance_pct'] >= lower) & (option_chain_df['distance_pct'] < upper)
            bin_iv = option_chain_df.loc[mask, 'iv'].mean()
            
            if not np.isnan(bin_iv):
                smiles.append(bin_iv)
        
        # Calculate smile curvature (higher = steeper smile)
        if len(smiles) >= 3:
            return smiles[0] - smiles[-1]  # ATM vs far OTM
        return 0.0
    
    def _calculate_option_skew(self, option_chain_df: pd.DataFrame) -> float:
        """Calculate option skew (put IV - call IV)."""
        if option_chain_df.empty:
            return 0.0
        
        put_iv = option_chain_df[option_chain_df['option_type'] == 'PE']['iv'].mean()
        call_iv = option_chain_df[option_chain_df['option_type'] == 'CE']['iv'].mean()
        
        if np.isnan(put_iv) or np.isnan(call_iv):
            return 0.0
        
        return put_iv - call_iv  # Positive = put skew (bearish), Negative = call skew (bullish)
    
    def _detect_spring_pattern(self, price_series: pd.Series, 
                              volume_series: pd.Series,
                              constituents_df: pd.DataFrame) -> float:
        """Detect Wyckoff spring pattern (bear trap)."""
        if len(price_series) < 10 or len(volume_series) < 10:
            return 0.0
        
        # Simplified spring detection
        recent_prices = price_series.iloc[-5:].values
        recent_volumes = volume_series.iloc[-5:].values
        
        # Spring: price makes lower low but closes above previous low
        if len(recent_prices) >= 5:
            low1 = np.min(recent_prices[:3])  # First low
            low2 = np.min(recent_prices[2:])  # Second low (potential spring)
            close = recent_prices[-1]
            
            # Volume spike on second low
            volume_spike = recent_volumes[-2] > np.mean(recent_volumes[:-1]) * 1.5
            
            if low2 < low1 * 0.995 and close > low1 and volume_spike:
                # Calculate spring probability
                price_recovery = (close - low2) / low2
                return min(price_recovery * 10, 1.0)
        
        return 0.0
    
    def _detect_upthrust_pattern(self, price_series: pd.Series,
                                volume_series: pd.Series,
                                constituents_df: pd.DataFrame) -> float:
        """Detect Wyckoff upthrust pattern (bull trap)."""
        if len(price_series) < 10 or len(volume_series) < 10:
            return 0.0
        
        recent_prices = price_series.iloc[-5:].values
        recent_volumes = volume_series.iloc[-5:].values
        
        # Upthrust: price makes higher high but closes below previous high
        if len(recent_prices) >= 5:
            high1 = np.max(recent_prices[:3])  # First high
            high2 = np.max(recent_prices[2:])  # Second high (potential upthrust)
            close = recent_prices[-1]
            
            # Volume spike on second high
            volume_spike = recent_volumes[-2] > np.mean(recent_volumes[:-1]) * 1.5
            
            if high2 > high1 * 1.005 and close < high1 and volume_spike:
                # Calculate upthrust probability
                price_rejection = (high2 - close) / high2
                return min(price_rejection * 10, 1.0)
        
        return 0.0
    
    def _calculate_accumulation_score(self, price_series: pd.Series,
                                     volume_series: pd.Series,
                                     oi_velocity: float) -> float:
        """Calculate accumulation/distribution score."""
        if len(price_series) < 20 or len(volume_series) < 20:
            return 0.0
        
        # Price in trading range
        price_range = price_series.iloc[-20:]
        range_high = price_range.max()
        range_low = price_range.min()
        range_width = (range_high - range_low) / range_low
        
        # Low volatility in range (accumulation)
        volatility = price_range.pct_change().std()
        
        # Volume analysis
        volume_trend = np.polyfit(range(len(volume_series.iloc[-20:])), 
                                 volume_series.iloc[-20:].values, 1)[0]
        
        # OI building during range (accumulation)
        oi_building = oi_velocity > 0.5
        
        # Calculate accumulation score
        score = 0.0
        
        # Narrow range with low volatility
        if range_width < 0.02 and volatility < 0.005:
            score += 0.3
        
        # Volume declining or stable (not distribution)
        if volume_trend <= 0:
            score += 0.2
        
        # OI building
        if oi_building:
            score += 0.3
        
        # Price near range lows (better accumulation)
        current_price = price_series.iloc[-1]
        if current_price < range_low * 1.02:
            score += 0.2
        
        return min(score, 1.0)

# ==============================
# ENHANCED FEATURE PIPELINE
# ==============================

# Global feature engine
_feature_engine = EnhancedFeatureEngine()

def build_and_store_features(
    *,
    timestamp: pd.Timestamp,
    option_chain_df: pd.DataFrame,
    spot_price: float,
    expiry_datetime,
    ltp: float,
    vwap: float,
    price_series: pd.Series,
    volume_series: pd.Series,
    constituents_df: pd.DataFrame,
    ccc_history: pd.Series,
    client: Optional[UpstoxClient] = None,
    option_keys: Optional[list] = None,
    mode: PipelineMode = PipelineMode.RESEARCH  # ‚¨ÖÔ∏è NEW
) -> Optional[Dict]:

    """
    Enhanced feature pipeline with research-based feature engineering.
    
    Args:
        client: UpstoxClient instance (required for advanced analytics)
        option_keys: Option keys for fetching enhanced analytics
    
    Returns:
        Dictionary with features and research analytics
    """
    
    # ------------------------------
    # TIMESTAMP VALIDATION
    # ------------------------------
    if not isinstance(timestamp, pd.Timestamp):
        raise TypeError("timestamp must be pandas.Timestamp")
    
    timestamp_str = timestamp.strftime("%Y-%m-%dT%H:%M:%S.%f")
    
    # ------------------------------
    # FETCH ENHANCED ANALYTICS (if client provided)
    # ------------------------------
    research_analytics = None

    if mode == PipelineMode.EXECUTION:
        if not client:
            raise RuntimeError("EXECUTION mode requires UpstoxClient")

        if not option_keys:
            raise RuntimeError("EXECUTION mode requires non-empty option_keys")

        try:
            research_analytics = client.fetch_option_chain_with_analytics(
                option_keys, spot_price
            )
        except Exception as e:
            raise RuntimeError(f"Execution analytics unavailable: {e}")

    else:
        # RESEARCH MODE ‚Äî best effort only
        if client and option_keys:
            try:
                research_analytics = client.fetch_option_chain_with_analytics(
                    option_keys, spot_price
                )
            except Exception as e:
                print(f"‚ö†Ô∏è Research analytics skipped: {e}")
                research_analytics = None

    
    # ------------------------------
    # COMPUTE BASE FEATURES
    # ------------------------------
    option_feats = compute_option_features(option_chain_df, spot_price, expiry_datetime)
    price_feats = compute_price_features(ltp, vwap, price_series, volume_series)
    breadth_feats = compute_breadth_features(constituents_df, ccc_history)
    
    # ------------------------------
    # COMPUTE RESEARCH FEATURES
    # ------------------------------
    research_feats_dict = {}
    if research_analytics:
        try:
            research_feats = _feature_engine.extract_research_features(
                research_analytics,
                spot_price,
                price_series,
                volume_series,
                constituents_df
            )
            
            # Convert research features to dictionary
            research_feats_dict = {
                # OI Velocity features
                "oi_velocity": research_feats.oi_velocity,
                "oi_velocity_ma": research_feats.oi_velocity_ma,
                "oi_velocity_std": research_feats.oi_velocity_std,
                "oi_regime_expansive": 1.0 if research_feats.oi_regime == "EXPANSIVE" else 0.0,
                "oi_regime_constricted": 1.0 if research_feats.oi_regime == "CONSTRICTED" else 0.0,
                
                # Gamma Exposure features
                "net_gamma": research_feats.net_gamma,
                "gamma_regime_positive": 1.0 if "POSITIVE" in research_feats.gamma_regime else 0.0,
                "gamma_regime_negative": 1.0 if "NEGATIVE" in research_feats.gamma_regime else 0.0,
                "gamma_flip_distance": research_feats.gamma_flip_distance,
                "max_gamma_strike_distance": abs(spot_price - research_feats.max_gamma_strike) / spot_price,
                
                # Structural features
                "wall_strength": research_feats.wall_strength,
                "wall_defense_score": research_feats.wall_defense_score,
                "trap_probability": research_feats.trap_probability,
                
                # Divergence features
                "price_oi_divergence": research_feats.price_oi_divergence,
                "price_gamma_divergence": research_feats.price_gamma_divergence,
                "divergence_score": research_feats.divergence_score,
                "has_divergence": 1.0 if research_feats.divergence_score > 0.3 else 0.0,
                
                # Market microstructure
                "put_call_ratio": research_feats.put_call_ratio,
                "max_pain_distance": research_feats.max_pain_distance,
                "vix_smile": research_feats.vix_smile,
                "skewness": research_feats.skewness,
                
                # Wyckoff features
                "spring_detection": research_feats.spring_detection,
                "upthrust_detection": research_feats.upthrust_detection,
                "accumulation_score": research_feats.accumulation_score,
                
                # Derived features
                "gamma_wall_interaction": research_feats.wall_strength * abs(research_feats.net_gamma),
                "velocity_divergence_composite": research_feats.oi_velocity * research_feats.divergence_score,
                "trap_gamma_composite": research_feats.trap_probability * (1.0 if research_feats.gamma_regime == "NEGATIVE" else 0.0)
            }
        except Exception as e:
            print(f"Warning: Research feature extraction failed: {e}")
            research_feats_dict = {}
    
    # ------------------------------
    # BUILD FEATURE ROW
    # ------------------------------
    feature_row = {
        "timestamp": timestamp_str,
        "feature_version": FEATURE_VERSION,
        "future_return_5m": None
    }
    
    # Fill ALL required feature columns explicitly
    for col in FEATURE_COLUMNS:
        if col == "timestamp":
            continue

        if col in option_feats:
            feature_row[col] = option_feats[col]
        elif col in price_feats:
            feature_row[col] = price_feats[col]
        elif col in breadth_feats:
            feature_row[col] = breadth_feats[col]
        else:
            feature_row[col] = 0.0

    
    # Add time to expiry with proper timezone handling
    def normalize_datetime(dt):
        """Normalize datetime to UTC timezone-aware."""
        if dt is None:
            return None
        
        # If it's already timezone-aware, convert to UTC
        if hasattr(dt, 'tzinfo') and dt.tzinfo is not None:
            return dt.astimezone(timezone.utc)
        
        # If it's timezone-naive, assume UTC and make it aware
        if isinstance(dt, pd.Timestamp):
            if dt.tz is None:
                return dt.tz_localize('UTC')
            else:
                return dt.tz_convert('UTC')
        elif isinstance(dt, datetime):
            return dt.replace(tzinfo=timezone.utc)
        
        return dt
    
    # Normalize both timestamps to UTC
    if expiry_datetime and timestamp:
        expiry_utc = normalize_datetime(expiry_datetime)
        timestamp_utc = normalize_datetime(timestamp)
        
        if expiry_utc and timestamp_utc:
            time_diff = (expiry_utc - timestamp_utc).total_seconds() / 60
            feature_row["time_to_expiry_minutes"] = max(int(time_diff), 0)
        else:
            feature_row["time_to_expiry_minutes"] = 0
    else:
        feature_row["time_to_expiry_minutes"] = 0
    
    # Merge research features
    feature_row.update(research_feats_dict)
    
    # ------------------------------
    # VALIDATE AND PERSIST
    # ------------------------------
    # Create DataFrame with all columns (base + research)
    all_columns = list(feature_row.keys())
    df = pd.DataFrame([feature_row], columns=all_columns)
    
    # Debug info
    print(f"FEATURE PIPELINE: Generated {len(feature_row)} features")
    print(f"Timestamp: {timestamp_str}")
    print(f"Spot Price: {spot_price}")
    
    if research_analytics:
        print(f"Research Analytics: {len(research_feats_dict)} research features added")
        # Log key metrics
        analytics = research_analytics.get("analytics", {})
        print(f"OI Velocity: {analytics.get('oi_velocity', 'N/A')}")
        print(f"Gamma Regime: {analytics.get('gamma_exposure', {}).get('regime', 'N/A')}")
        print(f"Market Regime: {analytics.get('market_regime', 'N/A')}")
    
    # Check for NULLs
    null_cols = df.columns[df.isnull().any()].tolist()
    if null_cols:
        print(f"WARNING: NULL values in columns: {null_cols}")
        # Fill NULLs with 0 for numeric columns
        for col in null_cols:
            if col in df.select_dtypes(include=[np.number]).columns:
                df[col] = df[col].fillna(0.0)
    
    assert not df.empty, "Feature DataFrame is empty"
    
    # Persist to database
    insert_market_features(df)
    
    print(f"‚úì Features stored successfully at {timestamp_str}")

    execution_ready = (
        mode == PipelineMode.EXECUTION and
        research_analytics is not None and
        len(research_feats_dict) > 0 and
        feature_row.get("oi_velocity_std", 0) > 0 and
        feature_row.get("time_to_expiry_minutes", 0) > 0
    )


    # Return comprehensive result
    return {
        "features": feature_row,
        "research_analytics": research_analytics.get("analytics", {}) if research_analytics else {},
        "market_insights": research_analytics.get("market_insights", []) if research_analytics else [],
        "timestamp": timestamp_str,
        "spot_price": spot_price,
        "execution_ready": execution_ready
    }


# ==============================
# UTILITY FUNCTIONS
# ==============================

def create_feature_summary(feature_row: Dict) -> Dict:
    """Create a summary of key features for display."""
    summary = {
        "timestamp": feature_row.get("timestamp", "N/A"),
        "base_features": {},
        "research_features": {},
        "signals": {}
    }
    
    # Base features
    base_keys = ["put_call_ratio", "vwap_distance", "price_momentum", 
                 "ccc_value", "ccc_slope", "time_to_expiry_minutes"]
    for key in base_keys:
        if key in feature_row:
            summary["base_features"][key] = feature_row[key]
    
    # Research features (top level)
    research_keys = ["oi_velocity", "net_gamma", "trap_probability", 
                    "divergence_score", "wall_strength"]
    for key in research_keys:
        if key in feature_row:
            summary["research_features"][key] = feature_row[key]
    
    # Generate signals
    signals = []
    
    # OI Velocity signal
    oi_vel = feature_row.get("oi_velocity", 0)
    if oi_vel > 1.5:
        signals.append("üìà Strong OI Buildup")
    elif oi_vel < -1.5:
        signals.append("üìâ OI Unwinding")
    
    # Gamma signal
    gamma_regime_pos = feature_row.get("gamma_regime_positive", 0)
    gamma_regime_neg = feature_row.get("gamma_regime_negative", 0)
    if gamma_regime_pos > 0.5:
        signals.append("üìå Positive Gamma (Stabilizing)")
    elif gamma_regime_neg > 0.5:
        signals.append("üöÄ Negative Gamma (Accelerating)")
    
    # Trap signal
    trap_prob = feature_row.get("trap_probability", 0)
    if trap_prob > 0.7:
        signals.append("üéØ High Trap Probability")
    elif trap_prob > 0.5:
        signals.append("‚ö†Ô∏è Moderate Trap Risk")
    
    # Divergence signal
    has_div = feature_row.get("has_divergence", 0)
    if has_div > 0.5:
        signals.append("üîç Divergence Detected")
    
    summary["signals"] = signals
    
    return summary

def validate_feature_contract(feature_row: Dict) -> bool:
    """Validate feature row against contract."""
    # Check required columns
    required_base = ["timestamp", "feature_version"]
    for col in required_base:
        if col not in feature_row:
            print(f"Missing required column: {col}")
            return False
    
    # Check data types
    for col, value in feature_row.items():
        if col == "timestamp":
            continue
        if col == "feature_version":
            if not isinstance(value, str):
                print(f"Invalid type for {col}: expected str, got {type(value)}")
                return False
        elif col in ["oi_regime_expansive", "oi_regime_constricted", 
                    "gamma_regime_positive", "gamma_regime_negative", "has_divergence"]:
            # Binary features
            if not isinstance(value, (int, float)):
                print(f"Invalid type for {col}: expected numeric, got {type(value)}")
                return False
        elif isinstance(value, (int, float)):
            # Numeric features - check for extreme values
            if abs(value) > 1e6:  # Unreasonable large value
                print(f"Extreme value for {col}: {value}")
                return False
            if pd.isna(value):
                print(f"NaN value for {col}")
                return False
    
    return True


--------------------------------------------------
FILE PATH : G:\trading_app\core\scheduler.py
SIZE      : 15173 bytes
--------------------------------------------------

from datetime import datetime, time, timedelta, timezone
from typing import Callable, Optional, Dict
import threading
import time as time_module  # Rename to avoid conflict with datetime.time
import streamlit as st
import numpy as np  # Add numpy import for metrics

class MarketScheduler:
    """
    Enhanced scheduler with research-aware execution control.
    
    Features:
    - Market hours detection
    - Intelligent interval adjustment
    - Execution throttling
    - Performance monitoring
    - Error recovery
    """
    
    def __init__(
        self,
        interval_seconds: int = 30,
        market_open: time = time(9, 15),  # CORRECT: use time objects
        market_close: time = time(15, 30),  # CORRECT: use time objects
        pre_market_minutes: int = 15,
        post_market_minutes: int = 15
    ):
        self.interval_seconds = interval_seconds
        self.market_open = market_open
        self.market_close = market_close
        self.pre_market_minutes = pre_market_minutes
        self.post_market_minutes = post_market_minutes
        
        # Execution tracking
        self._last_run: Optional[datetime] = None
        self._last_success: Optional[datetime] = None
        self._consecutive_failures: int = 0
        self._total_executions: int = 0
        self._lock = threading.Lock()
        
        # Performance metrics
        self.execution_times = []
        self.error_log = []
        
        # Adaptive interval (can adjust based on market conditions)
        self.min_interval = 10  # seconds
        self.max_interval = 300  # seconds
        self._current_interval = interval_seconds
        
    # ==============================
    # TIME MANAGEMENT
    # ==============================
    
    def _is_market_open(self, now: datetime) -> bool:
        """
        Check if market is open, including pre/post market.
        """
        t = now.time()
        
        # Pre-market period
        pre_market_start = time(
            self.market_open.hour,
            max(0, self.market_open.minute - self.pre_market_minutes)  # Ensure minutes don't go negative
        )
        
        # Post-market period
        post_market_end_minute = self.market_close.minute + self.post_market_minutes
        post_market_end_hour = self.market_close.hour + (post_market_end_minute // 60)
        post_market_end_minute = post_market_end_minute % 60
        post_market_end = time(
            post_market_end_hour,
            post_market_end_minute
        )
        
        return pre_market_start <= t <= post_market_end
    
    def _is_core_market_hours(self, now: datetime) -> bool:
        """Check if it's core trading hours."""
        t = now.time()
        return self.market_open <= t <= self.market_close
    
    def _get_time_to_market_open(self, now: datetime) -> Optional[float]:
        """Get seconds until market opens."""
        market_open_today = datetime.combine(now.date(), self.market_open)
        
        if now < market_open_today:
            return (market_open_today - now).total_seconds()
        
        # Market already open today, check tomorrow
        tomorrow = now.date() + timedelta(days=1)
        market_open_tomorrow = datetime.combine(tomorrow, self.market_open)
        return (market_open_tomorrow - now).total_seconds()
    
    def _get_market_status(self, now: datetime) -> Dict[str, any]:
        """Get detailed market status."""
        status = {
            "is_market_open": self._is_market_open(now),
            "is_core_hours": self._is_core_market_hours(now),
            "current_time": now,
            "market_open": self.market_open,
            "market_close": self.market_close
        }
        
        if not status["is_market_open"]:
            time_to_open = self._get_time_to_market_open(now)
            if time_to_open:
                status["time_to_open_hours"] = time_to_open / 3600
                status["next_open"] = now + timedelta(seconds=time_to_open)
        
        return status
    
    # ==============================
    # EXECUTION MANAGEMENT
    # ==============================
    
    def _is_due(self, now: datetime) -> bool:
        """Check if execution is due."""
        if self._last_run is None:
            return True
        
        delta = (now - self._last_run).total_seconds()
        return delta >= self._current_interval
    
    def _adjust_interval(self, execution_time: float, success: bool):
        """
        Adaptively adjust execution interval based on performance.
        """
        if not success:
            self._consecutive_failures += 1
            
            # Increase interval on consecutive failures
            if self._consecutive_failures >= 3:
                self._current_interval = min(
                    self._current_interval * 1.5,
                    self.max_interval
                )
                self._consecutive_failures = 0
        else:
            self._consecutive_failures = 0
            self._last_success = datetime.now()
            
            # Adjust interval based on execution time
            if execution_time > self._current_interval * 0.8:
                # Execution taking too long, increase interval
                self._current_interval = min(
                    self._current_interval * 1.2,
                    self.max_interval
                )
            elif execution_time < self._current_interval * 0.3:
                # Execution fast, can decrease interval (but not below min)
                self._current_interval = max(
                    self._current_interval * 0.9,
                    self.min_interval
                )
    
    def _should_execute(self, now: datetime, market_regime: Optional[str] = None) -> bool:
        """
        Determine if execution should proceed based on multiple factors.
        """
        # Basic checks
        if not self._is_market_open(now):
            return False
        
        if not self._is_due(now):
            return False
        
        # Check for too many recent failures
        if self._consecutive_failures >= 5:
            if self._last_success:
                time_since_success = (now - self._last_success).total_seconds()
                if time_since_success < 300:  # 5 minutes
                    return False  # Wait after multiple failures
        
        # Market regime-based adjustments
        if market_regime:
            # Execute more frequently during volatile regimes
            if market_regime in ["SQUEEZE", "BREAKOUT", "ACCELERATING"]:
                self._current_interval = max(self.min_interval, self.interval_seconds * 0.7)
            # Execute less frequently during stable regimes
            elif market_regime in ["RANGING", "STABILIZING"]:
                self._current_interval = min(self.max_interval, self.interval_seconds * 1.3)
        
        return True
    
    # ==============================
    # EXECUTION ENTRYPOINT
    # ==============================
    
    def run_if_due(self, run_cycle: Callable[[], any], market_regime: Optional[str] = None) -> Dict[str, any]:
        """
        Execute run_cycle() if due, with enhanced monitoring.
        
        Returns:
            Execution result dictionary
        """
        now = datetime.now()
        
        # Check if should execute
        if not self._should_execute(now, market_regime):
            return {
                "executed": False,
                "reason": "Not due or market closed",
                "next_execution_in": self._get_next_execution_time(now),
                "market_status": self._get_market_status(now)
            }
        
        # Thread-safe execution
        with self._lock:
            # Double-check inside lock
            if not self._is_due(now):
                return {
                    "executed": False,
                    "reason": "Already executed by another thread",
                    "next_execution_in": self._get_next_execution_time(now)
                }
            
            execution_start = datetime.now()
            result = {
                "executed": True,
                "start_time": execution_start.isoformat(),
                "success": False,
                "error": None,
                "execution_time": 0.0
            }
            
            try:
                # Execute the cycle
                cycle_result = run_cycle()
                execution_end = datetime.now()
                execution_time = (execution_end - execution_start).total_seconds()
                
                # Update tracking
                self._last_run = execution_start
                self._total_executions += 1
                self.execution_times.append(execution_time)
                
                # Keep only recent execution times
                if len(self.execution_times) > 100:
                    self.execution_times = self.execution_times[-100:]
                
                # Update result
                result.update({
                    "success": True,
                    "execution_time": execution_time,
                    "end_time": execution_end.isoformat(),
                    "cycle_result": cycle_result,
                    "current_interval": self._current_interval
                })
                
                # Adjust interval based on performance
                self._adjust_interval(execution_time, success=True)
                
            except Exception as e:
                execution_end = datetime.now()
                execution_time = (execution_end - execution_start).total_seconds()
                
                # Log error
                self.error_log.append({
                    "timestamp": execution_start.isoformat(),
                    "error": str(e),
                    "execution_time": execution_time
                })
                
                # Keep error log manageable
                if len(self.error_log) > 50:
                    self.error_log = self.error_log[-50:]
                
                # Update result
                result.update({
                    "success": False,
                    "error": str(e),
                    "execution_time": execution_time,
                    "end_time": execution_end.isoformat()
                })
                
                # Adjust interval on failure
                self._adjust_interval(execution_time, success=False)
                
                # Update last run time even on failure (to prevent rapid retry)
                self._last_run = execution_start
                self._total_executions += 1
            
            return result
    
    # ==============================
    # MONITORING & METRICS
    # ==============================
    
    def get_metrics(self) -> Dict[str, any]:
        """Get scheduler performance metrics."""
        metrics = {
            "total_executions": self._total_executions,
            "current_interval": self._current_interval,
            "last_run": self._last_run.isoformat() if self._last_run else None,
            "last_success": self._last_success.isoformat() if self._last_success else None,
            "consecutive_failures": self._consecutive_failures,
            "recent_error_count": len(self.error_log[-10:]),
            "is_market_open_now": self._is_market_open(datetime.now())
        }
        
        # Add execution time statistics
        if self.execution_times:
            metrics.update({
                "avg_execution_time": np.mean(self.execution_times),
                "max_execution_time": np.max(self.execution_times),
                "min_execution_time": np.min(self.execution_times),
                "recent_avg_execution_time": np.mean(self.execution_times[-10:]) if len(self.execution_times) >= 10 else None
            })
        
        return metrics
    
    def _get_next_execution_time(self, now: datetime) -> float:
        """Get seconds until next execution."""
        if self._last_run is None:
            return 0.0
        
        next_run = self._last_run + timedelta(seconds=self._current_interval)
        return max((next_run - now).total_seconds(), 0.0)
    
    def reset(self):
        """Reset scheduler state."""
        with self._lock:
            self._last_run = None
            self._last_success = None
            self._consecutive_failures = 0
            self._total_executions = 0
            self.execution_times = []
            self.error_log = []
            self._current_interval = self.interval_seconds
    
    def set_interval(self, interval_seconds: int):
        """Dynamically set execution interval."""
        with self._lock:
            self.interval_seconds = interval_seconds
            self._current_interval = max(
                min(interval_seconds, self.max_interval),
                self.min_interval
            )

# ==============================
# UTILITY FUNCTIONS
# ==============================

def create_market_scheduler(
    interval_seconds: int = 30,
    market_open: str = "09:15",
    market_close: str = "15:30"
) -> MarketScheduler:
    """
    Factory function to create market scheduler.
    
    Args:
        interval_seconds: Execution interval in seconds
        market_open: Market open time (HH:MM)
        market_close: Market close time (HH:MM)
    """
    # Parse time strings
    open_hour, open_minute = map(int, market_open.split(":"))
    close_hour, close_minute = map(int, market_close.split(":"))
    
    return MarketScheduler(
        interval_seconds=interval_seconds,
        market_open=time(open_hour, open_minute),  # CORRECT
        market_close=time(close_hour, close_minute)  # CORRECT
    )

def display_scheduler_status(scheduler: MarketScheduler):
    """Display scheduler status in Streamlit."""
    if not scheduler:
        return
    
    metrics = scheduler.get_metrics()
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if metrics["is_market_open_now"]:
            st.success("üü¢ Market Open")
        else:
            st.warning("üî¥ Market Closed")
        
        next_exec = scheduler._get_next_execution_time(datetime.now())
        if next_exec > 0:
            st.metric("Next Execution", f"{int(next_exec)}s")
    
    with col2:
        st.metric("Interval", f"{metrics['current_interval']}s")
        
        if metrics.get("avg_execution_time"):
            st.metric("Avg Exec Time", f"{metrics['avg_execution_time']:.1f}s")
    
    with col3:
        st.metric("Total Executions", metrics["total_executions"])
        
        if metrics["recent_error_count"] > 0:
            st.error(f"Recent Errors: {metrics['recent_error_count']}")


--------------------------------------------------
FILE PATH : G:\trading_app\core\session - Copy.py
SIZE      : 23310 bytes
--------------------------------------------------

"""
Enhanced Upstox Session Manager with persistent token storage,
refresh mechanism, and market state tracking.
Incorporates research insights: funding liquidity monitoring via OI velocity
and market microstructure analysis.
"""

import os
import json
import requests
import streamlit as st
from urllib.parse import urlencode
from typing import Optional, Dict, Tuple
from datetime import datetime, timedelta
import pickle
from pathlib import Path
from cryptography.fernet import Fernet
import hashlib

# ==============================
# CONFIG (EDIT THESE)
# ==============================

UPSTOX_AUTH_URL = "https://api.upstox.com/v2/login/authorization/dialog"
UPSTOX_TOKEN_URL = "https://api.upstox.com/v2/login/authorization/token"
UPSTOX_PROFILE_URL = "https://api.upstox.com/v2/user/profile"

# Get from Streamlit secrets or environment
try:
    CLIENT_ID = st.secrets["UPSTOX_CLIENT_ID"]
    CLIENT_SECRET = st.secrets["UPSTOX_CLIENT_SECRET"]
    REDIRECT_URI = st.secrets["UPSTOX_REDIRECT_URI"]
except:
    CLIENT_ID = os.getenv("UPSTOX_CLIENT_ID")
    CLIENT_SECRET = os.getenv("UPSTOX_CLIENT_SECRET")
    REDIRECT_URI = os.getenv("UPSTOX_REDIRECT_URI")

# ==============================
# SECURE TOKEN STORAGE
# ==============================

class TokenStorage:
    """
    Secure token storage with encryption and persistence.
    Stores tokens locally with automatic refresh capability.
    """
    
    def __init__(self, storage_path: str = "data/tokens"):
        self.storage_dir = Path(storage_path)
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate or load encryption key
        self.key_file = self.storage_dir / ".key"
        if self.key_file.exists():
            with open(self.key_file, 'rb') as f:
                self.encryption_key = f.read()
        else:
            self.encryption_key = Fernet.generate_key()
            with open(self.key_file, 'wb') as f:
                f.write(self.encryption_key)
        
        self.cipher = Fernet(self.encryption_key)
        self.token_file = self.storage_dir / "upstox_tokens.enc"
        
        # Track funding liquidity state (research concept)
        self.funding_state = {
            "last_oi_velocity": 0.0,
            "liquidity_regime": "NORMAL",  # NORMAL, CONSTRICTED, EXPANSIVE
            "last_update": datetime.utcnow()
        }
    
    def _generate_user_hash(self) -> str:
        """Generate unique user identifier for multi-user support"""
        # In production, use actual user ID. Here we use client ID as proxy
        return hashlib.sha256(f"{CLIENT_ID}_{REDIRECT_URI}".encode()).hexdigest()[:16]
    
    def save_tokens(self, access_token: str, refresh_token: str, expires_in: int):
        """Securely save tokens with expiration"""
        token_data = {
            "access_token": access_token,
            "refresh_token": refresh_token,
            "expires_at": (datetime.utcnow() + timedelta(seconds=expires_in)).isoformat(),
            "client_id": CLIENT_ID,
            "user_hash": self._generate_user_hash(),
            "created_at": datetime.utcnow().isoformat()
        }
        
        # Encrypt and save
        encrypted = self.cipher.encrypt(pickle.dumps(token_data))
        with open(self.token_file, 'wb') as f:
            f.write(encrypted)
        
        st.success("‚úì Authentication tokens saved securely")
    
    def load_tokens(self) -> Optional[Dict]:
        """Load and decrypt tokens, check expiration"""
        if not self.token_file.exists():
            return None
        
        try:
            with open(self.token_file, 'rb') as f:
                encrypted = f.read()
            
            token_data = pickle.loads(self.cipher.decrypt(encrypted))
            
            # Check expiration
            expires_at = datetime.fromisoformat(token_data["expires_at"])
            if datetime.utcnow() > expires_at - timedelta(minutes=5):  # 5 min buffer
                st.warning("‚ö†Ô∏è Access token expired or near expiry")
                return None
            
            # Verify client matches
            if token_data.get("client_id") != CLIENT_ID:
                st.error("Client ID mismatch - reauthentication required")
                return None
            
            return token_data
        except Exception as e:
            st.error(f"Token load error: {e}")
            return None
    
    def update_funding_state(self, oi_velocity: float):
        """
        Update funding liquidity state based on OI velocity.
        Research concept: Monitor capital flow intensity.
        """
        self.funding_state["last_oi_velocity"] = oi_velocity
        
        # Determine liquidity regime (research concept)
        if oi_velocity > 1.5:
            regime = "EXPANSIVE"  # High capital inflow
        elif oi_velocity < -1.5:
            regime = "CONSTRICTED"  # Capital outflow
        else:
            regime = "NORMAL"
        
        if self.funding_state["liquidity_regime"] != regime:
            st.info(f"üí∞ Liquidity regime changed: {regime} (OI Velocity: {oi_velocity:.2f})")
        
        self.funding_state["liquidity_regime"] = regime
        self.funding_state["last_update"] = datetime.utcnow()
    
    def get_funding_state(self) -> Dict:
        """Get current funding liquidity state"""
        return self.funding_state.copy()

# ==============================
# ENHANCED SESSION MANAGER
# ==============================

class UpstoxSession:
    """
    Enhanced session manager with:
    1. Persistent token storage
    2. Automatic token refresh
    3. Funding liquidity tracking
    4. Market state awareness
    """
    
    SESSION_KEY = "upstox_access_token"
    SESSION_REFRESH_KEY = "upstox_refresh_token"
    SESSION_PROFILE_KEY = "upstox_profile"
    
    _token_storage = TokenStorage()
    _market_state = {
        "current_regime": None,
        "last_velocity": 0.0,
        "gamma_regime": None,  # POSITIVE/NEGATIVE from research
        "wall_levels": {"call": None, "put": None},
        "trap_zones": []
    }
    
    @staticmethod
    def is_authenticated() -> bool:
        """Check if user has valid authentication"""
        if UpstoxSession.SESSION_KEY in st.session_state:
            return True
        
        # Check persistent storage
        tokens = UpstoxSession._token_storage.load_tokens()
        if tokens:
            # Load into session state
            st.session_state[UpstoxSession.SESSION_KEY] = tokens["access_token"]
            st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = tokens["refresh_token"]
            return True
        
        return False
    
    @staticmethod
    def get_access_token() -> Optional[str]:
        """Get current access token, refresh if needed"""
        if not UpstoxSession.is_authenticated():
            return None
        
        # Check if token is in session state
        if UpstoxSession.SESSION_KEY in st.session_state:
            return st.session_state[UpstoxSession.SESSION_KEY]
        
        return None
    
    @staticmethod
    def logout() -> None:
        """Secure logout - clear all session and storage"""
        for key in [UpstoxSession.SESSION_KEY, 
                   UpstoxSession.SESSION_REFRESH_KEY,
                   UpstoxSession.SESSION_PROFILE_KEY]:
            if key in st.session_state:
                del st.session_state[key]
        
        # Clear persistent storage
        storage_file = Path("data/tokens/upstox_tokens.enc")
        if storage_file.exists():
            storage_file.unlink()
        
        # Clear query params
        st.query_params.clear()
        st.success("‚úì Logged out successfully")
        st.rerun()
    
    # ==========================
    # TOKEN MANAGEMENT
    # ==========================
    
    @staticmethod
    def refresh_access_token(refresh_token: str) -> Optional[Tuple[str, str]]:
        """
        Refresh expired access token using refresh token.
        Returns (new_access_token, new_refresh_token) or None
        """
        headers = {
            "Content-Type": "application/x-www-form-urlencoded",
            "Accept": "application/json"
        }
        
        data = {
            "grant_type": "refresh_token",
            "refresh_token": refresh_token,
            "client_id": CLIENT_ID,
            "client_secret": CLIENT_SECRET
        }
        
        try:
            response = requests.post(
                UPSTOX_TOKEN_URL,
                headers=headers,
                data=data,
                timeout=10
            )
            
            if response.status_code == 200:
                token_data = response.json()
                return token_data.get("access_token"), token_data.get("refresh_token")
            else:
                st.error(f"Token refresh failed: {response.text}")
                return None
                
        except Exception as e:
            st.error(f"Token refresh error: {e}")
            return None
    
    @staticmethod
    def exchange_code_for_token(code: str) -> Optional[Dict]:
        """Exchange authorization code for tokens with full token data"""
        headers = {
            "Content-Type": "application/x-www-form-urlencoded",
            "Accept": "application/json"
        }
        
        data = {
            "code": code,
            "client_id": CLIENT_ID,
            "client_secret": CLIENT_SECRET,
            "redirect_uri": REDIRECT_URI,
            "grant_type": "authorization_code"
        }
        
        try:
            response = requests.post(
                UPSTOX_TOKEN_URL,
                headers=headers,
                data=data,
                timeout=10
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                st.error(f"Token exchange failed: {response.text}")
                return None
                
        except Exception as e:
            st.error(f"Token exchange error: {e}")
            return None
    
    # ==========================
    # USER PROFILE
    # ==========================
    
    @staticmethod
    def get_user_profile(access_token: str) -> Optional[Dict]:
        """Fetch user profile for session context"""
        headers = {
            "Accept": "application/json",
            "Authorization": f"Bearer {access_token}"
        }
        
        try:
            response = requests.get(UPSTOX_PROFILE_URL, headers=headers, timeout=10)
            if response.status_code == 200:
                return response.json().get("data", {})
            return None
        except Exception as e:
            st.warning(f"Profile fetch warning: {e}")
            return None
    
    # ==========================
    # MARKET STATE TRACKING (Research Integration)
    # ==========================
    
    @staticmethod
    def update_market_state(oi_velocity: float, gamma_exposure: float, 
                           wall_levels: Dict, spot_divergence: float):
        """
        Update comprehensive market state based on research concepts.
        
        Args:
            oi_velocity: Rate of change of Open Interest
            gamma_exposure: Net Gamma Exposure (positive/negative)
            wall_levels: Dict with 'call' and 'put' wall strike levels
            spot_divergence: Divergence between spot and derivative metrics
        """
        # Update funding liquidity state
        UpstoxSession._token_storage.update_funding_state(oi_velocity)
        
        # Determine gamma regime (research concept)
        if gamma_exposure > 0:
            gamma_regime = "POSITIVE"  # Stabilizing, pinning expected
        elif gamma_exposure < 0:
            gamma_regime = "NEGATIVE"  # Accelerating, squeezes possible
        else:
            gamma_regime = "NEUTRAL"
        
        # Detect potential traps (research concept)
        trap_zones = []
        current_price = 0  # Would be passed in production
        
        # Check if price is near walls with high OI unwinding
        for side, level in wall_levels.items():
            if level and abs(current_price - level) / level < 0.005:  # Within 0.5%
                # High probability of trap if OI velocity is negative (unwinding)
                if oi_velocity < -1.0:
                    trap_zones.append({
                        "side": side,
                        "level": level,
                        "type": "GAMMA_TRAP" if abs(gamma_exposure) > 0.5 else "OI_TRAP",
                        "confidence": min(abs(oi_velocity) / 3, 1.0)
                    })
        
        # Update market state
        UpstoxSession._market_state.update({
            "last_velocity": oi_velocity,
            "gamma_regime": gamma_regime,
            "wall_levels": wall_levels,
            "trap_zones": trap_zones,
            "spot_divergence": spot_divergence,
            "funding_regime": UpstoxSession._token_storage.get_funding_state()["liquidity_regime"],
            "timestamp": datetime.utcnow().isoformat()
        })
    
    @staticmethod
    def get_market_state() -> Dict:
        """Get current market state analysis"""
        return UpstoxSession._market_state.copy()
    
    # ==========================
    # STREAMLIT ENTRYPOINT (ENHANCED)
    # ==========================
    
    @staticmethod
    def authenticate() -> str:
        """
        Enhanced authentication flow with:
        1. Persistent token storage
        2. Automatic refresh
        3. User profile loading
        4. Market state initialization
        """
        
        # Already authenticated in session
        if UpstoxSession.is_authenticated():
            token = UpstoxSession.get_access_token()
            
            # Load profile if not loaded
            if UpstoxSession.SESSION_PROFILE_KEY not in st.session_state:
                profile = UpstoxSession.get_user_profile(token)
                if profile:
                    st.session_state[UpstoxSession.SESSION_PROFILE_KEY] = profile
            
            return token
        
        # Check persistent storage
        stored_tokens = UpstoxSession._token_storage.load_tokens()
        if stored_tokens:
            # Check if token needs refresh
            expires_at = datetime.fromisoformat(stored_tokens["expires_at"])
            if datetime.utcnow() > expires_at - timedelta(minutes=10):
                # Attempt refresh
                new_tokens = UpstoxSession.refresh_access_token(
                    stored_tokens["refresh_token"]
                )
                if new_tokens:
                    access_token, refresh_token = new_tokens
                    UpstoxSession._token_storage.save_tokens(
                        access_token, refresh_token, 86400
                    )
                    st.session_state[UpstoxSession.SESSION_KEY] = access_token
                    st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = refresh_token
                    st.success("‚úì Token refreshed automatically")
                else:
                    # Refresh failed, need re-auth
                    st.warning("Session expired, please login again")
            else:
                # Use stored token
                st.session_state[UpstoxSession.SESSION_KEY] = stored_tokens["access_token"]
                st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = stored_tokens["refresh_token"]
            
            # Load profile
            token = UpstoxSession.get_access_token()
            profile = UpstoxSession.get_user_profile(token)
            if profile:
                st.session_state[UpstoxSession.SESSION_PROFILE_KEY] = profile
            
            return token
        
        # Check for OAuth callback
        query_params = st.query_params
        if "code" in query_params:
            try:
                # Exchange code for tokens
                token_data = UpstoxSession.exchange_code_for_token(query_params["code"])
                if token_data:
                    # Save tokens
                    UpstoxSession._token_storage.save_tokens(
                        token_data["access_token"],
                        token_data.get("refresh_token", ""),
                        token_data.get("expires_in", 86400)
                    )
                    
                    # Store in session
                    st.session_state[UpstoxSession.SESSION_KEY] = token_data["access_token"]
                    if "refresh_token" in token_data:
                        st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = token_data["refresh_token"]
                    
                    # Load profile
                    profile = UpstoxSession.get_user_profile(token_data["access_token"])
                    if profile:
                        st.session_state[UpstoxSession.SESSION_PROFILE_KEY] = profile
                    
                    # Clean URL
                    st.query_params.clear()
                    st.rerun()
                    
            except Exception as e:
                st.error(f"Authentication failed: {str(e)}")
                st.stop()
        
        # Show login interface with market context
        st.markdown("""
        <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                    padding: 30px; border-radius: 15px; color: white;'>
            <h2 style='color: white;'>üîê Algorithmic Trading Platform Login</h2>
            <p style='font-size: 16px;'>
                Access real-time derivatives analytics including:<br>
                ‚Ä¢ OI Velocity & Gamma Exposure (GEX)<br>
                ‚Ä¢ Structural Walls & Trap Detection<br>
                ‚Ä¢ Spot Divergence Analysis<br>
                ‚Ä¢ Market Microstructure Insights
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("---")
        
        # Login button using Streamlit's native button
        login_url = UpstoxSession.get_login_url()
        
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            if st.button("üìà Login with Upstox", type="primary", use_container_width=True):
                # Redirect to Upstox login
                js = f'window.open("{login_url}", "_blank")'
                st.components.v1.html(f"""
                    <script>
                        {js}
                    </script>
                """, height=0)
            
            st.markdown("""
            <div style="margin-top: 20px; padding: 15px; background: #f8f9fa; border-radius: 10px;">
                <small>üîê <strong>Secure:</strong> Tokens are encrypted and stored locally</small><br>
                <small>üîÑ <strong>Persistent:</strong> Session persists across restarts</small><br>
                <small>üìä <strong>Enhanced:</strong> Market state tracking enabled</small>
            </div>
            """, unsafe_allow_html=True)
            
            st.markdown("""
            <div style="margin-top: 20px; padding: 15px; background: #f8f9fa; border-radius: 10px;">
                <small>üîê <strong>Secure:</strong> Tokens are encrypted and stored locally</small><br>
                <small>üîÑ <strong>Persistent:</strong> Session persists across restarts</small><br>
                <small>üìä <strong>Enhanced:</strong> Market state tracking enabled</small>
            </div>
            """, unsafe_allow_html=True)
        
        st.stop()
    
    @staticmethod
    def get_login_url() -> str:
        """Generate OAuth login URL"""
        params = {
            "response_type": "code",
            "client_id": CLIENT_ID,
            "redirect_uri": REDIRECT_URI,
            "scope": "order placement portfolio"  # Add required scopes
        }
        return f"{UPSTOX_AUTH_URL}?{urlencode(params)}"
    
    @staticmethod
    def get_session_info() -> Dict:
        """Get comprehensive session information"""
        return {
            "authenticated": UpstoxSession.is_authenticated(),
            "has_profile": UpstoxSession.SESSION_PROFILE_KEY in st.session_state,
            "profile": st.session_state.get(UpstoxSession.SESSION_PROFILE_KEY, {}),
            "funding_state": UpstoxSession._token_storage.get_funding_state(),
            "market_state": UpstoxSession.get_market_state(),
            "storage_path": str(UpstoxSession._token_storage.storage_dir)
        }

# ==============================
# SESSION UTILITIES
# ==============================

def display_session_status():
    """Display session status in Streamlit sidebar"""
    if UpstoxSession.is_authenticated():
        with st.sidebar:
            st.markdown("---")
            st.markdown("### üß† Session Status")
            
            # User info
            profile = st.session_state.get(UpstoxSession.SESSION_PROFILE_KEY, {})
            if profile:
                st.markdown(f"**User:** {profile.get('user_name', 'N/A')}")
                st.markdown(f"**Email:** {profile.get('email', 'N/A')}")
            
            # Market state
            market_state = UpstoxSession.get_market_state()
            if market_state.get("funding_regime"):
                regime = market_state["funding_regime"]
                color = {
                    "EXPANSIVE": "üü¢",
                    "NORMAL": "üü°", 
                    "CONSTRICTED": "üî¥"
                }.get(regime, "‚ö™")
                st.markdown(f"**Funding:** {color} {regime}")
            
            if market_state.get("gamma_regime"):
                gamma = market_state["gamma_regime"]
                icon = "üìå" if gamma == "POSITIVE" else "üöÄ" if gamma == "NEGATIVE" else "‚öñÔ∏è"
                st.markdown(f"**Gamma:** {icon} {gamma}")
            
            # Logout button
            if st.button("üö™ Logout", type="secondary", use_container_width=True):
                UpstoxSession.logout()

# ==============================
# INITIALIZATION
# ==============================

def initialize_session():
    """
    Initialize session with enhanced capabilities.
    Call this at the start of your app.
    """
    # Ensure token storage directory exists
    Path("data/tokens").mkdir(parents=True, exist_ok=True)
    
    # Initialize session state keys
    session_keys = [
        UpstoxSession.SESSION_KEY,
        UpstoxSession.SESSION_REFRESH_KEY,
        UpstoxSession.SESSION_PROFILE_KEY,
        "market_state",
        "funding_analysis"
    ]
    
    for key in session_keys:
        if key not in st.session_state:
            st.session_state[key] = None if "state" in key else {}


--------------------------------------------------
FILE PATH : G:\trading_app\core\session.py
SIZE      : 22551 bytes
--------------------------------------------------

"""
Enhanced Upstox Session Manager with persistent token storage,
refresh mechanism, and market state tracking.
Incorporates research insights: funding liquidity monitoring via OI velocity
and market microstructure analysis.
"""

import os
import json
import requests
import streamlit as st
from urllib.parse import urlencode
from typing import Optional, Dict, Tuple
from datetime import datetime, timedelta
import pickle
from pathlib import Path
from cryptography.fernet import Fernet
import hashlib

# ==============================
# CONFIG (EDIT THESE)
# ==============================

# Upstox API v3 endpoints
UPSTOX_AUTH_URL = "https://api.upstox.com/v2/login/authorization/dialog"
UPSTOX_TOKEN_URL = "https://api.upstox.com/v2/login/authorization/token"
UPSTOX_PROFILE_URL = "https://api.upstox.com/v2/user/profile"

# Get from Streamlit secrets or environment
def get_config():
    """Get configuration from secrets.toml or environment variables."""
    # Try Streamlit secrets first
    try:
        if hasattr(st, 'secrets'):
            return {
                'client_id': st.secrets.get("UPSTOX_CLIENT_ID"),
                'client_secret': st.secrets.get("UPSTOX_CLIENT_SECRET"),
                'redirect_uri': st.secrets.get("UPSTOX_REDIRECT_URI", "http://127.0.0.1:8501/callback")
            }
    except:
        pass
    
    # Fallback to environment variables
    return {
        'client_id': os.getenv("UPSTOX_CLIENT_ID"),
        'client_secret': os.getenv("UPSTOX_CLIENT_SECRET"),
        'redirect_uri': os.getenv("UPSTOX_REDIRECT_URI", "http://127.0.0.1:8501/callback")
    }

config = get_config()
CLIENT_ID = config['client_id']
CLIENT_SECRET = config['client_secret']
REDIRECT_URI = config['redirect_uri']

# ==============================
# SECURE TOKEN STORAGE
# ==============================

class TokenStorage:
    """
    Secure token storage with encryption and persistence.
    Stores tokens locally with automatic refresh capability.
    """
    
    def __init__(self, storage_path: str = "data/tokens"):
        self.storage_dir = Path(storage_path)
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate or load encryption key
        self.key_file = self.storage_dir / ".key"
        if self.key_file.exists():
            with open(self.key_file, 'rb') as f:
                self.encryption_key = f.read()
        else:
            self.encryption_key = Fernet.generate_key()
            with open(self.key_file, 'wb') as f:
                f.write(self.encryption_key)
        
        self.cipher = Fernet(self.encryption_key)
        self.token_file = self.storage_dir / "upstox_tokens.enc"
        
        # Track funding liquidity state (research concept)
        self.funding_state = {
            "last_oi_velocity": 0.0,
            "liquidity_regime": "NORMAL",  # NORMAL, CONSTRICTED, EXPANSIVE
            "last_update": datetime.utcnow()
        }
    
    def _generate_user_hash(self) -> str:
        """Generate unique user identifier for multi-user support"""
        # In production, use actual user ID. Here we use client ID as proxy
        return hashlib.sha256(f"{CLIENT_ID}_{REDIRECT_URI}".encode()).hexdigest()[:16]
    
    def save_tokens(self, access_token: str, refresh_token: str, expires_in: int):
        """Securely save tokens with expiration"""
        token_data = {
            "access_token": access_token,
            "refresh_token": refresh_token,
            "expires_at": (datetime.utcnow() + timedelta(seconds=expires_in)).isoformat(),
            "client_id": CLIENT_ID,
            "user_hash": self._generate_user_hash(),
            "created_at": datetime.utcnow().isoformat()
        }
        
        # Encrypt and save
        encrypted = self.cipher.encrypt(pickle.dumps(token_data))
        with open(self.token_file, 'wb') as f:
            f.write(encrypted)
        
        st.success("‚úì Authentication tokens saved securely")
    
    def load_tokens(self) -> Optional[Dict]:
        """Load and decrypt tokens, check expiration"""
        if not self.token_file.exists():
            return None
        
        try:
            with open(self.token_file, 'rb') as f:
                encrypted = f.read()
            
            token_data = pickle.loads(self.cipher.decrypt(encrypted))
            
            # Check expiration
            expires_at = datetime.fromisoformat(token_data["expires_at"])
            if datetime.utcnow() > expires_at - timedelta(minutes=5):  # 5 min buffer
                st.warning("‚ö†Ô∏è Access token expired or near expiry")
                return None
            
            # Verify client matches
            if token_data.get("client_id") != CLIENT_ID:
                st.error("Client ID mismatch - reauthentication required")
                return None
            
            return token_data
        except Exception as e:
            st.error(f"Token load error: {e}")
            return None
    
    def update_funding_state(self, oi_velocity: float):
        """
        Update funding liquidity state based on OI velocity.
        Research concept: Monitor capital flow intensity.
        """
        self.funding_state["last_oi_velocity"] = oi_velocity
        
        # Determine liquidity regime (research concept)
        if oi_velocity > 1.5:
            regime = "EXPANSIVE"  # High capital inflow
        elif oi_velocity < -1.5:
            regime = "CONSTRICTED"  # Capital outflow
        else:
            regime = "NORMAL"
        
        if self.funding_state["liquidity_regime"] != regime:
            st.info(f"üí∞ Liquidity regime changed: {regime} (OI Velocity: {oi_velocity:.2f})")
        
        self.funding_state["liquidity_regime"] = regime
        self.funding_state["last_update"] = datetime.utcnow()
    
    def get_funding_state(self) -> Dict:
        """Get current funding liquidity state"""
        return self.funding_state.copy()

# ==============================
# ENHANCED SESSION MANAGER
# ==============================

class UpstoxSession:
    """
    Enhanced session manager with:
    1. Persistent token storage
    2. Automatic token refresh
    3. Funding liquidity tracking
    4. Market state awareness
    """
    
    SESSION_KEY = "upstox_access_token"
    SESSION_REFRESH_KEY = "upstox_refresh_token"
    SESSION_PROFILE_KEY = "upstox_profile"
    
    _token_storage = TokenStorage()
    _market_state = {
        "current_regime": None,
        "last_velocity": 0.0,
        "gamma_regime": None,  # POSITIVE/NEGATIVE from research
        "wall_levels": {"call": None, "put": None},
        "trap_zones": []
    }
    
    @staticmethod
    def is_authenticated() -> bool:
        """Check if user has valid authentication"""
        if UpstoxSession.SESSION_KEY in st.session_state:
            return True
        
        # Check persistent storage
        tokens = UpstoxSession._token_storage.load_tokens()
        if tokens:
            # Load into session state
            st.session_state[UpstoxSession.SESSION_KEY] = tokens["access_token"]
            st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = tokens["refresh_token"]
            return True
        
        return False
    
    @staticmethod
    def get_access_token() -> Optional[str]:
        """Get current access token, refresh if needed"""
        if not UpstoxSession.is_authenticated():
            return None
        
        # Check if token is in session state
        if UpstoxSession.SESSION_KEY in st.session_state:
            return st.session_state[UpstoxSession.SESSION_KEY]
        
        return None
    
    @staticmethod
    def logout() -> None:
        """Secure logout - clear all session and storage"""
        for key in [UpstoxSession.SESSION_KEY, 
                   UpstoxSession.SESSION_REFRESH_KEY,
                   UpstoxSession.SESSION_PROFILE_KEY]:
            if key in st.session_state:
                del st.session_state[key]
        
        # Clear persistent storage
        storage_file = Path("data/tokens/upstox_tokens.enc")
        if storage_file.exists():
            storage_file.unlink()
        
        # Clear query params
        st.query_params.clear()
        st.success("‚úì Logged out successfully")
        st.rerun()
    
    # ==========================
    # TOKEN MANAGEMENT
    # ==========================
    
    @staticmethod
    def refresh_access_token(refresh_token: str) -> Optional[Tuple[str, str]]:
        """
        Refresh expired access token using refresh token.
        Returns (new_access_token, new_refresh_token) or None
        """
        headers = {
            "Content-Type": "application/x-www-form-urlencoded",
            "Accept": "application/json"
        }
        
        data = {
            "grant_type": "refresh_token",
            "refresh_token": refresh_token,
            "client_id": CLIENT_ID,
            "client_secret": CLIENT_SECRET
        }
        
        try:
            response = requests.post(
                UPSTOX_TOKEN_URL,
                headers=headers,
                data=data,
                timeout=10
            )
            
            if response.status_code == 200:
                token_data = response.json()
                return token_data.get("access_token"), token_data.get("refresh_token")
            else:
                st.error(f"Token refresh failed: {response.text}")
                return None
                
        except Exception as e:
            st.error(f"Token refresh error: {e}")
            return None
    
    @staticmethod
    def exchange_code_for_token(code: str) -> Optional[Dict]:
        """Exchange authorization code for tokens with full token data"""
        headers = {
            "Content-Type": "application/x-www-form-urlencoded",
            "Accept": "application/json"
        }
        
        data = {
            "code": code,
            "client_id": CLIENT_ID,
            "client_secret": CLIENT_SECRET,
            "redirect_uri": REDIRECT_URI,
            "grant_type": "authorization_code"
        }
        
        try:
            response = requests.post(
                UPSTOX_TOKEN_URL,
                headers=headers,
                data=data,
                timeout=10
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                st.error(f"Token exchange failed: {response.text}")
                return None
                
        except Exception as e:
            st.error(f"Token exchange error: {e}")
            return None
    
    # ==========================
    # USER PROFILE
    # ==========================
    
    @staticmethod
    def get_user_profile(access_token: str) -> Optional[Dict]:
        """Fetch user profile for session context"""
        headers = {
            "Accept": "application/json",
            "Authorization": f"Bearer {access_token}"
        }
        
        try:
            response = requests.get(UPSTOX_PROFILE_URL, headers=headers, timeout=10)
            if response.status_code == 200:
                return response.json().get("data", {})
            return None
        except Exception as e:
            st.warning(f"Profile fetch warning: {e}")
            return None
    
    # ==========================
    # MARKET STATE TRACKING (Research Integration)
    # ==========================
    
    @staticmethod
    def update_market_state(oi_velocity: float, gamma_exposure: float, 
                           wall_levels: Dict, spot_divergence: float):
        """
        Update comprehensive market state based on research concepts.
        
        Args:
            oi_velocity: Rate of change of Open Interest
            gamma_exposure: Net Gamma Exposure (positive/negative)
            wall_levels: Dict with 'call' and 'put' wall strike levels
            spot_divergence: Divergence between spot and derivative metrics
        """
        # Update funding liquidity state
        UpstoxSession._token_storage.update_funding_state(oi_velocity)
        
        # Determine gamma regime (research concept)
        if gamma_exposure > 0:
            gamma_regime = "POSITIVE"  # Stabilizing, pinning expected
        elif gamma_exposure < 0:
            gamma_regime = "NEGATIVE"  # Accelerating, squeezes possible
        else:
            gamma_regime = "NEUTRAL"
        
        # Detect potential traps (research concept)
        trap_zones = []
        current_price = 0  # Would be passed in production
        
        # Check if price is near walls with high OI unwinding
        for side, level in wall_levels.items():
            if level and abs(current_price - level) / level < 0.005:  # Within 0.5%
                # High probability of trap if OI velocity is negative (unwinding)
                if oi_velocity < -1.0:
                    trap_zones.append({
                        "side": side,
                        "level": level,
                        "type": "GAMMA_TRAP" if abs(gamma_exposure) > 0.5 else "OI_TRAP",
                        "confidence": min(abs(oi_velocity) / 3, 1.0)
                    })
        
        # Update market state
        UpstoxSession._market_state.update({
            "last_velocity": oi_velocity,
            "gamma_regime": gamma_regime,
            "wall_levels": wall_levels,
            "trap_zones": trap_zones,
            "spot_divergence": spot_divergence,
            "funding_regime": UpstoxSession._token_storage.get_funding_state()["liquidity_regime"],
            "timestamp": datetime.utcnow().isoformat()
        })
    
    @staticmethod
    def get_market_state() -> Dict:
        """Get current market state analysis"""
        return UpstoxSession._market_state.copy()
    
    # ==========================
    # STREAMLIT ENTRYPOINT (ENHANCED)
    # ==========================
    

    @staticmethod
    def authenticate() -> str:
        """
        Simplified authentication flow that ALWAYS shows login button when not authenticated
        """
        
        # DEBUG: Show what's in query params
        query_params = st.query_params
        st.sidebar.write(f"üîç Query params: {dict(query_params)}")
        
        # Check for OAuth callback FIRST (before checking anything else)
        if "code" in query_params:
            try:
                code = query_params["code"]
                st.sidebar.write(f"üîç Got OAuth code: {code[:20]}...")
                
                # Exchange code for tokens
                token_data = UpstoxSession.exchange_code_for_token(code)
                if token_data:
                    st.sidebar.success("‚úÖ Token exchange successful")
                    
                    # Save tokens
                    UpstoxSession._token_storage.save_tokens(
                        token_data["access_token"],
                        token_data.get("refresh_token", ""),
                        token_data.get("expires_in", 86400)
                    )
                    
                    # Store in session
                    st.session_state[UpstoxSession.SESSION_KEY] = token_data["access_token"]
                    if "refresh_token" in token_data:
                        st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = token_data["refresh_token"]
                    
                    # Load profile
                    profile = UpstoxSession.get_user_profile(token_data["access_token"])
                    if profile:
                        st.session_state[UpstoxSession.SESSION_PROFILE_KEY] = profile
                    
                    # Clean URL and rerun
                    st.query_params.clear()
                    st.rerun()
                else:
                    st.error("‚ùå Failed to exchange code for tokens")
                    
            except Exception as e:
                st.error(f"‚ùå Authentication failed: {str(e)}")
                import traceback
                st.error(traceback.format_exc())
        
        # Check if already authenticated
        if UpstoxSession.is_authenticated():
            token = UpstoxSession.get_access_token()
            return token
        
        # Check persistent storage
        stored_tokens = UpstoxSession._token_storage.load_tokens()
        if stored_tokens:
            # Use stored token
            st.session_state[UpstoxSession.SESSION_KEY] = stored_tokens["access_token"]
            st.session_state[UpstoxSession.SESSION_REFRESH_KEY] = stored_tokens["refresh_token"]
            
            token = UpstoxSession.get_access_token()
            profile = UpstoxSession.get_user_profile(token)
            if profile:
                st.session_state[UpstoxSession.SESSION_PROFILE_KEY] = profile
            
            return token
        
        # ==============================================
        # SHOW LOGIN INTERFACE (only if not authenticated)
        # ==============================================
        st.markdown("""
        <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                    padding: 30px; border-radius: 15px; color: white;'>
            <h2 style='color: white;'>üîê Algorithmic Trading Platform Login</h2>
            <p style='font-size: 16px;'>
                Access real-time derivatives analytics
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("---")
        
        # Login button
        login_url = UpstoxSession.get_login_url()
        
        st.markdown(f"""
        <div style="text-align: center;">
            <a href="{login_url}">
                <button style="
                    background-color: #00d09c;
                    color: white;
                    padding: 15px 30px;
                    font-size: 18px;
                    font-weight: bold;
                    border: none;
                    border-radius: 10px;
                    cursor: pointer;
                    width: 80%;
                    margin: 20px 0;
                    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
                    transition: all 0.3s ease;
                ">
                üìà Login with Upstox
                </button>
            </a>
        </div>
        """, unsafe_allow_html=True)
        
        st.info("""
        **Login Instructions:**
        1. Click the "Login with Upstox" button above
        2. You'll be redirected to Upstox authorization page
        3. Log in with your Upstox credentials
        4. Authorize the application
        5. You'll be redirected back to this app
        """)
        
        # CRITICAL: Stop execution here
        st.stop()
        return None
    
    @staticmethod
    def get_login_url() -> str:
        """Generate OAuth login URL"""
        params = {
            "response_type": "code",
            "client_id": CLIENT_ID,
            "redirect_uri": REDIRECT_URI,
            "scope": "order placement portfolio"
        }
        return f"{UPSTOX_AUTH_URL}?{urlencode(params)}"
    
    @staticmethod
    def get_session_info() -> Dict:
        """Get comprehensive session information"""
        return {
            "authenticated": UpstoxSession.is_authenticated(),
            "has_profile": UpstoxSession.SESSION_PROFILE_KEY in st.session_state,
            "profile": st.session_state.get(UpstoxSession.SESSION_PROFILE_KEY, {}),
            "funding_state": UpstoxSession._token_storage.get_funding_state(),
            "market_state": UpstoxSession.get_market_state(),
            "storage_path": str(UpstoxSession._token_storage.storage_dir)
        }

# ==============================
# SESSION UTILITIES
# ==============================

def display_session_status():
    """Display session status in Streamlit sidebar"""
    if UpstoxSession.is_authenticated():
        with st.sidebar:
            st.markdown("---")
            st.markdown("### üß† Session Status")
            
            # User info
            profile = st.session_state.get(UpstoxSession.SESSION_PROFILE_KEY, {})
            if profile:
                st.markdown(f"**User:** {profile.get('user_name', 'N/A')}")
                st.markdown(f"**Email:** {profile.get('email', 'N/A')}")
            
            # Market state
            market_state = UpstoxSession.get_market_state()
            if market_state.get("funding_regime"):
                regime = market_state["funding_regime"]
                color = {
                    "EXPANSIVE": "üü¢",
                    "NORMAL": "üü°", 
                    "CONSTRICTED": "üî¥"
                }.get(regime, "‚ö™")
                st.markdown(f"**Funding:** {color} {regime}")
            
            if market_state.get("gamma_regime"):
                gamma = market_state["gamma_regime"]
                icon = "üìå" if gamma == "POSITIVE" else "üöÄ" if gamma == "NEGATIVE" else "‚öñÔ∏è"
                st.markdown(f"**Gamma:** {icon} {gamma}")
            
            # Logout button
            if st.button("üö™ Logout", type="secondary", use_container_width=True):
                UpstoxSession.logout()

# ==============================
# INITIALIZATION
# ==============================

def initialize_session():
    """
    Initialize session with enhanced capabilities.
    Call this at the start of your app.
    """
    # Ensure token storage directory exists
    Path("data/tokens").mkdir(parents=True, exist_ok=True)
    
    # Initialize session state keys
    session_keys = [
        UpstoxSession.SESSION_KEY,
        UpstoxSession.SESSION_REFRESH_KEY,
        UpstoxSession.SESSION_PROFILE_KEY,
        "market_state",
        "funding_analysis"
    ]
    
    for key in session_keys:
        if key not in st.session_state:
            st.session_state[key] = None if "state" in key else {}


--------------------------------------------------
FILE PATH : G:\trading_app\core\signals\state_machine.py
SIZE      : 40237 bytes
--------------------------------------------------

"""
Enhanced Signal State Machine with Research-Based Decision Making.
Implements advanced signal generation using:
1. OI Velocity & Gamma Exposure analysis
2. Structural Walls & Traps detection
3. Spot Divergence analysis
4. Wyckoff pattern recognition
5. Market regime awareness
"""

from datetime import datetime, timedelta
import uuid
import pandas as pd
import numpy as np
from typing import Dict, Optional, Tuple, List
from dataclasses import dataclass
from enum import Enum
import json

# Import research features
from ml.feature_contract import FEATURE_VERSION, RESEARCH_FEATURES

# ==============================
# ENUMS & DATA CLASSES
# ==============================

class SignalStrength(Enum):
    """Signal strength classification"""
    WEAK = "WEAK"        # Low confidence, minor edge
    MODERATE = "MODERATE" # Decent confidence, clear edge
    STRONG = "STRONG"    # High confidence, strong edge
    VERY_STRONG = "VERY_STRONG" # Very high confidence, major edge

class MarketRegime(Enum):
    """Market regime classification"""
    ACCUMULATION = "ACCUMULATION"      # Wyckoff accumulation
    DISTRIBUTION = "DISTRIBUTION"      # Wyckoff distribution
    UPTREND = "UPTREND"                # Strong uptrend
    DOWNTREND = "DOWNTREND"            # Strong downtrend
    RANGING = "RANGING"                # Range-bound
    BREAKOUT = "BREAKOUT"              # Breakout from range
    SQUEEZE = "SQUEEZE"                # Gamma/OI squeeze
    REVERSAL = "REVERSAL"              # Trend reversal

class TrapType(Enum):
    """Types of market traps"""
    GAMMA_TRAP = "GAMMA_TRAP"          # Gamma-induced squeeze
    OI_TRAP = "OI_TRAP"                # OI unwinding trap
    WYCKOFF_SPRING = "WYCKOFF_SPRING"  # Wyckoff spring (bear trap)
    WYCKOFF_UPTHRUST = "WYCKOFF_UPTHRUST" # Wyckoff upthrust (bull trap)
    DIVERGENCE_TRAP = "DIVERGENCE_TRAP" # Divergence-based trap

@dataclass
class SignalComponents:
    """Components of signal decision"""
    # Core components
    trend_score: float                # -1 to 1 (bearish to bullish)
    momentum_score: float             # -1 to 1 (weak to strong)
    
    # Research components
    oi_velocity_score: float          # -1 to 1 (negative to positive velocity)
    gamma_score: float               # -1 to 1 (negative to positive gamma)
    wall_interaction_score: float    # -1 to 1 (wall defense vs trap)
    divergence_score: float          # -1 to 1 (bearish to bullish divergence)
    
    # Wyckoff components
    wyckoff_phase_score: float       # -1 to 1 (distribution to accumulation)
    pattern_score: float             # -1 to 1 (bearish to bullish patterns)
    
    # Composite scores
    composite_score: float           # Overall score (-1 to 1)
    confidence: float               # Signal confidence (0-1)
    
    # Regime classification
    market_regime: MarketRegime
    regime_confidence: float

@dataclass
class TrapAnalysis:
    """Analysis of potential trap"""
    trap_type: TrapType
    strike_level: float
    direction: str  # "BULLISH" or "BEARISH"
    confidence: float
    trigger_conditions: List[str]
    expected_move_pct: float

# ==============================
# ENHANCED SIGNAL STATE MACHINE
# ==============================

class SignalStateMachine:
    """
    Advanced signal generator with research-based decision making.
    Incorporates OI velocity, gamma exposure, walls/traps, divergence.
    """
    
    def __init__(
        self,
        signal_expiry_minutes: int = 5,
        confidence_threshold: float = 0.2,
        trap_confidence_threshold: float = 0.6
    ):
        self.signal_expiry_minutes = signal_expiry_minutes
        self.confidence_threshold = confidence_threshold
        self.trap_confidence_threshold = trap_confidence_threshold
        
        # State tracking
        self.signal_history = []
        self.regime_history = []
        self.trap_detections = []
        
        # Thresholds (configurable)
        self.thresholds = {
            "oi_velocity_high": 1.5,      # œÉ
            "oi_velocity_low": -1.5,      # œÉ
            "gamma_high": 500,           # Net gamma threshold
            "gamma_low": -500,           # Net gamma threshold
            "trap_prob_high": 0.7,       # High trap probability
            "divergence_high": 0.5,      # Significant divergence
            "wall_strength_high": 0.3,   # Strong wall
            "spring_detection_high": 0.6, # Strong spring pattern
            "upthrust_detection_high": 0.6 # Strong upthrust pattern
        }
    
    # ==============================
    # RESEARCH FEATURE EXTRACTION
    # ==============================
    
    def extract_research_features(self, feature_row: pd.Series) -> Dict:
        """
        Extract and normalize research features from feature row.
        """
        feats = {}
        
        # OI Velocity features
        feats["oi_velocity"] = feature_row.get("oi_velocity", 0.0)
        feats["oi_regime_expansive"] = feature_row.get("oi_regime_expansive", 0.0)
        feats["oi_regime_constricted"] = feature_row.get("oi_regime_constricted", 0.0)
        
        # Gamma Exposure features
        feats["net_gamma"] = feature_row.get("net_gamma", 0.0)
        feats["gamma_regime_positive"] = feature_row.get("gamma_regime_positive", 0.0)
        feats["gamma_regime_negative"] = feature_row.get("gamma_regime_negative", 0.0)
        feats["gamma_flip_distance"] = feature_row.get("gamma_flip_distance", 0.0)
        
        # Structural features
        feats["wall_strength"] = feature_row.get("wall_strength", 0.0)
        feats["wall_defense_score"] = feature_row.get("wall_defense_score", 0.0)
        feats["trap_probability"] = feature_row.get("trap_probability", 0.0)
        
        # Divergence features
        feats["price_oi_divergence"] = feature_row.get("price_oi_divergence", 0.0)
        feats["price_gamma_divergence"] = feature_row.get("price_gamma_divergence", 0.0)
        feats["divergence_score"] = feature_row.get("divergence_score", 0.0)
        feats["has_divergence"] = feature_row.get("has_divergence", 0.0)
        
        # Wyckoff features
        feats["spring_detection"] = feature_row.get("spring_detection", 0.0)
        feats["upthrust_detection"] = feature_row.get("upthrust_detection", 0.0)
        feats["accumulation_score"] = feature_row.get("accumulation_score", 0.0)
        
        # Composite features
        feats["gamma_wall_interaction"] = feature_row.get("gamma_wall_interaction", 0.0)
        feats["velocity_divergence_composite"] = feature_row.get("velocity_divergence_composite", 0.0)
        feats["trap_gamma_composite"] = feature_row.get("trap_gamma_composite", 0.0)
        
        # Base features (still important)
        feats["put_call_ratio"] = feature_row.get("put_call_ratio", 1.0)
        feats["price_momentum"] = feature_row.get("price_momentum", 0.0)
        feats["ccc_slope"] = feature_row.get("ccc_slope", 0.0)
        feats["vwap_distance"] = feature_row.get("vwap_distance", 0.0)
        
        return feats
    
    # ==============================
    # COMPONENT SCORING
    # ==============================
    
    def score_oi_velocity(self, feats: Dict) -> Tuple[float, str]:
        """
        Score OI velocity component.
        
        Returns:
            score (-1 to 1), analysis
        """
        velocity = feats.get("oi_velocity", 0.0)
        expansive = feats.get("oi_regime_expansive", 0.0)
        constricted = feats.get("oi_regime_constricted", 0.0)
        
        # Velocity scoring
        if velocity > self.thresholds["oi_velocity_high"]:
            score = 1.0  # Strong buildup
            analysis = "Strong OI buildup - initiative capital entering"
        elif velocity > 0.5:
            score = 0.5  # Moderate buildup
            analysis = "Moderate OI buildup"
        elif velocity < self.thresholds["oi_velocity_low"]:
            score = -1.0  # Strong unwinding
            analysis = "Strong OI unwinding - capital exiting"
        elif velocity < -0.5:
            score = -0.5  # Moderate unwinding
            analysis = "Moderate OI unwinding"
        else:
            score = 0.0  # Neutral
            analysis = "OI velocity neutral"
        
        # Regime adjustment
        if expansive > 0.5:
            score = max(score, 0.3)  # Bias bullish
            analysis += " (EXPANSIVE regime)"
        elif constricted > 0.5:
            score = min(score, -0.3)  # Bias bearish
            analysis += " (CONSTRICTED regime)"
        
        return score, analysis
    
    def score_gamma_exposure(self, feats: Dict) -> Tuple[float, str]:
        """
        Score Gamma Exposure component.
        
        Returns:
            score (-1 to 1), analysis
        """
        net_gamma = feats.get("net_gamma", 0.0)
        gamma_pos = feats.get("gamma_regime_positive", 0.0)
        gamma_neg = feats.get("gamma_regime_negative", 0.0)
        flip_distance = feats.get("gamma_flip_distance", 1.0)
        
        # Gamma scoring
        if gamma_neg > 0.5 and abs(net_gamma) > self.thresholds["gamma_high"]:
            score = -1.0  # Strong negative gamma (accelerating)
            analysis = "Strong negative gamma - volatility acceleration likely"
        elif gamma_neg > 0.5:
            score = -0.7  # Moderate negative gamma
            analysis = "Negative gamma regime - trending moves possible"
        elif gamma_pos > 0.5 and abs(net_gamma) > self.thresholds["gamma_high"]:
            score = 0.3  # Strong positive gamma (stabilizing)
            analysis = "Strong positive gamma - range-bound/pinning likely"
        elif gamma_pos > 0.5:
            score = 0.1  # Moderate positive gamma
            analysis = "Positive gamma regime - mean reversion favored"
        else:
            score = 0.0  # Neutral
            analysis = "Gamma exposure neutral"
        
        # Flip distance adjustment (closer to flip = more uncertainty)
        if flip_distance < 0.01:  # Very close to flip
            score *= 0.5  # Reduce confidence near flip
            analysis += " (Near gamma flip)"
        
        return score, analysis
    
    def score_structure(self, feats: Dict) -> Tuple[float, str, Optional[TrapAnalysis]]:
        """
        Score structural components (walls, traps, Wyckoff).
        
        Returns:
            score (-1 to 1), analysis, trap_analysis
        """
        wall_strength = feats.get("wall_strength", 0.0)
        wall_defense = feats.get("wall_defense_score", 0.0)
        trap_prob = feats.get("trap_probability", 0.0)
        spring = feats.get("spring_detection", 0.0)
        upthrust = feats.get("upthrust_detection", 0.0)
        accumulation = feats.get("accumulation_score", 0.0)
        
        score = 0.0
        analysis = []
        trap_analysis = None
        
        # Wall analysis
        if wall_strength > self.thresholds["wall_strength_high"]:
            if wall_defense > 0.7:
                score += 0.3  # Strong defense = continuation
                analysis.append(f"Strong wall defense ({wall_strength:.2f})")
            else:
                score -= 0.2  # Weak defense = potential break
                analysis.append(f"Weak wall defense ({wall_strength:.2f})")
        
        # Trap analysis
        if trap_prob > self.thresholds["trap_prob_high"]:
            # High trap probability
            trap_type = self._classify_trap_type(feats)
            if trap_type in [TrapType.GAMMA_TRAP, TrapType.WYCKOFF_SPRING]:
                # Bullish traps
                trap_analysis = TrapAnalysis(
                    trap_type=trap_type,
                    strike_level=0,  # Would be actual strike in production
                    direction="BULLISH",
                    confidence=trap_prob,
                    trigger_conditions=["Price breach", "OI unwinding"],
                    expected_move_pct=2.0  # Estimated move
                )
                score += 0.5  # Bullish bias
                analysis.append(f"{trap_type.value} detected (conf: {trap_prob:.2f})")
            elif trap_type in [TrapType.OI_TRAP, TrapType.WYCKOFF_UPTHRUST]:
                # Bearish traps
                trap_analysis = TrapAnalysis(
                    trap_type=trap_type,
                    strike_level=0,
                    direction="BEARISH",
                    confidence=trap_prob,
                    trigger_conditions=["Price rejection", "OI buildup"],
                    expected_move_pct=-2.0
                )
                score -= 0.5  # Bearish bias
                analysis.append(f"{trap_type.value} detected (conf: {trap_prob:.2f})")
        
        # Wyckoff pattern analysis
        if spring > self.thresholds["spring_detection_high"]:
            score += 0.4  # Spring = bullish
            analysis.append(f"Wyckoff Spring detected (strength: {spring:.2f})")
        
        if upthrust > self.thresholds["upthrust_detection_high"]:
            score -= 0.4  # Upthrust = bearish
            analysis.append(f"Wyckoff Upthrust detected (strength: {upthrust:.2f})")
        
        # Accumulation/distribution
        if accumulation > 0.7:
            score += 0.3  # Accumulation = bullish
            analysis.append(f"Accumulation phase (score: {accumulation:.2f})")
        elif accumulation < 0.3:
            score -= 0.3  # Distribution = bearish
            analysis.append(f"Distribution phase (score: {accumulation:.2f})")
        
        # Clamp score
        score = max(-1.0, min(1.0, score))
        
        return score, " | ".join(analysis) if analysis else "Structure neutral", trap_analysis
    
    def score_divergence(self, feats: Dict) -> Tuple[float, str]:
        """
        Score divergence components.
        
        Returns:
            score (-1 to 1), analysis
        """
        divergence_score = feats.get("divergence_score", 0.0)
        has_divergence = feats.get("has_divergence", 0.0)
        price_oi_div = feats.get("price_oi_divergence", 0.0)
        price_gamma_div = feats.get("price_gamma_divergence", 0.0)
        
        if has_divergence < 0.5:
            return 0.0, "No significant divergence"
        
        score = 0.0
        analysis = []
        
        # Price-OI divergence (research concept)
        if abs(price_oi_div) > 1.0:  # Significant divergence
            if price_oi_div > 0:  # Price up, OI down = bearish divergence
                score -= 0.4
                analysis.append("Bearish Price-OI divergence")
            else:  # Price down, OI up = bullish divergence
                score += 0.4
                analysis.append("Bullish Price-OI divergence")
        
        # Price-Gamma divergence
        if abs(price_gamma_div) > 0.5:
            if price_gamma_div > 0:  # Price moving against gamma regime
                score -= 0.3
                analysis.append("Price-Gamma regime divergence")
            else:
                score += 0.3
                analysis.append("Price-Gamma regime convergence")
        
        # Overall divergence score
        if divergence_score > self.thresholds["divergence_high"]:
            # High divergence confidence
            if divergence_score > 0:  # Bullish divergence
                score += 0.3
                analysis.append(f"Strong bullish divergence (conf: {divergence_score:.2f})")
            else:  # Bearish divergence
                score -= 0.3
                analysis.append(f"Strong bearish divergence (conf: {abs(divergence_score):.2f})")
        
        # Clamp score
        score = max(-1.0, min(1.0, score))
        
        return score, " | ".join(analysis) if analysis else "Divergence neutral"
    
    def score_trend_momentum(self, feats: Dict) -> Tuple[float, float, str]:
        """
        Score traditional trend and momentum components.
        
        Returns:
            trend_score, momentum_score, analysis
        """
        price_momentum = feats.get("price_momentum", 0.0)
        ccc_slope = feats.get("ccc_slope", 0.0)
        vwap_distance = feats.get("vwap_distance", 0.0)
        put_call_ratio = feats.get("put_call_ratio", 1.0)
        
        # Trend score (directional bias)
        trend_score = 0.0
        
        # Price momentum
        if price_momentum > 0.01:
            trend_score += 0.3
        elif price_momentum < -0.01:
            trend_score -= 0.3
        
        # Breadth momentum (CCC slope)
        if ccc_slope > 0.001:
            trend_score += 0.2
        elif ccc_slope < -0.001:
            trend_score -= 0.2
        
        # VWAP position
        if vwap_distance > 0.005:
            trend_score += 0.2  # Above VWAP = bullish
        elif vwap_distance < -0.005:
            trend_score -= 0.2  # Below VWAP = bearish
        
        # Put-Call ratio sentiment
        if put_call_ratio < 0.8:
            trend_score += 0.1  # Low PCR = bullish
        elif put_call_ratio > 1.2:
            trend_score -= 0.1  # High PCR = bearish
        
        # Momentum score (strength of move)
        momentum_score = abs(price_momentum) * 10  # Scale to 0-1 range
        momentum_score = min(momentum_score, 1.0)
        
        # Analysis
        analysis_parts = []
        if price_momentum > 0:
            analysis_parts.append(f"Price momentum: +{price_momentum*100:.1f}%")
        else:
            analysis_parts.append(f"Price momentum: {price_momentum*100:.1f}%")
        
        if ccc_slope > 0:
            analysis_parts.append(f"Breadth improving (CCC: +{ccc_slope:.3f})")
        elif ccc_slope < 0:
            analysis_parts.append(f"Breadth weakening (CCC: {ccc_slope:.3f})")
        
        analysis = " | ".join(analysis_parts) if analysis_parts else "Momentum neutral"
        
        return trend_score, momentum_score, analysis
    
    # ==============================
    # TRAP CLASSIFICATION
    # ==============================
    
    def _classify_trap_type(self, feats: Dict) -> TrapType:
        """
        Classify trap type based on feature combination.
        """
        trap_prob = feats.get("trap_probability", 0.0)
        gamma_neg = feats.get("gamma_regime_negative", 0.0)
        spring = feats.get("spring_detection", 0.0)
        upthrust = feats.get("upthrust_detection", 0.0)
        divergence = feats.get("has_divergence", 0.0)
        
        # Check for specific trap types
        if gamma_neg > 0.5 and trap_prob > 0.6:
            return TrapType.GAMMA_TRAP
        
        if trap_prob > 0.6 and gamma_neg < 0.5:
            return TrapType.OI_TRAP
        
        if spring > self.thresholds["spring_detection_high"]:
            return TrapType.WYCKOFF_SPRING
        
        if upthrust > self.thresholds["upthrust_detection_high"]:
            return TrapType.WYCKOFF_UPTHRUST
        
        if divergence > 0.5 and trap_prob > 0.5:
            return TrapType.DIVERGENCE_TRAP
        
        # Default
        return TrapType.OI_TRAP
    
    # ==============================
    # MARKET REGIME DETECTION
    # ==============================
    
    def detect_market_regime(self, feats: Dict, components: SignalComponents) -> Tuple[MarketRegime, float]:
        """
        Detect current market regime based on features.
        
        Returns:
            regime, confidence (0-1)
        """
        regime_scores = {
            MarketRegime.ACCUMULATION: 0.0,
            MarketRegime.DISTRIBUTION: 0.0,
            MarketRegime.UPTREND: 0.0,
            MarketRegime.DOWNTREND: 0.0,
            MarketRegime.RANGING: 0.0,
            MarketRegime.BREAKOUT: 0.0,
            MarketRegime.SQUEEZE: 0.0,
            MarketRegime.REVERSAL: 0.0
        }
        
        # Extract key features
        accumulation = feats.get("accumulation_score", 0.0)
        trap_prob = feats.get("trap_probability", 0.0)
        gamma_neg = feats.get("gamma_regime_negative", 0.0)
        divergence = feats.get("has_divergence", 0.0)
        price_momentum = feats.get("price_momentum", 0.0)
        
        # Score regimes
        # Accumulation/Distribution
        if accumulation > 0.7:
            regime_scores[MarketRegime.ACCUMULATION] = accumulation
        elif accumulation < 0.3:
            regime_scores[MarketRegime.DISTRIBUTION] = 1 - accumulation
        
        # Trend detection
        if abs(price_momentum) > 0.02:  # Strong trend
            if price_momentum > 0:
                regime_scores[MarketRegime.UPTREND] = abs(price_momentum) * 10
            else:
                regime_scores[MarketRegime.DOWNTREND] = abs(price_momentum) * 10
        else:
            regime_scores[MarketRegime.RANGING] = 0.7
        
        # Breakout/Squeeze
        if trap_prob > 0.6:
            if gamma_neg > 0.5:
                regime_scores[MarketRegime.SQUEEZE] = trap_prob
            else:
                regime_scores[MarketRegime.BREAKOUT] = trap_prob
        
        # Reversal
        if divergence > 0.5 and abs(price_momentum) > 0.01:
            regime_scores[MarketRegime.REVERSAL] = divergence
        
        # Find highest scoring regime
        best_regime_item = max(regime_scores.items(), key=lambda x: x[1])
        best_regime = best_regime_item[0]
        regime_confidence = best_regime_item[1]
        
        # If no clear regime, default to RANGING
        if regime_confidence < 0.3:
            return MarketRegime.RANGING, 0.3
        
        return best_regime, min(regime_confidence, 1.0)
    
    # ==============================
    # COMPOSITE SIGNAL GENERATION
    # ==============================
    
    def compute_composite_signal(self, components: SignalComponents) -> Tuple[str, float, SignalStrength]:
        """
        Compute final signal from components.
        
        Returns:
            signal_type, confidence, strength
        """
        composite = components.composite_score
        confidence = components.confidence
        
        # Determine signal type
        if composite > 0.3 and confidence > self.confidence_threshold:
            signal_type = "BUY"
            if composite > 0.6 and confidence > 0.7:
                strength = SignalStrength.VERY_STRONG
            elif composite > 0.4:
                strength = SignalStrength.STRONG
            elif composite > 0.3:
                strength = SignalStrength.MODERATE
            else:
                strength = SignalStrength.WEAK
                
        elif composite < -0.3 and confidence > self.confidence_threshold:
            signal_type = "SELL"
            if composite < -0.6 and confidence > 0.7:
                strength = SignalStrength.VERY_STRONG
            elif composite < -0.4:
                strength = SignalStrength.STRONG
            elif composite < -0.3:
                strength = SignalStrength.MODERATE
            else:
                strength = SignalStrength.WEAK
                
        else:
            signal_type = "NEUTRAL"
            strength = SignalStrength.WEAK
        
        return signal_type, confidence, strength
    
    # ==============================
    # MAIN DECISION ENGINE
    # ==============================
    
    def decide(self, feature_row: pd.Series) -> SignalComponents:
        """
        Main decision engine with research-based scoring.
        """
        # Extract research features
        feats = self.extract_research_features(feature_row)
        
        # Score all components
        trend_score, momentum_score, trend_analysis = self.score_trend_momentum(feats)
        oi_score, oi_analysis = self.score_oi_velocity(feats)
        gamma_score, gamma_analysis = self.score_gamma_exposure(feats)
        structure_score, structure_analysis, trap_analysis = self.score_structure(feats)
        divergence_score, divergence_analysis = self.score_divergence(feats)
        
        # Calculate Wyckoff phase score
        wyckoff_score = 0.0
        pattern_score = 0.0
        
        accumulation = feats.get("accumulation_score", 0.0)
        spring = feats.get("spring_detection", 0.0)
        upthrust = feats.get("upthrust_detection", 0.0)
        
        if accumulation > 0.5:
            wyckoff_score = accumulation - 0.5  # 0 to 0.5 range
        
        pattern_score = spring - upthrust  # Positive for springs, negative for upthrusts
        
        # Calculate composite score with weights
        weights = {
            "trend": 0.15,
            "momentum": 0.10,
            "oi_velocity": 0.20,     # High weight for OI velocity (research important)
            "gamma": 0.20,           # High weight for gamma (research important)
            "structure": 0.15,
            "divergence": 0.10,
            "wyckoff": 0.05,
            "pattern": 0.05
        }
        
        weighted_sum = (
            trend_score * weights["trend"] +
            momentum_score * np.sign(trend_score) * weights["momentum"] +  # Momentum amplifies trend
            oi_score * weights["oi_velocity"] +
            gamma_score * weights["gamma"] +
            structure_score * weights["structure"] +
            divergence_score * weights["divergence"] +
            wyckoff_score * weights["wyckoff"] +
            pattern_score * weights["pattern"]
        )
        
        # Calculate confidence
        component_scores = [
            abs(trend_score), abs(oi_score), abs(gamma_score),
            abs(structure_score), abs(divergence_score)
        ]
        confidence = np.mean([s for s in component_scores if s > 0.1]) if any(s > 0.1 for s in component_scores) else 0.0
        
        # Detect market regime
        market_regime, regime_confidence = self.detect_market_regime(feats, SignalComponents(
            trend_score=trend_score,
            momentum_score=momentum_score,
            oi_velocity_score=oi_score,
            gamma_score=gamma_score,
            wall_interaction_score=structure_score,
            divergence_score=divergence_score,
            wyckoff_phase_score=wyckoff_score,
            pattern_score=pattern_score,
            composite_score=weighted_sum,
            confidence=confidence,
            market_regime=MarketRegime.RANGING,  # Temporary placeholder
            regime_confidence=0.5  # Temporary placeholder
        ))
        
        # Store trap analysis if found
        if trap_analysis:
            self.trap_detections.append({
                "timestamp": feature_row.get("timestamp", ""),
                "trap_analysis": trap_analysis,
                "features": feats
            })
        
        return SignalComponents(
            trend_score=trend_score,
            momentum_score=momentum_score,
            oi_velocity_score=oi_score,
            gamma_score=gamma_score,
            wall_interaction_score=structure_score,
            divergence_score=divergence_score,
            wyckoff_phase_score=wyckoff_score,
            pattern_score=pattern_score,
            composite_score=weighted_sum,
            confidence=confidence,
            market_regime=market_regime,
            regime_confidence=regime_confidence
        )
    
    # ==============================
    # SIGNAL OBJECT GENERATION
    # ==============================
    
    def build_signal(
        self,
        feature_row: pd.Series,
        model_version: str = "research_v2"
    ) -> Dict:
        """
        Build complete signal record with research context.
        """

        # --- EXECUTION SAFETY GATE ---
        mode = feature_row.get("pipeline_mode", "research")
        execution_ready = feature_row.get("execution_ready", False)

        if mode == "execution" and not execution_ready:
            return {
                "signal_type": "NEUTRAL",
                "confidence": 0.0,
                "signal_strength": SignalStrength.WEAK.value,
                "reason": "Execution not allowed: feature row not execution-ready",
                "timestamp": feature_row.get("timestamp"),
                "feature_version": feature_row.get("feature_version"),
                "model_version": model_version,
                "status": "BLOCKED"
            }



        # Run decision engine
        components = self.decide(feature_row)
        
        # Compute final signal
        signal_type, confidence, strength = self.compute_composite_signal(components)
        
        # Generate signal ID
        signal_id = str(uuid.uuid4())
        
        # Current time
        now = datetime.utcnow()
        
        # Build research context
        research_context = {
            "components": {
                "trend_score": round(components.trend_score, 3),
                "momentum_score": round(components.momentum_score, 3),
                "oi_velocity_score": round(components.oi_velocity_score, 3),
                "gamma_score": round(components.gamma_score, 3),
                "wall_interaction_score": round(components.wall_interaction_score, 3),
                "divergence_score": round(components.divergence_score, 3),
                "wyckoff_phase_score": round(components.wyckoff_phase_score, 3),
                "pattern_score": round(components.pattern_score, 3),
                "composite_score": round(components.composite_score, 3)
            },
            "market_regime": components.market_regime.value,
            "regime_confidence": round(components.regime_confidence, 3),
            "signal_strength": strength.value,
            "thresholds_used": self.thresholds
        }
        
        # Build analytics summary
        analytics_summary = {
            "oi_velocity": feature_row.get("oi_velocity", 0),
            "net_gamma": feature_row.get("net_gamma", 0),
            "trap_probability": feature_row.get("trap_probability", 0),
            "divergence_score": feature_row.get("divergence_score", 0),
            "wall_strength": feature_row.get("wall_strength", 0),
            "confidence_breakdown": {
                "component_confidence": round(components.confidence, 3),
                "regime_confidence": round(components.regime_confidence, 3),
                "composite_confidence": round(confidence, 3)
            }
        }
        
        # Build rationale
        rationale_parts = []

        # Extract values from feature_row (not from components)
        trap_probability = feature_row.get("trap_probability", 0.0)
        has_divergence = feature_row.get("has_divergence", 0.0)

        # Add component analysis
        if abs(components.oi_velocity_score) > 0.3:
            direction = "bullish" if components.oi_velocity_score > 0 else "bearish"
            rationale_parts.append(f"{direction.capitalize()} OI velocity")

        if abs(components.gamma_score) > 0.3:
            regime = "negative" if components.gamma_score < 0 else "positive"
            rationale_parts.append(f"{regime.capitalize()} gamma regime")

        if trap_probability > 0.5:
            rationale_parts.append(f"High trap probability ({trap_probability:.2f})")

        if has_divergence > 0.5:
            rationale_parts.append("Significant divergence detected")

        rationale = " | ".join(rationale_parts) if rationale_parts else "rule_based_research_v2"
        
        # Build complete signal
        signal = {
            "signal_id": signal_id,
            "timestamp": feature_row["timestamp"],
            "feature_version": feature_row.get("feature_version", FEATURE_VERSION),
            "model_version": model_version,
            
            "signal_type": signal_type,
            "confidence": round(confidence, 3),
            
            "market_state": components.market_regime.value,
            "rationale": rationale,
            
            "expiry_time": (now + timedelta(minutes=self.signal_expiry_minutes)).isoformat(),
            "status": "NEW",
            
            "created_at": now.isoformat(),
            
            # Enhanced fields
            "research_context": research_context,
            "analytics_summary": analytics_summary,
            "signal_strength": strength.value,
            
            # Additional metadata
            "spot_price": feature_row.get("spot_price", 0),
            "put_call_ratio": feature_row.get("put_call_ratio", 1.0),
            "oi_velocity": feature_row.get("oi_velocity", 0),
            "net_gamma": feature_row.get("net_gamma", 0)
        }
        
        # Store in history
        self.signal_history.append({
            "timestamp": now.isoformat(),
            "signal": signal,
            "components": components
        })
        
        # Keep history manageable
        if len(self.signal_history) > 1000:
            self.signal_history = self.signal_history[-1000:]
        
        return signal
    
    # ==============================
    # UTILITY METHODS
    # ==============================
    
    def get_signal_history(self, limit: int = 10) -> List[Dict]:
        """Get recent signal history."""
        return self.signal_history[-limit:] if self.signal_history else []
    
    def get_recent_trap_detections(self, limit: int = 5) -> List[Dict]:
        """Get recent trap detections."""
        return self.trap_detections[-limit:] if self.trap_detections else []
    
    def get_performance_metrics(self) -> Dict:
        """Calculate performance metrics for recent signals."""
        if not self.signal_history:
            return {}
        
        recent_signals = self.signal_history[-100:]  # Last 100 signals
        
        buy_signals = [s for s in recent_signals if s["signal"]["signal_type"] == "BUY"]
        sell_signals = [s for s in recent_signals if s["signal"]["signal_type"] == "SELL"]
        
        metrics = {
            "total_signals": len(recent_signals),
            "buy_signals": len(buy_signals),
            "sell_signals": len(sell_signals),
            "neutral_signals": len(recent_signals) - len(buy_signals) - len(sell_signals),
            "avg_confidence": np.mean([s["signal"]["confidence"] for s in recent_signals]) if recent_signals else 0,
            "recent_regimes": {},
            "trap_detections": len(self.trap_detections)
        }
        
        # Count recent regimes
        for signal in recent_signals[-20:]:  # Last 20 signals
            regime = signal["signal"]["market_state"]
            metrics["recent_regimes"][regime] = metrics["recent_regimes"].get(regime, 0) + 1
        
        return metrics
    
    def reset(self):
        """Reset state machine history."""
        self.signal_history = []
        self.regime_history = []
        self.trap_detections = []

# ==============================
# SIGNAL VALIDATION & FILTERING
# ==============================

class SignalValidator:
    """
    Validates signals based on research criteria.
    """
    
    @staticmethod
    def validate_signal(signal: Dict, feature_row: pd.Series) -> Tuple[bool, str]:
        """
        Validate signal against research criteria.
        
        Returns:
            is_valid, reason
        """
        # Check confidence threshold
        if signal.get("confidence", 0) < 0.2:
            return False, "Confidence below threshold"
        
        # Check for trap confirmation
        trap_prob = feature_row.get("trap_probability", 0)
        if trap_prob > 0.7 and signal.get("signal_type") != "NEUTRAL":
            # High trap probability requires careful validation
            gamma_neg = feature_row.get("gamma_regime_negative", 0)
            if gamma_neg > 0.5:
                # Negative gamma with high trap = likely valid
                return True, "Gamma trap confirmed"
            else:
                return False, "High trap probability without negative gamma"
        
        # Check divergence consistency
        has_divergence = feature_row.get("has_divergence", 0)
        if has_divergence > 0.5:
            # Signal should align with divergence direction
            divergence_score = feature_row.get("divergence_score", 0)
            signal_type = signal.get("signal_type")
            
            if divergence_score > 0 and signal_type != "BUY":
                return False, "Signal contradicts bullish divergence"
            elif divergence_score < 0 and signal_type != "SELL":
                return False, "Signal contradicts bearish divergence"
        
        # Check OI velocity consistency
        oi_velocity = feature_row.get("oi_velocity", 0)
        if abs(oi_velocity) > 1.5:  # Strong OI movement
            if oi_velocity > 0 and signal.get("signal_type") != "BUY":
                return False, "Signal contradicts strong OI buildup"
            elif oi_velocity < 0 and signal.get("signal_type") != "SELL":
                return False, "Signal contradicts strong OI unwinding"
        
        return True, "Signal validated"
    
    @staticmethod
    def filter_weak_signals(signals: List[Dict], min_strength: str = "MODERATE") -> List[Dict]:
        """
        Filter signals by strength.
        
        Args:
            signals: List of signals
            min_strength: Minimum strength required
        
        Returns:
            Filtered signals
        """
        strength_order = {
            "WEAK": 0,
            "MODERATE": 1,
            "STRONG": 2,
            "VERY_STRONG": 3
        }
        
        min_strength_value = strength_order.get(min_strength, 0)
        
        filtered = []
        for signal in signals:
            signal_strength = signal.get("signal_strength", "WEAK")
            if strength_order.get(signal_strength, 0) >= min_strength_value:
                filtered.append(signal)
        
        return filtered

# ==============================
# SIGNAL ANALYTICS
# ==============================

def analyze_signal_patterns(signals: List[Dict]) -> Dict:
    """
    Analyze patterns in signal generation.
    """
    if not signals:
        return {}
    
    # Convert to DataFrame for analysis
    df = pd.DataFrame(signals)
    
    analysis = {
        "total_signals": len(df),
        "signal_distribution": df["signal_type"].value_counts().to_dict(),
        "avg_confidence": df["confidence"].mean() if "confidence" in df.columns else 0,
        "strength_distribution": df["signal_strength"].value_counts().to_dict() if "signal_strength" in df.columns else {},
        "regime_distribution": df["market_state"].value_counts().to_dict() if "market_state" in df.columns else {}
    }
    
    # Calculate success rate if PNL data available
    if "pnl" in df.columns and not df["pnl"].isna().all():
        profitable = df[df["pnl"] > 0]
        analysis["profitable_signals"] = len(profitable)
        analysis["success_rate"] = len(profitable) / len(df) * 100 if len(df) > 0 else 0
        analysis["avg_pnl"] = df["pnl"].mean()
    
    return analysis

# ==============================
# INITIALIZATION
# ==============================

def create_signal_engine(
    signal_expiry_minutes: int = 5,
    confidence_threshold: float = 0.2
) -> SignalStateMachine:
    """
    Factory function to create signal engine.
    """
    return SignalStateMachine(
        signal_expiry_minutes=signal_expiry_minutes,
        confidence_threshold=confidence_threshold
    )


--------------------------------------------------
FILE PATH : G:\trading_app\data\instrument_master.py
SIZE      : 18621 bytes
--------------------------------------------------

"""
Enhanced Instrument Master with robust key generation.
Handles multiple underlying formats and expiry matching.
"""

from typing import List, Dict, Any, Optional
import gzip
import json
from datetime import datetime, timedelta
from pathlib import Path
import pandas as pd
import warnings

# ==============================
# INSTRUMENT LOADING
# ==============================

def load_instruments() -> List[Dict[str, Any]]:
    """
    Load instruments from compressed JSON file.
    
    Returns:
        List of instrument dictionaries
    """
    instrument_file = Path("data/instruments.json.gz")
    
    if not instrument_file.exists():
        warnings.warn(f"Instrument file not found: {instrument_file}")
        return []
    
    try:
        with gzip.open(instrument_file, 'rt', encoding='utf-8') as f:
            data = json.load(f)
            
        if isinstance(data, list):
            print(f"‚úì Loaded {len(data)} instruments")
            return data
        else:
            warnings.warn(f"Unexpected data format: {type(data)}")
            return []
            
    except Exception as e:
        warnings.warn(f"Error loading instruments: {e}")
        return []

def save_instruments(instruments: List[Dict[str, Any]]) -> bool:
    """Save instruments to compressed JSON file."""
    try:
        instrument_file = Path("data/instruments.json.gz")
        instrument_file.parent.mkdir(parents=True, exist_ok=True)
        
        with gzip.open(instrument_file, 'wt', encoding='utf-8') as f:
            json.dump(instruments, f)
            
        print(f"‚úì Saved {len(instruments)} instruments")
        return True
    except Exception as e:
        warnings.warn(f"Error saving instruments: {e}")
        return False

# ==============================
# OPTION KEY GENERATION
# ==============================

def get_option_keys(
    underlying: str, 
    expiry: str, 
    max_keys: int = 200,
    instrument_type: Optional[str] = None
) -> List[str]:
    """
    Get option instrument keys for given underlying and expiry.
    Fixed: Matches by DATE only, not exact timestamp.
    
    Args:
        underlying: Underlying symbol (e.g., 'NIFTY', 'BANKNIFTY')
        expiry: Expiry date in 'YYYY-MM-DD' format
        max_keys: Maximum number of keys to return
        instrument_type: Filter by 'CE', 'PE', or None for both
    
    Returns:
        List of instrument keys
    """
    instruments = load_instruments()
    
    if not instruments:
        print("‚ö†Ô∏è No instruments loaded")
        return []
    
    # Parse target expiry date
    try:
        target_date = datetime.strptime(expiry, "%Y-%m-%d").date()
    except ValueError as e:
        print(f"‚ùå Invalid expiry format: {expiry}. Expected YYYY-MM-DD")
        return []
    
    option_keys = []
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        # Check if it's an option
        inst_type = instrument.get('instrument_type')
        if inst_type not in ['CE', 'PE']:
            continue
        
        # Filter by instrument type if specified
        if instrument_type and inst_type != instrument_type:
            continue
        
        # EXACT match for name
        name = instrument.get('name', '')
        if name != underlying:
            continue
        
        # Check expiry - convert timestamp to date
        instrument_expiry_ts = instrument.get('expiry')
        if not instrument_expiry_ts:
            continue
        
        # Convert timestamp to date (ignoring time component)
        try:
            instrument_expiry_date = datetime.fromtimestamp(instrument_expiry_ts / 1000).date()
        except:
            continue
        
        # Match by date only (not exact timestamp)
        if instrument_expiry_date != target_date:
            continue
        
        instrument_key = instrument.get('instrument_key')
        if instrument_key:
            option_keys.append({
                'key': instrument_key,
                'strike_price': float(instrument.get('strike_price', 0)),
                'instrument_type': inst_type,
                'instrument': instrument
            })
    
    if not option_keys:
        print(f"‚ö†Ô∏è No {underlying} options found for expiry {expiry}")
        return []
    
    # Sort by strike price
    option_keys.sort(key=lambda x: x['strike_price'])
    
    print(f"‚úÖ Found {len(option_keys)} {underlying} options for {expiry}")
    
    # Return just the keys
    return [item['key'] for item in option_keys[:max_keys]]

def get_option_keys_around_price(
    underlying: str,
    expiry: str,
    spot_price: float,
    num_strikes: int = 10,
    max_keys: int = 100
) -> List[str]:
    """
    Get option keys around current spot price.
    Fixed: Matches by DATE only, not exact timestamp.
    
    Args:
        underlying: Underlying symbol
        expiry: Expiry date in 'YYYY-MM-DD' format
        spot_price: Current spot price
        num_strikes: Number of strikes to get on each side
        max_keys: Maximum total keys to return
    
    Returns:
        List of instrument keys sorted by proximity to spot
    """
    instruments = load_instruments()
    
    if not instruments:
        return []
    
    # Parse target expiry date
    try:
        target_date = datetime.strptime(expiry, "%Y-%m-%d").date()
    except ValueError as e:
        print(f"‚ùå Invalid expiry format: {expiry}. Expected YYYY-MM-DD")
        return []
    
    all_options = []
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        # Check if it's an option
        if instrument.get('instrument_type') not in ['CE', 'PE']:
            continue
        
        # Check underlying
        name = instrument.get('name', '')
        if name != underlying:
            continue
        
        # Check expiry - convert timestamp to date
        instrument_expiry_ts = instrument.get('expiry')
        if not instrument_expiry_ts:
            continue
        
        # Convert timestamp to date (ignoring time component)
        try:
            instrument_expiry_date = datetime.fromtimestamp(instrument_expiry_ts / 1000).date()
        except:
            continue
        
        # Match by date only (not exact timestamp)
        if instrument_expiry_date != target_date:
            continue
        
        # Calculate distance from spot
        strike_price = float(instrument.get('strike_price', 0))
        distance = abs(strike_price - spot_price)
        
        instrument_key = instrument.get('instrument_key')
        if instrument_key:
            all_options.append({
                'key': instrument_key,
                'strike_price': strike_price,
                'distance': distance,
                'instrument_type': instrument.get('instrument_type'),
                'instrument': instrument
            })
    
    if not all_options:
        print(f"‚ö†Ô∏è No {underlying} options found for expiry {expiry} around spot {spot_price}")
        return []
    
    # Sort by distance from spot
    all_options.sort(key=lambda x: x['distance'])
    
    # Get closest strikes
    closest_options = all_options[:num_strikes * 2]  # CE and PE for each strike
    
    # Sort by strike price for consistency
    closest_options.sort(key=lambda x: x['strike_price'])
    
    print(f"‚úÖ Found {len(closest_options[:max_keys])} {underlying} options around spot {spot_price} for {expiry}")
    
    return [item['key'] for item in closest_options[:max_keys]]


def get_available_expiries(underlying: str) -> List[str]:
    """
    Get all available expiry dates for an underlying.
    Fixed: Uses date-only matching.
    
    Args:
        underlying: Underlying symbol
    
    Returns:
        List of expiry dates in 'YYYY-MM-DD' format
    """
    instruments = load_instruments()
    
    if not instruments:
        return []
    
    expiry_dates = set()
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        # Check if it's the right underlying
        name = instrument.get('name', '')
        if name != underlying:
            continue
        
        # Check if it's an option
        if instrument.get('instrument_type') not in ['CE', 'PE']:
            continue
        
        expiry_ts = instrument.get('expiry')
        if expiry_ts:
            try:
                # Convert timestamp to date only
                expiry_date = datetime.fromtimestamp(expiry_ts / 1000).date()
                expiry_dates.add(expiry_date.strftime('%Y-%m-%d'))
            except:
                pass
    
    return sorted(expiry_dates)

def get_nearest_expiry(underlying: str, min_days: int = 0) -> str:
    """
    Get nearest expiry date for an underlying.
    Fixed: Uses date-only comparison.
    
    Args:
        underlying: Underlying symbol
        min_days: Minimum days until expiry
    
    Returns:
        Expiry date in 'YYYY-MM-DD' format, or empty string if none found
    """
    expiry_dates = get_available_expiries(underlying)
    
    if not expiry_dates:
        return ""
    
    # Get today's date (without time)
    today = datetime.now().date()
    
    # Add min_days
    from datetime import timedelta
    target_date = today + timedelta(days=min_days)
    target_str = target_date.strftime('%Y-%m-%d')
    
    # Find first expiry on or after target date
    for expiry in expiry_dates:
        if expiry >= target_str:
            return expiry
    
    # If no future expiry, return the last one
    return expiry_dates[-1]

def get_weekly_expiry(underlying: str) -> str:
    """
    Get nearest Thursday expiry (standard weekly expiry).
    
    Args:
        underlying: Underlying symbol
    
    Returns:
        Expiry date in 'YYYY-MM-DD' format
    """
    # Find the nearest Thursday
    today = datetime.now()
    
    # Thursday is weekday 3 (Monday=0)
    days_until_thursday = (3 - today.weekday()) % 7
    if days_until_thursday == 0:
        days_until_thursday = 7  # If today is Thursday, get next Thursday
    
    next_thursday = today + timedelta(days=days_until_thursday)
    
    # Check if this Thursday is available
    thursday_str = next_thursday.strftime('%Y-%m-%d')
    available_expiries = get_available_expiries(underlying)
    
    if thursday_str in available_expiries:
        return thursday_str
    
    # If not, get the nearest available expiry
    return get_nearest_expiry(underlying)

# ==============================
# INSTRUMENT LOOKUP
# ==============================

def get_instrument_by_key(instrument_key: str) -> Optional[Dict[str, Any]]:
    """Get instrument details by instrument key."""
    instruments = load_instruments()
    
    for instrument in instruments:
        if isinstance(instrument, dict) and instrument.get('instrument_key') == instrument_key:
            return instrument
    
    return None

def get_instruments_by_type(
    instrument_type: str,
    underlying: Optional[str] = None
) -> List[Dict[str, Any]]:
    """
    Get instruments by type.
    
    Args:
        instrument_type: 'EQ', 'CE', 'PE', 'FUT', etc.
        underlying: Optional underlying symbol filter
    
    Returns:
        List of matching instruments
    """
    instruments = load_instruments()
    
    filtered = []
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        if instrument.get('instrument_type') != instrument_type:
            continue
        
        if underlying:
            name = instrument.get('name', '')
            if name != underlying:
                continue
        
        filtered.append(instrument)
    
    return filtered

# ==============================
# BATCH OPERATIONS
# ==============================

def get_batch_option_keys(
    underlying: str,
    expiries: List[str],
    max_per_expiry: int = 50
) -> Dict[str, List[str]]:
    """
    Get option keys for multiple expiries.
    
    Args:
        underlying: Underlying symbol
        expiries: List of expiry dates
        max_per_expiry: Maximum keys per expiry
    
    Returns:
        Dictionary mapping expiry to list of keys
    """
    result = {}
    
    for expiry in expiries:
        keys = get_option_keys(underlying, expiry, max_keys=max_per_expiry)
        if keys:
            result[expiry] = keys
    
    return result

def get_instrument_keys_for_strikes(
    underlying: str,
    expiry: str,
    strikes: List[float],
    option_types: List[str] = ['CE', 'PE']
) -> List[str]:
    """
    Get instrument keys for specific strikes and option types.
    
    Args:
        underlying: Underlying symbol
        expiry: Expiry date
        strikes: List of strike prices
        option_types: List of option types
    
    Returns:
        List of instrument keys
    """
    instruments = load_instruments()
    
    keys = []
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        inst_type = instrument.get('instrument_type')
        if inst_type not in option_types:
            continue
        
        # Check underlying
        name = instrument.get('name', '')
        if name != underlying:
            continue
        
        # Check expiry
        instrument_expiry = instrument.get('expiry')
        if not instrument_expiry:
            continue
        
        # Convert expiry string to timestamp
        try:
            expiry_dt = datetime.strptime(expiry, "%Y-%m-%d")
            expiry_ts = int(expiry_dt.timestamp() * 1000)
        except:
            continue
        
        if instrument_expiry != expiry_ts:
            continue
        
        # Check strike
        strike_price = float(instrument.get('strike_price', 0))
        if strike_price not in strikes:
            continue
        
        instrument_key = instrument.get('instrument_key')
        if instrument_key:
            keys.append(instrument_key)
    
    return keys

# ==============================
# UTILITIES
# ==============================

def print_instrument_summary():
    """Print summary of loaded instruments."""
    instruments = load_instruments()
    
    if not instruments:
        print("No instruments loaded")
        return
    
    print(f"Total instruments: {len(instruments)}")
    
    # Count by type
    type_counts = {}
    underlying_counts = {}
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        inst_type = instrument.get('instrument_type', 'UNKNOWN')
        type_counts[inst_type] = type_counts.get(inst_type, 0) + 1
        
        name = instrument.get('name', 'UNKNOWN')
        underlying_counts[name] = underlying_counts.get(name, 0) + 1
    
    print("\nBy instrument type:")
    for inst_type, count in sorted(type_counts.items()):
        print(f"  {inst_type}: {count}")
    
    print("\nTop underlying symbols:")
    for name, count in sorted(underlying_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"  {name}: {count}")

# ==============================
# TEST FUNCTION
# ==============================

def test_instrument_master():
    """Test the instrument master functions."""
    print("üß™ Testing Instrument Master...")
    
    # Load instruments
    instruments = load_instruments()
    print(f"‚úì Loaded {len(instruments)} instruments")
    
    # Get available expiries for NIFTY
    nifty_expiries = get_available_expiries("NIFTY")
    print(f"‚úì NIFTY expiries: {nifty_expiries}")
    
    if nifty_expiries:
        # Get nearest expiry
        nearest = get_nearest_expiry("NIFTY")
        print(f"‚úì Nearest NIFTY expiry: {nearest}")
        
        # Get option keys for nearest expiry
        if nearest:
            keys = get_option_keys("NIFTY", nearest, max_keys=10)
            print(f"‚úì Got {len(keys)} option keys for {nearest}")
            
            if keys:
                print("  Sample keys:")
                for key in keys[:3]:
                    print(f"    - {key}")
                
                # Get instrument details
                instrument = get_instrument_by_key(keys[0])
                if instrument:
                    print(f"  First instrument: {instrument.get('trading_symbol', 'N/A')}")
    
    # Test BANKNIFTY
    banknifty_expiries = get_available_expiries("BANKNIFTY")
    print(f"‚úì BANKNIFTY expiries: {banknifty_expiries}")
    
    print("\n‚úÖ Instrument master test complete")
# ==============================
# EXPIRY MANAGEMENT
# ==============================

def get_next_available_expiry(underlying: str) -> str:
    """
    Get the next available expiry date (excluding today if market closed).
    
    Args:
        underlying: Underlying symbol (e.g., 'NIFTY', 'BANKNIFTY')
    
    Returns:
        Next available expiry date in 'YYYY-MM-DD' format
    """
    from datetime import datetime
    
    # Get all available expiries
    expiries = get_available_expiries(underlying)
    if not expiries:
        return ""
    
    # Get today's date
    today = datetime.now().strftime('%Y-%m-%d')
    
    # Check if market is open (for testing, you can hardcode this)
    # For now, let's assume if today is expiry day, we should use next expiry
    market_open = False  # You can set this based on actual market hours
    
    if market_open and today in expiries:
        # Market is open and today is an expiry - use it
        return today
    else:
        # Market closed or today not available - get next expiry
        for expiry in sorted(expiries):
            if expiry > today:
                return expiry
        
        # If no future expiry, return the last one
        return expiries[-1]
# Run test if executed directly
if __name__ == "__main__":
    test_instrument_master()


--------------------------------------------------
FILE PATH : G:\trading_app\data\instrument_master_fixed.py
SIZE      : 5579 bytes
--------------------------------------------------

"""
FIXED version - matches by date, not exact timestamp
"""
from typing import List, Dict, Any, Optional
import gzip
import json
from datetime import datetime, timedelta
from pathlib import Path

def load_instruments() -> List[Dict[str, Any]]:
    """Load instruments from compressed JSON file."""
    instrument_file = Path("data/instruments.json.gz")
    
    if not instrument_file.exists():
        print(f"‚ùå Instrument file not found: {instrument_file}")
        return []
    
    try:
        with gzip.open(instrument_file, 'rt', encoding='utf-8') as f:
            data = json.load(f)
            
        if isinstance(data, list):
            return data
        else:
            print(f"‚ö†Ô∏è Unexpected data format: {type(data)}")
            return []
            
    except Exception as e:
        print(f"‚ùå Error loading instruments: {e}")
        return []

def get_option_keys_fixed(
    underlying: str, 
    expiry: str, 
    max_keys: int = 200,
    debug: bool = False
) -> List[str]:
    """
    Get option instrument keys for given underlying and expiry.
    Matches by DATE only, not exact timestamp.
    
    Args:
        underlying: Underlying symbol (e.g., 'NIFTY', 'BANKNIFTY')
        expiry: Expiry date in 'YYYY-MM-DD' format
        max_keys: Maximum number of keys to return
        debug: Enable debug logging
    
    Returns:
        List of instrument keys
    """
    instruments = load_instruments()
    
    if not instruments:
        if debug: print("‚ö†Ô∏è No instruments loaded")
        return []
    
    # Parse target expiry date
    try:
        target_date = datetime.strptime(expiry, "%Y-%m-%d").date()
        if debug: print(f"Looking for options expiring on: {target_date}")
    except ValueError as e:
        print(f"‚ùå Invalid expiry format: {expiry}. Expected YYYY-MM-DD")
        return []
    
    option_keys = []
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        # Check if it's an option
        inst_type = instrument.get('instrument_type')
        if inst_type not in ['CE', 'PE']:
            continue
        
        # EXACT match for name
        name = instrument.get('name', '')
        if name != underlying:
            continue
        
        # Check expiry - convert timestamp to date
        instrument_expiry_ts = instrument.get('expiry')
        if not instrument_expiry_ts:
            continue
        
        # Convert timestamp to date (ignoring time component)
        try:
            instrument_expiry_date = datetime.fromtimestamp(instrument_expiry_ts / 1000).date()
        except:
            continue
        
        # Match by date only
        if instrument_expiry_date != target_date:
            continue
        
        instrument_key = instrument.get('instrument_key')
        if instrument_key:
            option_keys.append({
                'key': instrument_key,
                'strike_price': float(instrument.get('strike_price', 0)),
                'instrument_type': inst_type,
                'instrument': instrument
            })
    
    if not option_keys:
        if debug: print(f"‚ö†Ô∏è No {underlying} options found for expiry {expiry}")
        return []
    
    # Sort by strike price
    option_keys.sort(key=lambda x: x['strike_price'])
    
    if debug: print(f"‚úÖ Found {len(option_keys)} {underlying} options for {expiry}")
    
    # Return just the keys
    return [item['key'] for item in option_keys[:max_keys]]

def get_available_expiries_fixed(underlying: str) -> List[str]:
    """
    Get all available expiry dates for an underlying.
    Fixed version that works with date-only matching.
    """
    instruments = load_instruments()
    
    if not instruments:
        return []
    
    expiry_dates = set()
    
    for instrument in instruments:
        if not isinstance(instrument, dict):
            continue
        
        # Check if it's the right underlying
        name = instrument.get('name', '')
        if name != underlying:
            continue
        
        # Check if it's an option
        if instrument.get('instrument_type') not in ['CE', 'PE']:
            continue
        
        expiry_ts = instrument.get('expiry')
        if expiry_ts:
            try:
                expiry_date = datetime.fromtimestamp(expiry_ts / 1000).date()
                expiry_dates.add(expiry_date.strftime('%Y-%m-%d'))
            except:
                pass
    
    return sorted(expiry_dates)

def test_fixed_functions():
    """Test the fixed functions"""
    print("üß™ Testing FIXED functions...")
    
    # Test get_available_expiries_fixed
    print("\nüîç Available expiries (fixed):")
    expiries = get_available_expiries_fixed("NIFTY")
    print(f"NIFTY expiries: {expiries[:5]}...")
    
    # Test get_option_keys_fixed
    if expiries:
        test_expiry = expiries[0] if len(expiries) > 0 else "2026-02-03"
        print(f"\nüîç Testing option keys for {test_expiry} (fixed):")
        keys = get_option_keys_fixed("NIFTY", test_expiry, max_keys=10, debug=True)
        print(f"Found {len(keys)} keys")
        
        if keys:
            print("Sample keys:")
            for key in keys[:3]:
                print(f"  - {key}")
        else:
            print("‚ùå Still no keys found!")

if __name__ == "__main__":
    test_fixed_functions()


--------------------------------------------------
FILE PATH : G:\trading_app\data\upstox_client.py
SIZE      : 39288 bytes
--------------------------------------------------

"""
Enhanced Upstox Client with advanced derivatives analytics.
Implements research concepts:
1. OI Velocity - Rate of change of Open Interest
2. Gamma Exposure (GEX) - Dealer hedging pressure
3. Structural Walls vs Traps detection
4. Spot Divergence analysis
5. Market microstructure insights
"""

import requests
import pandas as pd
import numpy as np
from typing import List, Dict, Optional, Tuple
import json
from datetime import datetime, timedelta
from scipy.stats import linregress
from dataclasses import dataclass
from enum import Enum
import streamlit as st

# ==============================
# CONSTANTS & CONFIG
# ==============================

BASE_URL = "https://api.upstox.com/v2"

# Research-based constants
OI_VELOCITY_LOOKBACK = 5  # periods for velocity calculation
GAMMA_LOOKBACK = 10  # strikes for gamma calculation
WALL_THRESHOLD = 0.15  # Min OI concentration for wall detection
TRAP_CONFIRMATION_WINDOW = 3  # periods for trap confirmation

class MarketRegime(Enum):
    """Market regimes based on research"""
    NORMAL = "NORMAL"
    EXPANSIVE = "EXPANSIVE"  # High OI velocity, capital inflow
    CONSTRICTED = "CONSTRICTED"  # Negative OI velocity, capital outflow
    GAMMA_POSITIVE = "GAMMA_POSITIVE"  # Stabilizing, pinning
    GAMMA_NEGATIVE = "GAMMA_NEGATIVE"  # Accelerating, squeezes
    TRAP_FORMING = "TRAP_FORMING"  # Wall breach with unwinding
    DIVERGENCE = "DIVERGENCE"  # Spot vs derivatives divergence

@dataclass
class WallAnalysis:
    """Analysis of structural walls"""
    strike: float
    oi_concentration: float
    option_type: str  # CE or PE
    is_defended: bool
    unwinding_rate: float  # OI velocity at this strike
    gamma_contribution: float
    distance_to_spot: float  # % distance from current spot

@dataclass
class TrapAnalysis:
    """Analysis of potential traps"""
    wall_strike: float
    breach_direction: str  # "UP" or "DOWN"
    confidence: float
    trigger_time: datetime
    gamma_impact: float
    oi_unwinding: float

@dataclass
class GammaProfile:
    """Gamma Exposure profile"""
    net_gamma: float
    positive_gamma_strikes: List[float]
    negative_gamma_strikes: List[float]
    flip_levels: List[float]  # Where gamma changes sign
    max_gamma_strike: float
    regime: str

# ==============================
# ENHANCED UPSTOX CLIENT
# ==============================

class UpstoxClient:
    def __init__(self, access_token: str):
        self.access_token = access_token
        self.session = requests.Session()
        self.session.headers.update({
            "Accept": "application/json",
            "Authorization": f"Bearer {access_token}"
        })
        
        # State tracking for research concepts
        self.oi_history = {}  # symbol -> list of OI values
        self.price_history = {}  # symbol -> list of prices
        self.gamma_history = {}  # symbol -> list of gamma values
        self.velocity_history = {}  # symbol -> list of velocity values
        
        # Market structure tracking
        self.walls_cache = {}
        self.traps_cache = {}
        self.gex_cache = {}
        
        # Research-based analytics
        self.analytics = MarketAnalytics()
    
    # ==============================
    # CORE REQUEST HANDLER
    # ==============================
    
    def _make_request(self, method: str, endpoint: str, **kwargs):
        """Helper method to make API requests with error handling"""
        url = f"{BASE_URL}/{endpoint}"
        
        try:
            response = self.session.request(method, url, **kwargs)
            
            # Debug logging
            # print(f"Request: {method} {url}")
            # print(f"Params: {kwargs.get('params', {})}")
            # print(f"Status: {response.status_code}")
            
            if response.status_code != 200:
                print(f"Error Response: {response.text}")
            
            response.raise_for_status()
            return response.json()
            
        except requests.exceptions.HTTPError as e:
            print(f"HTTP Error: {e}")
            try:
                error_data = response.json()
                print(f"Error Details: {json.dumps(error_data, indent=2)}")
            except:
                pass
            raise
        except Exception as e:
            print(f"Request Error: {e}")
            raise
    
    # ==============================
    # RESEARCH IMPLEMENTATION: OI VELOCITY
    # ==============================
    
    def calculate_oi_velocity(self, symbol: str, current_oi: float) -> Tuple[float, str]:
        """
        Calculate Open Interest Velocity with regime classification.
        
        Research Concept: OI Velocity measures the kinetic energy of market trends.
        High positive velocity = Initiative capital entering (Buildup)
        High negative velocity = Capital exiting (Unwinding/Covering)
        
        Returns: (velocity_score, regime)
        """
        if symbol not in self.oi_history:
            self.oi_history[symbol] = []
        
        # Add current OI to history
        self.oi_history[symbol].append(current_oi)
        
        # Keep only last N periods
        if len(self.oi_history[symbol]) > OI_VELOCITY_LOOKBACK:
            self.oi_history[symbol] = self.oi_history[symbol][-OI_VELOCITY_LOOKBACK:]
        
        if len(self.oi_history[symbol]) < 2:
            return 0.0, "INSUFFICIENT_DATA"
        
        # Calculate velocity (rate of change)
        oi_series = pd.Series(self.oi_history[symbol])
        velocity = oi_series.pct_change().iloc[-1] * 100  # % change
        
        # Normalize velocity (research concept)
        if len(oi_series) >= 3:
            std_dev = oi_series.pct_change().std() * 100
            if std_dev > 0:
                normalized_velocity = velocity / std_dev
            else:
                normalized_velocity = velocity
        else:
            normalized_velocity = velocity
        
        # Classify regime
        if normalized_velocity > 1.5:
            regime = "EXPANSIVE"  # High capital inflow
        elif normalized_velocity < -1.5:
            regime = "CONSTRICTED"  # Capital outflow
        else:
            regime = "NORMAL"
        
        # Store for trend analysis
        if symbol not in self.velocity_history:
            self.velocity_history[symbol] = []
        self.velocity_history[symbol].append({
            "timestamp": datetime.utcnow(),
            "velocity": normalized_velocity,
            "regime": regime,
            "raw_oi": current_oi
        })
        
        return normalized_velocity, regime
    
    # ==============================
    # RESEARCH IMPLEMENTATION: GAMMA EXPOSURE
    # ==============================
    
    def calculate_gamma_exposure(self, option_chain_df: pd.DataFrame, 
                                 spot_price: float) -> GammaProfile:
        """
        Calculate Gamma Exposure (GEX) with dealer hedging analysis.
        
        Research Concept: GEX predicts volatility regimes.
        Positive GEX: Dealers long gamma ‚Üí Stabilizing, pinning expected
        Negative GEX: Dealers short gamma ‚Üí Accelerating, squeezes possible
        """
        if option_chain_df.empty:
            return GammaProfile(0.0, [], [], [], 0.0, "NEUTRAL")
        
        # Calculate gamma for each strike (simplified Black-Scholes gamma)
        gamma_values = []
        flip_levels = []
        
        for _, row in option_chain_df.iterrows():
            strike = row['strike']
            option_type = row['option_type']
            oi = row['oi']
            iv = row.get('iv', 0.3)  # Default IV if not available
            
            # Simplified gamma calculation
            # In production, use proper Black-Scholes or gather from API
            if iv > 0:
                # Approximate gamma: highest ATM, decays as move OTM
                distance = abs(strike - spot_price) / spot_price
                gamma = np.exp(-distance * 100) / (iv * np.sqrt(252/365))  # Simplified
                
                # Adjust for option type
                if option_type == "PE":
                    gamma = -gamma  # Short gamma for put writers
                
                # Weight by OI
                gamma_weighted = gamma * oi
                gamma_values.append((strike, gamma_weighted))
                
                # Track sign changes for flip levels
                if len(gamma_values) > 1:
                    prev_sign = np.sign(gamma_values[-2][1])
                    curr_sign = np.sign(gamma_weighted)
                    if prev_sign != curr_sign:
                        flip_level = (strike + gamma_values[-2][0]) / 2
                        flip_levels.append(flip_level)
        
        if not gamma_values:
            return GammaProfile(0.0, [], [], [], 0.0, "NEUTRAL")
        
        # Calculate net gamma
        strikes, gammas = zip(*gamma_values)
        net_gamma = sum(gammas)
        
        # Separate positive and negative gamma strikes
        positive_strikes = [s for s, g in gamma_values if g > 0]
        negative_strikes = [s for s, g in gamma_values if g < 0]
        
        # Find strike with maximum gamma impact
        max_gamma_idx = np.argmax(np.abs(gammas))
        max_gamma_strike = strikes[max_gamma_idx]
        
        # Determine regime
        if net_gamma > 0:
            regime = "GAMMA_POSITIVE"
        elif net_gamma < 0:
            regime = "GAMMA_NEGATIVE"
        else:
            regime = "NEUTRAL"
        
        return GammaProfile(
            net_gamma=net_gamma,
            positive_gamma_strikes=positive_strikes[:5],  # Top 5
            negative_gamma_strikes=negative_strikes[:5],
            flip_levels=flip_levels,
            max_gamma_strike=max_gamma_strike,
            regime=regime
        )
    
    # ==============================
    # RESEARCH IMPLEMENTATION: WALLS VS TRAPS
    # ==============================
    
    def analyze_structural_walls(self, option_chain_df: pd.DataFrame, 
                                 spot_price: float) -> List[WallAnalysis]:
        """
        Identify structural walls with defense analysis.
        
        Research Concept: Walls are high OI concentrations that act as barriers.
        Traps form when walls breach with OI unwinding.
        """
        if option_chain_df.empty:
            return []
        
        walls = []
        
        # Group by strike to find OI concentrations
        strike_groups = option_chain_df.groupby('strike')
        
        for strike, group in strike_groups:
            total_oi = group['oi'].sum()
            call_oi = group[group['option_type'] == 'CE']['oi'].sum()
            put_oi = group[group['option_type'] == 'PE']['oi'].sum()
            
            # Determine dominant option type at this strike
            if call_oi > put_oi:
                dominant_type = "CE"
                dominant_oi = call_oi
            else:
                dominant_type = "PE"
                dominant_oi = put_oi
            
            # Calculate OI concentration (research concept)
            total_all_oi = option_chain_df['oi'].sum()
            if total_all_oi > 0:
                oi_concentration = dominant_oi / total_all_oi
            else:
                oi_concentration = 0
            
            # Check if this is a wall (high concentration)
            if oi_concentration > WALL_THRESHOLD:
                # Calculate distance from spot
                distance_pct = abs(strike - spot_price) / spot_price * 100
                
                # Analyze defense strength
                # Defense is stronger if:
                # 1. High OI concentration
                # 2. Close to spot price
                # 3. Recent OI increase (build-up)
                
                # Calculate unwinding rate (OI velocity at this strike)
                strike_key = f"{strike}_{dominant_type}"
                if strike_key in self.oi_history:
                    strike_oi_series = pd.Series(self.oi_history[strike_key])
                    if len(strike_oi_series) > 1:
                        unwinding_rate = strike_oi_series.pct_change().iloc[-1] * 100
                    else:
                        unwinding_rate = 0
                else:
                    unwinding_rate = 0
                
                # Determine if wall is being defended
                # Negative unwinding = defending (adding positions)
                # Positive unwinding = abandoning (closing positions)
                is_defended = unwinding_rate < 0  # Adding OI = defending
                
                walls.append(WallAnalysis(
                    strike=strike,
                    oi_concentration=oi_concentration,
                    option_type=dominant_type,
                    is_defended=is_defended,
                    unwinding_rate=unwinding_rate,
                    gamma_contribution=0,  # Would calculate in production
                    distance_to_spot=distance_pct
                ))
        
        # Sort by OI concentration (highest first)
        walls.sort(key=lambda x: x.oi_concentration, reverse=True)
        return walls[:5]  # Return top 5 walls
    
    def detect_traps(self, walls: List[WallAnalysis], spot_price: float,
                     oi_velocity: float) -> List[TrapAnalysis]:
        """
        Detect potential traps forming at walls.
        
        Research Concept: Traps occur when:
        1. Price breaches a wall
        2. OI starts unwinding (velocity negative)
        3. Gamma is negative (accelerating)
        4. High confidence of squeeze
        """
        traps = []
        
        for wall in walls:
            # Check if price is near wall (¬±1%)
            price_to_wall_ratio = spot_price / wall.strike
            is_near_wall = 0.99 <= price_to_wall_ratio <= 1.01
            
            if is_near_wall:
                # Check for trap conditions
                trap_confidence = 0.0
                
                # Condition 1: OI unwinding (negative velocity)
                if wall.unwinding_rate > 1.0:  # Rapid unwinding
                    trap_confidence += 0.3
                
                # Condition 2: Overall OI velocity negative
                if oi_velocity < -1.0:
                    trap_confidence += 0.3
                
                # Condition 3: Wall not defended
                if not wall.is_defended:
                    trap_confidence += 0.2
                
                # Condition 4: Price already breached (for detection)
                if (wall.option_type == "CE" and spot_price > wall.strike) or \
                   (wall.option_type == "PE" and spot_price < wall.strike):
                    trap_confidence += 0.2
                
                if trap_confidence > 0.5:  # Minimum confidence threshold
                    breach_direction = "UP" if wall.option_type == "CE" else "DOWN"
                    
                    trap = TrapAnalysis(
                        wall_strike=wall.strike,
                        breach_direction=breach_direction,
                        confidence=trap_confidence,
                        trigger_time=datetime.utcnow(),
                        gamma_impact=0,  # Would calculate actual gamma
                        oi_unwinding=wall.unwinding_rate
                    )
                    traps.append(trap)
        
        return traps
    
    # ==============================
    # RESEARCH IMPLEMENTATION: SPOT DIVERGENCE
    # ==============================
    
    def analyze_spot_divergence(self, spot_price: float, spot_velocity: float,
                               oi_velocity: float, gamma_profile: GammaProfile) -> Dict:
        """
        Analyze divergence between spot price and derivatives metrics.
        
        Research Concept: Divergence reveals internal market weakness.
        Bullish divergence: Price down but OI/Gamma improving
        Bearish divergence: Price up but OI/Gamma deteriorating
        """
        divergence_analysis = {
            "has_divergence": False,
            "type": None,  # BULLISH/BEARISH
            "confidence": 0.0,
            "metrics": {}
        }
        
        # Price vs OI divergence
        if spot_velocity > 0 and oi_velocity < -0.5:
            # Price up but OI unwinding (bearish divergence)
            divergence_analysis["has_divergence"] = True
            divergence_analysis["type"] = "BEARISH"
            divergence_analysis["confidence"] = min(abs(oi_velocity) / 2, 1.0)
            divergence_analysis["metrics"]["price_oi_divergence"] = {
                "price_change": spot_velocity,
                "oi_change": oi_velocity,
                "interpretation": "Hollow rally - price rising on covering, not new buying"
            }
        
        elif spot_velocity < 0 and oi_velocity > 0.5:
            # Price down but OI building (bullish divergence)
            divergence_analysis["has_divergence"] = True
            divergence_analysis["type"] = "BULLISH"
            divergence_analysis["confidence"] = min(oi_velocity / 2, 1.0)
            divergence_analysis["metrics"]["price_oi_divergence"] = {
                "price_change": spot_velocity,
                "oi_change": oi_velocity,
                "interpretation": "Accumulation - smart money buying despite price drop"
            }
        
        # Price vs Gamma divergence
        if gamma_profile.regime == "GAMMA_POSITIVE" and spot_velocity > 1.0:
            # Positive gamma (stabilizing) but price moving fast
            divergence_analysis["has_divergence"] = True
            divergence_analysis["type"] = "BEARISH"
            divergence_analysis["confidence"] = max(divergence_analysis["confidence"], 0.3)
            divergence_analysis["metrics"]["price_gamma_divergence"] = {
                "gamma_regime": gamma_profile.regime,
                "price_velocity": spot_velocity,
                "interpretation": "Price moving against gamma regime - likely to revert"
            }
        
        return divergence_analysis
    
    # ==============================
    # ENHANCED OPTION CHAIN FETCH
    # ==============================
    
    def fetch_option_chain_with_analytics(self, option_keys: list[str], 
                                         spot_price: float) -> Dict:
        """
        Enhanced option chain fetch with full research analytics.
        Returns comprehensive analysis including OI velocity, GEX, walls, traps.
        """
        # Fetch raw option chain
        option_chain_df = self.fetch_option_chain(option_keys)
        
        if option_chain_df.empty:
            return {
                "raw_data": option_chain_df,
                "analytics": {},
                "warnings": ["No option data available"]
            }
        
        # Calculate total OI for velocity
        total_oi = option_chain_df['oi'].sum()
        
        # OI Velocity analysis
        oi_velocity, oi_regime = self.calculate_oi_velocity("INDEX", total_oi)
        
        # Gamma Exposure analysis
        gamma_profile = self.calculate_gamma_exposure(option_chain_df, spot_price)
        
        # Structural walls analysis
        walls = self.analyze_structural_walls(option_chain_df, spot_price)
        
        # Trap detection
        traps = self.detect_traps(walls, spot_price, oi_velocity)
        
        # Spot price velocity (approximate)
        if "INDEX" not in self.price_history:
            self.price_history["INDEX"] = []
        self.price_history["INDEX"].append(spot_price)
        if len(self.price_history["INDEX"]) > OI_VELOCITY_LOOKBACK:
            self.price_history["INDEX"] = self.price_history["INDEX"][-OI_VELOCITY_LOOKBACK:]
        
        spot_velocity = 0
        if len(self.price_history["INDEX"]) >= 2:
            price_series = pd.Series(self.price_history["INDEX"])
            spot_velocity = price_series.pct_change().iloc[-1] * 100
        
        # Spot divergence analysis
        divergence = self.analyze_spot_divergence(spot_price, spot_velocity, 
                                                 oi_velocity, gamma_profile)
        
        # Market regime synthesis
        market_regime = self._synthesize_market_regime(
            oi_regime, gamma_profile.regime, divergence
        )
        
        # Compile comprehensive analysis
        analytics = {
            "timestamp": datetime.utcnow().isoformat(),
            "spot_price": spot_price,
            
            # OI Analysis
            "oi_velocity": round(oi_velocity, 3),
            "oi_regime": oi_regime,
            "total_oi": int(total_oi),
            
            # Gamma Analysis
            "gamma_exposure": {
                "net_gamma": round(gamma_profile.net_gamma, 3),
                "regime": gamma_profile.regime,
                "flip_levels": [round(x, 2) for x in gamma_profile.flip_levels],
                "max_impact_strike": gamma_profile.max_gamma_strike
            },
            
            # Structure Analysis
            "structural_walls": [
                {
                    "strike": wall.strike,
                    "type": wall.option_type,
                    "concentration": round(wall.oi_concentration, 3),
                    "defended": wall.is_defended,
                    "distance_pct": round(wall.distance_to_spot, 2)
                }
                for wall in walls
            ],
            
            # Trap Analysis
            "potential_traps": [
                {
                    "strike": trap.wall_strike,
                    "direction": trap.breach_direction,
                    "confidence": round(trap.confidence, 3),
                    "unwinding_rate": round(trap.oi_unwinding, 2)
                }
                for trap in traps
            ],
            
            # Divergence Analysis
            "spot_divergence": divergence,
            
            # Market Regime
            "market_regime": market_regime,
            "regime_confidence": self._calculate_regime_confidence(
                oi_velocity, gamma_profile.net_gamma, divergence
            )
        }
        
        # Store in cache for trend analysis
        self.walls_cache[datetime.utcnow()] = walls
        self.traps_cache[datetime.utcnow()] = traps
        self.gex_cache[datetime.utcnow()] = gamma_profile
        
        return {
            "raw_data": option_chain_df,
            "analytics": analytics,
            "market_insights": self._generate_market_insights(analytics)
        }
    
    # ==============================
    # ORIGINAL METHODS (ENHANCED)
    # ==============================
    
    def fetch_option_chain(self, option_keys: list[str]) -> pd.DataFrame:
        """Original method - fetch and normalize option chain quotes."""
        if not option_keys:
            return pd.DataFrame(
                columns=["strike", "option_type", "oi", "oi_change", "iv", "ltp", "volume"]
            )
        
        # Take max 200 keys (API limit)
        keys_to_fetch = option_keys[:200]
        
        data = self._make_request(
            "GET",
            "market-quote/quotes",
            params={"instrument_key": ",".join(keys_to_fetch)}
        )
        
        rows = []
        for key, payload in data.get("data", {}).items():
            symbol = payload.get("trading_symbol", "")
            
            rows.append({
                "strike": payload.get("strike_price"),
                "option_type": "CE" if symbol.endswith("CE") else "PE",
                "oi": payload.get("oi", 0),
                "oi_change": payload.get("oi_day_high", 0) - payload.get("oi_day_low", 0),
                "iv": payload.get("implied_volatility", 0),
                "ltp": payload.get("last_price", 0),
                "volume": payload.get("volume", 0),
                "timestamp": payload.get("timestamp")
            })
        
        return pd.DataFrame(rows)
    
    def fetch_index_quote(self, symbol: str = "NSE_INDEX|Nifty 50") -> Optional[dict]:
        """Original method - fetch index quote."""
        try:
            data = self._make_request(
                "GET",
                "market-quote/quotes",
                params={"symbol": symbol}
            )
            
            if data.get("status") == "success":
                response_key = symbol.replace("|", ":")
                quote_data = data.get("data", {}).get(response_key)
                
                if not quote_data:
                    quote_data = data.get("data", {}).get(symbol)
                
                if not quote_data and data.get("data"):
                    first_key = list(data["data"].keys())[0]
                    quote_data = data["data"][first_key]
                
                if not quote_data:
                    return None
                
                ohlc = quote_data.get("ohlc", {})
                
                return {
                    "symbol": symbol,
                    "ltp": quote_data.get("last_price"),
                    "open": ohlc.get("open"),
                    "high": ohlc.get("high"),
                    "low": ohlc.get("low"),
                    "close": ohlc.get("close"),
                    "change": quote_data.get("change"),
                    "net_change": quote_data.get("net_change"),
                    "percent_change": quote_data.get("percent_change"),
                    "volume": quote_data.get("volume"),
                    "timestamp": quote_data.get("timestamp")
                }
            else:
                print(f"API Error: {data}")
                return None
                
        except Exception as e:
            print(f"Error in fetch_index_quote: {e}")
            return None
    
    def fetch_equity_quotes(self, symbols: list[str]) -> pd.DataFrame:
        """
        Fetch equity quotes from Upstox.
        Accepts BOTH:
        - SYMBOL (e.g. HDFCBANK)
        - NSE_EQ|SYMBOL

        Internally normalizes to NSE_EQ|SYMBOL (Upstox requirement).
        NEVER raises validation errors for symbol format.
        """

        # ------------------------------
        # EMPTY GUARD
        # ------------------------------
        if not symbols:
            return pd.DataFrame(
                columns=[
                    "symbol", "ltp", "open", "high", "low",
                    "close", "change", "percent_change",
                    "volume", "timestamp"
                ]
            )

        # ------------------------------
        # üîë NORMALIZE SYMBOLS (CRITICAL FIX)
        # ------------------------------
        normalized_symbols: list[str] = []

        for s in symbols:
            if not isinstance(s, str):
                continue

            s = s.strip().upper()

            # Convert SYMBOL ‚Üí NSE_EQ|SYMBOL
            if "|" not in s:
                normalized_symbols.append(f"NSE_EQ|{s}")
            else:
                # Already formatted ‚Üí enforce correct prefix
                prefix, sym = s.split("|", 1)
                if prefix != "NSE_EQ":
                    normalized_symbols.append(f"NSE_EQ|{sym}")
                else:
                    normalized_symbols.append(s)

        if not normalized_symbols:
            return pd.DataFrame(
                columns=[
                    "symbol", "ltp", "open", "high", "low",
                    "close", "change", "percent_change",
                    "volume", "timestamp"
                ]
            )

        # Upstox limit safeguard
        normalized_symbols = normalized_symbols[:200]

        # ------------------------------
        # API CALL
        # ------------------------------
        data = self._make_request(
            "GET",
            "market-quote/quotes",
            params={"instrument_key": ",".join(normalized_symbols)}
        )

        # ------------------------------
        # PARSE RESPONSE
        # ------------------------------
        rows = []

        for key, payload in data.get("data", {}).items():
            try:
                # key format: NSE_EQ|HDFCBANK
                symbol = key.split("|", 1)[1]

                ohlc = payload.get("ohlc", {}) or {}

                rows.append({
                    "symbol": symbol,                     # ‚Üê IMPORTANT (NO PREFIX)
                    "ltp": payload.get("last_price"),
                    "open": ohlc.get("open"),
                    "high": ohlc.get("high"),
                    "low": ohlc.get("low"),
                    "close": ohlc.get("close"),
                    "change": payload.get("change"),
                    "percent_change": payload.get("percent_change"),
                    "volume": payload.get("volume", 0),
                    "timestamp": payload.get("timestamp")
                })

            except Exception as e:
                print(f"‚ö†Ô∏è Failed to parse equity quote {key}: {e}")

        return pd.DataFrame(rows)


    
    # ==============================
    # HELPER METHODS
    # ==============================
    
    def _synthesize_market_regime(self, oi_regime: str, gamma_regime: str, 
                                 divergence: Dict) -> str:
        """Synthesize overall market regime from multiple indicators."""
        regimes = []
        
        # OI-based regime
        if oi_regime == "EXPANSIVE":
            regimes.append("CAPITAL_INFLOW")
        elif oi_regime == "CONSTRICTED":
            regimes.append("CAPITAL_OUTFLOW")
        
        # Gamma-based regime
        if "POSITIVE" in gamma_regime:
            regimes.append("STABILIZING")
        elif "NEGATIVE" in gamma_regime:
            regimes.append("ACCELERATING")
        
        # Divergence-based
        if divergence["has_divergence"]:
            regimes.append(f"DIVERGENCE_{divergence['type']}")
        
        if not regimes:
            return "NEUTRAL"
        
        # Combine regimes
        if len(regimes) == 1:
            return regimes[0]
        else:
            return f"{regimes[0]}_{regimes[1]}"
    
    def _calculate_regime_confidence(self, oi_velocity: float, 
                                    net_gamma: float, divergence: Dict) -> float:
        """Calculate confidence score for market regime."""
        confidence = 0.5  # Base confidence
        
        # OI velocity confidence
        oi_confidence = min(abs(oi_velocity) / 3, 1.0)
        confidence = 0.7 * confidence + 0.3 * oi_confidence
        
        # Gamma confidence
        gamma_confidence = min(abs(net_gamma) / 1000, 1.0)  # Scale appropriately
        confidence = 0.7 * confidence + 0.3 * gamma_confidence
        
        # Divergence confidence
        if divergence["has_divergence"]:
            div_confidence = divergence["confidence"]
            confidence = 0.8 * confidence + 0.2 * div_confidence
        
        return round(confidence, 3)
    
    def _generate_market_insights(self, analytics: Dict) -> List[str]:
        """Generate human-readable market insights."""
        insights = []
        
        # OI insights
        oi_vel = analytics["oi_velocity"]
        if oi_vel > 1.5:
            insights.append("üìà Strong OI buildup - New capital entering market")
        elif oi_vel < -1.5:
            insights.append("üìâ OI unwinding - Positions being closed, watch for reversals")
        
        # Gamma insights
        gamma_regime = analytics["gamma_exposure"]["regime"]
        if gamma_regime == "GAMMA_POSITIVE":
            insights.append("üìå Positive Gamma - Market likely to pin/stabilize")
        elif gamma_regime == "GAMMA_NEGATIVE":
            insights.append("üöÄ Negative Gamma - Accelerating moves possible, watch for squeezes")
        
        # Wall insights
        walls = analytics["structural_walls"]
        if walls:
            top_wall = walls[0]
            if top_wall["defended"]:
                insights.append(f"üõ°Ô∏è Strong wall at {top_wall['strike']} ({top_wall['type']}) - Being defended")
            else:
                insights.append(f"‚ö†Ô∏è Wall at {top_wall['strike']} weakening - Monitor for breach")
        
        # Trap insights
        traps = analytics["potential_traps"]
        if traps:
            trap = traps[0]
            insights.append(f"üéØ Potential {trap['direction']} trap at {trap['strike']} - Confidence: {trap['confidence']*100:.0f}%")
        
        # Divergence insights
        if analytics["spot_divergence"]["has_divergence"]:
            div_type = analytics["spot_divergence"]["type"]
            if div_type == "BULLISH":
                insights.append("üîç Bullish divergence detected - Price down but smart money accumulating")
            else:
                insights.append("üîç Bearish divergence detected - Price up but internal weakness")
        
        return insights
    
    def get_market_analytics_summary(self) -> Dict:
        """Get summary of all market analytics."""
        return {
            "oi_velocity_history": self.velocity_history,
            "gamma_history": self.gex_cache,
            "walls_history": self.walls_cache,
            "traps_history": self.traps_cache,
            "current_regime": getattr(self, 'current_regime', 'UNKNOWN'),
            "last_update": datetime.utcnow().isoformat()
        }


# ==============================
# MARKET ANALYTICS HELPER CLASS
# ==============================

class MarketAnalytics:
    """Helper class for advanced market analytics."""
    
    @staticmethod
    def calculate_put_call_ratio(option_chain_df: pd.DataFrame) -> Dict:
        """Calculate Put-Call Ratio with breakdown."""
        put_oi = option_chain_df[option_chain_df["option_type"] == "PE"]["oi"].sum()
        call_oi = option_chain_df[option_chain_df["option_type"] == "CE"]["oi"].sum()
        put_volume = option_chain_df[option_chain_df["option_type"] == "PE"]["volume"].sum()
        call_volume = option_chain_df[option_chain_df["option_type"] == "CE"]["volume"].sum()
        
        pcr_oi = put_oi / call_oi if call_oi > 0 else 0
        pcr_volume = put_volume / call_volume if call_volume > 0 else 0
        
        return {
            "pcr_oi": round(pcr_oi, 3),
            "pcr_volume": round(pcr_volume, 3),
            "sentiment": "BEARISH" if pcr_oi > 1.2 else "BULLISH" if pcr_oi < 0.8 else "NEUTRAL",
            "put_oi": int(put_oi),
            "call_oi": int(call_oi),
            "total_oi": int(put_oi + call_oi)
        }
    
    @staticmethod
    def detect_max_pain(option_chain_df: pd.DataFrame) -> Dict:
        """Calculate Max Pain strike."""
        if option_chain_df.empty:
            return {"max_pain_strike": 0, "total_pain": 0}
        
        strikes = sorted(option_chain_df['strike'].unique())
        pain_values = []
        
        for strike in strikes:
            total_pain = 0
            
            # Calculate pain from puts
            puts = option_chain_df[(option_chain_df['strike'] == strike) & 
                                  (option_chain_df['option_type'] == 'PE')]
            for _, put in puts.iterrows():
                if strike < put['strike']:  # ITM
                    total_pain += (put['strike'] - strike) * put['oi']
            
            # Calculate pain from calls
            calls = option_chain_df[(option_chain_df['strike'] == strike) & 
                                   (option_chain_df['option_type'] == 'CE')]
            for _, call in calls.iterrows():
                if strike > call['strike']:  # ITM
                    total_pain += (strike - call['strike']) * call['oi']
            
            pain_values.append((strike, total_pain))
        
        if pain_values:
            max_pain_strike, min_pain = min(pain_values, key=lambda x: x[1])
            return {
                "max_pain_strike": max_pain_strike,
                "total_pain": int(min_pain),
                "all_pain_levels": pain_values[:10]  # Top 10
            }
        
        return {"max_pain_strike": 0, "total_pain": 0}


# ==============================
# UTILITY FUNCTIONS
# ==============================

def display_market_analytics(analytics: Dict):
    """Display market analytics in Streamlit."""
    if not analytics:
        st.info("No analytics available yet")
        return
    
    st.markdown("### üìä Advanced Market Analytics")
    
    # OI Velocity
    col1, col2, col3 = st.columns(3)
    with col1:
        oi_vel = analytics.get("oi_velocity", 0)
        regime = analytics.get("oi_regime", "N/A")
        color = "green" if oi_vel > 0 else "red"
        st.metric("OI Velocity", f"{oi_vel:.2f}œÉ", regime, delta_color="off")
    
    # Gamma Exposure
    with col2:
        gamma = analytics.get("gamma_exposure", {}).get("net_gamma", 0)
        gamma_regime = analytics.get("gamma_exposure", {}).get("regime", "N/A")
        icon = "üìå" if "POSITIVE" in gamma_regime else "üöÄ" if "NEGATIVE" in gamma_regime else "‚öñÔ∏è"
        st.metric("Gamma Exposure", f"{icon} {gamma_regime}", f"{gamma:.2f}")
    
    # Market Regime
    with col3:
        regime = analytics.get("market_regime", "N/A")
        confidence = analytics.get("regime_confidence", 0) * 100
        st.metric("Market Regime", regime, f"{confidence:.0f}% confidence")
    
    # Insights
    insights = analytics.get("market_insights", [])
    if insights:
        st.markdown("#### üí° Market Insights")
        for insight in insights[:5]:  # Top 5 insights
            st.info(insight)
    
    # Structural Walls
    walls = analytics.get("structural_walls", [])
    if walls:
        st.markdown("#### üß± Structural Walls")
        wall_df = pd.DataFrame(walls)
        st.dataframe(wall_df, use_container_width=True)
    
    # Traps
    traps = analytics.get("potential_traps", [])
    if traps:
        st.markdown("#### üéØ Potential Traps")
        for trap in traps:
            direction = trap.get("direction", "")
            strike = trap.get("strike", 0)
            confidence = trap.get("confidence", 0) * 100
            st.warning(f"{direction} trap at {strike} ({confidence:.0f}% confidence)")


--------------------------------------------------
FILE PATH : G:\trading_app\features\breadth.py
SIZE      : 18465 bytes
--------------------------------------------------

"""
Enhanced Breadth Features with Research Calculations.
Includes CCC analysis, market breadth, and constituent analysis.
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
from scipy import stats

# ==============================
# BREADTH FEATURE CALCULATIONS
# ==============================

def compute_breadth_features(
    constituents_df: pd.DataFrame,
    ccc_history: pd.Series
) -> Dict[str, float]:
    """
    Compute breadth-based features including research metrics.
    """
    # Base features
    ccc_value = compute_ccc_value(constituents_df)
    ccc_slope = compute_ccc_slope(ccc_history)
    
    # Advanced breadth features
    if not constituents_df.empty:
        breadth_metrics = compute_market_breadth(constituents_df)
        weight_distribution = analyze_weight_distribution(constituents_df)
        sector_analysis = analyze_sector_concentration(constituents_df)
        momentum_breadth = compute_momentum_breadth(constituents_df)
    else:
        breadth_metrics = {
            "advance_decline_ratio": 1.0,
            "percent_above_ma": 0.5,
            "breadth_momentum": 0.0
        }
        weight_distribution = {
            "top5_concentration": 0.3,
            "herfindahl_index": 0.1,
            "weight_skew": 0.0
        }
        sector_analysis = {
            "sector_concentration": 0.5,
            "dominant_sector_strength": 0.0
        }
        momentum_breadth = {
            "positive_momentum_ratio": 0.5,
            "momentum_dispersion": 0.0
        }
    
    # Combine all features
    features = {
        # Base CCC features
        "ccc_value": ccc_value,
        "ccc_slope": ccc_slope,
        
        # Market breadth
        "advance_decline_ratio": breadth_metrics["advance_decline_ratio"],
        "percent_above_ma": breadth_metrics["percent_above_ma"],
        "breadth_momentum": breadth_metrics["breadth_momentum"],
        
        # Weight distribution
        "top5_concentration": weight_distribution["top5_concentration"],
        "herfindahl_index": weight_distribution["herfindahl_index"],
        "weight_skew": weight_distribution["weight_skew"],
        
        # Sector analysis
        "sector_concentration": sector_analysis["sector_concentration"],
        "dominant_sector_strength": sector_analysis["dominant_sector_strength"],
        
        # Momentum breadth
        "positive_momentum_ratio": momentum_breadth["positive_momentum_ratio"],
        "momentum_dispersion": momentum_breadth["momentum_dispersion"],
        
        # Derived features
        "breadth_health": compute_breadth_health_score(
            ccc_value, ccc_slope, breadth_metrics
        ),
        "market_participation": compute_market_participation(constituents_df)
    }
    
    return features

# ==============================
# CCC CALCULATIONS
# ==============================

def compute_ccc_value(
    constituents_df: pd.DataFrame,
    weight_col: str = "weight",
    price_change_col: str = "price_change"
) -> float:
    """
    Compute Cumulative Constituent Contribution (CCC).
    """
    if constituents_df.empty:
        return 0.0
    
    required_cols = {weight_col, price_change_col}
    if not required_cols.issubset(constituents_df.columns):
        raise ValueError(
            f"Missing required columns: {required_cols - set(constituents_df.columns)}"
        )
    
    ccc = (constituents_df[weight_col] * constituents_df[price_change_col]).sum()
    return float(ccc)

def build_constituents_df(
    equity_quotes_df: pd.DataFrame,
    weights: Dict[str, float]
) -> pd.DataFrame:
    """
    Build constituents DataFrame.
    Handles BOTH:
      - weights keyed as SYMBOL
      - weights keyed as NSE_EQ|SYMBOL
    """

    if equity_quotes_df.empty or not weights:
        return pd.DataFrame()

    rows = []

    for _, row in equity_quotes_df.iterrows():
        symbol = row.get("symbol")

        if not symbol:
            continue

        # üîë Normalize weight lookup
        weight = (
            weights.get(symbol) or
            weights.get(f"NSE_EQ|{symbol}")
        )

        if weight is None:
            continue

        open_price = row.get("open") or row.get("close")
        ltp = row.get("ltp")

        if open_price is None or open_price <= 0 or ltp is None:
            continue


        price_change = (ltp - open_price) / open_price

        rows.append({
            "symbol": symbol,
            "weight": weight,
            "price_change": price_change,
            "ltp": ltp,
            "open": open_price
        })

    return pd.DataFrame(rows)


def compute_ccc_slope(
    ccc_series: pd.Series,
    lookback: int = 5
) -> float:
    """
    Compute slope of CCC over last N points.
    """
    if len(ccc_series) <= lookback:
        return 0.0
    
    y = ccc_series.iloc[-lookback:]
    x = np.arange(len(y))
    
    slope = np.polyfit(x, y, 1)[0]
    return float(slope)

# ==============================
# MARKET BREADTH ANALYSIS
# ==============================

def compute_market_breadth(constituents_df: pd.DataFrame) -> Dict[str, float]:
    """
    Compute comprehensive market breadth metrics.
    """
    if constituents_df.empty:
        return {
            "advance_decline_ratio": 1.0,
            "percent_above_ma": 0.5,
            "breadth_momentum": 0.0,
            "new_highs_lows": 0.0,
            "breadth_thrust": 0.0
        }
    
    # Advance-Decline ratio
    advances = constituents_df[constituents_df["price_change"] > 0]
    declines = constituents_df[constituents_df["price_change"] < 0]
    
    advance_count = len(advances)
    decline_count = len(declines)
    
    if decline_count == 0:
        advance_decline_ratio = float(advance_count) if advance_count > 0 else 1.0
    else:
        advance_decline_ratio = advance_count / decline_count
    
    # Percent above "moving average" (simplified as above open)
    above_open = constituents_df[constituents_df["price_change"] > 0]
    percent_above = len(above_open) / len(constituents_df)
    
    # Breadth momentum (weighted average of price changes)
    if constituents_df["weight"].sum() > 0:
        breadth_momentum = (constituents_df["weight"] * constituents_df["price_change"]).sum()
    else:
        breadth_momentum = constituents_df["price_change"].mean()
    
    # New highs/lows (simplified as extreme moves)
    price_change_std = constituents_df["price_change"].std()
    if price_change_std > 0:
        extreme_positives = constituents_df[
            constituents_df["price_change"] > 2 * price_change_std
        ]
        extreme_negatives = constituents_df[
            constituents_df["price_change"] < -2 * price_change_std
        ]
        new_highs_lows = (len(extreme_positives) - len(extreme_negatives)) / len(constituents_df)
    else:
        new_highs_lows = 0.0
    
    # Breadth thrust (rapid improvement in breadth)
    # Simplified as acceleration in advance-decline ratio
    breadth_thrust = 0.0  # Would require historical data
    
    return {
        "advance_decline_ratio": float(advance_decline_ratio),
        "percent_above_ma": float(percent_above),
        "breadth_momentum": float(breadth_momentum),
        "new_highs_lows": float(new_highs_lows),
        "breadth_thrust": float(breadth_thrust)
    }

# ==============================
# WEIGHT DISTRIBUTION ANALYSIS
# ==============================

def analyze_weight_distribution(constituents_df: pd.DataFrame) -> Dict[str, float]:
    """
    Analyze weight distribution of index constituents.
    """
    if constituents_df.empty:
        return {
            "top5_concentration": 0.3,
            "herfindahl_index": 0.1,
            "weight_skew": 0.0,
            "weight_gini": 0.5
        }
    
    weights = constituents_df["weight"].values
    
    # Top 5 concentration
    weights_sorted = np.sort(weights)[::-1]
    top5_concentration = np.sum(weights_sorted[:5]) if len(weights_sorted) >= 5 else np.sum(weights_sorted)
    
    # Herfindahl-Hirschman Index (concentration measure)
    herfindahl_index = np.sum(weights ** 2)
    
    # Weight skewness
    if len(weights) >= 3:
        weight_skew = stats.skew(weights)
    else:
        weight_skew = 0.0
    
    # Gini coefficient (inequality measure)
    if len(weights) >= 2:
        sorted_weights = np.sort(weights)
        n = len(sorted_weights)
        cumulative = np.cumsum(sorted_weights)
        gini = (n + 1 - 2 * np.sum(cumulative) / cumulative[-1]) / n
    else:
        gini = 0.5
    
    return {
        "top5_concentration": float(top5_concentration),
        "herfindahl_index": float(herfindahl_index),
        "weight_skew": float(weight_skew),
        "weight_gini": float(gini)
    }

# ==============================
# SECTOR ANALYSIS
# ==============================

def analyze_sector_concentration(constituents_df: pd.DataFrame) -> Dict[str, float]:
    """
    Analyze sector concentration (simplified version).
    In production, this would use actual sector data.
    """
    if constituents_df.empty:
        return {
            "sector_concentration": 0.5,
            "dominant_sector_strength": 0.0,
            "sector_correlation": 0.0
        }
    
    # Simplified: group by weight quartiles
    if len(constituents_df) >= 4:
        quartiles = pd.qcut(constituents_df["weight"], 4, labels=False)
        sector_concentration = 1.0 - len(set(quartiles)) / 4  # Higher = more concentrated
    else:
        sector_concentration = 0.5
    
    # Dominant sector strength (weight of top quartile)
    if len(constituents_df) >= 4:
        top_quartile = constituents_df.nlargest(len(constituents_df) // 4, "weight")
        dominant_sector_strength = top_quartile["weight"].sum()
    else:
        dominant_sector_strength = constituents_df["weight"].max()
    
    # Sector correlation (simplified as correlation of top weights)
    if len(constituents_df) >= 5:
        top_5 = constituents_df.nlargest(5, "weight")
        # Use price changes as proxy for sector performance
        if "price_change" in top_5.columns:
            sector_correlation = top_5["price_change"].std() / (abs(top_5["price_change"].mean()) + 1e-10)
            sector_correlation = min(sector_correlation, 1.0)
        else:
            sector_correlation = 0.5
    else:
        sector_correlation = 0.5
    
    return {
        "sector_concentration": float(sector_concentration),
        "dominant_sector_strength": float(dominant_sector_strength),
        "sector_correlation": float(sector_correlation)
    }

# ==============================
# MOMENTUM BREADTH
# ==============================

def compute_momentum_breadth(constituents_df: pd.DataFrame) -> Dict[str, float]:
    """
    Compute momentum breadth across constituents.
    """
    if constituents_df.empty or "price_change" not in constituents_df.columns:
        return {
            "positive_momentum_ratio": 0.5,
            "momentum_dispersion": 0.0,
            "momentum_trend": 0.0
        }
    
    price_changes = constituents_df["price_change"].values
    weights = constituents_df["weight"].values
    
    # Positive momentum ratio
    positive_momentum_ratio = np.sum(price_changes > 0) / len(price_changes)
    
    # Momentum dispersion (standard deviation of momentum)
    if len(price_changes) >= 2:
        momentum_dispersion = np.std(price_changes)
    else:
        momentum_dispersion = 0.0
    
    # Weighted momentum trend
    if np.sum(weights) > 0:
        weighted_momentum = np.sum(weights * price_changes) / np.sum(weights)
    else:
        weighted_momentum = np.mean(price_changes)
    
    return {
        "positive_momentum_ratio": float(positive_momentum_ratio),
        "momentum_dispersion": float(momentum_dispersion),
        "momentum_trend": float(weighted_momentum)
    }

# ==============================
# DERIVED FEATURES
# ==============================

def compute_breadth_health_score(
    ccc_value: float,
    ccc_slope: float,
    breadth_metrics: Dict[str, float]
) -> float:
    """
    Compute composite breadth health score (0-1).
    """
    scores = []
    
    # CCC value score
    ccc_score = min(abs(ccc_value) * 10, 1.0)
    scores.append(ccc_score * 0.3)
    
    # CCC slope score (positive slope is healthy)
    if ccc_slope > 0:
        slope_score = min(ccc_slope * 100, 1.0)
    else:
        slope_score = max(ccc_slope * 50, -1.0)  # Negative slope penalized
    scores.append(max(slope_score, 0) * 0.2)
    
    # Advance-decline ratio score
    adr = breadth_metrics.get("advance_decline_ratio", 1.0)
    if adr > 1.0:
        adr_score = min((adr - 1.0) * 2, 1.0)  # Cap at 1.0
    else:
        adr_score = max(adr - 0.5, 0) * 2  # Below 0.5 is bad
    scores.append(adr_score * 0.2)
    
    # Percent above MA score
    percent_above = breadth_metrics.get("percent_above_ma", 0.5)
    percent_score = abs(percent_above - 0.5) * 2  # Distance from 50%
    scores.append(percent_score * 0.15)
    
    # Breadth momentum score
    breadth_momentum = breadth_metrics.get("breadth_momentum", 0.0)
    momentum_score = min(abs(breadth_momentum) * 100, 1.0)
    scores.append(momentum_score * 0.15)
    
    return float(np.sum(scores))

def compute_market_participation(constituents_df: pd.DataFrame) -> float:
    """
    Compute market participation score (0-1).
    Higher = broader market participation in moves.
    """
    if constituents_df.empty or "price_change" not in constituents_df.columns:
        return 0.5
    
    price_changes = constituents_df["price_change"].values
    
    if len(price_changes) < 2:
        return 0.5
    
    # Participation = 1 - (fraction of stocks with near-zero changes)
    near_zero = np.sum(np.abs(price_changes) < 0.001) / len(price_changes)
    participation = 1.0 - near_zero
    
    # Adjust for direction consistency
    positive_fraction = np.sum(price_changes > 0) / len(price_changes)
    direction_consistency = max(positive_fraction, 1 - positive_fraction)
    
    participation = participation * direction_consistency
    
    return float(participation)

# ==============================
# DIVERGENCE DETECTION
# ==============================

def detect_breadth_divergence(
    price_change: float,
    breadth_metrics: Dict[str, float],
    historical_breadth: List[Dict]
) -> Tuple[bool, str, float]:
    """
    Detect divergence between price and breadth indicators.
    
    Returns:
        has_divergence, direction, confidence
    """
    if len(historical_breadth) < 5:
        return False, "NEUTRAL", 0.0
    
    # Get recent breadth values
    recent_breadth = historical_breadth[-5:]
    
    # Calculate breadth momentum
    breadth_values = [b.get("advance_decline_ratio", 1.0) for b in recent_breadth]
    breadth_momentum = np.polyfit(range(len(breadth_values)), breadth_values, 1)[0]
    
    # Price momentum (simplified)
    price_momentum = price_change
    
    # Detect divergence
    if price_momentum > 0.01 and breadth_momentum < -0.1:
        # Price up but breadth deteriorating (bearish divergence)
        confidence = min(abs(breadth_momentum) * 10, 1.0)
        return True, "BEARISH", confidence
    
    elif price_momentum < -0.01 and breadth_momentum > 0.1:
        # Price down but breadth improving (bullish divergence)
        confidence = min(abs(breadth_momentum) * 10, 1.0)
        return True, "BULLISH", confidence
    
    return False, "NEUTRAL", 0.0

# ==============================
# UTILITY FUNCTIONS
# ==============================

def calculate_breadth_indicators(constituents_df: pd.DataFrame) -> Dict[str, float]:
    """
    Calculate traditional breadth indicators.
    """
    if constituents_df.empty:
        return {}
    
    indicators = {}
    
    # McClellan Oscillator components
    advances = len(constituents_df[constituents_df["price_change"] > 0])
    declines = len(constituents_df[constituents_df["price_change"] < 0])
    
    indicators["advance_decline_line"] = advances - declines
    indicators["advance_decline_ratio"] = advances / declines if declines > 0 else advances
    
    # Arms Index (TRIN)
    if declines > 0:
        advance_volume = 1  # Simplified
        decline_volume = 1  # Simplified
        indicators["arms_index"] = (advances / declines) / (advance_volume / decline_volume)
    else:
        indicators["arms_index"] = 0.0
    
    # Percent above moving average (simplified)
    avg_change = constituents_df["price_change"].mean()
    indicators["percent_above_average"] = len(
        constituents_df[constituents_df["price_change"] > avg_change]
    ) / len(constituents_df)
    
    return indicators

def get_breadth_alerts(breadth_features: Dict[str, float]) -> List[str]:
    """
    Generate breadth-based alerts.
    """
    alerts = []
    
    # Check CCC value
    ccc_value = breadth_features.get("ccc_value", 0.0)
    if ccc_value > 0.01:
        alerts.append("Strong positive breadth (CCC > 1%)")
    elif ccc_value < -0.01:
        alerts.append("Strong negative breadth (CCC < -1%)")
    
    # Check advance-decline ratio
    adr = breadth_features.get("advance_decline_ratio", 1.0)
    if adr > 2.0:
        alerts.append("Extreme breadth: Advances >> Declines")
    elif adr < 0.5:
        alerts.append("Extreme breadth: Declines >> Advances")
    
    # Check breadth health
    health = breadth_features.get("breadth_health", 0.5)
    if health > 0.8:
        alerts.append("Excellent breadth health")
    elif health < 0.3:
        alerts.append("Poor breadth health")
    
    # Check market participation
    participation = breadth_features.get("market_participation", 0.5)
    if participation > 0.8:
        alerts.append("Broad market participation")
    elif participation < 0.3:
        alerts.append("Narrow market participation")
    
    return alerts[:3]  # Return top 3 alerts


--------------------------------------------------
FILE PATH : G:\trading_app\features\option_features.py
SIZE      : 18677 bytes
--------------------------------------------------

"""
Enhanced Option Features with Research Calculations.
Includes Gamma Exposure, OI Velocity, and structural analysis.
"""

import pandas as pd
import numpy as np
from datetime import datetime
from scipy.stats import norm
from typing import List, Dict, Tuple, Optional, Any, Union

# ==============================
# BLACK-SCHOLES CALCULATIONS
# ==============================

def black_scholes_greeks(
    S: float,           # Spot price
    K: float,           # Strike price
    T: float,           # Time to expiry (years)
    r: float,           # Risk-free rate
    sigma: float,       # Implied volatility
    option_type: str    # 'CE' or 'PE'
) -> Dict[str, float]:
    """
    Calculate Black-Scholes Greeks for options.
    """
    if T <= 0 or sigma <= 0:
        return {
            "delta": 0.5 if option_type == "CE" else -0.5,
            "gamma": 0.0,
            "theta": 0.0,
            "vega": 0.0
        }
    
    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))
    d2 = d1 - sigma * np.sqrt(T)
    
    if option_type == "CE":
        delta = norm.cdf(d1)
        theta = (-S * norm.pdf(d1) * sigma / (2 * np.sqrt(T)) 
                 - r * K * np.exp(-r * T) * norm.cdf(d2))
    else:  # PE
        delta = norm.cdf(d1) - 1
        theta = (-S * norm.pdf(d1) * sigma / (2 * np.sqrt(T))
                 + r * K * np.exp(-r * T) * norm.cdf(-d2))
    
    gamma = norm.pdf(d1) / (S * sigma * np.sqrt(T))
    vega = S * norm.pdf(d1) * np.sqrt(T)
    
    return {
        "delta": float(delta),
        "gamma": float(gamma),
        "theta": float(theta),
        "vega": float(vega)
    }

# ==============================
# OPTION CHAIN FEATURES
# ==============================

def compute_option_features(
    option_chain_df: pd.DataFrame,
    spot_price: float,
    expiry_datetime: datetime
) -> Dict[str, float]:
    """
    Compute option chain features including research metrics.
    """
    if option_chain_df.empty:
        return get_default_features()
    
    # Base features
    put_call_ratio = compute_put_call_ratio(option_chain_df)
    oi_delta = compute_oi_delta(option_chain_df)
    oi_concentration = compute_oi_concentration(option_chain_df)
    atm_iv = compute_atm_iv(option_chain_df, spot_price)
    iv_skew = compute_iv_skew(option_chain_df, spot_price)
    
    # Advanced features
    gamma_profile = compute_gamma_profile(option_chain_df, spot_price, expiry_datetime)
    oi_velocity = compute_oi_velocity(option_chain_df)
    max_pain = compute_max_pain(option_chain_df)
    vix_smile = compute_vix_smile(option_chain_df, spot_price)
    
    # Combine all features
    features = {
        # Base features
        "put_call_ratio": put_call_ratio,
        "oi_delta": oi_delta,
        "oi_concentration": oi_concentration,
        "atm_iv": atm_iv,
        "iv_skew": iv_skew,
        
        # Time feature
        "time_to_expiry_minutes": compute_time_to_expiry_minutes(expiry_datetime),
        
        # Research features
        "net_gamma": gamma_profile["net_gamma"],
        "gamma_skew": gamma_profile["skew"],
        "max_gamma_strike": gamma_profile["max_strike"],
        "oi_velocity": oi_velocity,
        "max_pain_strike": max_pain,
        "vix_smile": vix_smile,
        
        # Volume and OI ratios
        "call_oi_ratio": compute_call_oi_ratio(option_chain_df),
        "put_oi_ratio": compute_put_oi_ratio(option_chain_df),
        "volume_put_call_ratio": compute_volume_pcr(option_chain_df)
    }
    
    return features

# ==============================
# BASE FEATURE CALCULATIONS
# ==============================

def compute_put_call_ratio(option_chain_df: pd.DataFrame) -> float:
    """Calculate Put-Call Ratio (OI-based)."""
    if option_chain_df.empty:
        return 1.0
    
    put_oi = option_chain_df.loc[
        option_chain_df["option_type"] == "PE", "oi"
    ].sum()
    
    call_oi = option_chain_df.loc[
        option_chain_df["option_type"] == "CE", "oi"
    ].sum()
    
    if call_oi == 0:
        return 0.0
    
    return float(put_oi / call_oi)

def compute_oi_delta(option_chain_df: pd.DataFrame) -> float:
    """Calculate OI Delta (Put OI change - Call OI change)."""
    if option_chain_df.empty:
        return 0.0
    
    put_delta = option_chain_df.loc[
        option_chain_df["option_type"] == "PE", "oi_change"
    ].sum()
    
    call_delta = option_chain_df.loc[
        option_chain_df["option_type"] == "CE", "oi_change"
    ].sum()
    
    return float(put_delta - call_delta)

def compute_oi_concentration(option_chain_df: pd.DataFrame) -> float:
    """Calculate OI concentration at maximum OI strike."""
    if option_chain_df.empty:
        return 0.0
    
    total_oi = option_chain_df["oi"].sum()
    if total_oi == 0:
        return 0.0
    
    max_strike_oi = (
        option_chain_df
        .groupby("strike")["oi"]
        .sum()
        .max()
    )
    
    return float(max_strike_oi / total_oi)

def compute_atm_iv(option_chain_df: pd.DataFrame, spot_price: float) -> float:
    """Calculate At-The-Money Implied Volatility."""
    if option_chain_df.empty:
        return 0.3  # Default IV
    
    option_chain_df = option_chain_df.copy()
    option_chain_df["dist"] = abs(option_chain_df["strike"] - spot_price)
    
    atm_row = option_chain_df.sort_values("dist").iloc[0]
    iv = atm_row["iv"]
    
    return float(iv) if not np.isnan(iv) else 0.3

def compute_iv_skew(option_chain_df: pd.DataFrame, spot_price: float) -> float:
    """Calculate IV Skew (Call IV - Put IV)."""
    if option_chain_df.empty:
        return 0.0
    
    option_chain_df = option_chain_df.copy()
    option_chain_df["dist"] = abs(option_chain_df["strike"] - spot_price)
    
    # Get nearest strikes
    nearest_strikes = option_chain_df.sort_values("dist").head(10)
    
    ce_iv = nearest_strikes.loc[
        nearest_strikes["option_type"] == "CE", "iv"
    ].mean()
    
    pe_iv = nearest_strikes.loc[
        nearest_strikes["option_type"] == "PE", "iv"
    ].mean()
    
    if np.isnan(ce_iv) or np.isnan(pe_iv):
        return 0.0
    
    return float(ce_iv - pe_iv)

def compute_time_to_expiry_minutes(expiry_datetime: datetime) -> int:
    """Calculate minutes to expiry."""
    now = datetime.utcnow()
    delta = expiry_datetime - now
    
    minutes = int(delta.total_seconds() / 60)
    return max(minutes, 0)

# ==============================
# RESEARCH FEATURE CALCULATIONS
# ==============================

def compute_gamma_profile(
    option_chain_df: pd.DataFrame,
    spot_price: float,
    expiry_datetime: datetime
) -> Dict[str, float]:
    """
    Compute Gamma Exposure profile for research.
    """
    if option_chain_df.empty:
        return {
            "net_gamma": 0.0,
            "positive_gamma": 0.0,
            "negative_gamma": 0.0,
            "skew": 0.0,
            "max_strike": 0.0
        }
    
    
    # Calculate time to expiry in years
    now = datetime.utcnow()
    T = max((expiry_datetime - now).total_seconds() / (365 * 24 * 3600), 0.001)
    r = 0.05  # Risk-free rate approximation
    
    total_gamma = 0.0
    positive_gamma = 0.0
    negative_gamma = 0.0
    gamma_strikes = []
    
    for _, row in option_chain_df.iterrows():
        strike = row['strike']
        option_type = row['option_type']
        oi = row['oi']
        iv = row.get('iv', 0.3)
        
        # Calculate gamma for this strike
        greeks = black_scholes_greeks(
            S=spot_price,
            K=strike,
            T=T,
            r=r,
            sigma=iv,
            option_type=option_type
        )
        
        gamma = greeks["gamma"]
        
        # Adjust sign based on market maker position
        # Market makers are typically short options
        if option_type == "CE":
            gamma_contribution = -gamma * oi * 100  # Negative for short calls
        else:  # PE
            gamma_contribution = gamma * oi * 100   # Positive for short puts
        
        total_gamma += gamma_contribution
        
        if gamma_contribution > 0:
            positive_gamma += gamma_contribution
        else:
            negative_gamma += gamma_contribution
        
        gamma_strikes.append((strike, gamma_contribution))
    
    # Calculate gamma skew and max_strike
    if gamma_strikes:
        strikes, gammas = zip(*gamma_strikes)
        
        # Calculate skew
        if len(gammas) >= 10:
            gamma_skew = np.mean(gammas[:5]) - np.mean(gammas[-5:])
        else:
            gamma_skew = 0.0
        
        # Find max gamma strike safely
        if len(strikes) > 0 and len(gammas) > 0:
            max_idx = np.argmax(np.abs(gammas))
            max_gamma_strike = strikes[max_idx]
            # Ensure it's not None
            if max_gamma_strike is None or pd.isnull(max_gamma_strike):
                max_gamma_strike = 0.0
        else:
            max_gamma_strike = 0.0
            gamma_skew = 0.0
    else:
        gamma_skew = 0.0
        max_gamma_strike = 0.0
    
    return {
        "net_gamma": float(total_gamma),
        "positive_gamma": float(positive_gamma),
        "negative_gamma": float(negative_gamma),
        "skew": float(gamma_skew),
        "max_strike": float(max_gamma_strike)
    }

def compute_oi_velocity(option_chain_df: pd.DataFrame) -> float:
    """
    Calculate OI Velocity (rate of change of OI).
    In production, this would use historical data.
    """
    if option_chain_df.empty:
        return 0.0
    
    # Simplified velocity calculation
    # In real implementation, compare with previous snapshot
    total_oi = option_chain_df["oi"].sum()
    oi_change = option_chain_df["oi_change"].sum()
    
    if total_oi == 0:
        return 0.0
    
    velocity = oi_change / total_oi * 100
    return float(velocity)

def compute_max_pain(option_chain_df: pd.DataFrame) -> float:
    """Calculate Max Pain strike."""
    if option_chain_df.empty:
        return 0.0
    
    # Check if we have strikes
    if 'strike' not in option_chain_df.columns:
        return 0.0
    
    strikes = sorted(option_chain_df['strike'].unique())
    if not strikes:
        return 0.0
    
    pain_values = []
    
    for strike in strikes:
        total_pain = 0
        
        # Calculate pain from puts
        puts = option_chain_df[(option_chain_df['strike'] == strike) & 
                              (option_chain_df['option_type'] == 'PE')]
        for _, put in puts.iterrows():
            # Check if put is ITM (strike > spot for put pain calculation)
            # Actually for max pain, we calculate loss for option writers
            # For puts: writers lose when spot < strike
            put_strike = put['strike']
            if isinstance(put_strike, (int, float)):
                if strike < put_strike:  # ITM puts cause pain
                    put_oi = put['oi'] if pd.notnull(put['oi']) else 0
                    total_pain += (put_strike - strike) * put_oi
        
        # Calculate pain from calls
        calls = option_chain_df[(option_chain_df['strike'] == strike) & 
                               (option_chain_df['option_type'] == 'CE')]
        for _, call in calls.iterrows():
            # For calls: writers lose when spot > strike
            call_strike = call['strike']
            if isinstance(call_strike, (int, float)):
                if strike > call_strike:  # ITM calls cause pain
                    call_oi = call['oi'] if pd.notnull(call['oi']) else 0
                    total_pain += (strike - call_strike) * call_oi
        
        pain_values.append((strike, total_pain))
    
    if pain_values:
        # Find strike with minimum total pain
        min_pain_item = min(pain_values, key=lambda x: x[1])
        max_pain_strike = min_pain_item[0]
        
        # Ensure it's a valid number
        if max_pain_strike is None or pd.isnull(max_pain_strike):
            return 0.0
        return float(max_pain_strike)
    
    return 0.0

def compute_vix_smile(option_chain_df: pd.DataFrame, spot_price: float) -> float:
    """Calculate VIX smile curvature."""
    if option_chain_df.empty or spot_price <= 0:
        return 0.0
    
    # Group by distance from spot
    option_chain_df = option_chain_df.copy()
    option_chain_df['distance_pct'] = abs(option_chain_df['strike'] - spot_price) / spot_price * 100
    
    # Get IVs at different distances
    atm_iv = option_chain_df[
        option_chain_df['distance_pct'] < 2
    ]['iv'].mean()
    
    otm_iv = option_chain_df[
        (option_chain_df['distance_pct'] >= 5) & 
        (option_chain_df['distance_pct'] < 10)
    ]['iv'].mean()
    
    if np.isnan(atm_iv) or np.isnan(otm_iv):
        return 0.0
    
    # Smile = OTM IV - ATM IV (positive = smile, negative = smirk)
    return float(otm_iv - atm_iv)

# ==============================
# SUPPORTING CALCULATIONS
# ==============================

def compute_call_oi_ratio(option_chain_df: pd.DataFrame) -> float:
    """Calculate Call OI as percentage of total OI."""
    if option_chain_df.empty:
        return 0.5
    
    total_oi = option_chain_df["oi"].sum()
    call_oi = option_chain_df[option_chain_df["option_type"] == "CE"]["oi"].sum()
    
    if total_oi == 0:
        return 0.5
    
    return float(call_oi / total_oi)

def compute_put_oi_ratio(option_chain_df: pd.DataFrame) -> float:
    """Calculate Put OI as percentage of total OI."""
    if option_chain_df.empty:
        return 0.5
    
    total_oi = option_chain_df["oi"].sum()
    put_oi = option_chain_df[option_chain_df["option_type"] == "PE"]["oi"].sum()
    
    if total_oi == 0:
        return 0.5
    
    return float(put_oi / total_oi)

def compute_volume_pcr(option_chain_df: pd.DataFrame) -> float:
    """Calculate Volume-based Put-Call Ratio."""
    if option_chain_df.empty:
        return 1.0
    
    put_volume = option_chain_df.loc[
        option_chain_df["option_type"] == "PE", "volume"
    ].sum()
    
    call_volume = option_chain_df.loc[
        option_chain_df["option_type"] == "CE", "volume"
    ].sum()
    
    if call_volume == 0:
        return 0.0
    
    return float(put_volume / call_volume)

def get_default_features() -> Dict[str, float]:
    """Return default feature values when no data is available."""
    return {
        "put_call_ratio": 1.0,
        "oi_delta": 0.0,
        "oi_concentration": 0.0,
        "atm_iv": 0.3,
        "iv_skew": 0.0,
        "time_to_expiry_minutes": 0,
        "net_gamma": 0.0,
        "gamma_skew": 0.0,
        "max_gamma_strike": 0.0,
        "oi_velocity": 0.0,
        "max_pain_strike": 0.0,
        "vix_smile": 0.0,
        "call_oi_ratio": 0.5,
        "put_oi_ratio": 0.5,
        "volume_put_call_ratio": 1.0
    }

# ==============================
# UTILITY FUNCTIONS
# ==============================

def analyze_option_skew(option_chain_df: pd.DataFrame, spot_price: float) -> Dict[str, float]:
    """
    Analyze option skew across strikes.
    """
    if option_chain_df.empty:
        return {
            "call_skew": 0.0,
            "put_skew": 0.0,
            "total_skew": 0.0
        }
    
    # Separate calls and puts
    calls = option_chain_df[option_chain_df["option_type"] == "CE"].copy()
    puts = option_chain_df[option_chain_df["option_type"] == "PE"].copy()
    
    # Calculate distance from spot
    calls["distance_pct"] = (calls["strike"] - spot_price) / spot_price * 100
    puts["distance_pct"] = (spot_price - puts["strike"]) / spot_price * 100
    
    # Calculate skew (slope of IV vs distance)
    call_skew = 0.0
    put_skew = 0.0
    
    if len(calls) >= 5:
        # Sort by distance and take top 5 OTM calls
        otm_calls = calls[calls["distance_pct"] > 0].nlargest(5, "distance_pct")
        if len(otm_calls) >= 2:
            call_skew = np.polyfit(otm_calls["distance_pct"], otm_calls["iv"], 1)[0]
    
    if len(puts) >= 5:
        # Sort by distance and take top 5 OTM puts
        otm_puts = puts[puts["distance_pct"] > 0].nlargest(5, "distance_pct")
        if len(otm_puts) >= 2:
            put_skew = np.polyfit(otm_puts["distance_pct"], otm_puts["iv"], 1)[0]
    
    return {
        "call_skew": float(call_skew),
        "put_skew": float(put_skew),
        "total_skew": float(call_skew + put_skew)
    }

def detect_gamma_flip_levels(
    option_chain_df: pd.DataFrame,
    spot_price: float,
    expiry_datetime: datetime
) -> List[float]:
    """
    Detect price levels where gamma flips sign.
    """
    if option_chain_df.empty:
        return []
    
    # Calculate gamma at different price levels
    price_levels = np.linspace(spot_price * 0.95, spot_price * 1.05, 21)
    gamma_levels = []
    
    T = max((expiry_datetime - datetime.utcnow()).total_seconds() / (365 * 24 * 3600), 0.001)
    r = 0.05
    
    for price in price_levels:
        total_gamma = 0.0
        
        for _, row in option_chain_df.iterrows():
            strike = row['strike']
            option_type = row['option_type']
            oi = row['oi']
            iv = row.get('iv', 0.3)
            
            greeks = black_scholes_greeks(
                S=price,
                K=strike,
                T=T,
                r=r,
                sigma=iv,
                option_type=option_type
            )
            
            gamma = greeks["gamma"]
            
            if option_type == "CE":
                gamma_contribution = -gamma * oi
            else:
                gamma_contribution = gamma * oi
            
            total_gamma += gamma_contribution
        
        gamma_levels.append((price, total_gamma))
    
    # Find sign changes
    flip_levels = []
    for i in range(1, len(gamma_levels)):
        prev_sign = np.sign(gamma_levels[i-1][1])
        curr_sign = np.sign(gamma_levels[i][1])
        
        if prev_sign != curr_sign and prev_sign != 0 and curr_sign != 0:
            flip_price = (gamma_levels[i-1][0] + gamma_levels[i][0]) / 2
            flip_levels.append(float(flip_price))
    
    return flip_levels


--------------------------------------------------
FILE PATH : G:\trading_app\features\price_features.py
SIZE      : 12441 bytes
--------------------------------------------------

"""
Enhanced Price Features with Research Calculations.
Includes VWAP distance, momentum, volume analysis, and divergence detection.
"""

import pandas as pd
import numpy as np
from typing import Dict, Tuple, List
from scipy import stats

# ==============================
# PRICE FEATURE CALCULATIONS
# ==============================

def compute_price_features(
    ltp: float,
    vwap: float,
    price_series: pd.Series,
    volume_series: pd.Series
) -> Dict[str, float]:
    """
    Compute price-based features including research metrics.
    """
    # Base features
    vwap_distance = compute_vwap_distance(ltp, vwap)
    price_momentum = compute_price_momentum(price_series)
    volume_ratio = compute_volume_ratio(volume_series)
    
    # Advanced features
    trend_strength = compute_trend_strength(price_series)
    volatility = compute_volatility(price_series)
    rsi = compute_rsi(price_series)
    volume_profile = compute_volume_profile(price_series, volume_series)
    price_efficiency = compute_price_efficiency(price_series)
    
    # Divergence detection
    volume_price_divergence = compute_volume_price_divergence(price_series, volume_series)
    
    # Combine all features
    features = {
        # Base features
        "vwap_distance": vwap_distance,
        "price_momentum": price_momentum,
        "volume_ratio": volume_ratio,
        
        # Trend and momentum
        "trend_strength": trend_strength,
        "price_volatility": volatility,
        "rsi": rsi,
        "price_efficiency": price_efficiency,
        
        # Volume analysis
        "volume_trend": volume_profile["trend"],
        "volume_volatility": volume_profile["volatility"],
        "volume_clustering": volume_profile["clustering"],
        
        # Divergence
        "volume_price_divergence": volume_price_divergence,
        "has_volume_divergence": 1.0 if abs(volume_price_divergence) > 0.5 else 0.0
    }
    
    return features

# ==============================
# BASE FEATURE CALCULATIONS
# ==============================

def compute_vwap_distance(ltp: float, vwap: float) -> float:
    """Calculate distance from VWAP."""
    if vwap == 0:
        return 0.0
    
    return float((ltp - vwap) / vwap)

def compute_price_momentum(
    price_series: pd.Series,
    lookback: int = 5
) -> float:
    """Calculate price momentum over lookback period."""
    if len(price_series) <= lookback:
        return 0.0
    
    past_price = price_series.iloc[-lookback - 1]
    current_price = price_series.iloc[-1]
    
    if past_price == 0:
        return 0.0
    
    return float((current_price - past_price) / past_price)

def compute_volume_ratio(
    volume_series: pd.Series,
    lookback: int = 20
) -> float:
    """Calculate current volume relative to average."""
    if len(volume_series) < lookback:
        return 0.0
    
    current_volume = volume_series.iloc[-1]
    avg_volume = volume_series.iloc[-lookback:].mean()
    
    if avg_volume == 0:
        return 0.0
    
    return float(current_volume / avg_volume)

# ==============================
# ADVANCED FEATURE CALCULATIONS
# ==============================

def compute_trend_strength(price_series: pd.Series, lookback: int = 20) -> float:
    """
    Calculate trend strength using linear regression.
    Returns R¬≤ value (0-1) indicating trend strength.
    """
    if len(price_series) < lookback:
        return 0.0
    
    prices = price_series.iloc[-lookback:].values
    x = np.arange(len(prices))
    
    # Linear regression
    slope, intercept, r_value, p_value, std_err = stats.linregress(x, prices)
    
    # R¬≤ value indicates trend strength
    r_squared = r_value ** 2
    
    return float(r_squared)

def compute_volatility(price_series: pd.Series, lookback: int = 20) -> float:
    """Calculate price volatility (annualized)."""
    if len(price_series) <= 1:
        return 0.0
    
    returns = price_series.pct_change().dropna()
    
    if len(returns) < lookback:
        sample_returns = returns
    else:
        sample_returns = returns.iloc[-lookback:]
    
    if len(sample_returns) <= 1:
        return 0.0
    
    # Annualized volatility (assuming daily data)
    daily_vol = sample_returns.std()
    annual_vol = daily_vol * np.sqrt(252)
    
    return float(annual_vol)

def compute_rsi(price_series: pd.Series, period: int = 14) -> float:
    """Calculate Relative Strength Index."""
    if len(price_series) <= period:
        return 50.0  # Neutral
    
    delta = price_series.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
    
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    
    return float(rsi.iloc[-1]) if not pd.isna(rsi.iloc[-1]) else 50.0

def compute_price_efficiency(price_series: pd.Series) -> float:
    """
    Calculate price efficiency (random walk index).
    Higher values indicate more efficient/trending markets.
    """
    if len(price_series) < 10:
        return 0.5
    
    # Calculate Hurst exponent approximation
    n = min(len(price_series), 100)
    lags = range(2, min(n // 2, 20))
    
    if len(lags) < 2:
        return 0.5
    
    tau = []
    for lag in lags:
        # Calculate variance of lagged differences
        price_diff = np.diff(price_series.iloc[-n:], lag)
        if len(price_diff) > 1:
            tau.append(np.std(price_diff))
        else:
            tau.append(0)
    
    tau = np.array(tau)
    lags = np.array(lags)
    
    # Remove zeros
    mask = (tau > 0) & (lags > 0)
    if np.sum(mask) < 2:
        return 0.5
    
    # Linear regression in log space
    try:
        hurst = np.polyfit(np.log(lags[mask]), np.log(tau[mask]), 1)[0]
        efficiency = hurst  # H ‚âà 0.5 random walk, >0.5 trending, <0.5 mean-reverting
    except:
        efficiency = 0.5
    
    return float(efficiency)

# ==============================
# VOLUME ANALYSIS
# ==============================

def compute_volume_profile(
    price_series: pd.Series,
    volume_series: pd.Series,
    lookback: int = 20
) -> Dict[str, float]:
    """
    Compute volume profile features.
    """
    if len(volume_series) < lookback or len(price_series) < lookback:
        return {
            "trend": 0.0,
            "volatility": 0.0,
            "clustering": 0.0
        }
    
    recent_volume = volume_series.iloc[-lookback:]
    recent_prices = price_series.iloc[-lookback:]
    
    # Volume trend
    x = np.arange(len(recent_volume))
    volume_trend = np.polyfit(x, recent_volume.values, 1)[0]
    volume_trend_normalized = volume_trend / (recent_volume.mean() + 1e-10)
    
    # Volume volatility
    volume_volatility = recent_volume.std() / (recent_volume.mean() + 1e-10)
    
    # Volume clustering (autocorrelation)
    if len(recent_volume) >= 5:
        volume_clustering = recent_volume.autocorr(lag=1)
        if pd.isna(volume_clustering):
            volume_clustering = 0.0
    else:
        volume_clustering = 0.0
    
    return {
        "trend": float(volume_trend_normalized),
        "volatility": float(volume_volatility),
        "clustering": float(volume_clustering)
    }

def compute_volume_price_divergence(
    price_series: pd.Series,
    volume_series: pd.Series,
    lookback: int = 10
) -> float:
    """
    Detect divergence between price and volume.
    Positive = price up, volume down (bearish divergence)
    Negative = price down, volume up (bullish divergence)
    """
    if len(price_series) < lookback or len(volume_series) < lookback:
        return 0.0
    
    # Calculate price and volume changes
    price_change = (price_series.iloc[-1] - price_series.iloc[-lookback]) / price_series.iloc[-lookback]
    volume_change = (volume_series.iloc[-1] - volume_series.iloc[-lookback]) / (volume_series.iloc[-lookback] + 1e-10)
    
    # Normalize
    price_norm = price_change / (np.std(price_series.iloc[-lookback:].pct_change().dropna()) + 1e-10)
    volume_norm = volume_change / (np.std(volume_series.iloc[-lookback:].pct_change().dropna()) + 1e-10)
    
    # Divergence score
    divergence = price_norm - volume_norm
    
    return float(divergence)

# ==============================
# SUPPORTING CALCULATIONS
# ==============================

def detect_price_patterns(price_series: pd.Series) -> Dict[str, float]:
    """
    Detect common price patterns.
    """
    if len(price_series) < 20:
        return {
            "double_top": 0.0,
            "double_bottom": 0.0,
            "head_shoulders": 0.0,
            "triangle": 0.0
        }
    
    # Simplified pattern detection
    prices = price_series.iloc[-20:].values
    
    # Calculate peaks and troughs
    from scipy.signal import find_peaks
    
    peaks, _ = find_peaks(prices, prominence=np.std(prices) * 0.5)
    troughs, _ = find_peaks(-prices, prominence=np.std(prices) * 0.5)
    
    pattern_scores = {
        "double_top": 0.0,
        "double_bottom": 0.0,
        "head_shoulders": 0.0,
        "triangle": 0.0
    }
    
    # Double top detection
    if len(peaks) >= 2:
        peak1 = prices[peaks[-2]]
        peak2 = prices[peaks[-1]]
        if abs(peak1 - peak2) / peak1 < 0.02:  # Within 2%
            pattern_scores["double_top"] = 0.8
    
    # Double bottom detection
    if len(troughs) >= 2:
        trough1 = prices[troughs[-2]]
        trough2 = prices[troughs[-1]]
        if abs(trough1 - trough2) / trough1 < 0.02:
            pattern_scores["double_bottom"] = 0.8
    
    return pattern_scores

def calculate_support_resistance(
    price_series: pd.Series,
    window: int = 20
) -> Dict[str, float]:
    """
    Calculate support and resistance levels.
    """
    if len(price_series) < window:
        return {
            "support": 0.0,
            "resistance": 0.0,
            "current_position": 0.5
        }
    
    recent_prices = price_series.iloc[-window:]
    
    support = recent_prices.min()
    resistance = recent_prices.max()
    current = price_series.iloc[-1]
    
    if resistance - support == 0:
        position = 0.5
    else:
        position = (current - support) / (resistance - support)
    
    return {
        "support": float(support),
        "resistance": float(resistance),
        "current_position": float(position)
    }

# ==============================
# DIVERGENCE DETECTION
# ==============================

def detect_momentum_divergence(
    price_series: pd.Series,
    momentum_series: pd.Series
) -> Tuple[bool, str, float]:
    """
    Detect divergence between price and momentum oscillator.
    
    Returns:
        has_divergence, direction, confidence
    """
    if len(price_series) < 10 or len(momentum_series) < 10:
        return False, "NEUTRAL", 0.0
    
    # Get recent peaks in price and momentum
    price_peaks = []
    momentum_peaks = []
    
    # Simplified peak detection
    for i in range(5, len(price_series) - 5):
        if price_series.iloc[i] == price_series.iloc[i-5:i+5].max():
            price_peaks.append((i, price_series.iloc[i]))
        if momentum_series.iloc[i] == momentum_series.iloc[i-5:i+5].max():
            momentum_peaks.append((i, momentum_series.iloc[i]))
    
    if len(price_peaks) < 2 or len(momentum_peaks) < 2:
        return False, "NEUTRAL", 0.0
    
    # Check for divergence
    price_trend = price_peaks[-1][1] - price_peaks[-2][1]
    momentum_trend = momentum_peaks[-1][1] - momentum_peaks[-2][1]
    
    # Bullish divergence: price makes lower low, momentum makes higher low
    # Bearish divergence: price makes higher high, momentum makes lower high
    
    if price_trend > 0 and momentum_trend < 0:
        # Bearish divergence
        confidence = min(abs(momentum_trend) / 10, 1.0)
        return True, "BEARISH", confidence
    
    elif price_trend < 0 and momentum_trend > 0:
        # Bullish divergence
        confidence = min(abs(momentum_trend) / 10, 1.0)
        return True, "BULLISH", confidence
    
    return False, "NEUTRAL", 0.0


--------------------------------------------------
FILE PATH : G:\trading_app\FULL_CODEBASE_DUMP.txt
SIZE      : 569 bytes
--------------------------------------------------



--------------------------------------------------
FILE PATH : G:\trading_app\intelligence\market_state.py
SIZE      : 0 bytes
--------------------------------------------------



--------------------------------------------------
FILE PATH : G:\trading_app\intelligence\signal_logic.py
SIZE      : 0 bytes
--------------------------------------------------



--------------------------------------------------
FILE PATH : G:\trading_app\ml\feature_contract.py
SIZE      : 16686 bytes
--------------------------------------------------

"""
ENHANCED FEATURE CONTRACT - SINGLE SOURCE OF TRUTH
Incorporates research concepts: OI Velocity, Gamma Exposure, Walls/Traps, Spot Divergence

Used by:
- Live data collection
- ML training
- ML inference
- Backtesting
- Signal generation
"""

# ==============================
# FEATURE VERSION
# ==============================

FEATURE_VERSION = "v2.0"  # Updated for research features

# ==============================
# FEATURE CATEGORIES
# ==============================

# Base features (original set)
BASE_FEATURES = [
    "timestamp",
    
    # Option structure (original)
    "put_call_ratio",
    "oi_delta",
    "oi_concentration",
    "atm_iv",
    "iv_skew",
    
    # Price & flow (original)
    "vwap_distance",
    "price_momentum",
    "volume_ratio",
    
    # Breadth (original)
    "ccc_value",
    "ccc_slope",
    
    # Time context
    "time_to_expiry_minutes"
]

# Research features (enhanced)
RESEARCH_FEATURES = [
    # OI Velocity features
    "oi_velocity",
    "oi_velocity_ma",
    "oi_velocity_std",
    "oi_regime_expansive",      # 1.0 if EXPANSIVE, else 0.0
    "oi_regime_constricted",    # 1.0 if CONSTRICTED, else 0.0
    
    # Gamma Exposure features
    "net_gamma",
    "gamma_regime_positive",    # 1.0 if POSITIVE, else 0.0
    "gamma_regime_negative",    # 1.0 if NEGATIVE, else 0.0
    "gamma_flip_distance",      # Distance to nearest gamma flip (normalized)
    "max_gamma_strike_distance",# Distance to max gamma strike (normalized)
    
    # Structural features (Walls & Traps)
    "wall_strength",            # Combined strength of top walls (0-1)
    "wall_defense_score",       # How well walls are defended (0-1)
    "trap_probability",         # Probability of trap formation (0-1)
    
    # Divergence features
    "price_oi_divergence",      # Divergence between price and OI
    "price_gamma_divergence",   # Divergence between price and gamma
    "divergence_score",         # Combined divergence score (0-1)
    "has_divergence",           # 1.0 if significant divergence, else 0.0
    
    # Market microstructure
    "max_pain_distance",        # Distance to max pain (normalized)
    "vix_smile",                # Volatility smile curvature
    "skewness",                 # Option skew (put IV - call IV)
    
    # Wyckoff-inspired features
    "spring_detection",         # Bear trap probability (0-1)
    "upthrust_detection",       # Bull trap probability (0-1)
    "accumulation_score",       # Accumulation phase score (0-1)
    
    # Derived composite features
    "gamma_wall_interaction",   # wall_strength * abs(net_gamma)
    "velocity_divergence_composite",  # oi_velocity * divergence_score
    "trap_gamma_composite"      # trap_probability * gamma_regime_negative
]

# ==============================
# COMPLETE FEATURE SET
# ==============================

# All features (base + research)
FEATURE_COLUMNS = BASE_FEATURES + RESEARCH_FEATURES

# Target variable
TARGET_COLUMN = "future_return_5m"

# Metadata columns
METADATA_COLUMNS = [
    "feature_version",
    "timestamp"
]

# Primary keys for database
PRIMARY_KEYS = ["timestamp"]

# ==============================
# FEATURE GROUPS FOR ANALYSIS
# ==============================

FEATURE_GROUPS = {
    "option_structure": [
        "put_call_ratio",
        "oi_delta", 
        "oi_concentration",
        "atm_iv",
        "iv_skew",
        "skewness",
        "vix_smile"
    ],
    
    "price_momentum": [
        "vwap_distance",
        "price_momentum",
        "volume_ratio",
        "ccc_value",
        "ccc_slope"
    ],
    
    "oi_analysis": [
        "oi_velocity",
        "oi_velocity_ma",
        "oi_velocity_std",
        "oi_regime_expansive",
        "oi_regime_constricted"
    ],
    
    "gamma_exposure": [
        "net_gamma",
        "gamma_regime_positive",
        "gamma_regime_negative",
        "gamma_flip_distance",
        "max_gamma_strike_distance"
    ],
    
    "structure_analysis": [
        "wall_strength",
        "wall_defense_score",
        "trap_probability",
        "spring_detection",
        "upthrust_detection",
        "accumulation_score"
    ],
    
    "divergence_analysis": [
        "price_oi_divergence",
        "price_gamma_divergence",
        "divergence_score",
        "has_divergence"
    ],
    
    "composite_features": [
        "gamma_wall_interaction",
        "velocity_divergence_composite",
        "trap_gamma_composite"
    ],
    
    "context_features": [
        "time_to_expiry_minutes",
        "max_pain_distance"
    ]
}

# ==============================
# FEATURE DESCRIPTIONS
# ==============================

FEATURE_DESCRIPTIONS = {
    # Base features
    "put_call_ratio": "Put OI / Call OI ratio. >1.0 = bearish sentiment",
    "oi_delta": "Net OI change (Put OI change - Call OI change)",
    "oi_concentration": "Maximum OI concentration at any single strike",
    "atm_iv": "At-the-money implied volatility",
    "iv_skew": "Call IV - Put IV (positive = call skew, negative = put skew)",
    "vwap_distance": "(LTP - VWAP) / VWAP. Positive = above VWAP (bullish)",
    "price_momentum": "Normalized price momentum over lookback period",
    "volume_ratio": "Current volume / average volume",
    "ccc_value": "Cumulative Constituent Contribution - breadth indicator",
    "ccc_slope": "Slope of CCC over lookback period",
    "time_to_expiry_minutes": "Minutes remaining until option expiry",
    
    # OI Velocity features
    "oi_velocity": "Normalized rate of change of Open Interest (œÉ)",
    "oi_velocity_ma": "Moving average of OI velocity",
    "oi_velocity_std": "Standard deviation of OI velocity",
    "oi_regime_expansive": "1.0 if OI velocity > 1.5œÉ (capital inflow), else 0.0",
    "oi_regime_constricted": "1.0 if OI velocity < -1.5œÉ (capital outflow), else 0.0",
    
    # Gamma Exposure features
    "net_gamma": "Net Gamma Exposure of market makers",
    "gamma_regime_positive": "1.0 if net_gamma > 0 (stabilizing/pinning), else 0.0",
    "gamma_regime_negative": "1.0 if net_gamma < 0 (accelerating), else 0.0",
    "gamma_flip_distance": "Distance to nearest gamma flip level (normalized)",
    "max_gamma_strike_distance": "Distance to strike with maximum gamma impact",
    
    # Structural features
    "wall_strength": "Combined strength of structural walls (0-1 scale)",
    "wall_defense_score": "How strongly walls are being defended (0-1)",
    "trap_probability": "Probability of trap/squeeze formation (0-1)",
    "spring_detection": "Wyckoff spring pattern detection (0-1)",
    "upthrust_detection": "Wyckoff upthrust pattern detection (0-1)",
    "accumulation_score": "Accumulation phase score (0-1)",
    
    # Divergence features
    "price_oi_divergence": "Divergence between price change and OI change",
    "price_gamma_divergence": "Divergence between price change and gamma change",
    "divergence_score": "Combined divergence confidence (0-1)",
    "has_divergence": "1.0 if significant divergence detected, else 0.0",
    
    # Market microstructure
    "max_pain_distance": "Distance to max pain strike (normalized)",
    "vix_smile": "Volatility smile curvature (ATM IV - OTM IV)",
    "skewness": "Option skew (Put IV - Call IV)",
    
    # Composite features
    "gamma_wall_interaction": "Interaction between gamma and wall strength",
    "velocity_divergence_composite": "OI velocity multiplied by divergence score",
    "trap_gamma_composite": "Trap probability weighted by negative gamma regime"
}

# ==============================
# FEATURE IMPORTANCE GUIDELINES
# ==============================

# Expected impact on returns (for initial model weighting)
FEATURE_IMPACT = {
    "high_impact": [
        "oi_velocity",
        "net_gamma",
        "trap_probability",
        "divergence_score",
        "wall_strength"
    ],
    
    "medium_impact": [
        "put_call_ratio",
        "price_momentum",
        "ccc_slope",
        "gamma_regime_negative",
        "has_divergence",
        "spring_detection"
    ],
    
    "low_impact": [
        "oi_concentration",
        "vwap_distance",
        "volume_ratio",
        "max_pain_distance",
        "skewness"
    ],
    
    "contextual": [
        "time_to_expiry_minutes",
        "atm_iv",
        "vix_smile",
        "accumulation_score"
    ]
}

# ==============================
# FEATURE VALIDATION RULES
# ==============================

FEATURE_VALIDATION = {
    "value_ranges": {
        "put_call_ratio": (0, 5),
        "oi_velocity": (-10, 10),
        "net_gamma": (-1e6, 1e6),
        "trap_probability": (0, 1),
        "divergence_score": (0, 1),
        "wall_strength": (0, 1)
    },
    
    "required_features": [
        "timestamp",
        "feature_version",
        "put_call_ratio",
        "vwap_distance",
        "price_momentum",
        "ccc_value"
    ],
    
    "derived_features": [
        "oi_regime_expansive",
        "oi_regime_constricted",
        "gamma_regime_positive",
        "gamma_regime_negative",
        "has_divergence"
    ]
}

# ==============================
# MODEL INPUT CONFIGURATION
# ==============================

# Features for different model types
MODEL_FEATURE_SETS = {
    "full_model": FEATURE_COLUMNS,
    
    "research_model": RESEARCH_FEATURES,
    
    "momentum_model": [
        "price_momentum",
        "volume_ratio",
        "ccc_slope",
        "oi_velocity",
        "net_gamma"
    ],
    
    "divergence_model": [
        "price_oi_divergence",
        "price_gamma_divergence",
        "divergence_score",
        "has_divergence",
        "trap_probability"
    ],
    
    "structure_model": [
        "wall_strength",
        "wall_defense_score",
        "trap_probability",
        "spring_detection",
        "upthrust_detection"
    ],
    
    "quick_model": [
        "put_call_ratio",
        "oi_velocity",
        "net_gamma",
        "trap_probability",
        "divergence_score"
    ]
}

# ==============================
# UTILITY FUNCTIONS
# ==============================

def get_feature_group(feature_name: str) -> str:
    """Get the group name for a feature."""
    for group_name, features in FEATURE_GROUPS.items():
        if feature_name in features:
            return group_name
    return "unknown"

def validate_feature_name(feature_name: str) -> bool:
    """Check if a feature name is valid."""
    return feature_name in FEATURE_COLUMNS or feature_name in METADATA_COLUMNS

def get_feature_description(feature_name: str) -> str:
    """Get description for a feature."""
    return FEATURE_DESCRIPTIONS.get(feature_name, "No description available")

def get_features_by_impact(impact_level: str) -> list:
    """Get features by impact level."""
    return FEATURE_IMPACT.get(impact_level, [])

def get_model_features(model_type: str = "full_model") -> list:
    """Get feature set for specific model type."""
    return MODEL_FEATURE_SETS.get(model_type, FEATURE_COLUMNS)

def print_feature_summary():
    """Print summary of all features."""
    print(f"=== FEATURE CONTRACT v{FEATURE_VERSION} ===")
    print(f"Total features: {len(FEATURE_COLUMNS)}")
    print(f"Base features: {len(BASE_FEATURES)}")
    print(f"Research features: {len(RESEARCH_FEATURES)}")
    print()
    
    print("Feature Groups:")
    for group_name, features in FEATURE_GROUPS.items():
        print(f"  {group_name}: {len(features)} features")
    
    print()
    print("Top Impact Features:")
    for feature in FEATURE_IMPACT["high_impact"][:5]:
        print(f"  ‚Ä¢ {feature}: {FEATURE_DESCRIPTIONS.get(feature, '')}")

# ==============================
# DATABASE SCHEMA
# ==============================

# Schema for market_features table
MARKET_FEATURES_SCHEMA = {
    "table_name": "market_features",
    "columns": {
        "timestamp": "TEXT PRIMARY KEY",
        "feature_version": "TEXT",
        "future_return_5m": "REAL",
        
        # Base features
        "put_call_ratio": "REAL",
        "oi_delta": "REAL",
        "oi_concentration": "REAL",
        "atm_iv": "REAL",
        "iv_skew": "REAL",
        "vwap_distance": "REAL",
        "price_momentum": "REAL",
        "volume_ratio": "REAL",
        "ccc_value": "REAL",
        "ccc_slope": "REAL",
        "time_to_expiry_minutes": "INTEGER",
        
        # Research features (added in v2.0)
        "oi_velocity": "REAL",
        "oi_velocity_ma": "REAL",
        "oi_velocity_std": "REAL",
        "oi_regime_expansive": "REAL",
        "oi_regime_constricted": "REAL",
        "net_gamma": "REAL",
        "gamma_regime_positive": "REAL",
        "gamma_regime_negative": "REAL",
        "gamma_flip_distance": "REAL",
        "max_gamma_strike_distance": "REAL",
        "wall_strength": "REAL",
        "wall_defense_score": "REAL",
        "trap_probability": "REAL",
        "price_oi_divergence": "REAL",
        "price_gamma_divergence": "REAL",
        "divergence_score": "REAL",
        "has_divergence": "REAL",
        "max_pain_distance": "REAL",
        "vix_smile": "REAL",
        "skewness": "REAL",
        "spring_detection": "REAL",
        "upthrust_detection": "REAL",
        "accumulation_score": "REAL",
        "gamma_wall_interaction": "REAL",
        "velocity_divergence_composite": "REAL",
        "trap_gamma_composite": "REAL"
    },
    "indexes": [
        "CREATE INDEX idx_timestamp ON market_features(timestamp)",
        "CREATE INDEX idx_feature_version ON market_features(feature_version)",
        "CREATE INDEX idx_oi_velocity ON market_features(oi_velocity)",
        "CREATE INDEX idx_net_gamma ON market_features(net_gamma)"
    ]
}

# ==============================
# MIGRATION UTILITIES
# ==============================

def get_migration_sql(from_version: str, to_version: str) -> list:
    """
    Get SQL migration statements for feature version updates.
    
    Args:
        from_version: Current feature version
        to_version: Target feature version
    
    Returns:
        List of SQL statements to migrate
    """
    migrations = []
    
    if from_version == "v1.0" and to_version == "v2.0":
        # Add research feature columns
        research_columns = [
            "oi_velocity REAL DEFAULT 0.0",
            "oi_velocity_ma REAL DEFAULT 0.0",
            "oi_velocity_std REAL DEFAULT 0.0",
            "oi_regime_expansive REAL DEFAULT 0.0",
            "oi_regime_constricted REAL DEFAULT 0.0",
            "net_gamma REAL DEFAULT 0.0",
            "gamma_regime_positive REAL DEFAULT 0.0",
            "gamma_regime_negative REAL DEFAULT 0.0",
            "gamma_flip_distance REAL DEFAULT 0.0",
            "max_gamma_strike_distance REAL DEFAULT 0.0",
            "wall_strength REAL DEFAULT 0.0",
            "wall_defense_score REAL DEFAULT 0.0",
            "trap_probability REAL DEFAULT 0.0",
            "price_oi_divergence REAL DEFAULT 0.0",
            "price_gamma_divergence REAL DEFAULT 0.0",
            "divergence_score REAL DEFAULT 0.0",
            "has_divergence REAL DEFAULT 0.0",
            "max_pain_distance REAL DEFAULT 0.0",
            "vix_smile REAL DEFAULT 0.0",
            "skewness REAL DEFAULT 0.0",
            "spring_detection REAL DEFAULT 0.0",
            "upthrust_detection REAL DEFAULT 0.0",
            "accumulation_score REAL DEFAULT 0.0",
            "gamma_wall_interaction REAL DEFAULT 0.0",
            "velocity_divergence_composite REAL DEFAULT 0.0",
            "trap_gamma_composite REAL DEFAULT 0.0"
        ]
        
        for column_def in research_columns:
            column_name = column_def.split()[0]
            migrations.append(f"ALTER TABLE market_features ADD COLUMN {column_def}")
        
        # Add indexes for new features
        migrations.append("CREATE INDEX idx_oi_velocity ON market_features(oi_velocity)")
        migrations.append("CREATE INDEX idx_net_gamma ON market_features(net_gamma)")
        migrations.append("CREATE INDEX idx_trap_probability ON market_features(trap_probability)")
    
    return migrations

# ==============================
# INITIALIZATION
# ==============================

if __name__ == "__main__":
    print_feature_summary()


--------------------------------------------------
FILE PATH : G:\trading_app\ml\inference.py
SIZE      : 0 bytes
--------------------------------------------------



--------------------------------------------------
FILE PATH : G:\trading_app\ml\training.py
SIZE      : 0 bytes
--------------------------------------------------



--------------------------------------------------
FILE PATH : G:\trading_app\risk\position_sizing.py
SIZE      : 0 bytes
--------------------------------------------------



--------------------------------------------------
FILE PATH : G:\trading_app\risk\stoploss.py
SIZE      : 0 bytes
--------------------------------------------------



--------------------------------------------------
FILE PATH : G:\trading_app\scripts\fix_database.py
SIZE      : 4814 bytes
--------------------------------------------------

"""
Database Fix Script - Run this to fix migration issues.
"""

import sys
from pathlib import Path
import sqlite3

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))


# Add this function to scripts/fix_database.py or create a new script

def add_details_json_column():
    """Add missing details_json column to system_health table."""
    try:
        with get_connection() as conn:
            # Check if column exists
            cur = conn.execute("PRAGMA table_info(system_health)")
            columns = [row[1] for row in cur.fetchall()]
            
            if "details_json" not in columns:
                print("Adding details_json column to system_health table...")
                conn.execute("""
                    ALTER TABLE system_health 
                    ADD COLUMN details_json TEXT DEFAULT '{}'
                """)
                print("‚úì Added details_json column")
            else:
                print("‚úì details_json column already exists")
    except Exception as e:
        print(f"‚ùå Error adding details_json column: {e}")
        
def fix_database():
    """Fix database migration issues."""
    print("=" * 60)
    print("DATABASE FIX SCRIPT")
    print("=" * 60)
    
    db_path = Path("G:/trading_app/storage/trading.db")
    
    if not db_path.exists():
        print("Database file not found. Creating new database...")
        from storage.repository import initialize_storage
        initialize_storage()
        return
    
    print(f"Fixing database at: {db_path}")
    
    # Connect to database
    conn = sqlite3.connect(str(db_path))
    
    try:
        # Check if market_features table exists
        cur = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='market_features'")
        if not cur.fetchone():
            print("market_features table doesn't exist. Creating...")
            conn.close()
            from storage.repository import initialize_storage
            initialize_storage()
            return
        
        # Get current columns
        cur = conn.execute("PRAGMA table_info(market_features)")
        columns = [row[1] for row in cur.fetchall()]
        print(f"Current columns in market_features: {columns}")
        
        # Research columns that should exist
        research_columns = [
            "oi_velocity", "oi_velocity_ma", "oi_velocity_std",
            "oi_regime_expansive", "oi_regime_constricted",
            "net_gamma", "gamma_regime_positive", "gamma_regime_negative",
            "gamma_flip_distance", "max_gamma_strike_distance",
            "wall_strength", "wall_defense_score", "trap_probability",
            "price_oi_divergence", "price_gamma_divergence", "divergence_score",
            "has_divergence", "max_pain_distance", "vix_smile", "skewness",
            "spring_detection", "upthrust_detection", "accumulation_score",
            "gamma_wall_interaction", "velocity_divergence_composite", "trap_gamma_composite"
        ]
        
        # Add missing columns
        for column in research_columns:
            if column not in columns:
                print(f"Adding column: {column}")
                try:
                    conn.execute(f"ALTER TABLE market_features ADD COLUMN {column} REAL DEFAULT 0.0")
                    print(f"  ‚úì Added {column}")
                except sqlite3.OperationalError as e:
                    if "duplicate column" in str(e):
                        print(f"  ‚ö†Ô∏è Column {column} already exists")
                    else:
                        print(f"  ‚ùå Error adding {column}: {e}")
        
        # Check database_info table
        cur = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='database_info'")
        if not cur.fetchone():
            print("Creating database_info table...")
            conn.execute("""
                CREATE TABLE database_info (
                    key TEXT PRIMARY KEY,
                    value TEXT,
                    updated_at TEXT
                )
            """)
        
        # Set feature version to v2.0
        from datetime import datetime
        conn.execute(
            "INSERT OR REPLACE INTO database_info (key, value, updated_at) VALUES (?, ?, ?)",
            ("feature_version", "v2.0", datetime.utcnow().isoformat())
        )
        
        conn.commit()
        print("‚úì Database fixed successfully!")
        
    except Exception as e:
        print(f"‚ùå Error fixing database: {e}")
        import traceback
        traceback.print_exc()
    finally:
        conn.close()

if __name__ == "__main__":
    fix_database()


--------------------------------------------------
FILE PATH : G:\trading_app\scripts\initialize_database.py
SIZE      : 1361 bytes
--------------------------------------------------

"""
Database Initialization Script.
Run this once to set up the database with research features.
"""

import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from storage.repository import initialize_database, migrate_database
from ml.feature_contract import FEATURE_VERSION

def main():
    """Initialize and migrate database."""
    print("=" * 60)
    print("DATABASE INITIALIZATION SCRIPT")
    print("=" * 60)
    
    try:
        # Initialize database
        print("\n1. Initializing database...")
        initialize_database()
        
        # Migrate to current feature version
        print(f"\n2. Migrating to feature version {FEATURE_VERSION}...")
        success = migrate_database(FEATURE_VERSION)
        
        if success:
            print(f"\n‚úÖ Database initialized successfully!")
            print(f"   Feature Version: {FEATURE_VERSION}")
            print(f"   Database Path: storage/trading.db")
        else:
            print("\n‚ùå Database migration failed!")
            sys.exit(1)
            
    except Exception as e:
        print(f"\n‚ùå Error during initialization: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()


--------------------------------------------------
FILE PATH : G:\trading_app\storage\repository - Copy.py
SIZE      : 38870 bytes
--------------------------------------------------

"""
Enhanced Storage Repository with Research Features Support.
Handles database operations for market features, signals, and analytics.
Includes migration system for feature version updates.
"""

import sqlite3
import pandas as pd
from typing import Dict, Optional, List, Any, Tuple
from pathlib import Path
from datetime import datetime, timedelta
import json
import numpy as np
from contextlib import contextmanager

# Import feature contract for schema
from ml.feature_contract import (
    FEATURE_VERSION, 
    FEATURE_COLUMNS, 
    METADATA_COLUMNS,
    MARKET_FEATURES_SCHEMA,
    get_migration_sql
)

# =========================
# DATABASE CONFIG
# =========================

DB_PATH = Path("G:/trading_app/storage/trading.db")

# Ensure directory exists
DB_PATH.parent.mkdir(parents=True, exist_ok=True)

# =========================
# CONNECTION MANAGEMENT
# =========================

@contextmanager
def get_connection() -> sqlite3.Connection:
    """
    Context manager for database connections with automatic cleanup.
    """
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    
    # Enable WAL mode for better concurrency
    conn.execute("PRAGMA journal_mode=WAL")
    conn.execute("PRAGMA synchronous=NORMAL")
    conn.execute("PRAGMA foreign_keys=ON")
    
    try:
        yield conn
        conn.commit()
    except Exception as e:
        conn.rollback()
        raise e
    finally:
        conn.close()

def initialize_database():
    """
    Initialize database with all required tables and indexes.
    """
    with get_connection() as conn:
        # Create market_features table
        columns = MARKET_FEATURES_SCHEMA["columns"]
        column_defs = [f"{name} {dtype}" for name, dtype in columns.items()]
        
        create_table_sql = f"""
            CREATE TABLE IF NOT EXISTS market_features (
                {', '.join(column_defs)}
            )
        """
        
        conn.execute(create_table_sql)
        
        # Create indexes
        for index_sql in MARKET_FEATURES_SCHEMA.get("indexes", []):
            try:
                conn.execute(index_sql)
            except sqlite3.OperationalError as e:
                if "already exists" not in str(e):
                    raise
        
        # Create signals table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS signals (
                signal_id TEXT PRIMARY KEY,
                timestamp TEXT NOT NULL,
                feature_version TEXT NOT NULL,
                model_version TEXT NOT NULL,
                signal_type TEXT NOT NULL,
                confidence REAL,
                market_state TEXT,
                rationale TEXT,
                expiry_time TEXT,
                status TEXT,
                created_at TEXT,
                research_context TEXT,
                analytics_summary TEXT,
                pnl REAL DEFAULT NULL,
                exit_time TEXT,
                exit_price REAL,
                stop_loss_hit INTEGER DEFAULT 0,
                take_profit_hit INTEGER DEFAULT 0,
                trade_duration_seconds INTEGER
            )
        """)
        
        # Create signals indexes
        conn.execute("CREATE INDEX IF NOT EXISTS idx_signals_timestamp ON signals(timestamp)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_signals_status ON signals(status)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_signals_type ON signals(signal_type)")
        
        # Create model_registry table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS model_registry (
                model_version TEXT PRIMARY KEY,
                feature_version TEXT NOT NULL,
                algorithm TEXT NOT NULL,
                trained_on_start TEXT,
                trained_on_end TEXT,
                metrics_json TEXT,
                feature_importance_json TEXT,
                created_at TEXT,
                is_active INTEGER DEFAULT 0
            )
        """)
        
        # Create system_health table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS system_health (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                component TEXT NOT NULL,
                status TEXT NOT NULL,
                message TEXT,
                details_json TEXT
            )
        """)
        
        # Create research_analytics table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS research_analytics (
                timestamp TEXT PRIMARY KEY,
                oi_velocity REAL,
                oi_regime TEXT,
                net_gamma REAL,
                gamma_regime TEXT,
                wall_strength REAL,
                trap_probability REAL,
                divergence_score REAL,
                market_regime TEXT,
                confidence REAL,
                insights_json TEXT,
                walls_json TEXT,
                traps_json TEXT,
                created_at TEXT
            )
        """)
        
        # Create analytics indexes
        conn.execute("CREATE INDEX IF NOT EXISTS idx_analytics_timestamp ON research_analytics(timestamp)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_analytics_regime ON research_analytics(market_regime)")
        
        # Create feature_history table for tracking feature changes
        conn.execute("""
            CREATE TABLE IF NOT EXISTS feature_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                feature_name TEXT NOT NULL,
                old_value REAL,
                new_value REAL,
                change_pct REAL,
                created_at TEXT
            )
        """)
        
        # Create database_info table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS database_info (
                key TEXT PRIMARY KEY,
                value TEXT,
                updated_at TEXT
            )
        """)
        
        # Initialize database info
        conn.execute(
            "INSERT OR REPLACE INTO database_info (key, value, updated_at) VALUES (?, ?, ?)",
            ("feature_version", FEATURE_VERSION, datetime.utcnow().isoformat())
        )
        
        # Log initialization
        log_system_health(
            "database",
            "INITIALIZED",
            f"Database initialized with feature version {FEATURE_VERSION}"
        )
        
        print(f"‚úì Database initialized at {DB_PATH}")
        print(f"‚úì Feature version: {FEATURE_VERSION}")

# =========================
# DATABASE MIGRATION
# =========================

def migrate_database(target_version: str = FEATURE_VERSION) -> bool:
    """
    Migrate database to target feature version.
    
    Args:
        target_version: Target feature version
    
    Returns:
        True if migration successful
    """
    try:
        with get_connection() as conn:
            # Get current version
            cur = conn.execute("SELECT value FROM database_info WHERE key = 'feature_version'")
            row = cur.fetchone()
            current_version = row["value"] if row else "v1.0"
            
            if current_version == target_version:
                print(f"Database already at version {target_version}")
                return True
            
            print(f"Migrating database from {current_version} to {target_version}")
            
            # Get migration SQL
            migrations = get_migration_sql(current_version, target_version)
            
            if not migrations:
                print("No migration required")
                return True
            
            # Execute migrations
            for migration_sql in migrations:
                try:
                    conn.execute(migration_sql)
                    print(f"‚úì Executed: {migration_sql[:50]}...")
                except sqlite3.OperationalError as e:
                    if "already exists" in str(e) or "duplicate column" in str(e):
                        print(f"‚ö†Ô∏è Skipping (already exists): {migration_sql[:50]}...")
                    else:
                        raise
            
            # Update version
            conn.execute(
                "UPDATE database_info SET value = ?, updated_at = ? WHERE key = 'feature_version'",
                (target_version, datetime.utcnow().isoformat())
            )
            
            # Log migration
            log_system_health(
                "database",
                "MIGRATED",
                f"Migrated from {current_version} to {target_version}",
                json.dumps({"migrations_applied": len(migrations)})
            )
            
            print(f"‚úì Database migrated to version {target_version}")
            return True
            
    except Exception as e:
        print(f"‚ùå Migration failed: {e}")
        log_system_health(
            "database",
            "MIGRATION_FAILED",
            f"Failed to migrate: {str(e)}"
        )
        return False

# =========================
# MARKET FEATURES OPERATIONS
# =========================

def insert_market_features(df: pd.DataFrame) -> None:
    """
    Insert feature rows into market_features table.
    
    Args:
        df: DataFrame with feature columns matching MARKET_FEATURES_SCHEMA
    """
    if df.empty:
        return
    
    # Ensure feature_version is set
    if "feature_version" not in df.columns:
        df["feature_version"] = FEATURE_VERSION
    
    # Ensure timestamp is string
    df["timestamp"] = df["timestamp"].astype(str)
    
    # Ensure all required columns exist
    required_columns = list(MARKET_FEATURES_SCHEMA["columns"].keys())
    for col in required_columns:
        if col not in df.columns:
            df[col] = None  # Add missing columns with NULL
    
    # Select only columns that exist in schema
    df_to_insert = df[required_columns].copy()
    
    # Convert NaN to None for SQL
    df_to_insert = df_to_insert.replace({np.nan: None})
    
    try:
        with get_connection() as conn:
            # Insert data
            df_to_insert.to_sql(
                name="market_features",
                con=conn,
                if_exists="append",
                index=False
            )
            
            # Log successful insert
            log_system_health(
                "features",
                "INSERTED",
                f"Inserted {len(df)} feature rows",
                json.dumps({
                    "timestamp": df["timestamp"].iloc[0],
                    "feature_version": df["feature_version"].iloc[0],
                    "row_count": len(df)
                })
            )
            
            # Track feature changes for important features
            track_feature_changes(df_to_insert)
            
    except Exception as e:
        print(f"‚ùå Error inserting features: {e}")
        log_system_health(
            "features",
            "INSERT_FAILED",
            f"Failed to insert features: {str(e)}",
            json.dumps({"error": str(e)})
        )
        raise

def fetch_latest_features(feature_version: str = FEATURE_VERSION) -> Optional[pd.DataFrame]:
    """
    Fetch the most recent feature row for inference.
    
    Args:
        feature_version: Feature version to fetch
    
    Returns:
        DataFrame with latest features or None
    """
    query = """
        SELECT *
        FROM market_features
        WHERE feature_version = ?
        ORDER BY timestamp DESC
        LIMIT 1
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(feature_version,))
            
            if not df.empty:
                log_system_health(
                    "features",
                    "FETCHED",
                    f"Fetched latest features for version {feature_version}"
                )
            
            return df if not df.empty else None
            
    except Exception as e:
        print(f"‚ùå Error fetching features: {e}")
        return None

def fetch_features_by_time_range(
    start_time: str,
    end_time: str,
    feature_version: str = FEATURE_VERSION
) -> pd.DataFrame:
    """
    Fetch features within a time range.
    
    Args:
        start_time: Start timestamp (inclusive)
        end_time: End timestamp (inclusive)
        feature_version: Feature version filter
    
    Returns:
        DataFrame with features in range
    """
    query = """
        SELECT *
        FROM market_features
        WHERE feature_version = ?
          AND timestamp BETWEEN ? AND ?
        ORDER BY timestamp ASC
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(
                query,
                conn,
                params=(feature_version, start_time, end_time)
            )
            
            log_system_health(
                "features",
                "FETCHED_RANGE",
                f"Fetched {len(df)} features from {start_time} to {end_time}"
            )
            
            return df
            
    except Exception as e:
        print(f"‚ùå Error fetching features by range: {e}")
        return pd.DataFrame()

def fetch_training_data(
    feature_version: str = FEATURE_VERSION,
    start_ts: Optional[str] = None,
    end_ts: Optional[str] = None,
    min_confidence: float = 0.0
) -> pd.DataFrame:
    """
    Fetch labeled data for ML training with research features.
    
    Args:
        feature_version: Feature version
        start_ts: Start timestamp
        end_ts: End timestamp
        min_confidence: Minimum signal confidence for training
    
    Returns:
        DataFrame with features and labels
    """
    # Build WHERE clause
    conditions = ["feature_version = ?", "future_return_5m IS NOT NULL"]
    params = [feature_version]
    
    if start_ts:
        conditions.append("timestamp >= ?")
        params.append(start_ts)
    
    if end_ts:
        conditions.append("timestamp <= ?")
        params.append(end_ts)
    
    where_clause = " AND ".join(conditions)
    
    query = f"""
        SELECT mf.*, s.confidence as signal_confidence
        FROM market_features mf
        LEFT JOIN signals s ON mf.timestamp = s.timestamp AND s.status = 'VALIDATED'
        WHERE {where_clause}
        ORDER BY mf.timestamp ASC
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=params)
            
            # Filter by confidence if needed
            if min_confidence > 0 and 'signal_confidence' in df.columns:
                df = df[df['signal_confidence'] >= min_confidence]
            
            log_system_health(
                "training",
                "FETCHED",
                f"Fetched {len(df)} training samples"
            )
            
            return df
            
    except Exception as e:
        print(f"‚ùå Error fetching training data: {e}")
        return pd.DataFrame()

# =========================
# RESEARCH ANALYTICS OPERATIONS
# =========================

def insert_research_analytics(analytics: Dict) -> None:
    """
    Insert research analytics for tracking and visualization.
    
    Args:
        analytics: Dictionary with research analytics data
    """
    if not analytics:
        return
    
    # Prepare data
    timestamp = analytics.get("timestamp", datetime.utcnow().isoformat())
    
    data = {
        "timestamp": timestamp,
        "oi_velocity": analytics.get("oi_velocity"),
        "oi_regime": analytics.get("oi_regime"),
        "net_gamma": analytics.get("gamma_exposure", {}).get("net_gamma"),
        "gamma_regime": analytics.get("gamma_exposure", {}).get("regime"),
        "wall_strength": analytics.get("structural_walls", [{}])[0].get("concentration", 0) if analytics.get("structural_walls") else 0,
        "trap_probability": analytics.get("potential_traps", [{}])[0].get("confidence", 0) if analytics.get("potential_traps") else 0,
        "divergence_score": analytics.get("spot_divergence", {}).get("confidence", 0),
        "market_regime": analytics.get("market_regime"),
        "confidence": analytics.get("regime_confidence"),
        "insights_json": json.dumps(analytics.get("market_insights", [])),
        "walls_json": json.dumps(analytics.get("structural_walls", [])),
        "traps_json": json.dumps(analytics.get("potential_traps", [])),
        "created_at": datetime.utcnow().isoformat()
    }
    
    # Clean NaN values
    for key, value in data.items():
        if isinstance(value, float) and np.isnan(value):
            data[key] = None
    
    try:
        with get_connection() as conn:
            columns = ", ".join(data.keys())
            placeholders = ", ".join(["?"] * len(data))
            values = list(data.values())
            
            conn.execute(
                f"INSERT OR REPLACE INTO research_analytics ({columns}) VALUES ({placeholders})",
                values
            )
            
    except Exception as e:
        print(f"‚ùå Error inserting research analytics: {e}")

def fetch_recent_analytics(limit: int = 100) -> pd.DataFrame:
    """
    Fetch recent research analytics for visualization.
    
    Args:
        limit: Number of rows to fetch
    
    Returns:
        DataFrame with analytics
    """
    query = """
        SELECT *
        FROM research_analytics
        ORDER BY timestamp DESC
        LIMIT ?
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(limit,))
            return df
    except Exception as e:
        print(f"‚ùå Error fetching analytics: {e}")
        return pd.DataFrame()

# =========================
# SIGNAL STORE OPERATIONS (ENHANCED)
# =========================

def insert_signal(signal: Dict) -> None:
    """
    Insert a trading signal with research context.
    
    Args:
        signal: Signal dictionary with research context
    """
    if not signal:
        return
    
    # Add research context if available
    research_context = signal.get("research_context", {})
    if research_context and not isinstance(research_context, str):
        signal["research_context"] = json.dumps(research_context)
    
    # Add analytics summary
    analytics_summary = signal.get("analytics_summary", {})
    if analytics_summary and not isinstance(analytics_summary, str):
        signal["analytics_summary"] = json.dumps(analytics_summary)
    
    # Ensure all required fields
    required_fields = [
        "signal_id", "timestamp", "feature_version", "model_version",
        "signal_type", "confidence", "status", "created_at"
    ]
    
    for field in required_fields:
        if field not in signal:
            print(f"‚ö†Ô∏è Missing required field in signal: {field}")
            signal[field] = ""
    
    try:
        with get_connection() as conn:
            columns = ", ".join(signal.keys())
            placeholders = ", ".join(["?"] * len(signal))
            values = list(signal.values())
            
            conn.execute(
                f"INSERT INTO signals ({columns}) VALUES ({placeholders})",
                values
            )
            
            log_system_health(
                "signals",
                "INSERTED",
                f"Signal {signal['signal_id'][:8]} inserted: {signal['signal_type']} (conf: {signal['confidence']})",
                json.dumps({
                    "signal_id": signal["signal_id"],
                    "type": signal["signal_type"],
                    "confidence": signal["confidence"]
                })
            )
            
    except Exception as e:
        print(f"‚ùå Error inserting signal: {e}")
        log_system_health(
            "signals",
            "INSERT_FAILED",
            f"Failed to insert signal: {str(e)}"
        )
        raise

def signal_exists(feature_timestamp: str) -> bool:
    """
    Check if a signal already exists for given timestamp.
    
    Args:
        feature_timestamp: Feature timestamp
    
    Returns:
        True if signal exists
    """
    query = """
        SELECT 1 FROM signals WHERE timestamp = ? LIMIT 1
    """
    
    try:
        with get_connection() as conn:
            cur = conn.execute(query, (feature_timestamp,))
            return cur.fetchone() is not None
    except Exception as e:
        print(f"‚ùå Error checking signal existence: {e}")
        return False

def validate_new_signals(confidence_threshold: float = 0.2):
    """
    Promote NEW signals to VALIDATED if confidence is sufficient.
    
    Args:
        confidence_threshold: Minimum confidence for validation
    """
    query = """
        UPDATE signals
        SET status = 'VALIDATED'
        WHERE status = 'NEW'
          AND confidence >= ?
    """
    
    try:
        with get_connection() as conn:
            result = conn.execute(query, (confidence_threshold,))
            updated_count = result.rowcount
            
            if updated_count > 0:
                log_system_health(
                    "signals",
                    "VALIDATED",
                    f"Validated {updated_count} signals (conf >= {confidence_threshold})"
                )
                
    except Exception as e:
        print(f"‚ùå Error validating signals: {e}")
        log_system_health(
            "signals",
            "VALIDATION_FAILED",
            f"Signal validation failed: {str(e)}"
        )

def expire_old_signals():
    """
    Expire VALIDATED signals whose expiry_time has passed.
    """
    query = """
        UPDATE signals
        SET status = 'EXPIRED'
        WHERE status = 'VALIDATED'
          AND expiry_time < ?
    """
    
    now = datetime.utcnow().isoformat()
    
    try:
        with get_connection() as conn:
            result = conn.execute(query, (now,))
            expired_count = result.rowcount
            
            if expired_count > 0:
                log_system_health(
                    "signals",
                    "EXPIRED",
                    f"Expired {expired_count} signals"
                )
                
    except Exception as e:
        print(f"‚ùå Error expiring signals: {e}")

def update_signal_status(signal_id: str, new_status: str, 
                        pnl: Optional[float] = None,
                        exit_time: Optional[str] = None,
                        exit_price: Optional[float] = None) -> None:
    """
    Update signal lifecycle status with trade results.
    
    Args:
        signal_id: Signal ID
        new_status: New status
        pnl: Profit/Loss if applicable
        exit_time: Exit timestamp
        exit_price: Exit price
    """
    # Build update query
    updates = ["status = ?"]
    params = [new_status]
    
    if pnl is not None:
        updates.append("pnl = ?")
        params.append(pnl)
    
    if exit_time:
        updates.append("exit_time = ?")
        params.append(exit_time)
    
    if exit_price is not None:
        updates.append("exit_price = ?")
        params.append(exit_price)
    
    # Calculate trade duration if exit_time is provided
    if exit_time:
        updates.append("""
            trade_duration_seconds = (
                CAST((julianday(?) - julianday(created_at)) * 86400 AS INTEGER)
            )
        """)
        params.append(exit_time)
    
    query = f"""
        UPDATE signals
        SET {', '.join(updates)}
        WHERE signal_id = ?
    """
    
    params.append(signal_id)
    
    try:
        with get_connection() as conn:
            conn.execute(query, params)
            
            log_system_health(
                "signals",
                "UPDATED",
                f"Signal {signal_id[:8]} updated to {new_status}"
            )
            
    except Exception as e:
        print(f"‚ùå Error updating signal: {e}")
        log_system_health(
            "signals",
            "UPDATE_FAILED",
            f"Failed to update signal {signal_id}: {str(e)}"
        )

def fetch_active_signals(limit: int = 10) -> pd.DataFrame:
    """
    Fetch active signals for display.
    
    Args:
        limit: Maximum number of signals to fetch
    
    Returns:
        DataFrame with active signals
    """
    query = """
        SELECT *
        FROM signals
        WHERE status IN ('NEW','VALIDATED')
        ORDER BY created_at DESC
        LIMIT ?
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(limit,))
            
            # Parse JSON fields
            for col in ["research_context", "analytics_summary"]:
                if col in df.columns:
                    df[col] = df[col].apply(
                        lambda x: json.loads(x) if x and isinstance(x, str) else {}
                    )
            
            return df
            
    except Exception as e:
        print(f"‚ùå Error fetching active signals: {e}")
        return pd.DataFrame()

def fetch_signal_performance(days: int = 30) -> Dict:
    """
    Fetch signal performance metrics.
    
    Args:
        days: Number of days to analyze
    
    Returns:
        Dictionary with performance metrics
    """
    cutoff_date = (datetime.utcnow() - timedelta(days=days)).isoformat()
    
    query = """
        SELECT 
            signal_type,
            status,
            COUNT(*) as count,
            AVG(confidence) as avg_confidence,
            AVG(pnl) as avg_pnl,
            SUM(CASE WHEN pnl > 0 THEN 1 ELSE 0 END) as winners,
            SUM(CASE WHEN pnl <= 0 THEN 1 ELSE 0 END) as losers
        FROM signals
        WHERE created_at >= ?
        GROUP BY signal_type, status
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(cutoff_date,))
            
            if df.empty:
                return {}
            
            # Calculate overall metrics
            total_signals = df['count'].sum()
            profitable_signals = df[df['signal_type'] == 'BUY']['winners'].sum() + \
                                df[df['signal_type'] == 'SELL']['winners'].sum()
            
            performance = {
                "total_signals": int(total_signals),
                "profitable_signals": int(profitable_signals),
                "win_rate": round(profitable_signals / total_signals * 100, 1) if total_signals > 0 else 0,
                "breakdown": df.to_dict('records'),
                "period_days": days
            }
            
            return performance
            
    except Exception as e:
        print(f"‚ùå Error fetching performance: {e}")
        return {}

# =========================
# MODEL REGISTRY OPERATIONS
# =========================

def register_model(model_info: Dict) -> None:
    """
    Register trained ML model metadata.
    
    Args:
        model_info: Model information dictionary
    """
    required_keys = [
        "model_version", "feature_version", "algorithm",
        "trained_on_start", "trained_on_end", "metrics_json"
    ]
    
    for key in required_keys:
        if key not in model_info:
            raise ValueError(f"Missing required key: {key}")
    
    # Set created_at if not provided
    if "created_at" not in model_info:
        model_info["created_at"] = datetime.utcnow().isoformat()
    
    try:
        with get_connection() as conn:
            # Deactivate other models for this feature version
            conn.execute("""
                UPDATE model_registry
                SET is_active = 0
                WHERE feature_version = ?
            """, (model_info["feature_version"],))
            
            # Insert new model as active
            columns = ", ".join(model_info.keys())
            placeholders = ", ".join(["?"] * len(model_info))
            values = list(model_info.values())
            
            conn.execute(
                f"INSERT INTO model_registry ({columns}) VALUES ({placeholders})",
                values
            )
            
            log_system_health(
                "models",
                "REGISTERED",
                f"Model {model_info['model_version']} registered"
            )
            
    except Exception as e:
        print(f"‚ùå Error registering model: {e}")
        raise

def fetch_active_model(feature_version: str = FEATURE_VERSION) -> Optional[Dict]:
    """
    Fetch the active model for a given feature version.
    
    Args:
        feature_version: Feature version
    
    Returns:
        Model info dictionary or None
    """
    query = """
        SELECT *
        FROM model_registry
        WHERE feature_version = ?
          AND is_active = 1
        ORDER BY created_at DESC
        LIMIT 1
    """
    
    try:
        with get_connection() as conn:
            row = conn.execute(query, (feature_version,)).fetchone()
            return dict(row) if row else None
    except Exception as e:
        print(f"‚ùå Error fetching active model: {e}")
        return None

# =========================
# SYSTEM HEALTH LOGGING
# =========================

def log_system_health(
    component: str,
    status: str,
    message: Optional[str] = None,
    details_json: Optional[str] = None
) -> None:
    """
    Log system health status.
    
    Args:
        component: System component
        status: Status (OK, ERROR, WARNING, etc.)
        message: Optional message
        details_json: Optional JSON details
    """
    query = """
        INSERT INTO system_health (timestamp, component, status, message, details_json)
        VALUES (?, ?, ?, ?, ?)
    """
    
    params = (
        datetime.utcnow().isoformat(),
        component,
        status,
        message,
        details_json
    )
    
    try:
        with get_connection() as conn:
            conn.execute(query, params)
    except Exception as e:
        print(f"‚ùå Error logging system health: {e}")

def fetch_recent_health_logs(limit: int = 50) -> pd.DataFrame:
    """
    Fetch recent system health logs.
    
    Args:
        limit: Number of logs to fetch
    
    Returns:
        DataFrame with health logs
    """
    query = """
        SELECT *
        FROM system_health
        ORDER BY timestamp DESC
        LIMIT ?
    """
    
    try:
        with get_connection() as conn:
            return pd.read_sql_query(query, conn, params=(limit,))
    except Exception as e:
        print(f"‚ùå Error fetching health logs: {e}")
        return pd.DataFrame()

# =========================
# FEATURE HISTORY TRACKING
# =========================

def track_feature_changes(feature_df: pd.DataFrame) -> None:
    """
    Track significant feature changes for monitoring.
    
    Args:
        feature_df: DataFrame with features
    """
    if feature_df.empty or len(feature_df) < 2:
        return
    
    # Only track important features
    important_features = [
        "oi_velocity", "net_gamma", "trap_probability",
        "divergence_score", "wall_strength", "put_call_ratio"
    ]
    
    current = feature_df.iloc[-1]
    previous = feature_df.iloc[-2] if len(feature_df) > 1 else current
    
    changes = []
    for feature in important_features:
        if feature in current and feature in previous:
            old_val = previous[feature]
            new_val = current[feature]
            
            if old_val is not None and new_val is not None and old_val != 0:
                change_pct = ((new_val - old_val) / abs(old_val)) * 100
                
                # Only track significant changes (>10%)
                if abs(change_pct) > 10:
                    changes.append({
                        "timestamp": current["timestamp"],
                        "feature_name": feature,
                        "old_value": old_val,
                        "new_value": new_val,
                        "change_pct": change_pct,
                        "created_at": datetime.utcnow().isoformat()
                    })
    
    if changes:
        try:
            with get_connection() as conn:
                for change in changes:
                    conn.execute("""
                        INSERT INTO feature_history 
                        (timestamp, feature_name, old_value, new_value, change_pct, created_at)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (
                        change["timestamp"], change["feature_name"],
                        change["old_value"], change["new_value"],
                        change["change_pct"], change["created_at"]
                    ))
        except Exception as e:
            print(f"‚ùå Error tracking feature changes: {e}")

# =========================
# DATABASE MAINTENANCE
# =========================

def cleanup_old_data(days_to_keep: int = 30) -> Dict:
    """
    Clean up old data from database.
    
    Args:
        days_to_keep: Number of days of data to keep
    
    Returns:
        Dictionary with cleanup statistics
    """
    cutoff_date = (datetime.utcnow() - timedelta(days=days_to_keep)).isoformat()
    
    cleanup_stats = {}
    
    try:
        with get_connection() as conn:
            # Clean old market features
            result = conn.execute(
                "DELETE FROM market_features WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["market_features"] = result.rowcount
            
            # Clean old signals
            result = conn.execute(
                "DELETE FROM signals WHERE created_at < ? AND status = 'EXPIRED'",
                (cutoff_date,)
            )
            cleanup_stats["signals"] = result.rowcount
            
            # Clean old system health logs
            result = conn.execute(
                "DELETE FROM system_health WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["system_health"] = result.rowcount
            
            # Clean old research analytics
            result = conn.execute(
                "DELETE FROM research_analytics WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["research_analytics"] = result.rowcount
            
            # Clean old feature history
            result = conn.execute(
                "DELETE FROM feature_history WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["feature_history"] = result.rowcount
            
            # Vacuum database
            conn.execute("VACUUM")
            
            log_system_health(
                "database",
                "CLEANED",
                f"Cleaned up {sum(cleanup_stats.values())} old records",
                json.dumps(cleanup_stats)
            )
            
            return cleanup_stats
            
    except Exception as e:
        print(f"‚ùå Error during cleanup: {e}")
        log_system_health(
            "database",
            "CLEANUP_FAILED",
            f"Cleanup failed: {str(e)}"
        )
        return {}

# =========================
# DATABASE INFO & STATS
# =========================

def get_database_stats() -> Dict:
    """
    Get database statistics.
    
    Returns:
        Dictionary with database statistics
    """
    stats = {}
    
    try:
        with get_connection() as conn:
            # Table counts
            tables = [
                "market_features", "signals", "model_registry",
                "system_health", "research_analytics", "feature_history"
            ]
            
            for table in tables:
                cur = conn.execute(f"SELECT COUNT(*) as count FROM {table}")
                row = cur.fetchone()
                stats[f"{table}_count"] = row["count"] if row else 0
            
            # Latest feature timestamp
            cur = conn.execute("SELECT MAX(timestamp) as latest FROM market_features")
            row = cur.fetchone()
            stats["latest_feature"] = row["latest"] if row else None
            
            # Feature version
            cur = conn.execute("SELECT value FROM database_info WHERE key = 'feature_version'")
            row = cur.fetchone()
            stats["feature_version"] = row["value"] if row else "unknown"
            
            # Active signals
            cur = conn.execute("""
                SELECT COUNT(*) as count 
                FROM signals 
                WHERE status IN ('NEW','VALIDATED')
            """)
            row = cur.fetchone()
            stats["active_signals"] = row["count"] if row else 0
            
            # Database size
            db_size = DB_PATH.stat().st_size if DB_PATH.exists() else 0
            stats["database_size_mb"] = round(db_size / (1024 * 1024), 2)
            
            return stats
            
    except Exception as e:
        print(f"‚ùå Error getting database stats: {e}")
        return {}

# =========================
# INITIALIZATION
# =========================

def initialize_storage():
    """
    Initialize storage system.
    """
    print("Initializing storage system...")
    initialize_database()
    
    # Check and run migrations if needed
    current_version = get_database_stats().get("feature_version", "v1.0")
    if current_version != FEATURE_VERSION:
        print(f"Migrating from {current_version} to {FEATURE_VERSION}")
        migrate_database(FEATURE_VERSION)
    
    print("‚úì Storage system initialized")

# Run initialization when module is imported
initialize_storage()


--------------------------------------------------
FILE PATH : G:\trading_app\storage\repository.py
SIZE      : 41221 bytes
--------------------------------------------------

"""
Enhanced Storage Repository with Research Features Support - FIXED VERSION
Handles database operations for market features, signals, and analytics.
Includes migration system for feature version updates.
"""

import sqlite3
import pandas as pd
from typing import Dict, Optional, List, Any, Tuple
from pathlib import Path
from datetime import datetime, timedelta
import json
import numpy as np
from contextlib import contextmanager

# Import feature contract for schema
from ml.feature_contract import (
    FEATURE_VERSION, 
    FEATURE_COLUMNS, 
    METADATA_COLUMNS,
    MARKET_FEATURES_SCHEMA,
    get_migration_sql
)

# =========================
# DATABASE CONFIG
# =========================

DB_PATH = Path("G:/trading_app/storage/trading.db")

# Ensure directory exists
DB_PATH.parent.mkdir(parents=True, exist_ok=True)

# =========================
# CONNECTION MANAGEMENT
# =========================

@contextmanager
def get_connection() -> sqlite3.Connection:
    """
    Context manager for database connections with automatic cleanup.
    """
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    
    # Enable WAL mode for better concurrency
    conn.execute("PRAGMA journal_mode=WAL")
    conn.execute("PRAGMA synchronous=NORMAL")
    conn.execute("PRAGMA foreign_keys=ON")
    
    try:
        yield conn
        conn.commit()
    except Exception as e:
        conn.rollback()
        raise e
    finally:
        conn.close()

def initialize_database():
    """
    Initialize database with all required tables and indexes.
    """
    with get_connection() as conn:
        # Check if market_features table exists and get its columns
        cur = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='market_features'")
        table_exists = cur.fetchone() is not None
        
        if table_exists:
            # Get existing columns
            cur = conn.execute("PRAGMA table_info(market_features)")
            existing_columns = {row[1] for row in cur.fetchall()}
            print(f"Existing columns in market_features: {existing_columns}")
        else:
            existing_columns = set()
        
        # Get expected columns from schema
        expected_columns = set(MARKET_FEATURES_SCHEMA["columns"].keys())
        print(f"Expected columns in market_features: {expected_columns}")
        
        # Create table if it doesn't exist
        if not table_exists:
            print("Creating market_features table from scratch...")
            columns = MARKET_FEATURES_SCHEMA["columns"]
            column_defs = [f"{name} {dtype}" for name, dtype in columns.items()]
            
            create_table_sql = f"""
                CREATE TABLE IF NOT EXISTS market_features (
                    {', '.join(column_defs)}
                )
            """
            
            conn.execute(create_table_sql)
            print("market_features table created successfully")
        
        # Check for missing columns and add them
        missing_columns = expected_columns - existing_columns
        if missing_columns:
            print(f"Adding missing columns: {missing_columns}")
            
            for column in missing_columns:
                dtype = MARKET_FEATURES_SCHEMA["columns"][column]
                try:
                    conn.execute(f"ALTER TABLE market_features ADD COLUMN {column} {dtype}")
                    print(f"  ‚úì Added column: {column}")
                except sqlite3.OperationalError as e:
                    if "duplicate column" in str(e):
                        print(f"  ‚ö†Ô∏è Column {column} already exists")
                    else:
                        raise
        
        # Create other tables
        print("Creating other tables...")
        
        # Create signals table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS signals (
                signal_id TEXT PRIMARY KEY,
                timestamp TEXT NOT NULL,
                feature_version TEXT NOT NULL,
                model_version TEXT NOT NULL,
                signal_type TEXT NOT NULL,
                confidence REAL,
                market_state TEXT,
                rationale TEXT,
                expiry_time TEXT,
                status TEXT,
                created_at TEXT,
                research_context TEXT,
                analytics_summary TEXT,
                pnl REAL DEFAULT NULL,
                exit_time TEXT,
                exit_price REAL,
                stop_loss_hit INTEGER DEFAULT 0,
                take_profit_hit INTEGER DEFAULT 0,
                trade_duration_seconds INTEGER
            )
        """)
        
        # Create model_registry table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS model_registry (
                model_version TEXT PRIMARY KEY,
                feature_version TEXT NOT NULL,
                algorithm TEXT NOT NULL,
                trained_on_start TEXT,
                trained_on_end TEXT,
                metrics_json TEXT,
                feature_importance_json TEXT,
                created_at TEXT,
                is_active INTEGER DEFAULT 0
            )
        """)
        
        # Create system_health table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS system_health (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                component TEXT NOT NULL,
                status TEXT NOT NULL,
                message TEXT,
                details_json TEXT
            )
        """)
        
        # Create research_analytics table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS research_analytics (
                timestamp TEXT PRIMARY KEY,
                oi_velocity REAL,
                oi_regime TEXT,
                net_gamma REAL,
                gamma_regime TEXT,
                wall_strength REAL,
                trap_probability REAL,
                divergence_score REAL,
                market_regime TEXT,
                confidence REAL,
                insights_json TEXT,
                walls_json TEXT,
                traps_json TEXT,
                created_at TEXT
            )
        """)
        
        # Create feature_history table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS feature_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                feature_name TEXT NOT NULL,
                old_value REAL,
                new_value REAL,
                change_pct REAL,
                created_at TEXT
            )
        """)
        
        # Create database_info table
        conn.execute("""
            CREATE TABLE IF NOT EXISTS database_info (
                key TEXT PRIMARY KEY,
                value TEXT,
                updated_at TEXT
            )
        """)
        
        # Create indexes (skip if they already exist)
        print("Creating indexes...")
        index_sqls = [
            # market_features indexes
            "CREATE INDEX IF NOT EXISTS idx_timestamp ON market_features(timestamp)",
            "CREATE INDEX IF NOT EXISTS idx_feature_version ON market_features(feature_version)",
            "CREATE INDEX IF NOT EXISTS idx_oi_velocity ON market_features(oi_velocity)",
            "CREATE INDEX IF NOT EXISTS idx_net_gamma ON market_features(net_gamma)",
            
            # signals indexes
            "CREATE INDEX IF NOT EXISTS idx_signals_timestamp ON signals(timestamp)",
            "CREATE INDEX IF NOT EXISTS idx_signals_status ON signals(status)",
            "CREATE INDEX IF NOT EXISTS idx_signals_type ON signals(signal_type)",
            
            # research_analytics indexes
            "CREATE INDEX IF NOT EXISTS idx_analytics_timestamp ON research_analytics(timestamp)",
            "CREATE INDEX IF NOT EXISTS idx_analytics_regime ON research_analytics(market_regime)"
        ]
        
        for index_sql in index_sqls:
            try:
                conn.execute(index_sql)
            except sqlite3.OperationalError as e:
                if "already exists" not in str(e):
                    print(f"  ‚ö†Ô∏è Could not create index: {e}")
        
        # Initialize database info
        current_version = get_current_feature_version(conn)
        if not current_version:
            conn.execute(
                "INSERT INTO database_info (key, value, updated_at) VALUES (?, ?, ?)",
                ("feature_version", FEATURE_VERSION, datetime.utcnow().isoformat())
            )
            print(f"‚úì Set feature version to {FEATURE_VERSION}")
        else:
            print(f"‚úì Current feature version: {current_version}")
        
        # Log initialization
        _log_system_health_conn(
            conn,
            "database",
            "INITIALIZED",
            f"Database initialized with feature version {FEATURE_VERSION}"
        )
        
        print(f"‚úì Database initialized at {DB_PATH}")

def get_current_feature_version(conn: sqlite3.Connection) -> Optional[str]:
    """Get current feature version from database."""
    try:
        cur = conn.execute("SELECT value FROM database_info WHERE key = 'feature_version'")
        row = cur.fetchone()
        return row["value"] if row else None
    except:
        return None

# =========================
# DATABASE MIGRATION
# =========================

def migrate_database(target_version: str = FEATURE_VERSION) -> bool:
    """
    Migrate database to target feature version.
    """
    try:
        with get_connection() as conn:
            # Get current version
            current_version = get_current_feature_version(conn)
            if not current_version:
                current_version = "v1.0"
            
            if current_version == target_version:
                print(f"Database already at version {target_version}")
                return True
            
            print(f"Migrating database from {current_version} to {target_version}")
            
            # Get migration SQL
            migrations = get_migration_sql(current_version, target_version)
            
            if not migrations:
                print("No migration required")
                return True
            
            # Execute migrations
            for migration_sql in migrations:
                try:
                    print(f"Executing: {migration_sql[:80]}...")
                    conn.execute(migration_sql)
                except sqlite3.OperationalError as e:
                    if "already exists" in str(e) or "duplicate column" in str(e):
                        print(f"  ‚ö†Ô∏è Skipping (already exists): {migration_sql[:50]}...")
                    else:
                        print(f"  ‚ùå Error: {e}")
                        raise
            
            # Update version
            conn.execute(
                "INSERT OR REPLACE INTO database_info (key, value, updated_at) VALUES (?, ?, ?)",
                (target_version, target_version, datetime.utcnow().isoformat())
            )
            
            # Log migration
            _log_system_health_conn(
                conn,
                "database",
                "MIGRATED",
                f"Migrated from {current_version} to {target_version}",
                json.dumps({"migrations_applied": len(migrations)})
            )
            
            print(f"‚úì Database migrated to version {target_version}")
            return True
            
    except Exception as e:
        print(f"‚ùå Migration failed: {e}")
        import traceback
        traceback.print_exc()
        return False

# =========================
# SYSTEM HEALTH LOGGING (PRIVATE HELPER)
# =========================

def _log_system_health_conn(
    conn: sqlite3.Connection,
    component: str,
    status: str,
    message: Optional[str] = None,
    details_json: Optional[str] = None
) -> None:
    """
    PRIVATE: Log system health status (requires connection).
    """
    query = """
        INSERT INTO system_health (timestamp, component, status, message, details_json)
        VALUES (?, ?, ?, ?, ?)
    """
    
    params = (
        datetime.utcnow().isoformat(),
        component,
        status,
        message,
        details_json
    )
    
    try:
        conn.execute(query, params)
    except Exception as e:
        print(f"‚ùå Error logging system health: {e}")

# =========================
# PUBLIC SYSTEM HEALTH LOGGING
# =========================

def log_system_health(
    component: str,
    status: str,
    message: Optional[str] = None,
    details_json: Optional[str] = None
) -> None:
    """
    PUBLIC: Log system health status.
    """
    with get_connection() as conn:
        _log_system_health_conn(conn, component, status, message, details_json)

# =========================
# INITIALIZATION (MODIFIED)
# =========================

def initialize_storage():
    """
    Initialize storage system.
    """
    print("Initializing storage system...")
    
    try:
        # Initialize database structure
        initialize_database()
        
        # Get current version
        with get_connection() as conn:
            current_version = get_current_feature_version(conn)
        
        # Check and run migrations if needed
        if current_version != FEATURE_VERSION:
            print(f"Current version: {current_version}, Target: {FEATURE_VERSION}")
            print(f"Migrating from {current_version} to {FEATURE_VERSION}")
            success = migrate_database(FEATURE_VERSION)
            
            if not success:
                print("‚ö†Ô∏è Migration may have issues, but continuing...")
        
        print("‚úì Storage system initialized")
        
    except Exception as e:
        print(f"‚ùå Error initializing storage: {e}")
        import traceback
        traceback.print_exc()
        raise

# =========================
# MARKET FEATURES OPERATIONS
# =========================

def insert_market_features(df: pd.DataFrame) -> None:
    """
    Insert feature rows into market_features table.
    """
    if df.empty:
        return
    
    # Ensure feature_version is set
    if "feature_version" not in df.columns:
        df["feature_version"] = FEATURE_VERSION
    
    # Ensure timestamp is string
    df["timestamp"] = df["timestamp"].astype(str)
    
    # Get expected columns from schema
    expected_columns = set(MARKET_FEATURES_SCHEMA["columns"].keys())
    
    # Add any missing columns with NULL
    for col in expected_columns:
        if col not in df.columns:
            df[col] = None
    
    # Select only columns that exist in schema
    df_to_insert = df[list(expected_columns)].copy()
    
    # Convert NaN to None for SQL
    df_to_insert = df_to_insert.replace({np.nan: None})
    
    try:
        with get_connection() as conn:
            # Insert data
            df_to_insert.to_sql(
                name="market_features",
                con=conn,
                if_exists="append",
                index=False
            )
            
            print(f"‚úì Inserted {len(df)} feature rows")
            
    except Exception as e:
        print(f"‚ùå Error inserting features: {e}")
        raise

def fetch_latest_features(feature_version: str = FEATURE_VERSION) -> Optional[pd.DataFrame]:
    """
    Fetch the most recent feature row for inference.
    """
    query = """
        SELECT *
        FROM market_features
        WHERE feature_version = ?
        ORDER BY timestamp DESC
        LIMIT 1
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(feature_version,))
            return df if not df.empty else None
            
    except Exception as e:
        print(f"‚ùå Error fetching features: {e}")
        return None

def fetch_features_by_time_range(
    start_time: str,
    end_time: str,
    feature_version: str = FEATURE_VERSION
) -> pd.DataFrame:
    """
    Fetch features within a time range.
    
    Args:
        start_time: Start timestamp (inclusive)
        end_time: End timestamp (inclusive)
        feature_version: Feature version filter
    
    Returns:
        DataFrame with features in range
    """
    query = """
        SELECT *
        FROM market_features
        WHERE feature_version = ?
          AND timestamp BETWEEN ? AND ?
        ORDER BY timestamp ASC
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(
                query,
                conn,
                params=(feature_version, start_time, end_time)
            )
            
            log_system_health(
                "features",
                "FETCHED_RANGE",
                f"Fetched {len(df)} features from {start_time} to {end_time}"
            )
            
            return df
            
    except Exception as e:
        print(f"‚ùå Error fetching features by range: {e}")
        return pd.DataFrame()

def fetch_training_data(
    feature_version: str = FEATURE_VERSION,
    start_ts: Optional[str] = None,
    end_ts: Optional[str] = None,
    min_confidence: float = 0.0
) -> pd.DataFrame:
    """
    Fetch labeled data for ML training with research features.
    
    Args:
        feature_version: Feature version
        start_ts: Start timestamp
        end_ts: End timestamp
        min_confidence: Minimum signal confidence for training
    
    Returns:
        DataFrame with features and labels
    """
    # Build WHERE clause
    conditions = ["feature_version = ?", "future_return_5m IS NOT NULL"]
    params = [feature_version]
    
    if start_ts:
        conditions.append("timestamp >= ?")
        params.append(start_ts)
    
    if end_ts:
        conditions.append("timestamp <= ?")
        params.append(end_ts)
    
    where_clause = " AND ".join(conditions)
    
    query = f"""
        SELECT mf.*, s.confidence as signal_confidence
        FROM market_features mf
        LEFT JOIN signals s ON mf.timestamp = s.timestamp AND s.status = 'VALIDATED'
        WHERE {where_clause}
        ORDER BY mf.timestamp ASC
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=params)
            
            # Filter by confidence if needed
            if min_confidence > 0 and 'signal_confidence' in df.columns:
                df = df[df['signal_confidence'] >= min_confidence]
            
            log_system_health(
                "training",
                "FETCHED",
                f"Fetched {len(df)} training samples"
            )
            
            return df
            
    except Exception as e:
        print(f"‚ùå Error fetching training data: {e}")
        return pd.DataFrame()

# =========================
# RESEARCH ANALYTICS OPERATIONS
# =========================

def insert_research_analytics(analytics: Dict) -> None:
    """
    Insert research analytics for tracking and visualization.
    
    Args:
        analytics: Dictionary with research analytics data
    """
    if not analytics:
        return
    
    # Prepare data
    timestamp = analytics.get("timestamp", datetime.utcnow().isoformat())
    
    data = {
        "timestamp": timestamp,
        "oi_velocity": analytics.get("oi_velocity"),
        "oi_regime": analytics.get("oi_regime"),
        "net_gamma": analytics.get("gamma_exposure", {}).get("net_gamma"),
        "gamma_regime": analytics.get("gamma_exposure", {}).get("regime"),
        "wall_strength": analytics.get("structural_walls", [{}])[0].get("concentration", 0) if analytics.get("structural_walls") else 0,
        "trap_probability": analytics.get("potential_traps", [{}])[0].get("confidence", 0) if analytics.get("potential_traps") else 0,
        "divergence_score": analytics.get("spot_divergence", {}).get("confidence", 0),
        "market_regime": analytics.get("market_regime"),
        "confidence": analytics.get("regime_confidence"),
        "insights_json": json.dumps(analytics.get("market_insights", [])),
        "walls_json": json.dumps(analytics.get("structural_walls", [])),
        "traps_json": json.dumps(analytics.get("potential_traps", [])),
        "created_at": datetime.utcnow().isoformat()
    }
    
    # Clean NaN values
    for key, value in data.items():
        if isinstance(value, float) and np.isnan(value):
            data[key] = None
    
    try:
        with get_connection() as conn:
            columns = ", ".join(data.keys())
            placeholders = ", ".join(["?"] * len(data))
            values = list(data.values())
            
            conn.execute(
                f"INSERT OR REPLACE INTO research_analytics ({columns}) VALUES ({placeholders})",
                values
            )
            
    except Exception as e:
        print(f"‚ùå Error inserting research analytics: {e}")

def fetch_recent_analytics(limit: int = 100) -> pd.DataFrame:
    """
    Fetch recent research analytics for visualization.
    
    Args:
        limit: Number of rows to fetch
    
    Returns:
        DataFrame with analytics
    """
    query = """
        SELECT *
        FROM research_analytics
        ORDER BY timestamp DESC
        LIMIT ?
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(limit,))
            return df
    except Exception as e:
        print(f"‚ùå Error fetching analytics: {e}")
        return pd.DataFrame()

# =========================
# SIGNAL STORE OPERATIONS (ENHANCED)
# =========================

def insert_signal(signal: Dict) -> None:
    """
    Insert a trading signal with research context.
    
    Args:
        signal: Signal dictionary with research context
    """
    if not signal:
        return
    
    # Add research context if available
    research_context = signal.get("research_context", {})
    if research_context and not isinstance(research_context, str):
        signal["research_context"] = json.dumps(research_context)
    
    # Add analytics summary
    analytics_summary = signal.get("analytics_summary", {})
    if analytics_summary and not isinstance(analytics_summary, str):
        signal["analytics_summary"] = json.dumps(analytics_summary)
    
    # Ensure all required fields
    required_fields = [
        "signal_id", "timestamp", "feature_version", "model_version",
        "signal_type", "confidence", "status", "created_at"
    ]
    
    for field in required_fields:
        if field not in signal:
            print(f"‚ö†Ô∏è Missing required field in signal: {field}")
            signal[field] = ""
    
    try:
        with get_connection() as conn:
            columns = ", ".join(signal.keys())
            placeholders = ", ".join(["?"] * len(signal))
            values = list(signal.values())
            
            conn.execute(
                f"INSERT INTO signals ({columns}) VALUES ({placeholders})",
                values
            )
            
            log_system_health(
                "signals",
                "INSERTED",
                f"Signal {signal['signal_id'][:8]} inserted: {signal['signal_type']} (conf: {signal['confidence']})",
                json.dumps({
                    "signal_id": signal["signal_id"],
                    "type": signal["signal_type"],
                    "confidence": signal["confidence"]
                })
            )
            
    except Exception as e:
        print(f"‚ùå Error inserting signal: {e}")
        log_system_health(
            "signals",
            "INSERT_FAILED",
            f"Failed to insert signal: {str(e)}"
        )
        raise

def signal_exists(feature_timestamp: str) -> bool:
    """
    Check if a signal already exists for given timestamp.
    
    Args:
        feature_timestamp: Feature timestamp
    
    Returns:
        True if signal exists
    """
    query = """
        SELECT 1 FROM signals WHERE timestamp = ? LIMIT 1
    """
    
    try:
        with get_connection() as conn:
            cur = conn.execute(query, (feature_timestamp,))
            return cur.fetchone() is not None
    except Exception as e:
        print(f"‚ùå Error checking signal existence: {e}")
        return False

def validate_new_signals(confidence_threshold: float = 0.2):
    """
    Promote NEW signals to VALIDATED if confidence is sufficient.
    
    Args:
        confidence_threshold: Minimum confidence for validation
    """
    query = """
        UPDATE signals
        SET status = 'VALIDATED'
        WHERE status = 'NEW'
          AND confidence >= ?
    """
    
    try:
        with get_connection() as conn:
            result = conn.execute(query, (confidence_threshold,))
            updated_count = result.rowcount
            
            if updated_count > 0:
                log_system_health(
                    "signals",
                    "VALIDATED",
                    f"Validated {updated_count} signals (conf >= {confidence_threshold})"
                )
                
    except Exception as e:
        print(f"‚ùå Error validating signals: {e}")
        log_system_health(
            "signals",
            "VALIDATION_FAILED",
            f"Signal validation failed: {str(e)}"
        )

def expire_old_signals():
    """
    Expire VALIDATED signals whose expiry_time has passed.
    """
    query = """
        UPDATE signals
        SET status = 'EXPIRED'
        WHERE status = 'VALIDATED'
          AND expiry_time < ?
    """
    
    now = datetime.utcnow().isoformat()
    
    try:
        with get_connection() as conn:
            result = conn.execute(query, (now,))
            expired_count = result.rowcount
            
            if expired_count > 0:
                log_system_health(
                    "signals",
                    "EXPIRED",
                    f"Expired {expired_count} signals"
                )
                
    except Exception as e:
        print(f"‚ùå Error expiring signals: {e}")

def update_signal_status(signal_id: str, new_status: str, 
                        pnl: Optional[float] = None,
                        exit_time: Optional[str] = None,
                        exit_price: Optional[float] = None) -> None:
    """
    Update signal lifecycle status with trade results.
    
    Args:
        signal_id: Signal ID
        new_status: New status
        pnl: Profit/Loss if applicable
        exit_time: Exit timestamp
        exit_price: Exit price
    """
    # Build update query
    updates = ["status = ?"]
    params = [new_status]
    
    if pnl is not None:
        updates.append("pnl = ?")
        params.append(pnl)
    
    if exit_time:
        updates.append("exit_time = ?")
        params.append(exit_time)
    
    if exit_price is not None:
        updates.append("exit_price = ?")
        params.append(exit_price)
    
    # Calculate trade duration if exit_time is provided
    if exit_time:
        updates.append("""
            trade_duration_seconds = (
                CAST((julianday(?) - julianday(created_at)) * 86400 AS INTEGER)
            )
        """)
        params.append(exit_time)
    
    query = f"""
        UPDATE signals
        SET {', '.join(updates)}
        WHERE signal_id = ?
    """
    
    params.append(signal_id)
    
    try:
        with get_connection() as conn:
            conn.execute(query, params)
            
            log_system_health(
                "signals",
                "UPDATED",
                f"Signal {signal_id[:8]} updated to {new_status}"
            )
            
    except Exception as e:
        print(f"‚ùå Error updating signal: {e}")
        log_system_health(
            "signals",
            "UPDATE_FAILED",
            f"Failed to update signal {signal_id}: {str(e)}"
        )

def fetch_active_signals(limit: int = 10) -> pd.DataFrame:
    """Fetch active signals for display."""
    query = """
        SELECT *
        FROM signals
        WHERE status IN ('NEW','VALIDATED')
        ORDER BY created_at DESC
        LIMIT ?
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(limit,))
            return df
    except Exception as e:
        print(f"‚ùå Error fetching active signals: {e}")
        return pd.DataFrame()

def fetch_signal_performance(days: int = 30) -> Dict:
    """
    Fetch signal performance metrics.
    
    Args:
        days: Number of days to analyze
    
    Returns:
        Dictionary with performance metrics
    """
    cutoff_date = (datetime.utcnow() - timedelta(days=days)).isoformat()
    
    query = """
        SELECT 
            signal_type,
            status,
            COUNT(*) as count,
            AVG(confidence) as avg_confidence,
            AVG(pnl) as avg_pnl,
            SUM(CASE WHEN pnl > 0 THEN 1 ELSE 0 END) as winners,
            SUM(CASE WHEN pnl <= 0 THEN 1 ELSE 0 END) as losers
        FROM signals
        WHERE created_at >= ?
        GROUP BY signal_type, status
    """
    
    try:
        with get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=(cutoff_date,))
            
            if df.empty:
                return {}
            
            # Calculate overall metrics
            total_signals = df['count'].sum()
            profitable_signals = df[df['signal_type'] == 'BUY']['winners'].sum() + \
                                df[df['signal_type'] == 'SELL']['winners'].sum()
            
            performance = {
                "total_signals": int(total_signals),
                "profitable_signals": int(profitable_signals),
                "win_rate": round(profitable_signals / total_signals * 100, 1) if total_signals > 0 else 0,
                "breakdown": df.to_dict('records'),
                "period_days": days
            }
            
            return performance
            
    except Exception as e:
        print(f"‚ùå Error fetching performance: {e}")
        return {}

# =========================
# MODEL REGISTRY OPERATIONS
# =========================

def register_model(model_info: Dict) -> None:
    """
    Register trained ML model metadata.
    
    Args:
        model_info: Model information dictionary
    """
    required_keys = [
        "model_version", "feature_version", "algorithm",
        "trained_on_start", "trained_on_end", "metrics_json"
    ]
    
    for key in required_keys:
        if key not in model_info:
            raise ValueError(f"Missing required key: {key}")
    
    # Set created_at if not provided
    if "created_at" not in model_info:
        model_info["created_at"] = datetime.utcnow().isoformat()
    
    try:
        with get_connection() as conn:
            # Deactivate other models for this feature version
            conn.execute("""
                UPDATE model_registry
                SET is_active = 0
                WHERE feature_version = ?
            """, (model_info["feature_version"],))
            
            # Insert new model as active
            columns = ", ".join(model_info.keys())
            placeholders = ", ".join(["?"] * len(model_info))
            values = list(model_info.values())
            
            conn.execute(
                f"INSERT INTO model_registry ({columns}) VALUES ({placeholders})",
                values
            )
            
            log_system_health(
                "models",
                "REGISTERED",
                f"Model {model_info['model_version']} registered"
            )
            
    except Exception as e:
        print(f"‚ùå Error registering model: {e}")
        raise

def fetch_active_model(feature_version: str = FEATURE_VERSION) -> Optional[Dict]:
    """
    Fetch the active model for a given feature version.
    
    Args:
        feature_version: Feature version
    
    Returns:
        Model info dictionary or None
    """
    query = """
        SELECT *
        FROM model_registry
        WHERE feature_version = ?
          AND is_active = 1
        ORDER BY created_at DESC
        LIMIT 1
    """
    
    try:
        with get_connection() as conn:
            row = conn.execute(query, (feature_version,)).fetchone()
            return dict(row) if row else None
    except Exception as e:
        print(f"‚ùå Error fetching active model: {e}")
        return None

# =========================
# SYSTEM HEALTH LOGGING (PUBLIC)
# =========================

def fetch_recent_health_logs(limit: int = 50) -> pd.DataFrame:
    """
    Fetch recent system health logs.
    
    Args:
        limit: Number of logs to fetch
    
    Returns:
        DataFrame with health logs
    """
    query = """
        SELECT *
        FROM system_health
        ORDER BY timestamp DESC
        LIMIT ?
    """
    
    try:
        with get_connection() as conn:
            return pd.read_sql_query(query, conn, params=(limit,))
    except Exception as e:
        print(f"‚ùå Error fetching health logs: {e}")
        return pd.DataFrame()

# =========================
# FEATURE HISTORY TRACKING
# =========================

def track_feature_changes(feature_df: pd.DataFrame) -> None:
    """
    Track significant feature changes for monitoring.
    
    Args:
        feature_df: DataFrame with features
    """
    if feature_df.empty or len(feature_df) < 2:
        return
    
    # Only track important features
    important_features = [
        "oi_velocity", "net_gamma", "trap_probability",
        "divergence_score", "wall_strength", "put_call_ratio"
    ]
    
    current = feature_df.iloc[-1]
    previous = feature_df.iloc[-2] if len(feature_df) > 1 else current
    
    changes = []
    for feature in important_features:
        if feature in current and feature in previous:
            old_val = previous[feature]
            new_val = current[feature]
            
            if old_val is not None and new_val is not None and old_val != 0:
                change_pct = ((new_val - old_val) / abs(old_val)) * 100
                
                # Only track significant changes (>10%)
                if abs(change_pct) > 10:
                    changes.append({
                        "timestamp": current["timestamp"],
                        "feature_name": feature,
                        "old_value": old_val,
                        "new_value": new_val,
                        "change_pct": change_pct,
                        "created_at": datetime.utcnow().isoformat()
                    })
    
    if changes:
        try:
            with get_connection() as conn:
                for change in changes:
                    conn.execute("""
                        INSERT INTO feature_history 
                        (timestamp, feature_name, old_value, new_value, change_pct, created_at)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (
                        change["timestamp"], change["feature_name"],
                        change["old_value"], change["new_value"],
                        change["change_pct"], change["created_at"]
                    ))
        except Exception as e:
            print(f"‚ùå Error tracking feature changes: {e}")

# =========================
# DATABASE MAINTENANCE
# =========================

def cleanup_old_data(days_to_keep: int = 30) -> Dict:
    """
    Clean up old data from database.
    
    Args:
        days_to_keep: Number of days of data to keep
    
    Returns:
        Dictionary with cleanup statistics
    """
    cutoff_date = (datetime.utcnow() - timedelta(days=days_to_keep)).isoformat()
    
    cleanup_stats = {}
    
    try:
        with get_connection() as conn:
            # Clean old market features
            result = conn.execute(
                "DELETE FROM market_features WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["market_features"] = result.rowcount
            
            # Clean old signals
            result = conn.execute(
                "DELETE FROM signals WHERE created_at < ? AND status = 'EXPIRED'",
                (cutoff_date,)
            )
            cleanup_stats["signals"] = result.rowcount
            
            # Clean old system health logs
            result = conn.execute(
                "DELETE FROM system_health WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["system_health"] = result.rowcount
            
            # Clean old research analytics
            result = conn.execute(
                "DELETE FROM research_analytics WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["research_analytics"] = result.rowcount
            
            # Clean old feature history
            result = conn.execute(
                "DELETE FROM feature_history WHERE timestamp < ?",
                (cutoff_date,)
            )
            cleanup_stats["feature_history"] = result.rowcount
            
            # Vacuum database
            conn.execute("VACUUM")
            
            log_system_health(
                "database",
                "CLEANED",
                f"Cleaned up {sum(cleanup_stats.values())} old records",
                json.dumps(cleanup_stats)
            )
            
            return cleanup_stats
            
    except Exception as e:
        print(f"‚ùå Error during cleanup: {e}")
        log_system_health(
            "database",
            "CLEANUP_FAILED",
            f"Cleanup failed: {str(e)}"
        )
        return {}
# =========================
# DATABASE INFO & STATS
# =========================

def get_database_stats() -> Dict:
    """Get database statistics."""
    stats = {}
    
    try:
        with get_connection() as conn:
            # Table counts
            tables = [
                "market_features", "signals", "model_registry",
                "system_health", "research_analytics", "feature_history"
            ]
            
            for table in tables:
                try:
                    cur = conn.execute(f"SELECT COUNT(*) as count FROM {table}")
                    row = cur.fetchone()
                    stats[f"{table}_count"] = row["count"] if row else 0
                except:
                    stats[f"{table}_count"] = 0
            
            # Latest feature timestamp
            try:
                cur = conn.execute("SELECT MAX(timestamp) as latest FROM market_features")
                row = cur.fetchone()
                stats["latest_feature"] = row["latest"] if row else None
            except:
                stats["latest_feature"] = None
            
            # Feature version
            stats["feature_version"] = get_current_feature_version(conn) or "unknown"
            
            # Database size
            if DB_PATH.exists():
                db_size = DB_PATH.stat().st_size
                stats["database_size_mb"] = round(db_size / (1024 * 1024), 2)
            else:
                stats["database_size_mb"] = 0
            
            return stats
            
    except Exception as e:
        print(f"‚ùå Error getting database stats: {e}")
        return {}


# =========================
# INITIALIZATION
# =========================

def initialize_storage():
    """
    Initialize storage system.
    """
    print("Initializing storage system...")
    initialize_database()
    
    # Check and run migrations if needed
    current_version = get_database_stats().get("feature_version", "v1.0")
    if current_version != FEATURE_VERSION:
        print(f"Migrating from {current_version} to {FEATURE_VERSION}")
        migrate_database(FEATURE_VERSION)
    
    print("‚úì Storage system initialized")

# Run initialization when module is imported
initialize_storage()


--------------------------------------------------
FILE PATH : G:\trading_app\storage\schema.sql
SIZE      : 2496 bytes
--------------------------------------------------

-- ================================
-- FEATURE STORE (IMMUTABLE)
-- ================================
CREATE TABLE IF NOT EXISTS market_features (
    timestamp TEXT NOT NULL,
    feature_version TEXT NOT NULL,

    -- Option structure
    put_call_ratio REAL NOT NULL,
    oi_delta REAL NOT NULL,
    oi_concentration REAL NOT NULL,
    atm_iv REAL NOT NULL,
    iv_skew REAL NOT NULL,

    -- Price & flow
    vwap_distance REAL NOT NULL,
    price_momentum REAL NOT NULL,
    volume_ratio REAL NOT NULL,

    -- Breadth
    ccc_value REAL NOT NULL,
    ccc_slope REAL NOT NULL,

    -- Time context
    time_to_expiry_minutes INTEGER NOT NULL,

    -- ML target (NULL in live mode)
    future_return_5m REAL,

    PRIMARY KEY (timestamp, feature_version)
);

-- ================================
-- SIGNAL STATE MACHINE
-- ================================
CREATE TABLE IF NOT EXISTS signals (
    signal_id TEXT PRIMARY KEY,
    timestamp TEXT NOT NULL,

    feature_version TEXT NOT NULL,
    model_version TEXT NOT NULL,

    signal_type TEXT CHECK(signal_type IN ('BUY','SELL','NEUTRAL')) NOT NULL,
    confidence REAL NOT NULL,

    market_state TEXT NOT NULL,
    rationale TEXT,

    expiry_time TEXT NOT NULL,
    status TEXT CHECK(status IN ('NEW','VALIDATED','EXPIRED','EVALUATED')) NOT NULL,

    created_at TEXT NOT NULL
);

-- ================================
-- TRADE EXECUTION (FUTURE)
-- ================================
CREATE TABLE IF NOT EXISTS trades (
    trade_id TEXT PRIMARY KEY,
    signal_id TEXT NOT NULL,

    entry_price REAL NOT NULL,
    exit_price REAL,
    quantity INTEGER NOT NULL,

    stop_loss REAL,
    take_profit REAL,

    pnl REAL,
    executed_at TEXT,
    closed_at TEXT,

    FOREIGN KEY(signal_id) REFERENCES signals(signal_id)
);

-- ================================
-- MODEL REGISTRY
-- ================================
CREATE TABLE IF NOT EXISTS model_registry (
    model_version TEXT PRIMARY KEY,
    feature_version TEXT NOT NULL,

    algorithm TEXT NOT NULL,
    trained_on_start TEXT NOT NULL,
    trained_on_end TEXT NOT NULL,

    metrics_json TEXT NOT NULL,
    created_at TEXT NOT NULL
);

-- ================================
-- SYSTEM HEALTH & DEBUG
-- ================================
CREATE TABLE IF NOT EXISTS system_health (
    timestamp TEXT PRIMARY KEY,
    component TEXT NOT NULL,
    status TEXT NOT NULL,
    message TEXT
);



--------------------------------------------------
FILE PATH : G:\trading_app\ui\charts.py
SIZE      : 0 bytes
--------------------------------------------------



--------------------------------------------------
FILE PATH : G:\trading_app\ui\heatmap.py
SIZE      : 0 bytes
--------------------------------------------------



--------------------------------------------------
FILE PATH : G:\trading_app\ui\option_chain.py
SIZE      : 0 bytes
--------------------------------------------------



--------------------------------------------------
FILE PATH : G:\trading_app\ui\signals.py
SIZE      : 0 bytes
--------------------------------------------------



--------------------------------------------------
FILE PATH : G:\trading_app\utils\expiry_utils.py
SIZE      : 2524 bytes
--------------------------------------------------

"""
Utility functions for expiry date selection
"""
from datetime import datetime, time
from data.instrument_master import get_available_expiries

def is_market_open() -> bool:
    """
    Check if market is currently open.
    NSE market hours: 9:15 AM to 3:30 PM IST, Monday to Friday
    """
    now = datetime.now()
    
    # Check if it's a weekday
    if now.weekday() >= 5:  # Saturday (5) or Sunday (6)
        return False
    
    # Check time
    market_open = time(9, 15)  # 9:15 AM IST
    market_close = time(15, 30)  # 3:30 PM IST
    
    current_time = now.time()
    return market_open <= current_time <= market_close

def get_trading_expiry(underlying: str) -> str:
    """
    Get appropriate expiry date for trading.
    - If market is open and today is expiry: use today
    - If market is closed and today is expiry: use next expiry
    - Otherwise: use nearest expiry
    """
    expiries = get_available_expiries(underlying)
    if not expiries:
        return ""
    
    today = datetime.now().strftime('%Y-%m-%d')
    
    # Check if today is an expiry day
    if today in expiries:
        if is_market_open():
            # Market is open on expiry day - use today
            return today
        else:
            # Market closed on expiry day - use next expiry
            for expiry in sorted(expiries):
                if expiry > today:
                    return expiry
            return expiries[-1]  # Fallback to last expiry
    else:
        # Today is not expiry day - get nearest future expiry
        for expiry in sorted(expiries):
            if expiry >= today:
                return expiry
        return expiries[-1]  # Fallback to last expiry

def get_expiry_for_backtesting(underlying: str, reference_date: str = None) -> str:
    """
    Get expiry for backtesting (can use historical expiries).
    
    Args:
        underlying: Underlying symbol
        reference_date: Reference date in 'YYYY-MM-DD' format (default: today)
    
    Returns:
        Expiry date for backtesting
    """
    expiries = get_available_expiries(underlying)
    if not expiries:
        return ""
    
    if reference_date:
        ref_date = reference_date
    else:
        ref_date = datetime.now().strftime('%Y-%m-%d')
    
    # Find the nearest expiry on or after reference date
    for expiry in sorted(expiries):
        if expiry >= ref_date:
            return expiry
    
    return expiries[-1]


========== END FILE CONTENTS ==========

===============================================
SCAN COMPLETE
===============================================
